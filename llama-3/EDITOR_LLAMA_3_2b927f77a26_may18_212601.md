#### 00:00:01.994

Welcome back to a special bonus session of the Cognitive Revolution. Yep, there's always more to learn. It's happening quickly today. So   
(00:00:11.601) ~~I guess~~(00:00:11.840)  
super quickly, what we recorded this morning, by the time we got off the recording, Jan Leca had posted his tweet thread statement about his reasons for leaving OpenAI, in which he puts it pretty plainly that he's had pretty fundamental disagreements with leadership and has had trouble getting the resources that he needed to do the work, including compute resources, and certainly had some nice, fond things to say to his teammates, but basically was like, I don't think we're on the right track, and seems to be resigning pretty much in protest. That doesn't seem like a good thing, doesn't seem like a good situation. 


---


#### 00:00:52.987

What else have we learned and what do we make of it?   
(00:00:56.549) ~~Well,~~ (00:00:56.710)  
so we also got coverage from Vox, from Bloomberg, from TechCrunch. We got Sam Altman's response actually to Leike, which was extremely graceful. Essentially saying, yes, we have a lot of work to do and we're going to do it and expect a longer response later. It's the basically best possible thing you can say there, but then you're on the hook for doing it. We have Kelsey Piper confirming the nature of the draconian non-disparagement clauses, which apparently have lifetime duration and include an NDA that you can't reveal about violating the NDA. And she claims that when employees come on board for the first time and are given equity heavy compensation, they are not told that they will be required to sign this disparagement clause or have their existing vested equity confiscated upon departure. 


---


#### 00:01:46.597

So that seems like a really bad equilibrium and way to run a company. I'm honestly confused as to why that's legal.   
(00:01:53.962) ~~Well, it maybe shouldn't be.~~(00:01:55.203)  
Yeah. I think you should have to   
(00:01:56.963) ~~like~~ (00:01:57.103)  
very much acknowledge the disparagement clause rules very clearly. Initially, if you're going to confiscate something of immense value for someone not signing them, it doesn't seem reasonable at all. It also doesn't obviously speak well of the company in its openness in the good sense, right? forcing every employee to never disparage you for life no matter what, or else, that's just not a reasonable position to take if you want people to not assume the worst. And   
(00:02:25.658) ~~so,~~ (00:02:25.919)  
yeah, we look at Leckie's statement, essentially saying that for years, they have had the trouble of shiny new products becoming the priority of a move away from safety culture. that the culture is not amenable to or compatible with safety. 


---


#### 00:02:42.920

I'm paraphrasing here a bit. I'm not looking at the words. And that he had trouble getting compute over the last few months, despite the explicit 20% of existing compute commitment from OpenAI, which should have been sufficient for current purposes. And indeed TechCrunch confirms that they have not been honoring their commitments. They have asked for a fraction of the 20% commitment and have repeatedly not gotten what they asked for. that is part and parcel of the whole idea of running anything in AI these days, is compute. You need your compute to do your thing. And he was reporting this has specifically become a substantial barrier to doing their work. So this is not only a philosophical approach, because he said they should be spending vastly more, right? 


---


#### 00:03:28.640

Like not just a little bit more, but they should be spending vastly more of their resources on preparing for our AGI future. Also, if you'll notice what he actually said, for just the next generation, that essentially he doesn't think they're ready for GPT-5. He doesn't think they're on pace to have the tools they need for GPT-5 to be safe in a pedestrian mundane utility sense, like not in an existential sense. And then later on, there's the problem of super alignment. There's the problem of AGI, which many people at OpenAI have said they expect within several years. It's a very short timeline. And now the super alignment team has been dissolved. And if people have been dispersed throughout the company, they claim they will still continue to work on that level. 


---


#### 00:04:14.269

But having dishonored their commitment and having dissolved the team and having lost the leadership, it doesn't look like the kind of effort they promised us, but they said they were going to do. Yeah. Does the timeline still hold, Sam? We're now,   
(00:04:30.312) ~~what,~~ (00:04:30.533)  
nine months into the four years. If he still expects to need superalignment to succeed within four years, then these actions do not reflect somebody who understands that and understands, as he has repeatedly told us he understands, what is at stake with superalignment. And it's clear, like he's breaking point with Ilya departing, according to the Bloomberg article, that makes perfect sense. But we have a series of departures. We have obvious justifications for that. 


---


#### 00:05:00.858

And this all makes sense. There was all this speculation about what did Ilya see? It was the whole thing. What did Jan see became the thing after Jan quit. And the answer is what they saw was a company that's not committed to safety, that's unwilling to put its money where its mouth is, that has a culture that is hostile to safety efforts, and that is pivoting towards the shiny new It's a product company. It's a scaling startup. It's devoted to making money. There's nothing wrong with making money. But in this case, they're building smarter than human intelligence as their explicit company mission and goal. And as Leike points out and as Altman has repeatedly acknowledged, this is not a safe thing to do. 


---


#### 00:05:45.500

This is not a default thing that will go well by accident. We can debate how likely this is to go badly. with good, real efforts to make it go well. But I think any reasonable person can see that there's a very good chance that if we do not put in the effort, we do not do the work, that things would then go badly. And it doesn't have to necessarily involve some sort of specific AI catastrophe or accidental risk scenario, which simply means this could go very badly for people as experienced by people on the planet Earth. And we have to think carefully about these questions. But what's clear is that OpenAI's leadership, as embodied by Sam Altman, increasingly is not doing this, and specifically in the last few months, is not honoring their commitments. 


---


#### 00:06:30.391

Yeah, that compute one for me is pretty troubling. I could try to make a, in the absence of that, it would be a lot harder to parse, I feel like. Obviously, people can have all sorts of disagreements, and you can imagine the Sam Altman defense being like, You're doing your thing over here on the super alignment team. We're doing our thing over here on the product team. Why is that inherently a problem? But yeah, if you can't get the compute to do the alignment work. It's not just that they promised us X and we got Y. It's beyond specifically saying, you promised us X. We needed X over N, where N is a lot more than one, to do our work. 


---


#### 00:07:12.086

And we couldn't get it. And the super alignment commitment, like it sounded a lot, 20% of our currently available compute, that's not a lot over four years because two years from now, OpenAI is going to have 10 times as much compute, like for sure, unless something very strange happens. So over the extended period, this is a reasonably small, very minus commitment, as opposed to in certain similar industries where safety is paramount, most of research costs, most of development costs can become safety. And again, that's for mundane level, just make sure the plant doesn't melt down, levels of safety. So I don't understand it. I also don't understand why it's not simply good business to give   
(00:07:52.764) ~~Mikey~~ (00:07:53.124)  
and people like that their compute. 


---


#### 00:07:55.565

Certainly relative to this outcome, it seems if we are indeed talking about something like 5% of compute, then that could only allow you to move 5% faster than you could without that compute. It does seem like a very strange decision to allow people to walk and allow this to become this big of a story over a couple percentage points of compute availability. Yeah, it's almost always the straw that breaks the camel's back, right? It's clearly, Ilya was being shut out of decisions, couldn't play his ambassador role that he had previously played inside the company, if you're reading   
(00:08:37.769) ~~between,~~ (00:08:38.129)  
not even between the lines, just reading the lines of these various articles and reports, that people were turning against the very idea, the board battles embodied this struggle, the way they played out in practice, whether or not this is at all fair or deserved by anyone or any philosophy or any approach or anything. turn people against them. 


---


#### 00:08:57.664

Every person that leaves turns you further against them. The firings, you have a cascade of every person that leaves, you lose more trust. Like why did they leave? What caused them to leave? How was that handled? And yeah, presumably it was just like, I'm fed up. We're not getting what we need. Wake up call. I can't just sit here and try to pretend that I have the resources I need because I don't. The way he talks about it, they're just not investing what a company in their position needs to invest in what I call mundane safety either, right? And in fact, I haven't investigated this, but you see this in their reports that 4.0 has much less tendency to refuse inappropriate requests, like building a bomb, than GPT-4 Turbo or its rivals. that somehow this new model that was sent out is actually just very not robust in the jailbreak slash mundane harm sense. 


---


#### 00:09:53.299

It just is very willing to fill your requests. And I haven't tried to stress test it or red team it in this sense, because why would I? I don't think it's particularly a dangerous thing for the most part to have it be jailbroken. It was already jailbroken for those who cared enough. But what's very clear is that the 4.0 development process did not involve a robust attempt to make it safe in a conventional sense, they mostly just decided the capabilities that this model possessed were not so dangerous and they just weren't going to give it much care. So I'll see you chain to the open AI fence or what do we do? What do we do from here? 


---


#### 00:10:31.197

I mean, I think we have to treat going forward until we see, Paulman could respond with an amazing set of commitments. He could respond with a new set of hires of people he's bringing in, he could do any number of things. So we're always hopeful. And as much as AI moves fast, I don't think we need to move this week or anything like that. And part of it is that it's very clear from Jan's testimony that we don't have to move this week, that if Jan and Ilya and others were concerned about something imminently happening, they would have said so. It is very clear from this statement that these are long-term concerns driven by things that the ships get steered slowly, no matter what you do. 


---


#### 00:11:11.261

So they have time to fix it. Barring evidence to the contrary, given everything that we've learned, I think we just have to assume that OpenAI is functionally a fully for-profit business, fully a move-fast-and-break-things, hockey-stick-graph startup business run by Sam Altman, who is running it on that basis, and that they are not taking the safety problem seriously, that their culture, internally, is hostile to the idea of safety, to the idea of worrying about things. And that, therefore, we should expect them to handle this future badly. That we should expect them not to be prepared for what is to come, to be extremely cavalier. And there's the possibility that for a while, cavalier works out. A cavalier GPT-5 might well be the best thing to happen for the world. 


---


#### 00:12:04.491

It's possible. if they're just not that dangerous. We talked about not safe for work. We talked about sex and gore and these other capabilities. And yeah, it might just be good to let that stuff happen. It might be completely net positive. I actually expect that. And Pliny broke all the major models. He broke all of them fully. And he broke GBD 4.0 in about two minutes during the announcement speech, right?   
(00:12:31.105) ~~Like~~ (00:12:31.245)  
the moment he got access to it, because his first hunch just worked, because why wouldn't it? But at some point, that's going to stop being an acceptable situation. At some point, these things are going to be highly capable, and we're going to have to worry about societal implications, and then catastrophic risk, and then existential risk. 


---


#### 00:12:47.336

And all that's probabilistic, because you don't know when it's going to happen. Like when GPT-5 comes out, you're probably not going to be in an existential situation at all. But how many nines are you going to put on that statement? And if your answer is more than two, you're crazy. And I would probably put one. So you can come crying and say, you said it was only 97% and it turned out not to happen. I'm like, well, chalk it up to a slightly worse calibration than you on this one and let's keep betting on sporting events. But I don't know what else to say. I assume you have seen the Dwarkesh interview with John Schulman out this week. 


---


#### 00:13:23.167

I unfortunately haven't because I have a thing with Dwarkesh where I move slowly. I pause. I write notes. I often go up entire posts. And I simply have decided I have not had the time. to give this my proper attention. And I don't want to, but this is even more important now, right? I expect it's an hour and a half podcast. You're going to have to do a close reading. Yeah. A couple. Yeah. So as it's reported now that he's taking on the responsibility for safety writ large.   
(00:13:53.817) ~~I guess~~(00:13:54.038)  
he was already responsible. As Dwarkesh presented him in the interview, he was responsible for post-training the models. As described in some of these articles today, he's responsible for making today's models safe. 


---


#### 00:14:05.770

Now he's going to have this kind of additional responsibility rolling up to him of big picture long-term safety. In that context, that interview,   
(00:14:15.081) ~~I think~~(00:14:15.500)  
is going to give you serious pause. And obviously you can evaluate it for yourself, but there were multiple moments. Just hearing what he was doing previously. This is, you took the mundane safety guy who is doing the job with mundane safety, that it's not necessarily better or worse than the rivals, at least until, I haven't evaluated for O in a sense, and there's again, of course, there's potentially a problem. But that's just completely irrelevant problem to the problem he is now being asked. And if he was doing that job properly, then   
(00:14:45.835) ~~like~~ (00:14:45.975)  
he wouldn't necessarily have mentioned that they have these concerns about the next generation of models not being prepared for, that should be part of his job as well. 


---


#### 00:14:56.158

But yeah, the idea that you're going to use post-training on your AGI, or even your ASI, right, to render it like HHH or like net useful or any of those terms.   
(00:15:09.166) ~~I think~~(00:15:09.365)  
that's basically a pipe dream to rely only on that. And in fact, you have to worry during even the training regimen of some of these things, but it's just the kind of strategy, certainly in a way, but the strategies that he's been using so far are the kind of things that   
(00:15:22.928) ~~like~~ (00:15:23.129)  
he himself acknowledged explicitly,   
(00:15:25.690) ~~like~~ (00:15:25.850)  
both online and literally five feet away from me during a talk would not work. He took on the 80,000 Hours podcast. He explained why ROHF is not a solution to this problem, and he has other proposed solutions that he wants to try where I don't think they'll work. 


---


#### 00:15:42.191

And I tried to debate him and explain to him why they wouldn't work, and I was unable to make my case sufficiently convincingly, and he didn't buy it. I think it was reasonable for him not to buy it in the sense that I was making some very bold claims that are very different from his worldview, and I didn't back them up specifically enough because it's hard for me to do without technical training. But like he was thinking about the problem on a different level than someone who's thinking about post-training was thinking about the problem. So I haven't seen the interview yet. Again, I want to be very careful with it. But yeah, if you think it's going to give me pause, 


---


#### 00:16:12.979

I'm confident you're right. It's going to get popped. A couple, well, just for folks who might be listening to this and might not have time to go through that or inclination to, it is definitely worth it. It's very interesting in multiple ways. One of the things that he says is that he's expecting quote unquote AGI on   
(00:16:30.759) ~~kind of~~(00:16:30.979)  
a two to three year timeframe. Dworkin asks him, could it be as soon as next year? And he's like, I don't think so. That would be surprising. But two to three, he was pretty much willing to co-sign on. And then Drakesh asked a number of questions that were like, pretty fundamental. Like what happens when this happens? 


---


#### 00:16:49.924

How are we going to deal with it? Or like... At one point he said, I'm not sure that doesn't sound like a super robust plan that you're outlining here. And I appreciate the candor, but at multiple points he was, yeah, I don't really have a great account for how that's going to go. Or hopefully we'll be able to work together with the other leading developers in that situation. And   
(00:17:10.574) ~~yeah,~~ (00:17:10.714)  
we don't really have a robust plan for that at this point. That's the second best plan. The best plan is to actually figure out what you're going to do and how we're going to handle this. And the second best plan is to know you don't know, to start with a blank beginner mind. 


---


#### 00:17:25.452

So if he comes to this, if he comes day one and he says, we don't have a plan, we don't have a solution, we don't know how to align this thing, we don't know what to do with it if we did align it, we don't know how society can handle this. We don't know how to make the transition from AGI to ASI, again, even if we handle alignment and even if we handle the interim. We don't know any of these things. We're lost. We need to figure this out. And he starts from scratch. I am perfectly happy with that as the answer for a person who's highly capable. He founded OpenAI. He's co-founded it. he's done a lot of impressive things. 


---


#### 00:18:00.480

Ilya, I thought, Ilya cares deeply about these issues and appreciates the depths and stakes of the problem. But I always thought, and I still, going off of Ilya's publicly stated remarks, I just don't think any of the things he was thinking about are anywhere near what you need to be thinking or what would work. My likey stuff wouldn't work, but I thought he was reasonably grounded and trying hard. All of this stuff just felt like it often felt, okay, that just seems like a misconception to me. It feels like you're thinking about this problem in a   
(00:18:31.345) ~~kind of~~(00:18:31.826)  
fuzzy, not sufficiently geared way, often, and in a way that just wouldn't survive in,   
(00:18:39.491) ~~like,~~ (00:18:39.632)  
an encounter with the enemy, right? 


---


#### 00:18:41.794

When he looks at, when he and Nike and the other people run their super alignment team, and   
(00:18:46.698) ~~they run,~~(00:18:47.077)  
they build these papers, they run these experiments, and they actually try to make these things work, they'll understand that their plans aren't working. And they'll either find ways to modify them so they work or throw them out and try new ones. Because one of the things Ilya is famous for is the kind of attitude of, I'm going to try this as many ways as I have to and throw out everything I think I know until I make this thing work because I'm committed to making this thing work. And that's what you love to see. And that's why I had a lot of faith that Ilya and Jan would find a way Not every time, because I don't think this problem is even necessarily theoretically solvable by humans in a reasonable timeframe with any attitude. 


---


#### 00:19:23.770

But I thought they had a reasonably good shot, because with a lot of resources and a lot of time, four years in some sense is a lot of time, they would figure out at least some ways not to align an AI. And then we get to try again. And the fact that their first way definitely won't work in my model is, that's fine. They were a mix of ideas that were like reasonable and I thought promising and they're hopeless, but nobody knows how to solve these problems.   
(00:19:49.999) ~~Right?~~ (00:19:50.179)  
So I can't really get that mad at you for being excited by ideas that I think won't solve these problems. What do I have? A better suggestion? 


---


#### 00:19:56.961

You try something, you learn something, you try again. I think there's a decent chance that looking at these generalization questions, looking at these supervision questions, will inform your approach to trying to find alternate solutions that might themselves be more promising. If none of those types of solutions, if that entire solution class is entirely hopeless,   
(00:20:20.663) ~~right?~~ (00:20:20.804)  
If the worst case scenario that is as bad as it looks to me on first glance and there's no fixing it, then those are pretty bleak worlds in many ways, because that cuts off a lot of people's plans. So leaving room for their being, places to maneuver there. So with this clarity, and while we await a proper response from Altman and leadership, what else do you think people should be doing? 


---


#### 00:20:47.390

Joking, but maybe I'm not entirely joking about chaining oneself to the open AI fence. For reference, there was a person who's done that this week. there is like mundane consumer protest. Now that ChatsGPT is free, it's gonna be hard to cancel our accounts, but   
(00:21:03.276) ~~we could all,~~(00:21:03.816)  
we could rally app developers to boycott and switch to Claude or something. We could, obviously we're on the record on being at least   
(00:21:13.461) ~~like~~ (00:21:13.663)  
generally positively disposed to SB1047. We could think about supporting that even more forcefully or, suggesting possible amendments   
(00:21:24.209) ~~to that,~~(00:21:24.608)  
to strengthen that. Something like the non-disparagement clause being made illegal could be an interesting one. I like that. When I look at 1047, I wrote it up, right? 


---


#### 00:21:34.255

I was mostly looking for ways in which it was too strong, but I identified a number of ways in which this bill might go too far in the sense of it has these serious downsides. And unless we're getting a lot in exchange for these downsides, That makes it politically hard to pass. It makes it harder to get buy-in. It makes it harder to get cooperation. And nobody actually wants to tank the economy. Nobody wants to actually slow down the mundane utility.   
(00:21:59.594) ~~Not nobody, but I don't.~~(00:22:00.595)  
And so how can we strengthen this bill? The exception being the derivative versus non-derivative definition clause, where I thought this was just a bug. It's like literally there's just a major definitional mistake in this bill. 


---


#### 00:22:12.502

Maybe it's just making it worse on every level. We need to fix it because if I can pass off all of my blame to you, that makes your situation terrible, but it also makes all the safety guarantees something I can skirt.   
(00:22:24.720) ~~I can then ignore anything.~~(00:22:26.662)  
And that's not good either, right? Like we can't pay for really cheap, we have to stop this. So some of the others were about how do we weaken this?   
(00:22:33.846) ~~How do we, I don't know if weaken is the right word, but how do we clarify this bill?~~(00:22:36.849)  
How do we prevent potential overreach or misinterpretation of this bill in a stronger sense? And then other people pointed out ways in which the bill was like potentially too weak. 


---


#### 00:22:46.856

Everyone was complaining about the criminal liability, but it's only under perjury. Maybe that's not enough, right? One could ask potentially. And some people, I think for now it isn't, it is where we want to be. And I think the reaction to even the perjury showed us that this is just a third rail and   
(00:23:02.903) ~~like~~ (00:23:03.183)  
people just get so scared of such things that   
(00:23:05.565) ~~like, let's not, it just gonna,~~(00:23:07.826)  
it might have very bad dynamics for the good people to stop touching things or like they panic and they don't want that. But yeah, like we have whistleblower, a clause in the bill. And if you don't enforce non-compete agreements, this seems so much worse than a non-compete agreement, right? 


---


#### 00:23:25.703

An ordinary non-compete agreement, which now, not only California has not enforced them forever, but now, like they're going to be illegal for high paid, all but the most prominent employees, the entire country, because the FTC, unless that doesn't go through. But yeah, I have a hard time believing that it is in the interests   
(00:23:44.193) ~~of the public.~~(00:23:45.394)  
of the United States to allow a company to hold most of somebody's wealth hostage to signing a lifetime full non-disparagement clause on the company they are leaving in an area in which things that are wrong are in the vital national interest. If there is something wrong with the safety at OpenAI, that's something we need to know. If there's anything wrong with the culture at OpenAI in other ways, that's something we need to know. 


---


#### 00:24:12.344

If you have a whistleblower provision, this is like, how do you blow the whistle if you're not legally allowed to blow the whistle? Or the price is millions and millions of dollars in equity,   
(00:24:22.207) ~~right?~~ (00:24:22.326)  
That you can't sell, so they can just confiscate it,   
(00:24:25.968) ~~right?~~ (00:24:26.127)  
So even,   
(00:24:26.587) ~~they don't even have to sue you necessarily,~~(00:24:28.068)  
they can just confiscate it in that situation. And then he knows what else they might be threatening or holding over people. But you just don't know, again, because they can't talk about it. So I can't know these things. So we have to assume the worst. in some senses, because we can't talk about it. And yeah, I think it'd be very reasonable to say that AI companies should not be able to sign non-disparagement clauses as pertains to certain aspects of the company, certainly, and potentially universally. 


---


#### 00:24:56.821

Like I think it's just, why is it good for the two of us to get to an agreement where we agree that no matter what happens, I can never say anything bad about you and you can never say anything bad about me. I understand why it's better for us in some sense, but People not being able to talk in that way   
(00:25:12.808) ~~just~~ (00:25:12.950)  
doesn't seem great. And if we're in the business of not enforcing contracts that are against the public interest, this seems like a private place to look, right? Even though I have libertarian instincts that like avoiding contracts is bad. Yeah, this seems to be a reasonable place to consider that, obviously. But beyond that, well, if you're not going to do the safety work yourself, necessarily, someone has to make sure that you do, right? 


---


#### 00:25:37.526

Someone has to be doing the check-in. And this is a reason to doubt, right? If I'm going to trust a company, I have to be able to trust their commitments, I have to be able to trust their statements, their attestments, and I need that to be under some sort of punishment, right? That's the whole idea, right? If you lie, right? The idea is that you don't have to do anything. You have to tell us what you are doing and you have to be held responsible if you lied your ass off. That is what the key provisions of SB 1047 are about. They're about saying what you're doing and being responsible if you didn't do it, and saying what your logic is, and being responsible if your logic is lies. 


---


#### 00:26:16.207

It just makes no sense, willful disregard, beyond the pale. That's what it's about. And also just having a mechanism where if you discover that there is actually catastrophic risk in the room, you can get the model shut down. Both that they have the ability to shut it down, at least locally, and that you can order that reasonably. These are the fundamental things that this bill is about. These things seem important now more than ever in the light of this information. Fundamentally speaking, yeah, I think we have to understand that until proven otherwise, OpenAI is much less of a confusing company than it was a week ago or six months ago. There were reasonable arguments to be made when they announced super alignment, when they put out their reasonably good- I made some of them myself. 


---


#### 00:27:05.375

Yeah, they put out a reasonably good fairness framework. Like they've done some good things. They've hired a bunch of good people. They had a bunch of people who moved in circles where like they're credibly spending a lot of their time talking the right ways, asking the right questions, even if I don't agree with their specific beliefs. And a lot of that's just gone now. Their credibility is shot from a safety perspective. And so I think it's a lot less confusing now. And yeah, I think that if you have a choice in whose technology to use, in some sense at this point, and you choose to go with open AIs in a way that matters,   
(00:27:41.821) ~~well,~~ (00:27:41.961)  
this is part of what you are considering, part of what you're doing. 


---


#### 00:27:45.022

And I think it also means that you are taking on a risk, a concrete risk yourself, because I don't think you should necessarily trust their mundane safety, right, in this world going forward.   
(00:27:58.347) ~~I think it's, I'm still just,~~(00:27:59.627)  
again, there's just not much that can go wrong with the GBT 4.0 that I am that scared of. if they don't have a culture of safety. You need a security mindset to build AIs, to make these AIs do the things you want them to do and have it not go up in your face, even in an ordinary, normal, mundane way. You need to be thinking about these problems and working on these problems and giving them the respect they are due. 


---


#### 00:28:22.196

And so, the entropic business case for we deeply care about this and we have a culture of this, which they clearly do amongst their employees, where they encourage this concern, where they have this concern. And we're going to make sure that when you use our product, you get what you're trying to get and not something else you did not expect and did not want. It becomes a lot more interesting. And then where Google lies on that spectrum, you can evaluate for yourself.   
(00:28:48.471) ~~I'm not saying there are any angels in this room, right?~~(00:28:50.232)  
I'm not saying there's anybody that I trust, but there are levels. And again, we'll see what Altman says next week. What do you think about the revisiting the notion of third party testing as well? 


---


#### 00:29:02.000

This was something we chatted a little bit about prior to my interview with State Senator Scott Wiener and who, of course, is sponsoring the bill. The current bill just says including third party auditors as appropriate. And now it does seem   
(00:29:19.348) ~~OK.~~ (00:29:19.450)  
Maybe we step that up a little bit. We got this UK AI Safety Institute not quite getting the access. Now we have the resignations clearly in protest. You're a good game design guy. How would you think about designing that game so that the right people get the right kind of access to do the right kind of testing and it doesn't collapse into something stupid? The thing that I emphasized when I analyzed both the preparedness framework and the responsible scaling policy for Entropic was these are potentially very good policies if the spirit of the rules is being honored. 


---


#### 00:30:03.056

If the people at these companies care about safety, they have a culture of safety, and they don't just look at this as a bunch of check boxes to get through so that they can get through compliance and release their thing and satisfy the scolds, but they genuinely care about this result And when the answer comes back, technically you passed, but that's funny. The response is no, wait, stop. Think about what's going on. Oh, well, that wasn't okay. If something's going on here, we need to investigate   
(00:30:32.348) ~~this. We need to~~(00:30:32.888)  
stop this and react accordingly. And certainly you can't do it if people are potentially gaming to sabotage benchmarks to be like under worse of thresholds. 


---


#### 00:31:24.304

Do you trust them to isolate these groups, right? I can imagine a world in which I would have that trust. But after what we just saw, do you have that trust to open AI? I don't know that I do, in the sense that I don't, definitely don't. But we saw that I trust their benchmarks, right? When OpenAI comes up with benchmarks, I trust that they're not gaming the benchmarks. They're not trying to do it in that way. But would I trust them for GPT-5 in the same way that I did previously? Now, maybe not as much,   
(00:31:49.382) ~~right?~~ (00:31:49.521)  
I don't know what direction they're going to do these things in, but I think Colin Fraser was the first person to say, let's not assume they invented the operating system from her until we get our hands on it. 


---


#### 00:31:59.028

Let's not jump to conclusions in any way, not just safety, but also capabilities. Because if you are a hype machine that is trying to hype, that is a very different world than what chatTBT was, right? ChatTBT was just, here is our very sterilely named   
(00:32:17.349) ~~very~~ (00:32:17.670)  
simply presented, very clean. I still give major props to them for not having it up in various ways. There are things to love about OpenAI. But they just presented this thing, OK, here's a cool thing. Let's see what you do with it. We're not going to tell you how awesome it is. We're just going to put it up there. And the GPT-4 was the same way, mostly. And now, this is the third, 


---


#### 00:32:39.547

I think, time they've gone up there and gone, here are our amazing abilities, some of which we don't have yet. And that's very different. And then Sam Altman went on Twitter the day after Google's I.O. to gloat about how his hype was so much of a better vibe than Google's hype. But he hasn't addressed, to my knowledge, any of the concrete things that Google announced or any comparisons as to who's building the better product, even to just congratulate them on a great set of offerings or anything like that. He's just, oh, get a hold of the nerd, basically. They're trying so hard. That's not a good sign either. Yeah. It's not great. It's not great all the way around. 


---


#### 00:33:17.390

I would say I definitely strongly noticed the shift from the earlier releases to the current releases. I would say last fall, the demo day, was really the first time where it was   
(00:33:31.060) ~~like,~~ (00:33:31.320)  
and even that was more buttoned up than this one. It was like, the GPTs weren't really ready. They didn't really work that well. And it was the first time where it felt like, man, you guys shipped this even though it wasn't really in shape to ship. There's no safety concern, but they felt like vaporware, right? Yeah, they just didn't work. Yeah, the retrieval was not good.   
(00:33:55.460) ~~I think~~(00:33:55.640)  
that has been improved, but it was a few months later. that, and I need to do a little more testing with this myself, but what I've been hearing in my app development circles is that the assistance API has rounded into form now where the retrieval actually does work much better than it originally did. 


---


#### 00:34:13.112

And it's more like what they described in that first release, but that's relatively recent and it's been a number of months. And this one just seems even messier where it's like, They weren't even really clear on what we were supposed to be getting, or people are just all confused at the moment. I was confused. I was trying to   
(00:34:29.342) ~~get,~~ (00:34:29.501)  
get ChatGPT to modify a picture of me and my son and it was not doing it right or whatever. And then I was like, what's going on with this thing? And then I went into go on Twitter and I see, Oh, okay. I'm still using the old system and it's just not clear to me what I'm even dealing with. 


---


#### 00:34:43.726

They're not updating a lot of the angles and they're not being clear. experienced by a lot of people complaining, oh, I just realized I'm not using the new version of this thing. I understood it from what they were doing.   
(00:34:55.195) ~~Like~~ (00:34:56.076)  
I was being a journalist and being paying very close attention. But   
(00:34:59.797) ~~like,~~ (00:34:59.996)  
to a normal person who's paying ordinary attention, it was very unfair.   
(00:35:04.338) ~~Well,~~ (00:35:04.538)  
any other thoughts on I guess one of the question I had was, are there any new safety related agendas or developments that you think are worth extra attention or   
(00:35:15.300) ~~that~~ (00:35:15.440)  
that we should maybe be increasing our bets on at the moment. I'm always looking out for something that seems like it really could work. 


---


#### 00:35:24.327

And I'm always struck by the fact that... The most interesting thing to happen that I think is getting no attention in the alignment. So did you hear about SOFON? I don't think so.   
(00:35:35.358) ~~So, I mentioned this a few, I forget which week, how many weeks ago it is, because time blurbs.~~(00:35:39.141)  
  
(00:35:39.382) ~~But,~~ (00:35:39.603)  
so, the Chinese have proposed a technique called a SOFON, because, let's build a SOFON from   
(00:35:47.909) ~~the ancient,~~(00:35:48.510)  
from the dystopian offering, the aliens oppress us using SOFONs. If you read the book, anyone, see the show? But, the idea is, you can trap a model, open source or closed, but in particular, open source, in a local maximum with respect to certain specified topics, such that if you attempt to fine-tune it to get it out of the local maximum, it won't work. 


---


#### 00:36:16.961

Ordinary fine-tuning techniques to try and escape from a failure won't work. The proposal was you could actually teach, not just not, because you don't teach biology to Lama 3 or Lama 4, let's say. Lama 4 doesn't learn biology. there's only so much biology that you have to learn, feed it a bunch of textbooks, suddenly it knows biology, right? Even if you somehow manage to not have it learn it by implication. But now the so-called protocol is you can specifically teach it to not understand the biology and be really dense about it. The way that certain people were like, I can't do math and just refuse to learn no matter how much you teach them and how many examples you give them, because they had trauma. potentially give the thing trauma, by metaphor. 


---


#### 00:36:58.501

And so the idea is, if you can make sure that the thing can't learn biology, now you've got an open model that can't give up biology, potentially, very early. We haven't run it through its paces, we haven't tried it at scale, we haven't, but it's an idea. It's the beginning of the first proposal I ever made.   
(00:37:19.083) ~~Well,~~ (00:37:19.342)  
in theory, maybe we could do something that raises the cost above epsilon compared to the training charge of the model to take somebody's general model and turn it to whatever specific end we have in mind. Maybe this would start to require enough work that we're not just making it easy. If we can do that, now, we still have the problem of you have to enumerate all of the specific things you want it not to know. but you have to figure out all the things you want to stop it from knowing and block them, right? 


---


#### 00:37:52.777

Again, very similar to what we saw in the book, not a spoiler, but the idea being, you see this in sci-fi all the time, right? You see the villain or the oppressors or whatever it is. And they say, oh, all that matters to us is that you don't do X, Y, Z. If you can't do X, Y, Z, we're fine. And someone finds out a way to do W,   
(00:38:11.465) ~~right?~~ (00:38:11.644)  
Someone finds out a way to do something that they don't detect, that doesn't count to them,   
(00:38:15.306) ~~like,~~ (00:38:15.706)  
they don't understand what's going on, And their response to that is to ignore it. This constantly happens,   
(00:38:21.684) ~~right? Like,~~(00:38:22.143)  
people start doing weird shit, right? And the aliens that are trying to take, they've taken over the enterprise or whatever it is. 


---


#### 00:38:29.925

They go, I don't know what weird shit is going on, but eh, whatever. Whereas the correct answer, of course, is I don't know what weird shit is going on. So stop what you're doing until I know. That's not okay. But If you only can have it   
(00:38:44.275) ~~kind of~~(00:38:44.514)  
scripted, like I detect you can't do bioweapons, you can't do nuclear bombs, you can't do chemical weapons, you can't do cyber attacks, blah, blah, blah.   
(00:38:53.181) ~~Well,~~ (00:38:53.460)  
that's fine for now. It's incredibly helpful for Model 4. But it's not that helpful for Model 6, even if it works. because it's no longer going to be the threat that you knew was coming. You're not going to be able to enumerate what a smarter thing than you comes up with. 


---


#### 00:39:12.094

So it works up to a point, but it's still incredibly helpful. And it potentially raises the bar quite substantially to the point where we can all reach an agreement. If this works great, I don't know. If you want like a moment of hope or something, there are at least some proposals. Yeah, that's good. I hadn't heard of that. And it definitely sounds like something I need to go do a little more homework on. Anything else? Cause you're one for one in terms of new and very interesting pointers there. Yeah. I haven't seen that much in the alignment sphere lately, unfortunately. It's new evidence that things won't work particularly, but it's just been like relatively quiet. 


---


#### 00:39:46.708

I would say on that level, I guess something has to be quiet, right? You can't have everything happening all the time. If you were to pitch a movie concept that you think would be most influential right now in the way that like her seems to be inspiring the current moment of technology, mine might be the social network meets the Lord of the Rings. where the sort of central figure would be the Sam Altman type who is on this   
(00:40:17.652) ~~like~~ (00:40:17.793)  
meteoric rise of technology, but it's also being corrupted by it in the way that the Ring is corrupting. That's a weird example, just because the Ring was a metaphor for the actual AGI decades. That seems like the story that we might need to all hear. 


---


#### 00:40:36.186

We used to talk about the Fellowship taking the Ring to Mordor, right, as a Yeah, sort of metaphor for some of the things that might happen in some scenarios. But yeah, certainly you could tell that story.   
(00:40:47.092) ~~I think,~~(00:40:47.893)  
like my instincts tell me that's not the most interesting approach to that. I think if I was going to do it,   
(00:40:56.382) ~~I think~~(00:40:56.702)  
I would maybe just do a very kind of straightforward AI takeover scenario with not such a smart intelligence anywhere. Like just show the humans like suddenly giving up control. Show the humans, because in everyone's individual interest, no one can stop it. Just show things just spiraling out of control. One thing leads to another. 


---


#### 00:41:22.159

There aren't even any idiots, and there are no villains. Things just go wrong. And there's that. You could also have a law and order artificial intelligence set in 2035. Might be fun. The idea being that,   
(00:41:37.090) ~~well,~~ (00:41:37.329)  
not these open models have given everybody these extra capabilities. We have to be very proactive about hunting down people who try to implement catastrophic threats. And then, obviously, the police have all their AIs, and everyone's acting on a high level. It's one of these things where you notice that the world almost blows up every other week. Do you think we can write that? That's one of the challenges that I have with this in general, is I feel like the leap from here to there is tough. 


---


#### 00:42:04.103

What does the procedural look like? Can you imagine that getting concrete enough to be shown on TV in a way that... It is procedural, right? Is Star Trek not a procedural in its own way? We explore a brand new world, we find an ethical dilemma, we find a technological problem, and we have our debates, and then we encounter a setback, and then we implement our solution, we solve the dilemma, and we go on our way. It's not that simple, but it also is. And so you find a way to do a version of that, potentially. But yeah, a fun game for watching any sci-fi show. is note how often things almost go horribly wrong, right? 


---


#### 00:42:52.972

Like just watch a season of any Star Trek, and watch how often the ship almost blows up, or like somehow the Federation is almost in dire danger, and how often it happened because someone was being a complete idiot, and how often it happens so naturally, but like being kind of all these different problems. And then ask yourself, well, if you just looked at the 45 minute mark of every episode, You had to assign probabilities. If this wasn't a narrative someone wrote, what's the chance that humanity would have survived from here? Or what's the chance the ship would have survived from here? What's the chance the Enterprise actually makes it through seven seasons? The answer is zero. 


---


#### 00:43:29.659

It's so challenged in so many different ways. And the Federation probably does too. The Federation is in a lot of trouble reasonably often. Luckily, we got out of it because we wrote it. But this is not a utopia in the sense that if you actually were there, the safeguards aren't there.   
(00:43:44.894) ~~Like~~ (00:43:45.155)  
we have no, there's no robustness in this world. This world is fragile, right? The Star Trek universe is so fragile. And so we get lucky a lot, but   
(00:43:53.442) ~~like,~~ (00:43:53.722)  
why are we getting lucky unless it's you protecting us or the travelers or something, right? In a way that we don't understand. And so you carry that forward.   
(00:44:02.148) ~~The other,~~(00:44:02.369)  
the other game is like, how often does somebody come within five minutes of building, right? 


---


#### 00:44:05.871

Like, or how often would AI just run completely random here if you didn't have the rule that mysteriously doesn't. And that also is   
(00:44:11.516) ~~like,~~ (00:44:11.615)  
just, right, in most of these scenarios.   
(00:44:14.460) ~~So,~~ (00:44:14.820)  
yeah, you could also just have it just runs a normal sci-fi world, except that every now and then, and by every now and then, one episode in three or something, someone accidentally follows through on logic, and   
(00:44:29.394) ~~like,~~ (00:44:29.594)  
the super intelligence emerges, and everybody dies, or someone takes over the world, or everything gets picked up, or some new regime change happens or there's a recursive self-improvement. Who knows? I haven't thought this through. I'm brainstorming with you now. But the idea being that imagine if you got to just... 


---


#### 00:44:47.967

And then, of course, the world just... You see the rewind where everything goes back in reverse and then the person just chooses not to do it. But once every episode or two, somebody almost enters the world and they just decide not to. And there's really no explanation why they don't. Yeah, the sort of Garden of Forking Paths is a pretty interesting idea. Minded of the three body problem, too, has   
(00:45:08.711) ~~like,~~ (00:45:08.911)  
part of its story   
(00:45:10.932) ~~kind of~~(00:45:11.494)  
goes that way, where   
(00:45:13.094) ~~the like,~~(00:45:13.755)  
the civilization is being restarted and rerun over and over again. And it just ends at various times, and then it gets booted up again. But it's   
(00:45:23.380) ~~like,~~ (00:45:23.621)  
they last different lengths of time, and some of them are short and others are longer, but they all   
(00:45:28.565) ~~kind of~~(00:45:28.844)  
end and get rerun. 


---


#### 00:45:30.847

And I do think that would be an also a pretty interesting way to present the future that like some branches of this tree are terminal. Yeah. The three body problem is such a weird, I don't want to spoil anything, but it's such a weird, and I'm going to do my best not to, but it's such a weird mix of this kind of fully cynical, hard realism beyond what I think is even accurate. where the universe is this cold place that wants to kill you so badly, and you can afford not the slightest bit of kindness and decency if you want to survive. And at the same time, we don't just all die. In some sense, it may have been. 


---


#### 00:46:09.927

Obviously, there's a book two, and then there's a book three, but it's just not the way the book exists. And the book isn't what happens to Tricellaris after we get wiped out. The book is about people in some sense. Okay, last question,   
(00:46:21.487) ~~I think.~~(00:46:21.827)  
Do you find yourself shifting at all in terms of your sense of whether or not we may be in some form of simulation? My answer to the simulation hypothesis has always been, essentially, all of the value lies in the world, lies in the places where we're not in one. If you make nine simulated copies of me, and then there's me, and you put us in 10 copies of the situation, but one is real and the other nine will just be   
(00:46:48.920) ~~like,~~ (00:46:49.061)  
recorded into videotape and viewed back later or something. 


---


#### 00:46:52.835

Well, shouldn't I just act as if I'm the real one?   
(00:46:55.918) ~~Right,~~ (00:46:56.038)  
isn't that just currently the correct strategy? Even if there's 9,999 of them, maybe I still have the correct strategy. Or rather, if you are the ancestor, if this is an ancestor simulation,   
(00:47:06.246) ~~right?~~ (00:47:06.447)  
And   
(00:47:06.706) ~~so like,~~(00:47:08.289)  
then 20,000 years later, we try to run a bunch of sims of the ancestors. Well, the ancestors who decided they were the ancestors who, when faced with the ancestral situation, figured out that given the situation, they were probably in an ancestor simulation and therefore didn't need to actually make sure that the civilization progressed to a point where they could run the future ancestor simulations. Those guys don't get simulated, right? 


---


#### 00:47:32.360

Because those civilizations don't make it. So there's a real sense in which your only simulation hypothesis is only valid if you treat it as invalid or something like that, right? You have to take the situation seriously. And also, what's the point of a simulation when the person finds out? and acts like it's true. There are a bunch of movies like that,   
(00:47:49.454) ~~right? No spoilers, even naming them.~~(00:47:51.653)  
But in general, the point is to treat it as real. The whole goal is to treat it as real.   
(00:47:58.976) ~~I don't really see any, I don't know,~~(00:48:01.617)  
right? Obviously, there is some probability that this is a sim of some kind. I can't rule it out. I will sometimes jokingly refer to things in that kind of way. 


---


#### 00:48:13.699

The writers were a bit on the nose today, and one of my things I'll sometimes say. But you can't take it seriously in the sense of changing Europe. What about moving to the Caribbean and unplugging? I just saw an interesting tweet from Amanda Eskell from Anthropic the other day, where she said, I don't think AI is definitely going to kill us all. I'm not a doomer. If I were, I wouldn't be working on this. And she was also, I do think it's a real risk, but I think it's something that we can shape and hopefully I can have an impact on. But if I really was a doomer, I would just head to the Caribbean and spend the rest of my days there. 


---


#### 00:48:47.472

I do know, I do have a close friend who basically has that attitude. that he said, I just want to enjoy the good times that we have and not worry about it too much. And then she also said the downside of this or flip side of this is if I ever do get burned out and decide to take some time off in the Caribbean, people will take it as a sign of doom. Yeah, I think it was Jeffrey Miller, I'm not sure exactly who it was though, who said it wouldn't work. The food would turn to ash in your mouth. You wouldn't get no joy because you would know this. And   
(00:49:14.994) ~~I think that was,~~(00:49:15.653)  
that's largely true for me, 


---


#### 00:49:17.655

I think. Knowing I just walked away from this thing and was ignoring it, it just wouldn't sit well with me and I wouldn't be able to just go eat a nest. It just wouldn't work.   
(00:49:27.498) ~~You can,~~(00:49:27.818)  
you want Mr. Reagan to plug back into the Matrix.   
(00:49:30.239) ~~Well,~~ (00:49:30.398)  
you need your memory in some sense. You need to   
(00:49:32.920) ~~like~~ (00:49:33.059)  
really not notice. for some people. Whereas I enjoy fighting. I enjoy struggling. I enjoy the striving to do better and to solve problems, right? That's what my thing is. I would never go to the Caribbean because what am I doing in the Caribbean? I'd just be bored for a week anyway. But Unless I'm just   
(00:49:50.269) ~~like~~ (00:49:50.429)  
posting, making sports betting, bookmaking decisions again. 


---


#### 00:49:53.431

That's the warnings I've ever made to the Caribbean and had a good time. So there you go. But   
(00:49:58.094) ~~I think~~(00:49:58.375)  
it highlights, by the way, the fact that the word doomer is just a slur and has been completely misappropriated and misallocated. Who is a real doomer? The doomer is the person who says that there's nothing to be done, that what we do doesn't matter, right? The doomer is the person whose PDoom is 0.9 bar. or otherwise it's all over. There's nothing you can do. And you see those people on climate change, you see doomers, the people who think that humanity is doomed and there is nothing you can do about it. Your decision doesn't matter. 


---


#### 00:50:31.851

Whereas if you think your decision matters, as Amanda points out, that makes you not a doomer, right? Yeah, so what if we could lose? We can win. And you can help fight. It's an ancient Judaic idea, right? The universe hangs in the balance,   
(00:50:45.494) ~~right?~~ (00:50:45.653)  
The scales oscillate. could be up to you which way they were good and evil in the original, right? God's judgment. You can decide which way this goes. And obviously,   
(00:50:55.641) ~~you know,~~(00:50:55.902)  
this isn't a God's judgment thing. It's not a, there isn't a moral tone to this. It's about solving a problem. But if you take it from 34.007% to 34.008% chance of victory, that's a great life in some sense. Look at all the utils, including just for you. 


---


#### 00:51:10.862

Imagine what happens. But yeah, if you don't have anybody to interact with the problem, and you just decide to go off and do something else, it makes perfect sense to me. And you can't burn out. What Amanda needs to do once a year is sit   
(00:51:22.909) ~~Mai Tai's~~(00:51:23.369)  
on a beach in the Caribbean for two weeks so that she can regain her mental health and she can go back and resume. that you should do that. You should do it for an entire year after five years of working because otherwise you won't be able to think clearly or make a good decision. You should do that. There's nothing wrong with understanding your limitations. Life is not all the fight. 


---


#### 00:51:46.990

I make an effort to work on other things, think about other things. I make time for family. I try to have fun. I'm going to Madison Square Garden tonight. with some of my old Magic friends, and we're going to do a watch party, where 40,000 of us Knicks fans are going to watch Game 6 on a video screen, because they're in Indiana. But it's going to be fun as hell,   
(00:52:10.733) ~~right?~~ (00:52:10.954)  
And I don't claim that I'm saving the world here. I'm not. I just want to root for my team. Yeah. I feel you. I think it's probably a good place to end it. Any other closing thoughts? Yeah. Indeed, many amazing and wonderful things and weird things come to pass and best of luck to everyone. 


---


#### 00:52:29.820

And I look forward to Altman's response and where all these people land next. Like who are, this is some great talent. Someone's going to snap them up or they're going to do something. So we'll see what happens. Yeah, no doubt about that. Well, the saga will continue, but for now, I appreciate the extra time today, Svima, for being part, again, of Revolution. Absolutely, it's been a pleasure. 


---


