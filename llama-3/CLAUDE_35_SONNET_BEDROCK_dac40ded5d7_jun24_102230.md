#### 00:00:00.149

Martin Casado, General Partner at A16Z. Welcome to the Cognitive Revolution. Very happy to be here.   
(00:00:06.671) ~~I~~ (00:00:06.711)  
  
(00:00:06.751) ~~think~~ (00:00:06.852)  
this is going to be a great conversation. We are primarily here to talk about the importance of open source and to dig into   
(00:00:13.535) ~~a~~ (00:00:13.554)  
  
(00:00:13.615) ~~lot~~ (00:00:13.714)  
  
(00:00:13.734) ~~of~~ (00:00:13.775)  
the details of the current debates around early regulation of AI and how that might impact open source and what the pros, if any, and cons, if potentially many   
(00:00:23.320) ~~of~~ (00:00:23.399)  
that might be. Before we get into it, I just wanted to get a little   
(00:00:27.120) ~~bit~~ (00:00:27.222)  
of your super big picture AI worldview and ask you to project out a little bit. How powerful do you think AI is going to be over the next couple of years? 


---


#### 00:00:40.040

We've obviously heard AGI 2027 recently. Is that a story you're buying? What do you think we're going to see over the next two to three years? Listen,   
(00:00:49.343) ~~I~~ (00:00:49.404)  
  
(00:00:49.423) ~~think~~ (00:00:49.584)  
  
(00:00:49.603) ~~like~~ (00:00:49.743)  
all things, the past is probably the best predictor of the future. My hope is this continues to be a very useful tool to help us with progress and to stave off this universe that seems pretty destined on killing us. So I'm hoping it continues on that trajectory. What is interesting is if you actually look~~,~~(00:01:08.052)  
  
(00:01:09.015) ~~let's~~ (00:01:09.155)  
  
(00:01:09.174) ~~say~~ (00:01:09.254)  
the past 80 years of   
(00:01:11.096) ~~quote,~~ (00:01:11.277)  
  
(00:01:11.298) ~~unquote,~~ (00:01:11.638)  
It's been steady progress, independent of there being winters and summers. It's been very steady progress. It's been progress on economics, progress on problem solves. 


---


#### 00:01:19.906

And every time we tackle a problem, everybody's like, oh, goodness, this is it. We're almost at AGI. And then it just turns out that's one modality, and then we go on to the next, and we say, oh, that wasn't really actually AGI or AI. So this whole AGI thing has been a moving goalpost for 70 years.   
(00:01:34.878) ~~And~~ (00:01:34.957)  
  
(00:01:34.977) ~~so~~ (00:01:35.097)  
  
(00:01:35.177) ~~I~~ (00:01:35.218)  
  
(00:01:35.257) ~~would~~ (00:01:35.399)  
  
(00:01:35.438) ~~say~~, listen, it's been this kind of very steady progress. We've gotten good at many things. It's been a very useful tool.   
(00:01:39.942) ~~I~~ (00:01:40.003)  
  
(00:01:40.043) ~~hope~~ (00:01:40.143)  
it continues to be so.   
(00:01:41.144) ~~I~~ (00:01:41.203)  
  
(00:01:41.224) ~~think~~ (00:01:41.364)  
it's a very important thing for us to continue to develop and use. But   
(00:01:44.566) ~~I~~ (00:01:44.587)  
  
(00:01:44.847) ~~don't~~ (00:01:45.087)  
  
(00:01:45.147) ~~think~~ (00:01:45.709)  
  
(00:01:47.269) ~~that~~ (00:01:47.489)  
there's   
(00:01:47.950) ~~any~~ (00:01:48.331)  
  
(00:01:48.852) ~~step~~ (00:01:49.191)  
  
(00:01:49.451) ~~change~~ (00:01:49.953)  
  
(00:01:50.253) ~~or~~ (00:01:50.513)  
  
(00:01:50.533) ~~it~~ (00:01:50.572)  
  
(00:01:51.334) ~~changes~~ (00:01:51.733)  
  
(00:01:51.793) ~~the~~ (00:01:51.894)  
  
(00:01:51.995) ~~nature~~ (00:01:52.334)  
  
(00:01:52.415) ~~of~~ (00:01:52.474)  
computers or software in ways that we haven't seen before. 


---


#### 00:01:55.317

I don't think that. Let me just say a few more concrete things, because I'm a big believer in threshold effects, where sometimes I see this, it's not necessarily the topics of emergency are super hotly debated, and is emergence   
(00:02:12.415) ~~of~~ (00:02:12.475)  
  
(00:02:12.515) ~~mirage~~ (00:02:13.016)  
or not a mirage, and maybe the underlying curves are smooth. But in terms of   
(00:02:17.219) ~~like~~ (00:02:17.340)  
practical utility, even if the capability of the model hasn't taken a sudden shift, the   
(00:02:23.598) ~~like~~ (00:02:23.798)  
utility and what it can actually practically do in the world sometimes seems to take   
(00:02:28.301) ~~like~~ (00:02:28.481)  
a notable leap.   
(00:02:30.242) ~~I~~ (00:02:30.302)  
  
(00:02:30.323) ~~guess~~, right now, it seems like   
(00:02:31.905) ~~I~~ (00:02:31.965)  
  
(00:02:31.985) ~~thought~~ (00:02:32.145)  
Dworkish put this really well on a recent podcast where he said, it seems like the models are token for token, as smart as most people. 


---


#### 00:02:40.450

And yet there's some things that they're missing. Like they don't have this great long-term memory or the ability to integrate memories. They're not great at autonomy. They fall down and can't get back up or don't know how to dust themselves off. But it seems like the core intelligence is there to do a lot of jobs. And it's this packaging, memory, agentic finishing that's needed to get to the point where you'd have like this sort of knowledge worker. Is that basically the story that you see? And does that translate to   
(00:03:08.381) ~~a~~,   
(00:03:08.580) ~~I~~ (00:03:08.681)  
  
(00:03:08.920) ~~think~~ (00:03:09.080)  
AGI is   
(00:03:09.602) ~~yes,~~ (00:03:09.782)  
certainly very ill-defined, but like drop in knowledge worker that could do a large majority of jobs,   
(00:03:16.790) ~~like~~ (00:03:16.950)  
passively well is maybe better defined. 


---


#### 00:03:19.914

Do you see that coming soon? So again,   
(00:03:23.209) ~~I~~ (00:03:23.269)  
  
(00:03:23.288) ~~think~~ (00:03:23.408)  
I like to draw from a historical analogy, because   
(00:03:25.830) ~~I~~ (00:03:25.889)  
  
(00:03:25.909) ~~think~~ (00:03:26.030)  
they're very useful in this context. Because   
(00:03:27.730) ~~I~~ (00:03:27.751)  
  
(00:03:28.211) ~~just~~ (00:03:28.330)  
  
(00:03:28.471) ~~feel~~ (00:03:28.651)  
  
(00:03:28.691) ~~like~~ (00:03:28.830)  
we see these very often, and they all look the same, and it's played out. And so when I first joined Stanford to do my PhD, so I did my PhD in computer science and systems. It was in 2003. And around then,   
(00:03:41.456) ~~I~~ (00:03:41.497)  
  
(00:03:41.537) ~~don't~~ (00:03:41.637)  
  
(00:03:41.657) ~~know~~ (00:03:41.736)  
  
(00:03:41.757) ~~if~~ (00:03:41.877)  
  
(00:03:41.896) ~~it~~ (00:03:41.997)  
  
(00:03:42.037) ~~was~~ (00:03:42.296)  
  
(00:03:42.456) ~~2003~~ (00:03:42.456)  
  
(00:03:42.477) ~~or~~ (00:03:42.516)  
  
(00:03:42.537) ~~2004~~, Sebastian Thrun had won the DARPA Grand Challenge.   
(00:03:46.253) ~~To~~ (00:03:46.334)  
  
(00:03:46.354) ~~the~~ (00:03:46.413)  
  
(00:03:46.433) ~~driver~~ (00:03:46.633)  
  
(00:03:46.653) ~~grand~~ (00:03:46.835)  
  
(00:03:46.854) ~~challenge~~, he drove a van autonomously, fully autonomously for 1200 miles. And everybody was like, hooray,   
(00:03:53.920) ~~like~~ (00:03:54.161)  
AGI is solved. Robo taxi is solved. 


---


#### 00:03:57.604

This is   
(00:03:57.943) ~~amazing~~ (00:03:58.344)  
  
(00:03:58.384) ~~thing~~. And a hundred percent, we've hit one of these threshold moments. A hundred percent,   
(00:04:02.187) ~~like~~ (00:04:02.307)  
you could actually do things you couldn't do before. It was the start of a new era of vision and perception and self-driving. And now, 20 years later, and about $100 billion invested, a little bit less than $100 billion invested, the unit economics of self-driving are still three times worse than Uber. And so I am a systems person, meaning all I've been doing for the last 30 years of my life is building computer systems, large distributed systems. This is what I do. And I just know that Capabilities, like scalar capabilities,   
(00:04:37.656) ~~A~~, they're not necessarily parametric. So they don't necessarily just go up and to the right. 


---


#### 00:04:41.781

And there's so many examples of this, and we can talk about that. I know that the universe is this very heavy-tailed system, and there's no single solution that tends to reign in its complexity. I know that the economics for these things are very hard. What gets lost in these conversations is AI has been better   
(00:04:57.216) ~~at~~ (00:04:57.276)  
  
(00:04:57.317) ~~humans~~ (00:04:57.596)  
for a very long time at many things. Handwriting detection. Diagnosis for a very long time. Game playing for a very long time. Mathematics, since the creation of the computer. Four orders of magnitude better   
(00:05:10.286) ~~than~~ (00:05:10.386)  
  
(00:05:10.406) ~~mathematics~~ (00:05:10.826)  
than us. And yet, none of these things have resulted in kind of general economically viable solutions for everything, just for subsets. Another way to rephrase what you said about the language stuff is, these models are very good at predicting what   
(00:05:31.411) ~~a~~ (00:05:31.451)  
  
(00:05:31.511) ~~mean~~ (00:05:31.730)  
  
(00:05:31.771) ~~person~~ (00:05:32.112)  
  
(00:05:32.172) ~~would~~ (00:05:32.372)  
  
(00:05:32.892) ~~do~~ (00:05:33.012)  
  
(00:05:33.072) ~~next~~. 


---


#### 00:05:34.180

A mean person would do that. So sure, they basically do kernel smoothing over positional embeddings. That's great. Which means if you have a corpus of data, it'll give you the average response, which is great. And it's very useful for a number of things. Does that provide economic utility for a broad range of stuff?   
(00:05:49.572) ~~I'll~~ (00:05:49.692)  
  
(00:05:49.732) ~~tell~~ (00:05:49.831)  
  
(00:05:49.851) ~~you,~~ (00:05:49.951)  
I look at these companies basically full-time and I have for three years, we probably have the largest portfolio of them. And I don't see that yet. Now there are areas where they do work, but it's not that, it's not this kind of general knowledge worker. That's actually not working. And the areas where it is working is stuff like the marginal cost of content creation goes to zero. 


---


#### 00:06:08.901

So now we have   
(00:06:09.841) ~~like~~ (00:06:09.980)  
amazing new content.   
(00:06:11.262) ~~Or~~ (00:06:11.362)  
computers are creating an emotional connection with humans. That's new. And that's exciting. And that's amazing. But this idea of this general knowledge worker working, I haven't seen it yet. And maybe it'll happen. But just like self-driving 20 years ago, that's just not how the universe tends to play out.   
(00:06:27.644) ~~Yeah,~~ (00:06:27.845)  
where do you think we are in self-driving today? I recently took a~~,~~(00:06:30.966)  
  
(00:06:30.987) ~~and~~ (00:06:31.127)  
  
(00:06:31.146) ~~it's~~ (00:06:31.247)  
  
(00:06:31.266) ~~been~~ (00:06:31.387)  
  
(00:06:31.406) ~~actually~~ (00:06:31.627)  
  
(00:06:31.646) ~~a~~ (00:06:31.666)  
  
(00:06:31.687) ~~couple~~ (00:06:32.048)  
  
(00:06:32.708) ~~upgrades~~ (00:06:33.148)  
  
(00:06:33.608) ~~since~~ (00:06:33.889)  
  
(00:06:33.949) ~~I~~ (00:06:34.009)  
  
(00:06:34.048) ~~took~~ (00:06:34.209)  
  
(00:06:34.269) ~~my~~ (00:06:34.569)  
  
(00:06:35.069) ~~last~~ (00:06:35.329)  
ride in a Tesla FSD. At that time, I was like, this was last summer, actually. So it's been basically a year since I did it last. At that point, I was~~,~~(00:06:44.134)  
  
(00:06:44.274) ~~I~~ (00:06:44.334)  
  
(00:06:44.375) ~~think~~ (00:06:44.495)  
this is actually safer than the average person I drive with and probably the average person I see around me on the road, who sometimes I'm like, man, how are you, how did you survive long enough to get in front of me and do that? 


---


#### 00:06:56.482

And they're not deployed at super scale, we're not quite ready to trust them. But if you believe   
(00:07:02.088) ~~like~~ (00:07:02.267)  
the statistics from the company, and certainly my just episodic experience seems to suggest that they probably are roughly on par, maybe even better. And maybe we need~~,~~(00:07:11.678)  
maybe society is going to demand like a 10x better to actually deploy at scale. But It seems like we're there and then   
(00:07:19.014) ~~I~~ (00:07:19.033)  
  
(00:07:19.053) ~~guess~~.   
(00:07:19.435) ~~So~~ (00:07:19.514)  
  
(00:07:19.574) ~~I~~ (00:07:19.615)  
  
(00:07:19.634) ~~don't~~ (00:07:19.795)  
  
(00:07:19.855) ~~know~~ (00:07:19.975)  
  
(00:07:20.095) ~~what~~ (00:07:20.295)  
  
(00:07:20.475) ~~their~~ (00:07:20.834)  
  
(00:07:20.954) ~~kind~~ (00:07:21.115)  
  
(00:07:21.134) ~~of~~ (00:07:21.194)  
unit economic~~s~~(00:07:21.975)  
and   
(00:07:22.095) ~~I~~ (00:07:22.134)  
  
(00:07:22.154) ~~don't~~ (00:07:22.274)  
  
(00:07:22.295) ~~know~~ (00:07:22.394)  
  
(00:07:22.454) ~~if~~ (00:07:22.495)  
they're there on an exception basis.   
(00:07:24.875) ~~I~~ (00:07:24.935)  
  
(00:07:24.995) ~~do~~ (00:07:25.175)  
  
(00:07:25.235) ~~feel~~ (00:07:25.415)  
with a lot of these AI things, you enter this area where you get good enough that the economics suggest that you change the environment rather than make the AI more general. 


---


#### 00:07:35.259

And   
(00:07:35.418) ~~I~~ (00:07:35.478)  
  
(00:07:35.499) ~~think~~ (00:07:35.639)  
this is probably best described with agriculture. So we don't have any unit economically viable way to pick strawberries. It's like human beings and animals have been forging food forever. We're very efficient at it. To actually compete with a human on that is very tough. On the other hand, what we can do is we can play strawberries in certain rows and we can add these optical cues,   
(00:07:57.391) ~~et~~ (00:07:57.490)  
  
(00:07:57.511) ~~cetera~~, that kind of get it much closer. And so   
(00:07:59.132) ~~I~~ (00:07:59.192)  
  
(00:07:59.232) ~~definitely~~ (00:07:59.612)  
  
(00:07:59.632) ~~think~~ (00:07:59.773)  
  
(00:07:59.793) ~~that~~ (00:07:59.892)  
we're at a place where for many cases, AI is better than humans at driving, which   
(00:08:04.557) ~~I~~ (00:08:04.596)  
  
(00:08:04.617) ~~think~~ (00:08:04.737)  
is great. And   
(00:08:05.237) ~~I~~ (00:08:05.257)  
  
(00:08:05.278) ~~think~~ (00:08:05.377)  
it's a huge positive.   
(00:08:06.298) ~~I~~ (00:08:06.338)  
  
(00:08:06.379) ~~think~~ (00:08:06.478)  
these are hugely specialized systems. 


---


#### 00:08:08.379

I don't think you can take the one that's good at self-driving and make it~~,~~(00:08:11.002)  
  
(00:08:11.163) ~~I~~ (00:08:11.202)  
  
(00:08:11.223) ~~don't~~ (00:08:11.362)  
  
(00:08:11.382) ~~know~~, be a virtual girlfriend.   
(00:08:12.923) ~~I~~ (00:08:12.963)  
  
(00:08:12.983) ~~think~~ (00:08:13.084)  
These are totally different modalities. So it's not general, right? It's not general. It's just good at driving~~.~~(00:08:16.747)  
just like something is good at chess. And now   
(00:08:19.850) ~~I~~ (00:08:19.889)  
  
(00:08:19.930) ~~think~~ (00:08:20.050)  
we're at the long tail of the problem. For fully autonomous stuff like Waymo, the unit economics are just very tough, right? They're very tough if you're competing with a human driver. For other ones,   
(00:08:28.857) ~~I~~ (00:08:28.916)  
  
(00:08:28.976) ~~do~~ (00:08:29.077)  
  
(00:08:29.117) ~~think~~ (00:08:29.237)  
  
(00:08:29.257) ~~that~~ (00:08:29.557)  
if we just start evolving how we make streets and so forth,   
(00:08:33.900) ~~I~~ (00:08:33.961)  
  
(00:08:34.000) ~~think~~ (00:08:34.142)  
we're pretty close. And   
(00:08:34.981) ~~I~~ (00:08:35.022)  
  
(00:08:35.042) ~~think~~ (00:08:35.162)  
that's great.   
(00:08:35.722) ~~I~~ (00:08:35.763)  
  
(00:08:35.802) ~~think~~ (00:08:35.923)  
This is a positive for everybody. 


---


#### 00:08:38.149

I love your comment. First of all, I love the narrowness of the full self-driving. I actually think that's a huge plus that it is an engineered system that does one thing. And I also love your comment about modifying the environment. That was actually very notable even a year ago in the FSD that the places where it got tripped up, I was like, there's an exit, the closest exit to my house off the highway has one of these service roads and has a stop sign. And it's genuinely quite ambiguous as to who the stop sign is for as you're getting off the highway. It's not for you as you're getting off the highway, but it's pointed right at you. 


---


#### 00:09:15.090

And so you can understand how it would get confused about that. And that's the kind of thing that if we're serious, we can just go around fixing these signs and we'd make it so much better. Can we talk~~,~~(00:09:23.972)  
can we just   
(00:09:24.474) ~~very~~ (00:09:24.754)  
just quick digression, because this is just so important to this discussion on how the heavy tailed universe makes this conversation very complicated between human beings, because I don't think people appreciate how heavy tailed the universe is. So I just want to describe this, because it just comes up right now. And it's   
(00:09:38.623) ~~gonna~~ (00:09:38.722)  
  
(00:09:38.743) ~~be~~ (00:09:38.802)  
so relevant to what we're   
(00:09:39.644) ~~gonna~~ (00:09:39.744)  
  
(00:09:39.764) ~~be~~ (00:09:39.844)  
talking   
(00:09:40.163) ~~going~~ (00:09:40.323)  
forward, which is, there are many~~,~~(00:09:41.865)  
  
(00:09:42.066) ~~there~~ (00:09:42.166)  
  
(00:09:42.186) ~~are~~ (00:09:42.245)  
  
(00:09:42.285) ~~many~~ (00:09:42.446)  
things that systems deal with that are   
(00:09:44.326) ~~that~~ (00:09:44.427)  
  
(00:09:44.447) ~~are~~ (00:09:44.527)  
heavy tailed. 


---


#### 00:09:45.488

What heavy tailed means is, if you draw an occurrence at random, just draw one at random, the chances are   
(00:09:52.452) ~~is~~ (00:09:52.493)  
that it's a very rare occurrence. So a very classic case of this is search, right? And so if you draw a unique search query, say Google at random, the chances are it's pretty unique actually, even after all of this time. Now, this is unique searches. So if you draw,   
(00:10:09.755) ~~so~~ (00:10:09.796)  
  
(00:10:09.816) ~~if,~~ (00:10:09.855)  
  
(00:10:10.076) ~~I'm~~ (00:10:10.517)  
  
(00:10:11.597) ~~just~~ (00:10:11.738)  
  
(00:10:11.758) ~~trying,~~ (00:10:11.878)  
  
(00:10:11.898) ~~I'm~~ (00:10:11.998)  
  
(00:10:12.019) ~~trying~~ (00:10:12.158)  
  
(00:10:12.178) ~~to,~~ (00:10:12.259)  
  
(00:10:12.599) ~~I'm~~ (00:10:12.719)  
  
(00:10:12.739) ~~trying~~ (00:10:12.879)  
  
(00:10:12.918) ~~to~~ (00:10:12.979)  
  
(00:10:13.039) ~~say~~ (00:10:13.159)  
  
(00:10:13.179) ~~this~~ (00:10:13.340)  
  
(00:10:13.399) ~~as~~ (00:10:13.460)  
  
(00:10:13.519) ~~clearly~~ (00:10:13.820)  
  
(00:10:13.860) ~~as~~ (00:10:13.919)  
  
(00:10:13.980) ~~I~~ (00:10:14.020)  
  
(00:10:14.041) ~~can.~~ (00:10:14.221)  
So if you take all of the searches that go into Google, say, and then you don't de-dupe them, so like you can have a lot of repetitive ones, the vast majority of searches are the same. 


---


#### 00:10:24.471

Let's say 90% of them are common. But if you reduce it to just singular intents, so you don't have any duplications, the majority are exceptions. And that's how the universe tends to be.   
(00:10:39.147) ~~Like~~, I come from networking. It's a very famously heavy tale   
(00:10:41.908) ~~to~~ (00:10:41.948)  
discipline. A lot of things in the tale. So if you're building a general system, the majority of new things it has to do, not things it has to do, the majority of new things it has to do are exceptions. Now, it's very easy to get tripped up in these conversations because the majority of the stuff that you're doing is not new,   
(00:10:57.933) ~~right~~? The majority of stuff you're doing is very common. 


---


#### 00:11:02.178

But anytime you get into new areas, then you have to come up with new stuff. And it's like self-driving is a very~~,~~(00:11:07.485)  
  
(00:11:07.784) ~~I~~ (00:11:07.825)  
  
(00:11:07.865) ~~think~~, great case of this. It's like nobody expected basically a 2D vision problem. Let's be honest~~,~~(00:11:14.750)  
  
(00:11:14.809) ~~man~~. Self-driving is 2D. It's not even 3D. It's like you have streets and you have signs. You don't really worry about the Z dimension, really. And yet, $100 billion in, $100 billion of investment, we're almost there, but not in an economically positive way. And   
(00:11:32.827) ~~I~~ (00:11:32.908)  
  
(00:11:32.947) ~~think~~ (00:11:33.087)  
The reason I want to make this point now is   
(00:11:36.048) ~~I~~ (00:11:36.087)  
  
(00:11:36.128) ~~think~~ (00:11:36.288)  
so much of these discussions is people~~,~~(00:11:38.149)  
  
(00:11:38.969) ~~I~~ (00:11:39.048)  
  
(00:11:39.149) ~~believe~~, underestimating how heavy tail the universe is and how hard it is to make progress. 


---


#### 00:11:44.330

And so we should be working as hard to make progress so that we can do stuff like self-driving and not slow it down because the task is enormously complex.   
(00:11:56.131) ~~I~~ (00:11:56.172)  
  
(00:11:56.192) ~~think~~ (00:11:56.312)  
that's definitely a good description of where things are today   
(00:11:59.972) ~~in~~ (00:12:00.013)  
  
(00:12:00.832) ~~my~~ (00:12:01.072)  
  
(00:12:01.113) ~~kind~~ (00:12:01.273)  
  
(00:12:01.312) ~~of~~ (00:12:01.393)  
  
(00:12:02.485) ~~summary~~ (00:12:03.086)  
  
(00:12:03.206) ~~communications~~ (00:12:03.826)  
  
(00:12:03.866) ~~about~~ (00:12:04.086)  
  
(00:12:04.126) ~~the~~ (00:12:04.226)  
  
(00:12:04.246) ~~state~~ (00:12:04.427)  
  
(00:12:04.447) ~~of~~ (00:12:04.486)  
  
(00:12:04.547) ~~AI,~~ (00:12:04.927)  
  
(00:12:05.447) ~~I~~ (00:12:05.506)  
  
(00:12:05.567) ~~always~~ (00:12:05.866)  
  
(00:12:05.947) ~~say~~ (00:12:06.347)  
  
(00:12:07.368) ~~that~~ (00:12:07.607)  
the best systems are closing in on expert performance on routine tasks. And the word routine there definitely is critical, because when you get outside of routine tasks, they are not comparable to expert performance.   
(00:12:23.894) ~~Okay,~~ (00:12:24.033)  
  
(00:12:24.053) ~~so~~ (00:12:24.173)  
  
(00:12:24.193) ~~let's~~ (00:12:24.333)  
  
(00:12:24.374) ~~move~~ (00:12:24.514)  
  
(00:12:24.553) ~~this~~ (00:12:24.714)  
  
(00:12:24.855) ~~to~~ (00:12:24.975)  
  
(00:12:25.014) ~~here.~~ (00:12:25.215)  
  
(00:12:25.274) ~~So~~ (00:12:25.475)  
  
(00:12:26.495) ~~as~~ (00:12:26.595)  
  
(00:12:26.695) ~~far~~ (00:12:26.955)  
  
(00:12:27.034) ~~as~~ (00:12:27.115)  
  
(00:12:27.235) ~~I~~ (00:12:27.315)  
  
(00:12:27.375) ~~can~~ (00:12:27.556)  
  
(00:12:27.615) ~~tell~~ (00:12:27.875)  
what LLMs are doing, And there's actually papers that have shown this pretty concretely, is they take a corpus of data, they create positional embeddings, and they basically average over it. 


---


#### 00:12:38.436

And based on that, they can predict what a human being would do or say given a certain situation. And they do that basically via averaging. For 80% of the tasks, let's say that's great. This is for 80% as far as non-unique~~,~~(00:12:59.267)  
  
(00:12:59.567) ~~right~~? So you're not deduping tasks. But let me give you   
(00:13:01.969) ~~analogy~~. What if I came to you, I'm a startup founder, I'm like, hey, Nathan, I've got this amazing chat bot that for IT help tickets, it answers 90% of the time. And you're like, this is amazing. Like 90% of the work goes away~~,~~(00:13:17.878)  
  
(00:13:18.357) ~~right~~? This is what you say. But then you actually do diligence on my company and you realize that That 90% of that, 85% of that is just answering password resets. 


---


#### 00:13:30.408

So I was factually correct saying that 90% gets answered. That's the case. But the reality is the situations that I help with are very easy to automate if you wanted to. And they're not that much work. And you still have, from a unique standpoint, you still have to have humans do 50% of the unique things that come in, 50%. So that was the state of chatbots for the last 10 years. And we had the exact same discussions. They seem to answer 90%. They answer 85%. They can do these things. We're almost done. But that's not how heavy tail distributions work. So   
(00:14:03.221) ~~again~~,   
(00:14:03.500) ~~so~~ (00:14:03.681)  
let's go to this case that you're making.   
(00:14:06.143) ~~A~~ (00:14:06.202)  
  
(00:14:06.283) ~~hundred~~ (00:14:06.484)  
  
(00:14:06.504) ~~percent~~, these LLMs solve problems we couldn't before. 


---


#### 00:14:09.225

And a hundred percent, they're a very useful computer science primitive.   
(00:14:12.587) ~~we're~~ (00:14:12.788)  
nowhere close to understanding what these distributions look like, how heavy tailed they are, and how they can handle that tail, just like we weren't with AV, just like we weren't with original chatbots. And so   
(00:14:25.931) ~~I~~ (00:14:25.951)  
just think that without being specific on uniqueness and the distribution and whether we think it's parametric, it's very hard to have these conversations. So we resulted generalities that sound good, but   
(00:14:39.072) ~~I~~ (00:14:39.133)  
  
(00:14:39.153) ~~think~~ (00:14:39.312)  
from a system standpoint, aren't very. So what do you think? How far do you think this goes? Best guess, if you are imagining yourself in 2027 and you have your AI assistant, can you say, hey, 


---


#### 00:14:53.500

I'm going to New York next week for this event, here's the invitation, book me a flight, book me a hotel, and get a good result?   
(00:15:04.642) ~~I~~ (00:15:04.701)  
  
(00:15:04.721) ~~think~~ (00:15:04.841)  
it's going to be like self-driving.   
(00:15:06.222) ~~I~~ (00:15:06.263)  
  
(00:15:06.302) ~~think~~ (00:15:06.482)  
we'll get 80% there with these agentic systems and then people are going to start changing the environment to make them more effective because human output is also heavy tailed. And so   
(00:15:19.447) ~~my~~,   
(00:15:19.687) ~~my~~, my best guess. Again, I'm   
(00:15:21.509) ~~going~~ (00:15:21.609)  
  
(00:15:21.629) ~~to~~,   
(00:15:21.708) ~~I'm~~ (00:15:21.828)  
  
(00:15:21.849) ~~sorry~~,   
(00:15:21.969) ~~I'm~~ (00:15:22.028)  
  
(00:15:22.048) ~~going~~ (00:15:22.149)  
  
(00:15:22.168) ~~to~~ (00:15:22.208)  
  
(00:15:22.229) ~~repeat~~ (00:15:22.448)  
  
(00:15:22.469) ~~myself~~, but   
(00:15:22.950) ~~I~~ (00:15:23.009)  
  
(00:15:23.110) ~~actually~~ (00:15:23.269)  
  
(00:15:23.289) ~~think~~ (00:15:23.409)  
that's how it's going to play out is right now we're anytime we're always at like the beginning of what looks like an exponential, but there's actually a sigmoid. And what we think is like a fat head system, but it ends a heavy tail system. 


---


#### 00:15:34.332

It always feels like this,   
(00:15:35.173) ~~like~~ (00:15:35.293)  
it feels right now. And everybody's saying all this stuff that they're saying right now. And we're like, it's amazing.   
(00:15:38.875) ~~You~~ (00:15:38.934)  
  
(00:15:38.975) ~~know,~~ (00:15:39.075)  
Everything's going to work out and then we're going to realize, oh my goodness, the universe is heavy tail. This is really hard. These things are good at 80% of stuff, but the 80% of stuff that it's good at~~.~~(00:15:47.337)  
is not that hard. And the stuff that I need it to be good at is really hard. And so then we're going to enter an area that we're like, oh boy, we talked about scaling laws, throwing 10 times more compute at something over a period of time is so impossible that what we're going to do is we're going to actually change the way websites work. 


---


#### 00:16:04.211

And we're going to actually have indicators and we're actually going to have lanes. And then we're going to spend the time actually changing the systems. And so over time between changing the way websites work and where these agents work, we'll enter a period where these things are more autonomous.   
(00:16:17.900) ~~don't~~ (00:16:18.160)  
  
(00:16:18.221) ~~think~~ (00:16:18.360)  
it's because we've cracked the generality problem.   
(00:16:20.782) ~~I~~ (00:16:20.822)  
  
(00:16:20.841) ~~think~~ (00:16:20.961)  
it's because we've evolved computer science as a discipline and tech and software as an industry in order to solve the problem. What do you make of these sort of,   
(00:16:31.505) ~~I~~ (00:16:31.566)  
find language to be a confusing domain in some ways because there is so much, it's   
(00:16:40.229) ~~like~~ (00:16:40.350)  
hard to get truly out of distribution in   
(00:16:42.990) ~~like~~ (00:16:43.150)  
interesting ways where it's   
(00:16:44.530) ~~like~~ (00:16:44.610)  
very hard to say,   
(00:16:45.530) ~~is~~ (00:16:45.572)  
  
(00:16:45.591) ~~this~~, can write a sonnet about the NBA finals or whatever. 


---


#### 00:16:50.625

And that's definitely out of distribution, but you could say it's interpolating between things in distribution. Sometimes I'd like to just go to things that humans can't do. So when I look at things like,   
(00:17:00.710) ~~I~~ (00:17:00.730)  
  
(00:17:00.750) ~~don't~~ (00:17:00.850)  
  
(00:17:00.870) ~~know~~ (00:17:00.950)  
  
(00:17:00.970) ~~if~~ (00:17:01.009)  
  
(00:17:01.029) ~~you've~~ (00:17:01.149)  
  
(00:17:01.169) ~~studied~~ (00:17:01.409)  
Evo, the early foundation model for DNA sequencing, It's an interesting one. I did an episode just yesterday on models for basically simulating solutions, like salt solutions. And in both   
(00:17:16.925) ~~of~~ (00:17:16.965)  
these cases, you see these sort of remarkable generalization moments where in the Evo case, it's trained on DNA sequence data, just like bacteria and phage data. And so it's next token prediction, it's next base pair prediction. Same thing as a language model, right? So you could say, oh, it's only predicting the next base pair. 


---


#### 00:17:36.913

But like the language models, it seems to have developed these higher order internal representations that mean something that it was not specifically trained to learn, but that it's learned in service of making the next base pair prediction. And one of the experiments that they did that is so striking to me, this is from the Ark Institute, they did what they call gene essentiality scoring, where they basically say, we'll give you a gene sequence and we'll perturb it. And then we'll look at your perplexity, your being the model, your level of confidence or uncertainty downstream of that prediction. If you remain confident, then we can believe that gene must not be super essential.   
(00:18:20.448) ~~Cause~~ (00:18:20.567)  
  
(00:18:20.607) ~~if~~ (00:18:20.667)  
  
(00:18:20.708) ~~it~~ (00:18:20.768)  
  
(00:18:20.807) ~~gets~~ (00:18:20.968)  
  
(00:18:21.008) ~~messed~~ (00:18:21.268)  
  
(00:18:21.428) ~~up,~~ (00:18:21.508)  
  
(00:18:21.847) ~~then~~ (00:18:21.968)  
  
(00:18:21.988) ~~you~~ (00:18:22.087)  
  
(00:18:22.107) ~~can~~ (00:18:22.208)  
  
(00:18:22.228) ~~still~~ (00:18:22.387)  
  
(00:18:22.428) ~~predict~~ (00:18:22.749)  
  
(00:18:22.788) ~~like~~ (00:18:22.949)  
  
(00:18:23.068) ~~how~~ (00:18:23.209)  
  
(00:18:23.229) ~~this~~ (00:18:23.368)  
  
(00:18:23.409) ~~fits~~ (00:18:23.628)  
  
(00:18:23.669) ~~into~~ (00:18:23.888)  
  
(00:18:23.949) ~~life~~. 


---


#### 00:18:24.628

But if   
(00:18:25.148) ~~you~~,   
(00:18:25.308) ~~if~~ (00:18:25.368)  
we make that change and then you become very~~,~~(00:18:27.069)  
radically unconfident in your downstream predictions, then we can infer that this gene is actually really important because now when it's off,   
(00:18:35.494) ~~like~~ (00:18:35.654)  
now you have no idea, now you recognize you're out of distribution and you can't make any confident predictions anymore. It seems to me like that signals, and that's like a 7 billion parameter model, 300 billion tokens, that's 2% of what LLAMA, or 1.5% of what LLAMA 3 is trained on. So it does feel like there's some potential in these systems to learn things that we don't know, to grok, if you will, patterns in the universe that are   
(00:19:08.167) ~~like~~, not known to anyone really, to essentially speak DNA. 


---


#### 00:19:12.913

Yeah, sure. So if you buy all that,   
(00:19:14.595) ~~like~~, how does that, how do you backport that to language? Because I feel like I start to see, well, maybe there are, we're just modeling the economy or modeling software. I do start to see things. If you were to train, keep scaling and keep training these models on distributed systems or software or whatever, how do we know that they don't start to learn things that people don't know and truly generalize beyond the training set? Because it does seem like that's happening in some profound way.   
(00:19:42.886) ~~I~~ (00:19:42.946)  
  
(00:19:42.987) ~~think~~ (00:19:43.106)  
  
(00:19:43.146) ~~that's~~ (00:19:43.267)  
  
(00:19:43.287) ~~a~~ (00:19:43.366)  
  
(00:19:43.386) ~~great~~ (00:19:43.508)  
  
(00:19:43.528) ~~question~~.   
(00:19:43.847) ~~I~~ (00:19:43.867)  
  
(00:19:44.228) ~~think~~ (00:19:44.367)  
  
(00:19:44.407) ~~maybe~~ (00:19:44.607)  
  
(00:19:44.647) ~~this~~ (00:19:44.768)  
  
(00:19:44.827) ~~is~~ (00:19:44.907)  
  
(00:19:44.968) ~~why~~ (00:19:45.148)  
  
(00:19:45.209) ~~I~~ (00:19:45.288)  
  
(00:19:45.328) ~~come~~ (00:19:45.489)  
  
(00:19:45.528) ~~to~~ (00:19:45.588)  
  
(00:19:45.628) ~~this~~ (00:19:45.749)  
  
(00:19:45.769) ~~from~~ (00:19:45.888)  
  
(00:19:45.929) ~~a~~ (00:19:45.969)  
  
(00:19:45.989) ~~different~~ (00:19:46.209)  
  
(00:19:46.229) ~~thing~~. Prior to my PhD, my life was computational physics. 


---


#### 00:19:50.971

I worked at Lawrence Livermore National Lab. I did large physics simulation. I just lived this life. And   
(00:19:55.555) ~~I~~ (00:19:55.615)  
  
(00:19:55.654) ~~think~~ (00:19:55.815)  
the Occam's razor in all of this is distribution in, distribution out. These things are very good at learning distributions of the training set. And so this is just   
(00:20:07.762) ~~into~~ (00:20:07.903)  
this kind of weird, quirky coincidence of this moment. I actually worked on protein folding at IBM T.G. Watson Research Center in 1999 on the Blue Team project. And at that time, there was a belief that we knew enough of the fundamental forces in order to actually, from first principles, calculate protein folding, right? That was the thesis, and that we just didn't have enough compute. And so they actually built a whole computer to do this called Blue Gene. 


---


#### 00:20:38.622

And through corporate machinations, that ended up becoming an entirely new computer because they couldn't fund it. It never really happened. But there was the belief that if you had enough compute that you could do protein folding and a bunch of other stuff. And so   
(00:20:51.230) ~~I~~ (00:20:51.309)  
  
(00:20:51.410) ~~think~~ (00:20:51.589)  
it's very reasonable. This is just strict Occam's razor that if you have enough compute and you have enough data and you're dealing with an axiomatic system that you can reduce to first principles, then you will learn that distribution and you can use it for predictive stuff. And if you want to call predictive emergent, that's fine. But it's just predictive, like anything is predictive. Like I've worked on   
(00:21:13.605) ~~simulations~~ (00:21:14.165)  
codes that were predictive. 


---


#### 00:21:15.086

Could I have predicted the yield of a nuclear weapon? No, I could not have predicted that as a human being. Only a computer can. But it's really just understanding first principles to create these things. I very strongly believe these models are very useful. In science, where you've got fundamental laws of nature that are being learned that can be predictive, I think the models that are good at that is not going to turn around and solve a different problem, most likely, because I think you've probably learned one distribution from one domain, and that's a very useful tool that we should use. Now let's   
(00:21:50.113) ~~go~~ (00:21:50.212)  
  
(00:21:50.232) ~~ahead~~ (00:21:50.333)  
  
(00:21:50.353) ~~and~~ (00:21:50.413)  
move that over to language   
(00:21:51.574) ~~now~~. So if my Occam's Razor is these things learn structure in a data corpus. 


---


#### 00:21:57.259

It's distribution in, distribution out. That means that they're learning structure of the text corpus that they've been fed. And   
(00:22:07.067) ~~I~~ (00:22:07.107)  
  
(00:22:07.147) ~~think~~ (00:22:07.288)  
a great example of this, there's a recent paper I saw,   
(00:22:09.490) ~~I~~ (00:22:09.549)  
  
(00:22:09.569) ~~don't~~ (00:22:09.670)  
  
(00:22:09.690) ~~know~~ (00:22:09.769)  
  
(00:22:09.789) ~~if~~ (00:22:09.829)  
  
(00:22:09.869) ~~it~~ (00:22:09.930)  
  
(00:22:09.950) ~~got~~ (00:22:10.049)  
  
(00:22:10.069) ~~accepted~~ (00:22:10.410)  
  
(00:22:10.450) ~~or~~ (00:22:10.490)  
  
(00:22:10.529) ~~not~~,   
(00:22:10.671) ~~I~~ (00:22:10.691)  
  
(00:22:10.730) ~~think~~ (00:22:10.830)  
  
(00:22:10.851) ~~it~~ (00:22:10.891)  
  
(00:22:10.911) ~~was~~ (00:22:10.990)  
  
(00:22:11.010) ~~just~~ (00:22:11.171)  
  
(00:22:11.250) ~~on~~ (00:22:11.351)  
  
(00:22:11.832) ~~archive~~, but that showed that if you could gzip a text corpus,   
(00:22:16.316) ~~do~~ (00:22:16.455)  
  
(00:22:16.496) ~~you~~ (00:22:16.655)  
  
(00:22:16.695) ~~see~~ (00:22:16.836)  
  
(00:22:16.875) ~~this~~? And the compression was good, then the accuracy was good. Which suggests almost everything you need to know about this, that all it's doing is learning structure in the text. It has nothing to do with underlying meaning, it's just structure that's in the text. And so you can understand the distribution of text. You can actually spit out text. 


---


#### 00:22:36.271

But this doesn't say anything about learning fundamental principles of the world from which the text is based. And so it almost seems to be that the magic in this sequence of texts is you've got these humans that have spent~~,~~(00:22:49.218)  
  
(00:22:49.538) ~~say~~, 3,000 years looking at the universe. And the universe is~~,~~(00:22:53.059)  
  
(00:22:53.099) ~~again~~,   
(00:22:53.319) ~~like~~ (00:22:53.440)  
heavy-tailed, and it's nonlinear, and it's very complex~~.~~(00:22:55.760)  
  
(00:22:56.421) ~~and~~ (00:22:56.500)  
  
(00:22:56.540) ~~it's~~ (00:22:56.641)  
fractal and it's self-similar. And these are notoriously complicated systems to simulate. So then the human brain is~~,~~(00:23:04.684)  
  
(00:23:04.704) ~~I'm~~ (00:23:04.805)  
  
(00:23:04.825) ~~going~~ (00:23:04.984)  
  
(00:23:05.005) ~~to~~ (00:23:05.045)  
abstract this out as a tree. And   
(00:23:06.605) ~~I'm~~ (00:23:06.685)  
  
(00:23:06.705) ~~going~~ (00:23:06.806)  
  
(00:23:06.826) ~~to~~ (00:23:06.885)  
abstract this out as   
(00:23:09.106) ~~like~~ (00:23:09.267)  
this concept. And so we're like these machines that take the universe and abstract it into things that are words. 


---


#### 00:23:14.369

But it's a very lossy representation.   
(00:23:16.010) ~~Like~~ (00:23:16.111)  
you can't   
(00:23:17.090) ~~You~~ (00:23:17.171)  
  
(00:23:17.211) ~~can't~~ (00:23:17.391)  
describe something and get back the universe. But it turns out, because we have those abstractions, they're very predictable.   
(00:23:23.035) ~~Like~~, we've made the universe more linear. And we've made the universe less heavy-tailed. And we've made the universe less self-similar. And once we've done that, we've added structure that's predictable. So the fact that you can take a corpus of text, which a human being has pulled from the universe and then made the universe much simpler, and find structure in that is not surprising at all. But to think that somehow then you can go from that to the universe is a step that just simply has not been demonstrated. 


---


#### 00:23:49.914

And   
(00:23:50.056) ~~it~~ (00:23:50.115)  
actually, it doesn't even stand to reason. Two follow-ups there. One on the question or the subject of meaning. How do you square the idea, if I understood you correctly, you're saying that doesn't mean that they have any understanding of meaning. How would you square that with a sparse autoencoder line of research or sort of Golden Gate Claw, if you will? Like they're able to now say, these sparse auto encoder techniques that they can isolate~~,~~(00:24:18.008)  
  
(00:24:18.028) ~~I~~ (00:24:18.048)  
  
(00:24:18.887) ~~think~~ (00:24:19.009)  
  
(00:24:19.028) ~~it's~~ (00:24:19.128)  
30 some million different features, each of which is a direction in activation space, and then inject those at runtime. And they're starting to create these sort of control mechanisms where it's like they jack up the Golden Gate Bridge feature, then all it wants to talk about is Golden Gate Bridge, or more practically, insert kindness or insert~~,~~(00:24:41.085)  
  
(00:24:41.184) ~~you~~ (00:24:41.244)  
  
(00:24:41.265) ~~know,~~ (00:24:41.384)  
deviousness or whatever. 


---


#### 00:24:42.786

There seems to be some meaning there. There's structure. That's very different than meaning. So we're talking about three things.   
(00:24:49.990) ~~And~~ (00:24:50.089)  
  
(00:24:50.130) ~~this~~ (00:24:50.250)  
  
(00:24:50.289) ~~is~~ (00:24:50.369)  
  
(00:24:50.390) ~~a~~ (00:24:50.410)  
  
(00:24:50.710) ~~great~~ (00:24:50.869)  
  
(00:24:50.910) ~~conversation,~~ (00:24:51.289)  
  
(00:24:51.309) ~~because~~ (00:24:51.451)  
  
(00:24:51.471) ~~I~~ (00:24:51.490)  
  
(00:24:51.510) ~~think~~ (00:24:51.631)  
  
(00:24:51.671) ~~it~~ (00:24:51.711)  
  
(00:24:51.750) ~~gets~~ (00:24:51.871)  
  
(00:24:51.891) ~~to~~ (00:24:51.931)  
  
(00:24:51.990) ~~the~~ (00:24:52.090)  
  
(00:24:52.131) ~~heart~~ (00:24:52.290)  
  
(00:24:52.310) ~~of~~ (00:24:52.371)  
  
(00:24:52.411) ~~it.~~ (00:24:52.471)  
So the universe is self-similar. It's fractal. Meaning no matter what zoom level you look at it, it has the same stochastic properties, right? So   
(00:25:03.503) ~~like~~ (00:25:03.644)  
you can spend an entire life studying a cell and a planet~~,~~(00:25:06.246)  
  
(00:25:06.506) ~~right~~? That's how much complexity is in the universe. The universe is heavy-tailed, which means the exception is the norm if you dedupe. And it's nonlinear, which means that you can't computationally predict out too far just because we don't have closed form solutions for nonlinear stuff and it's just a very hard computations problem. 


---


#### 00:25:25.163

That's the universe. Now, human beings have had to navigate this crazy universe. And so we've created this amazing engine, which is the brain. And it has reduced this universe to~~,~~(00:25:36.971)  
  
(00:25:37.030) ~~like~~, concepts and words and stuff that we use and we talk about. That   
(00:25:40.271) ~~kind~~ (00:25:40.392)  
  
(00:25:40.412) ~~of~~ (00:25:40.451)  
makes it a little bit predictable. And so at least you and I can communicate about it. But if I tell you   
(00:25:45.992) ~~like~~, this is a tree. There's a concept tree in my brain. But that's   
(00:25:50.355) ~~a~~, it's almost an arbitrary distinction that it's a tree.   
(00:25:54.258) ~~Like~~, I could talk about branches versus leaves.   
(00:25:57.339) ~~I~~ (00:25:57.400)  
  
(00:25:57.460) ~~took~~ (00:25:57.661)  
  
(00:25:57.721) ~~about~~, could talk about networks of trees. That's actually one tree, like the big aspen grove. I could talk about cells of the tree. 


---


#### 00:26:02.943

It's like this kind of arbitrarily useful distinction. So it has some semblance to the real world. But if I say a tree, it's probably not   
(00:26:10.987) ~~an~~ (00:26:11.047)  
accurate relative to how the world is. Is it one tree? Is it a family tree? It's just a useful abstraction. So these models will 100% recover the abstractions that we've put in text because the structure is all there. That's not surprising. Like, compression would do that. All it's doing is taking advantage of structure, and that structure is real. But let's say that it's finding a tree. Does a tree actually map to the universe in a meaningful way? It's a human-created concept that has some vague semblance to something we all agree on. 


---


#### 00:26:43.435

Unless you're a scientist, then you probably disagree with the common understanding. And   
(00:26:47.837) ~~oh,~~ (00:26:47.917)  
  
(00:26:47.958) ~~by~~ (00:26:48.097)  
  
(00:26:48.137) ~~the~~ (00:26:48.218)  
  
(00:26:48.258) ~~way,~~ (00:26:48.397)  
my concept of a tree also includes a toy and a cartoon picture of a tree, which is entirely different. And so text is a way that we as humans represent the world that's very different than the actual universe because we find it useful. There's structure there. And these LLMs are exploiting that structure just like compression would or anything else. And it's very useful for us. But it doesn't necessarily mean that these things can enact on the world~~,~~(00:27:17.395)  
  
(00:27:17.497) ~~right~~? These are very different domains.   
(00:27:19.962) ~~Yeah,~~ (00:27:20.202)  
  
(00:27:20.423) ~~I~~ (00:27:20.462)  
  
(00:27:20.482) ~~guess~~ (00:27:20.624)  
  
(00:27:20.644) ~~I'm~~ (00:27:20.763)  
  
(00:27:20.784) ~~not~~ (00:27:20.903)  
  
(00:27:21.003) ~~quite~~ (00:27:21.183)  
  
(00:27:21.223) ~~getting~~ (00:27:21.443)  
  
(00:27:21.463) ~~the~~ (00:27:21.523)  
  
(00:27:21.564) ~~gap.~~ (00:27:21.845)  
There's so many interesting results to point to recently. 


---


#### 00:27:25.807

Did you see the one about GPT-4 finding and exploiting new zero-day exploits? This was just in the last week or two. And it's increasingly impossible to keep up with everything, so I don't expect you've seen everything. No, that one I have. I actually think,   
(00:27:40.901) ~~so~~ (00:27:41.040)  
my two minutes,   
(00:27:41.662) ~~and~~ (00:27:41.741)  
  
(00:27:41.761) ~~I~~ (00:27:41.842)  
  
(00:27:41.862) ~~think~~ (00:27:42.041)  
when you're dealing with this much compute, and this much data,   
(00:27:46.885) ~~I~~ (00:27:46.945)  
  
(00:27:46.987) ~~think~~ (00:27:47.146)  
the human intuition just totally fails. And we as humans are really bad about thinking in distributions. Anyway, this is not how we think. We assume the world is parametric. We assume, and by the way, which is why the text that we create is so well structured and why it could be exploited by LLMs. 


---


#### 00:28:00.038

Like you have to navigate this universe. You have to make   
(00:28:02.400) ~~the~~ (00:28:02.480)  
simplifying assumptions, which we do. And so it's good to actually come up with mental frameworks about how these things work. So I'll tell you a few of mine. The first one of mine is, as far as I can tell, these things are explaining structure in whatever data that they're reading. And like we've mentioned before,   
(00:28:19.535) ~~and~~ (00:28:19.654)  
it's not clear whether that distribution extends beyond that. And if it does, then you're basically back to simulating the universe, which I've spent a lot of time with. I think that's very tough. The second one is the actual mechanisms. We're talking about transformers. The actual mechanism is basically kernel smoothing. 


---


#### 00:28:34.374

It's averaging. Which means   
(00:28:36.035) ~~to~~ (00:28:36.115)  
  
(00:28:36.174) ~~me~~ (00:28:36.355)  
is the further you get out to where the data is rare, the greater the inaccuracies come. And that doesn't mean that for systems that you can actually extrapolate from, that you don't get great results. That would be~~,~~(00:28:50.861)  
  
(00:28:50.901) ~~quote~~ (00:28:51.080)  
  
(00:28:51.101) ~~unquote~~, out of distribution. It turns out some systems are linear, or you have enough data, you can map the distribution. So that one is totally fine. Now, the third one is this in-context learning one. And   
(00:29:00.144) ~~I~~ (00:29:00.164)  
  
(00:29:00.204) ~~think~~ (00:29:00.345)  
Vishal Mishra, who's a professor at Columbia, did the best work on this, where he actually shows that for in-context learning, where you actually put the context in the prompt and you can move the posterior distribution to get interesting results, he mapped it specifically to Bayesian learning. 


---


#### 00:29:15.944

And it's a beautiful paper.   
(00:29:17.567) ~~I~~ (00:29:17.606)  
  
(00:29:17.626) ~~don't~~ (00:29:17.747)  
  
(00:29:17.767) ~~know~~ (00:29:17.846)  
  
(00:29:17.866) ~~why~~ (00:29:17.967)  
more people   
(00:29:18.428) ~~don't~~ (00:29:18.607)  
read it.   
(00:29:18.969) ~~So~~ (00:29:19.088)  
  
(00:29:19.128) ~~listen~~, we know that these things can do some basic Bayesian reasoning. And this is where the prompt is basically the new evidence, which changes the posterior function. So you'll get new stuff there. We know that if you average enough stuff, You'll get new stuff there. It just has to be linearly interpolatable. If it's not linearly interpolatable, you're not going to get new stuff. So none of these things suggest that you're not going to get new stuff. It just puts constraints. We know how Bayesian systems work. We've got 20 years of understanding convergence properties to them. We have   
(00:29:49.806) ~~a~~ (00:29:49.826)  
work that's specifically mapped ICL to Bayesian learning. 


---


#### 00:29:54.846

So let's just go ahead and use that corpus of work to understand the properties. It doesn't say it's out of distribution or indistributed. It doesn't say that at all. It just bounds what that means. And then we also know the mechanics of the way Transformers works, which is this kernel split. It's more complex than that. And so that can create new things, but it means there's a linear interpolation. And so   
(00:30:16.763) ~~I~~ (00:30:16.804)  
  
(00:30:17.065) ~~think~~ (00:30:17.184)  
the problem is we have these anecdotal conversations where it's whack-a-mole. It feels almost like the intelligent design discussions where someone will come up with a new set of evidence of something that there's no way evolution could have created this~~.~~(00:30:30.982)  
  
(00:30:31.624) ~~to~~ (00:30:31.763)  
  
(00:30:31.784) ~~spend~~ (00:30:31.984)  
  
(00:30:32.105) ~~all~~ (00:30:32.265)  
  
(00:30:32.285) ~~of~~ (00:30:32.325)  
  
(00:30:32.384) ~~this~~ (00:30:32.525)  
  
(00:30:32.585) ~~time~~ (00:30:32.965)  
  
(00:30:33.487) ~~to~~ (00:30:33.606)  
  
(00:30:33.646) ~~show~~ (00:30:33.826)  
  
(00:30:33.926) ~~actually~~ (00:30:34.208)  
  
(00:30:34.248) ~~you~~ (00:30:34.327)  
  
(00:30:34.367) ~~can~~ (00:30:34.647)  
  
(00:30:34.688) ~~but~~ (00:30:34.807)  
  
(00:30:34.868) ~~we~~ (00:30:34.929)  
  
(00:30:34.949) ~~don't~~ (00:30:35.068)  
  
(00:30:35.088) ~~know~~ (00:30:35.229)  
  
(00:30:35.288) ~~everything~~ (00:30:35.730)  
  
(00:30:35.769) ~~and~~ (00:30:35.849)  
  
(00:30:35.890) ~~here's~~ (00:30:36.089)  
  
(00:30:36.109) ~~the~~ (00:30:36.210)  
  
(00:30:36.250) ~~theory~~ (00:30:36.590)  
  
(00:30:36.671) ~~and~~ (00:30:36.750)  
  
(00:30:36.810) ~~i~~ (00:30:36.891)  
  
(00:30:37.010) ~~i~~ (00:30:37.092)  
  
(00:30:37.192) ~~feel~~ (00:30:37.392)  
  
(00:30:37.412) ~~like~~ (00:30:37.551)  
  
(00:30:37.632) ~~when~~ (00:30:37.751)  
  
(00:30:37.772) ~~we~~ (00:30:37.893)  
  
(00:30:37.932) ~~have~~ (00:30:38.053)  
  
(00:30:38.093) ~~these~~ (00:30:38.212)  
  
(00:30:38.252) ~~discussions~~ (00:30:38.634)  
  
(00:30:38.693) ~~it~~ (00:30:38.733)  
  
(00:30:38.753) ~~should~~ (00:30:38.874)  
  
(00:30:38.894) ~~be~~ (00:30:38.973)  
  
(00:30:38.993) ~~a~~ (00:30:39.013)  
  
(00:30:39.114) ~~bit~~ (00:30:39.275)  
  
(00:30:39.295) ~~more~~ (00:30:39.434)  
  
(00:30:39.515) ~~principled~~ (00:30:39.996)  
  
(00:30:40.036) ~~as~~ (00:30:40.115)  
  
(00:30:40.175) ~~opposed~~ (00:30:40.476)  
  
(00:30:40.496) ~~to~~ (00:30:40.615)  
  
(00:30:40.856) ~~i've~~ (00:30:40.957)  
  
(00:30:40.997) ~~got~~ (00:30:41.096)  
  
(00:30:41.116) ~~this~~ (00:30:41.257)  
  
(00:30:41.356) ~~anecdote~~ (00:30:41.758)  
  
(00:30:41.778) ~~that~~ (00:30:41.877)  
  
(00:30:41.897) ~~seems~~ (00:30:42.077)  
  
(00:30:42.097) ~~something's~~ (00:30:42.419)  
  
(00:30:42.499) ~~new~~ (00:30:42.659)  
  
(00:30:42.699) ~~because~~ (00:30:42.878)  
  
(00:30:42.920) ~~nobody~~ (00:30:43.180)  
  
(00:30:43.200) ~~says~~ (00:30:43.359)  
  
(00:30:43.380) ~~that~~ (00:30:43.460)  
  
(00:30:43.480) ~~you're~~ (00:30:43.599)  
  
(00:30:43.619) ~~not~~ (00:30:43.701)  
  
(00:30:43.740) ~~going~~ (00:30:43.840)  
  
(00:30:43.861) ~~to~~ (00:30:43.901)  
  
(00:30:43.921) ~~see~~ (00:30:44.000)  
  
(00:30:44.040) ~~new~~ (00:30:44.201)  
  
(00:30:44.221) ~~stuff~~ (00:30:44.481)  
  
(00:30:44.501) ~~it's~~ (00:30:44.582)  
  
(00:30:45.021) ~~very~~ (00:30:45.182)  
  
(00:30:45.363) ~~obviously~~ (00:30:45.864)  
  
(00:30:45.884) ~~if~~ (00:30:45.923)  
  
(00:30:45.943) ~~you're~~ (00:30:46.144)  
  
(00:30:46.203) ~~doing~~ (00:30:46.605)  
  
(00:30:46.845) ~~interpolation~~ (00:30:47.405)  
  
(00:30:47.425) ~~it's~~ (00:30:47.526)  
  
(00:30:47.586) ~~new~~ (00:30:47.726)  
  
(00:30:48.487) ~~very~~ (00:30:48.626)  
  
(00:30:48.767) ~~obvious~~ (00:30:49.007)  
  
(00:30:49.047) ~~if~~ (00:30:49.086)  
  
(00:30:49.126) ~~you're~~ (00:30:49.247)  
  
(00:30:49.267) ~~doing~~ (00:30:49.386)  
  
(00:30:49.467) ~~Bayesian~~ (00:30:50.007)  
  
(00:30:50.166) ~~reasoning,~~ (00:30:50.547)  
  
(00:30:50.587) ~~like~~ (00:30:50.728)  
  
(00:30:50.768) ~~something's~~ (00:30:51.028)  
  
(00:30:51.048) ~~going~~ (00:30:51.147)  
  
(00:30:51.167) ~~to~~ (00:30:51.208)  
  
(00:30:51.228) ~~be~~ (00:30:51.307)  
  
(00:30:51.387) ~~new,~~ (00:30:51.587)  
  
(00:30:51.647) ~~and~~ (00:30:51.728)  
  
(00:30:51.768) ~~talk~~ (00:30:52.008)  
  
(00:30:52.167) ~~more~~ (00:30:52.468)  
  
(00:30:52.548) ~~about~~ (00:30:52.867)  
  
(00:30:53.407) ~~the~~ (00:30:53.488)  
  
(00:30:53.548) ~~distributions~~ (00:30:54.367)  
  
(00:30:54.448) ~~and~~ (00:30:54.548)  
  
(00:30:54.567) ~~the~~ (00:30:54.667)  
  
(00:30:54.807) ~~theory~~ (00:30:55.169)  
  
(00:30:55.189) ~~of~~ (00:30:55.229)  
  
(00:30:55.269) ~~why~~ (00:30:55.409)  
  
(00:30:55.469) ~~we're~~ (00:30:55.608)  
  
(00:30:55.628) ~~doing~~ (00:30:55.828)  
  
(00:30:55.848) ~~that.~~ (00:30:55.929)  



---


#### 00:30:56.249

But as far as I can tell, that's totally missing. Nobody's come and said, here's my theory of out-of-distribution stuff. Here is my thesis for what is going on functionally to create this new data. And on the other hand, you've got mounting   
(00:31:10.991) ~~and~~ (00:31:11.071)  
tons of evidence that map these to existing systems that we know~~.~~(00:31:14.032)  
that people just seem to not want to follow. And I just feel like, listen, humans love to see things in clouds and complex systems. Clouds are like, they're just, there's so many facets, and they're so complicated, and they're so huge, and they're so ethereal, and then we see things. And we just do this historically. And we've got these   
(00:31:31.170) ~~kind~~ (00:31:31.309)  
  
(00:31:31.329) ~~of~~ (00:31:31.390)  
amazing compute elements that are huge~~.~~(00:31:33.912)  
and they surprise us, and they're amazing. 


---


#### 00:31:38.253

But we can map them to formal systems, and we know how they work. And that doesn't mean that they're dangerous or not dangerous. I'm not saying that. I'm just saying that we can actually map them to reasonable systems to have a discussion. And that just seems to be missing. This conversation is a great example. I'm very happy to map these things to formal systems   
(00:31:52.178) ~~We~~ (00:31:52.298)  
  
(00:31:52.337) ~~understand~~ (00:31:52.978)  
and have that discussion. But it's always this kind of anecdotal whack-a-mole instead, which I just don't know how to answer to every instance of what seems like emergent behavior, when, of course, emergent behavior is expected anyways.   
(00:32:03.000) ~~Yeah~~. There's a lot there~~,~~(00:32:05.028)  
  
(00:32:05.087) ~~obviously~~, to chew on. I didn't mean to rant so long. 


---


#### 00:32:10.811

No apology needed. But I am definitely a materialist. And so I do believe that in the fullness of time, we can probably figure out   
(00:32:20.419) ~~in~~ (00:32:20.558)  
an arbitrary level of detail how these things work. They're actually way easier to study than life, for example.   
(00:32:27.083) ~~Of~~ (00:32:27.143)  
  
(00:32:27.182) ~~course~~. And I would say we've made tremendous progress in interpretability and general understanding   
(00:32:34.827) ~~paper~~ (00:32:35.048)  
  
(00:32:35.068) ~~you~~ (00:32:35.148)  
  
(00:32:35.169) ~~mentioned~~ (00:32:35.388)  
is a great example. I've seen another one too that's pretty similar where somebody designed a matrix implementation of gradient descent and then looked for that in the wild and found it.   
(00:32:47.103) ~~I~~ (00:32:47.123)  
  
(00:32:47.182) ~~thought~~, look, these things have learned to implement gradient descent at runtime and that's another kind of mechanism, at least sometimes, probably have different mechanisms   
(00:32:54.151) ~~and~~ (00:32:54.211)  
  
(00:32:54.230) ~~different~~ (00:32:54.471)  
  
(00:32:54.510) ~~models~~. 


---


#### 00:32:54.951

And there's this various, almost like phase change type of dynamics that are being explored too, where even for something as simple as like the modular addition grokking, there's like multiple algorithms that it can lock into each of which work, but they're different. And so there's memorization and then there's multiple like actual algorithmic solves and which one you end up with depends on initial conditions. So there's,   
(00:33:18.349) ~~I~~ (00:33:18.390)  
  
(00:33:18.430) ~~do~~ (00:33:18.509)  
  
(00:33:18.549) ~~believe~~ (00:33:18.730)  
  
(00:33:18.769) ~~that~~ (00:33:18.910)  
  
(00:33:19.109) ~~we~~,   
(00:33:20.338) ~~I~~ (00:33:20.378)  
  
(00:33:20.398) ~~don't~~ (00:33:20.499)  
  
(00:33:20.519) ~~think~~ (00:33:20.618)  
there's any magic in the machine.   
(00:33:21.579) ~~I'm~~ (00:33:21.640)  
  
(00:33:21.660) ~~definitely~~ (00:33:21.859)  
  
(00:33:21.880) ~~with~~ (00:33:21.960)  
  
(00:33:22.000) ~~you~~ (00:33:22.079)  
  
(00:33:22.599) ~~on~~ (00:33:22.680)  
  
(00:33:22.720) ~~that~~. It does seem that, in principle, we should be able to figure it all out. It doesn't seem like we've made a lot of progress, but it doesn't seem like we're that close to having it all figured out. 


---


#### 00:33:33.346

On the sigmoid question, I tend to also agree that it does not seem like there's reason to believe that this is going to be an exponential forever. It does seem like it probably levels off. But then I'm also reminded of the old joke of two guys in the woods and the bear is coming and the one's putting on his shoes and he says, I don't have to outrun the bear, I just have to outrun you. And so I do wonder if we imagine continuing to scale up as we have been scaling up and there's all these trend lines and   
(00:34:01.553) ~~times~~ (00:34:01.814)  
more compute and however much advantage from algorithmic efficiency or whatever. I was just talking to somebody at one of the   
(00:34:10.010) ~~I~~ (00:34:10.050)  
  
(00:34:10.070) ~~guess~~ (00:34:10.190)  
  
(00:34:10.230) ~~I'll~~ (00:34:10.670)  
  
(00:34:10.690) ~~just~~ (00:34:10.851)  
  
(00:34:10.891) ~~say~~ (00:34:11.070)  
generally one of the top five hyperscalers in the world, and they are using a new replacement for the Atom optimizer that is giving them something like 30% savings on compute. 


---


#### 00:34:23.802

This was published open source, the technique is called SOFIA, and it's just incredibly 30% off the top. You're saving like $100 million on the biggest runs. Anyway, let's imagine we continue to scale up and it's a few more orders of magnitude. And let's say we don't just put in the text, but we also put in   
(00:34:39.550) ~~like~~ (00:34:39.789)  
this   
(00:34:40.570) ~~sort~~ (00:34:40.710)  
  
(00:34:40.731) ~~of~~ (00:34:40.791)  
low level solution data and the protein, you have the DNA data and the protein and the gene expression. And we work our way up all these levels of~~,~~(00:34:48.012)  
  
(00:34:48.771) ~~of~~ (00:34:48.851)  
orders of magnitude. And then it's like computer systems,   
(00:34:51.853) ~~like~~ (00:34:51.992)  
all the cloud logs from AWS and Google cloud and all this stuff gets in there. And you've got all these different self-similar but overlapping orders of magnitude of ways of understanding the world. 


---


#### 00:35:06.101

I have a hard time imagining how that doesn't, even if it asymptotes or levels off at some point, I have a hard time imagining that doesn't level off at a higher point than human is able to achieve today. But I feel like you probably see that differently still. So a lot of this reduces to how you view the universe. If you don't view the universe as fractal self-similar, and if you don't view it as heavy-tailed, and if you don't view it as non-linear, then you could imagine that. But it is all of those things. We know it is all of those things. And so there's no distribution of data that we know of that's not the universe that will produce something that's predictive of the universe. 


---


#### 00:35:48.413

It's really that simple. That doesn't mean that we can't focus on an area and reproduce that distribution. I could become very good at predicting   
(00:35:59.318) ~~whatever~~, protein folding. I could get really good at playing chess. But it's distribution in, distribution out. If you notice the ones that stuff like recursive self-similarity works, and control loops work, and simulated data works, like synthetic data works, there are these axiomatic areas where the axioms define, like they constrain the search space and you're basically converging on search. And you can get very good at those. I can get much better at unit arithmetic. I can get much better than you at game playing. I can get much better   
(00:36:29.884) ~~at~~ (00:36:29.925)  
  
(00:36:29.945) ~~humans~~ (00:36:30.144)  
  
(00:36:30.184) ~~at~~ (00:36:30.244)  
  
(00:36:30.385) ~~all~~ (00:36:30.545)  
  
(00:36:30.565) ~~of~~ (00:36:30.625)  
  
(00:36:30.664) ~~these~~ (00:36:30.824)  
  
(00:36:30.885) ~~things~~. 


---


#### 00:36:31.226

But none of that   
(00:36:33.157) ~~None~~ (00:36:33.317)  
  
(00:36:33.336) ~~of~~ (00:36:33.376)  
  
(00:36:33.436) ~~it~~ (00:36:33.498)  
talks to the fact that can you find the right level of abstraction in a fractal system and can you tackle a heavy-tailed universe where not by occurrence, but by uniqueness, the complexities in the tail. And there's just zero. Even in this discussion, the use cases that you've used tend to be these kind of axiomatic. Of course we can do full search. Like we thought in the late 90s we could do protein folding by fully searching the search space. It's just not surprising to me that we can learn distributions and spit them out. I feel that's a very different statement than saying, now we have a model that can navigate the universe in a way that is predictive of all   
(00:37:21.585) ~~of~~ (00:37:21.644)  
the complexities of it. 


---


#### 00:37:22.666

I think maybe another way to think of this is we've spent 3000 years doing our best and writing it down. And we can create a model that can learn from all of that and do what the mean human being would have done in the last 3000 years. But the problem is the stuff that we're doing tomorrow by uniqueness, a lot of it's going to be new. And that's just how the universe works. And we're going to have to either build a machine that can do that, which we don't know how to do, or we're gonna have to do it ourselves, and let these machines do the mean task. So Do you have an account   
(00:37:51.630) ~~of~~, or a theory of, what it is that you think humans are doing that the models can't do today? 


---


#### 00:37:59.114

Because,   
(00:37:59.896) ~~yeah~~,   
(00:38:00.036) ~~so~~ (00:38:00.135)  
tell me what it is.   
(00:38:00.695) ~~Yeah~~,   
(00:38:00.996) ~~yeah~~,   
(00:38:01.117) ~~yeah~~. Two things~~,~~(00:38:01.516)  
  
(00:38:01.536) ~~two~~ (00:38:01.717)  
  
(00:38:01.737) ~~things~~. One of them, and the most important one, is we experience the universe and we abstract it into concepts. That's very common. Why do you think all the budget is moving to post-training,   
(00:38:11.782) ~~by~~ (00:38:11.882)  
  
(00:38:11.902) ~~the~~ (00:38:11.961)  
  
(00:38:11.981) ~~way~~? What's your thesis of why so much budget right now is moving from compute to post-training? Ease of use is a huge one.   
(00:38:19.030) ~~I~~ (00:38:19.050)  
  
(00:38:20.132) ~~guess~~,   
(00:38:20.411) ~~I~~ (00:38:20.452)  
  
(00:38:20.492) ~~don't~~ (00:38:20.612)  
  
(00:38:20.652) ~~know~~ (00:38:20.771)  
  
(00:38:20.811) ~~that~~ (00:38:20.911)  
  
(00:38:21.052) ~~I~~ (00:38:21.431)  
  
(00:38:21.592) ~~necessarily~~ (00:38:22.072)  
  
(00:38:22.092) ~~think~~, I would still guess it's probably a minority of budget. If you're talking like, 400.   
(00:38:27.914) ~~I~~ (00:38:27.994)  
  
(00:38:28.514) ~~don't~~ (00:38:29.135)  
  
(00:38:30.775) ~~know~~. Listen, they had~~,~~(00:38:32.556)  
  
(00:38:32.576) ~~they~~ (00:38:32.675)  
  
(00:38:32.715) ~~said~~ (00:38:32.815)  
  
(00:38:32.835) ~~they~~ (00:38:32.936)  
  
(00:38:32.976) ~~had~~ (00:38:33.076)  
10 million messages. I know how much multi-pass costs. Let's say it costs $15. 


---


#### 00:38:37.777

Like we're talking a budget between a hundred and 200 million, which anecdotally it seems about right based on what I know of the industry. But yeah, that's about half. And so what I would argue is we're very good~~.~~(00:38:49.503)  
Models are very good at taking the output that humans create and being able to reproduce that distribution. They're very good at that. This is why Scale.ai is Scale.ai, and all these companies are so successful. That's not the game. The game is looking at the universe and creating the supervised data. That's the game. And so   
(00:39:11.188) ~~I~~ (00:39:11.228)  
  
(00:39:11.268) ~~think~~ (00:39:11.427)  
one model to look at this is, again, human beings have been around, let's say, in a capacity for writing things down for 5,000 years. 


---


#### 00:39:18.971

So you've got humans for 5,000 years that have been looking at the universe and doing this thing that models cannot do, which is making a decision like, that is a rock, and that is   
(00:39:28.297) ~~a~~ (00:39:28.318)  
dust, and this is a concept, and this is a relationship, and I'm going to write it down. And then as a group, we're going to synthesize these ideas and work at these. And so we've got this almost platonic representation in our heads. of the universe, and that is very structured. So we did all the hard work. That is hard work to take this untamed universe and reduce it into words and concepts. That's hard. No LLM that I know can do that. 


---


#### 00:39:55.251

Not even close. But then once we've done all   
(00:39:57.733) ~~of~~ (00:39:57.773)  
that hard work, is it surprising to you that there's structure? We did all this work. Of course there's structure. So you take that structure, you put it into an LLM, it learns the structure, and it can spit it back out. So the very specific thing that these LLMs can't do is look at the universe and recreate this kind of structure. And to be super clear, that structure is not the universe. That's very different. Like a rock is a rock. It's an idea in our head. It's not representative of any single thing. Is a grain of sand a rock?   
(00:40:26.518) ~~I~~ (00:40:26.559)  
  
(00:40:26.579) ~~don't~~ (00:40:26.759)  
  
(00:40:26.798) ~~know~~. Is a boulder a rock? 


---


#### 00:40:28.099

I don't know. These are concepts we've created in order to navigate the universe. They're not the universe. So human beings take the universe and create the concepts. And that's structured because we need structure in order to do anything. And the LLMs learn that structure. And I haven't seen a shred of it. This is why all of this feels like the only thing that works is basically you can do exhaustive~~,~~(00:40:48.355)  
  
(00:40:48.574) ~~quote~~ (00:40:48.775)  
  
(00:40:48.795) ~~unquote~~, AI, which converges on search to learn distributions. Or you do supervised learning, which human beings are doing the hard work~~,~~(00:40:55.458)  
  
(00:40:55.559) ~~in~~ (00:40:55.639)  
  
(00:40:55.699) ~~my~~ (00:40:55.878)  
  
(00:40:55.958) ~~opinion~~, by labeling things and everything else. And then you just learn what the human beings have done. But to take the universe and actually to   
(00:41:01.722) ~~reign~~ (00:41:02.083)  
structure   
(00:41:02.882) ~~and~~ (00:41:02.943)  
all that complexity, that's what we do. 


---


#### 00:41:05.664

And it would be great if machines can do it. I've never seen any evidence that they can.   
(00:41:10.407) ~~Yeah~~, maybe some glimmers of it, but that's just not where we are.   
(00:41:14.927) ~~I~~ (00:41:14.987)  
  
(00:41:15.007) ~~just~~ (00:41:15.387)  
  
(00:41:15.447) ~~want~~ (00:41:15.668)  
  
(00:41:15.768) ~~to~~ (00:41:15.887)  
  
(00:41:16.027) ~~be~~ (00:41:16.148)  
  
(00:41:16.407) ~~super~~ (00:41:16.608)  
  
(00:41:16.628) ~~quick~~,   
(00:41:16.987) ~~because~~ (00:41:17.268)  
  
(00:41:17.288) ~~you~~ (00:41:17.349)  
  
(00:41:17.369) ~~asked~~ (00:41:17.528)  
  
(00:41:17.548) ~~me~~ (00:41:17.608)  
  
(00:41:17.628) ~~a~~ (00:41:17.648)  
  
(00:41:17.688) ~~very~~ (00:41:17.809)  
  
(00:41:17.849) ~~specific~~ (00:41:18.168)  
  
(00:41:18.188) ~~question~~,   
(00:41:18.989) ~~a~~ (00:41:19.009)  
  
(00:41:19.048) ~~very~~ (00:41:19.208)  
  
(00:41:19.228) ~~specific~~ (00:41:19.628)  
  
(00:41:19.708) ~~answer~~. Look at the universe, and then come up with these concepts that are useful, that are not the universe. They're concepts. They're totally separate. Like a rock is not a thing. It's a human concept. But to take the universe and decide something is a rock, that's actually all of the complexity and all of the energy   
(00:41:34.510) ~~is~~ (00:41:34.590)  
  
(00:41:34.710) ~~that~~ (00:41:34.911)  
  
(00:41:34.972) ~~step~~, which LLMs just don't do.   
(00:41:38.092) ~~Yes~~, this is why   
(00:41:38.713) ~~I~~ (00:41:38.733)  
  
(00:41:38.773) ~~think~~ (00:41:38.932)  
language is such a tricky domain. 


---


#### 00:41:42.094

By the way, the platonic representation, you probably saw there's a paper titled exactly that recently. But it doesn't tell us much about this particular question because it is still operating in the language domain. And it is in that way able to piggyback on our prior art in terms of useful abstraction of the universe.   
(00:42:01.606) ~~I~~ (00:42:01.666)  
  
(00:42:01.726) ~~do~~ (00:42:01.847)  
  
(00:42:01.887) ~~think~~ (00:42:02.068)  
if there was a line of research that might, where I would venture a guess, that I think you maybe wouldn't agree with, that maybe we could come back in a certain finite amount of time and say, has this happened or not? And what would we infer from it? I probably would look at this biology foundation model space and the sequence modeling. 


---


#### 00:42:27.190

And you can imagine that's going to be elaborated a lot. They didn't even use eukaryotic sequences in that initial Evo paper, let alone   
(00:42:35.177) ~~like~~ (00:42:35.378)  
expression data, transcriptome, proteome, whatever. But   
(00:42:39.262) ~~I~~ (00:42:39.302)  
  
(00:42:39.322) ~~guess~~ (00:42:39.501)  
my expectation would be...   
(00:42:40.822) ~~I~~ (00:42:40.842)  
  
(00:42:41.403) ~~just,~~ (00:42:41.563)  
  
(00:42:41.603) ~~I~~ (00:42:41.623)  
  
(00:42:41.643) ~~would~~ (00:42:41.804)  
  
(00:42:41.824) ~~be~~ (00:42:41.884)  
  
(00:42:41.923) ~~very,~~ (00:42:42.083)  
  
(00:42:42.164) ~~I~~ (00:42:42.284)  
very much agree. We know enough about the natural sciences to build predictive models and we have for a very long time,   
(00:42:49.228) ~~right~~? Like I can simulate a supernova on a computer pretty accurately.   
(00:42:57.289) ~~I~~ (00:42:57.349)  
  
(00:42:57.389) ~~think~~ (00:42:57.528)  
it's phenomenal. It's phenomenal that we have an approach to throw a computer at a problem that learning a fundamental law of physics. But again, how is that any different than the fact that we've been modeling physical systems for a very long time other than the fact that In these cases, it allows us to apply more compute at the problem because we don't have to, and we can solve problems that we don't have close form analytics solutions to. 


---


#### 00:44:05.722

Why is this not just a straightforward extension of exactly that, like we've been doing for the last 80 years?   
(00:44:12.467) ~~I~~ (00:44:12.547)  
  
(00:44:12.588) ~~think~~ (00:44:12.728)  
we're, here's my expectation. Tell me if you~~,~~(00:44:14.869)  
maybe we can make a little friendly wager on this.   
(00:44:18.572) ~~I~~ (00:44:18.652)  
  
(00:44:18.693) ~~think~~ (00:44:18.853)  
  
(00:44:18.873) ~~that~~ (00:44:19.032)  
over the next year, we are going to start to see scaled up foundation models for biology that are going to start to understand the super complicated interactions between genes, between proteins in cells, in ways that are inferred from inputs and outputs, learning these higher order concepts in the middle, which we could not simulate because it's computationally just intractable, and which we don't have any closed form, certainly don't have a closed form solution for either. 


---


#### 00:44:58.438

If that does happen, that would seem to constitute to me an instance of looking at the world, looking at basically raw data of sequences and just lysed cells and what proteins were found in them and whatever, and learning meaningful abstractions. And   
(00:45:15.949) ~~I~~ (00:45:15.989)  
  
(00:45:16.030) ~~think~~ (00:45:16.170)  
we~~,~~(00:45:16.331)  
  
(00:45:16.911) ~~I~~ (00:45:16.971)  
would expect that we'll start to discover stuff by doing counterfactual experiments on those models. In other words, tweak a thing, see what happens. Find medicines, find disease patterns by make a tweak, see what happens. What if we change this counterfactually and then go validate those things in the wet lab? If we start to see that happening, would that, to you, represent the phenomenon? I fully expect that. But how does that not constitute looking at the universe and figuring out what's what? 


---


#### 00:45:47.891

  
(00:45:47.891) ~~Oh,~~ (00:45:48.211)  
  
(00:45:48.251) ~~for~~ (00:45:48.652)  
  
(00:45:48.692) ~~sure.~~ (00:45:49.012)  
For specialized subdomains, you absolutely can learn distributions.   
(00:45:52.675) ~~A~~ (00:45:52.735)  
  
(00:45:52.815) ~~hundred~~ (00:45:53.036)  
  
(00:45:53.076) ~~percent,~~ (00:45:53.396)  
  
(00:45:53.456) ~~right?~~ (00:45:53.596)  
  
(00:45:53.635) ~~Listen,~~ (00:45:53.856)  
I've implemented Navier-Stokes, like fluid dynamics.   
(00:45:58.199) ~~where,~~ (00:45:58.579)  
  
(00:45:59.101) ~~and~~ (00:45:59.181)  
this is a turbulent chaotic system. So we've known how to   
(00:46:02.083) ~~like~~ (00:46:02.304)  
implement very complex systems with computers~~,~~(00:46:06.907)  
  
(00:46:07.708) ~~for~~ (00:46:07.867)  
  
(00:46:07.887) ~~sure~~. And especially in very specific domains where we can reduce these things to a few fundamental forces or we can reduce these things to mostly linear systems. Like finite element is~~,~~(00:46:17.956)  
  
(00:46:17.996) ~~like~~ (00:46:18.217)  
the previous version of this is just all the computational methods where we would take a problem that we know and we'd actually experimentally determine It   
(00:46:25.762) ~~says~~ (00:46:25.943)  
  
(00:46:26.023) ~~experimentally,~~ (00:46:26.583)  
  
(00:46:26.603) ~~we'd~~ (00:46:26.744)  
  
(00:46:26.764) ~~say,~~ (00:46:26.925)  
  
(00:46:27.065) ~~okay,~~ (00:46:27.344)  
  
(00:46:27.385) ~~like~~ (00:46:27.545)  
this~~,~~(00:46:27.786)  
  
(00:46:27.905) ~~this~~ (00:46:28.085)  
material behaves this way under this pressure and this heat, and it has these properties. 


---


#### 00:46:31.730

And we take what we learned experimentally and we'd create these models and they would do pretty good at simulation. And   
(00:46:36.155) ~~I~~ (00:46:36.195)  
  
(00:46:36.215) ~~would~~ (00:46:36.356)  
  
(00:46:36.396) ~~say~~ (00:46:36.556)  
this is a very straightforward extension of that, which is you look at how the world works in a constrained situation. And then you can predict what would happen in the constrained situation. But just like simulations, remember, we could do this with simulation. We've been able to do this for a very long time~~,~~(00:46:53.472)  
  
(00:46:53.572) ~~right~~? So you could experimentally determine how different materials work, and then you can actually simulate a new system based on those. This is how we do most industrial design today anyways. So my question to you is, how is this fundamentally different than that?   
(00:47:09.476) ~~or~~ (00:47:09.577)  
  
(00:47:09.677) ~~not~~ (00:47:09.896)  
  
(00:47:10.416) ~~a~~ (00:47:10.456)  
  
(00:47:10.637) ~~natural~~ (00:47:11.018)  
  
(00:47:11.057) ~~extension~~ (00:47:11.637)  
  
(00:47:11.657) ~~than~~ (00:47:11.797)  
  
(00:47:11.838) ~~that,~~ (00:47:12.097)  
where you're giving it a system, you're learning some fundamental properties, you can do something new. 


---


#### 00:47:16.719

But that doesn't mean that you can disobey the laws of physics in predicting stuff that computers can't predict. It doesn't mean that all of a sudden they can   
(00:47:25.103) ~~like~~, it's not obvious to me that it can simulate complex nonlinear systems that are chaotic for long periods of time.   
(00:47:34.494) ~~I~~ (00:47:34.554)  
  
(00:47:34.594) ~~think~~ (00:47:34.733)  
this is just yet another step on this   
(00:47:37.135) ~~kind~~ (00:47:37.315)  
  
(00:47:37.335) ~~of~~ (00:47:37.396)  
  
(00:47:37.436) ~~like~~ (00:47:37.576)  
we have computers simulate physical stuff and we're simulating the next thing with the next tool. Does the parallel making   
(00:47:45.481) ~~make~~ (00:47:45.621)  
sense? If you go to   
(00:47:47.664) ~~like~~ (00:47:47.824)  
in the 80s and 90s, we would literally empirically test physical matter. We didn't know how the physical matter worked. We   
(00:47:55.067) ~~didn't~~ (00:47:55.186)  
empirically test~~ed~~(00:47:55.847)  
it. It has this opacity under this heat. 


---


#### 00:47:58.106

It has this tensile strength. We'd use that to build databases of materials, equations of state, we call them. This is how they interoperate. We'd use those when we're doing simulations. And we'd simulate what happens when a car explodes, what happens if an airplane runs into a building, all of these things. None of those instances worked. They were simulations. And they were very accurate. And none of them came from first principles. They were all empirically~~.~~(00:48:22.072)  
But that has its limits because it didn't solve climate prediction past 15 days. It didn't allow us to simulate life. And so to me, this is just computers being attached to another domain, which thank goodness we've come up with a great tool that's going to give us a little bit more insight. 


---


#### 00:48:40.630

But it's a little bit more insight   
(00:48:41.911) ~~is~~ (00:48:41.972)  
  
(00:48:42.012) ~~what~~ (00:48:42.132)  
  
(00:48:42.172) ~~it~~ (00:48:42.231)  
  
(00:48:42.331) ~~is~~. By the way, we had the same discussions in the simulation. This is why, by the way, we stopped Playstations from going to the Middle East. I actually worked in the nuclear weapons program at Livermore, so I was very close to the previous version of these discussions, like, oh my goodness, if   
(00:49:00.800) ~~like~~ (00:49:00.940)  
Saddam Hussein gets PlayStations, he's going to be able to simulate nuclear weapons,   
(00:49:04.764) ~~right~~? Like just totally misunderstanding that the ability to simulate something is not some runaway process that's going to allow you to recreate a world or anything like that. It's a very specific tool for a very specific situation. But we were here before   
(00:49:20.894) ~~when~~ (00:49:21.335)  
  
(00:49:21.434) ~~it~~ (00:49:21.534)  
  
(00:49:21.574) ~~was~~ (00:49:21.695)  
25 years ago. 


---


#### 00:49:23.056

Yeah, there's certainly no arguing that past predictions of AI progress often didn't pan out and that does seem like it's not a near-term future that we can dismiss either. The other argument is that they have all panned out exactly as you'd expect, and they're going to continue to. And this has been steady progress for 80 years, and it's been a super useful tool. And every time we unlock something, people are like, oh~~,~~(00:49:47.454)  
  
(00:49:47.514) ~~poo~~, that wasn't AGI, so that must not have been AI. And they're focused on the wrong goal, and it's a very unspecified goal. And instead, it's like any other computer science primitive~~.~~(00:49:55.898)  
where it keeps solving problems and we should continue to solve problems with it, which is the way that I would view it. 


---


#### 00:50:02.342

It's interesting because people talk about summers and winters and not panning out   
(00:50:04.884) ~~like~~ (00:50:05.003)  
  
(00:50:05.023) ~~you~~ (00:50:05.123)  
  
(00:50:05.164) ~~just~~ (00:50:05.324)  
  
(00:50:05.344) ~~did~~, but it couldn't be further from the truth. It's been a foundational primitive of computer science that we've been using very effectively and we continue to use more effectively and I don't see why that won't just continue to be the case.   
(00:50:16.826) ~~Yeah~~, I like your vision. I think what you're describing is almost an ideal scenario. And I think we've~~,~~(00:50:25.527)  
I spent more of the time that we have today here than I had anticipated, because I wanted to get into all these debates around SB1047 and compute thresholds. We've got half an hour. We have half an hour now. So we~~,~~(00:50:36.389)  
  
(00:50:36.728) ~~yeah~~, we have half an hour. 


---


#### 00:50:37.530

We can dig into it. But   
(00:50:38.429) ~~I~~ (00:50:38.449)  
  
(00:50:38.469) ~~do~~ (00:50:39.309)  
  
(00:50:39.369) ~~think~~ (00:50:39.690)  
this is an important foundation. Let me try to bottom line your perspective. It seems like you~~,~~(00:50:47.552)  
  
(00:50:47.572) ~~I'm~~ (00:50:47.632)  
  
(00:50:48.494) ~~still~~ (00:50:48.653)  
  
(00:50:48.673) ~~a~~ (00:50:48.693)  
  
(00:50:48.713) ~~little~~ (00:50:48.893)  
  
(00:50:48.934) ~~confused~~ (00:50:49.353)  
  
(00:50:49.414) ~~around~~ (00:50:49.894)  
what would count. Is there, what would be the evidence of the fundamental thing that humans can do and have done through our history that the AIs can't do is look at the universe and figure out the right abstractions and come up with   
(00:51:03.884) ~~the~~ (00:51:03.963)  
the right concepts that compress it in order to make sense of it. And I agree it's very hard to say what they're doing in the language domain because we already did that work and they're learning it from us. I try to describe something in the biology domain where it's like it seems like they're starting to show signs of doing that and I could believe that they would and then you agreed, but then now I'm confused as to wouldn't that count as doing that? 


---


#### 00:51:22.532

And then it seemed like the response was, well, that's just in one domain, but it doesn't seem like there's anything that would prevent it. Certainly there's a huge leap in generality with this latest generation of systems. So   
(00:51:31.976) ~~I~~ (00:51:31.996)  
  
(00:51:32.255) ~~do~~ (00:51:32.356)  
imagine just shoveling all the modalities into one model. We've already got text, vision, and audio in GPT-4.0. Why not the true GPT-5.0 would be like biology data and weather data and   
(00:51:44.360) ~~like~~ (00:51:44.501)  
pictures of deep space and solution simulations and   
(00:51:48.806) ~~just~~ (00:51:48.925)  
battery simulations, material science, whatever. Throw that all into one thing. And if it can do that, then it's definitely not going to be constrained by one domain anymore. So   
(00:51:58.496) ~~I'm~~ (00:51:58.556)  
  
(00:51:58.577) ~~still~~ (00:51:58.717)  
  
(00:51:58.737) ~~a~~ (00:51:58.757)  
  
(00:51:58.777) ~~little~~ (00:51:59.016)  
  
(00:51:59.117) ~~lost~~ (00:51:59.376)  
  
(00:51:59.436) ~~as~~ (00:51:59.496)  
  
(00:51:59.516) ~~to~~ (00:51:59.556)  
  
(00:51:59.577) ~~like~~ (00:51:59.737)  
exactly what the limit that you see is in terms of why it doesn't become a system that's more powerful than people. 


---


#### 00:52:07.920

I'm so glad you reduced it to this.   
(00:52:09.239) ~~I~~ (00:52:09.280)  
  
(00:52:09.320) ~~think~~ (00:52:09.440)  
This is great.   
(00:52:10.039) ~~Right.~~ (00:52:10.239)  
  
(00:52:10.320) ~~And~~ (00:52:10.400)  
  
(00:52:10.539) ~~there~~ (00:52:10.699)  
There's two things. There's this notion that language reasoning is general, which   
(00:52:13.820) ~~you,~~ (00:52:13.900)  
which a lot of people believe, but you don't seem to be on that   
(00:52:16.641) ~~kind~~ (00:52:16.842)  
  
(00:52:16.882) ~~of~~. So let's put that one aside. And you're more on the learning properties and simulating properties of the universe side, which is totally fine. So what I would do is I would just bring you back to everything we've learned about simulation, which is even when you know all of the properties of the system and you can simulate~~,~~(00:52:34.820)  
You just don't have the computational capacity to simulate nonlinear systems. We don't have the materials or the energy or anything. 


---


#### 00:52:44.617

It's literally like a compute problem. We have code bases that have been around for 20, 30 years that simulate all sorts of crazy stuff. And yet, they've got limited utility for exactly this reason. The universe is just so complex that they're useful for a little bit. And so if it turns out, and I'll give you a very specific one that we can bet. If it turns out that these models somehow change that compute trade-off where it can simulate nonlinear systems in ways that traditional stuff can't, I'm 100% with you. But that's not what they're doing. What they're doing is they're inferring stuff that we haven't been able to infer by looking at data. That doesn't mean that they can be predictive in a way that   
(00:53:30.838) ~~kind~~ (00:53:30.998)  
  
(00:53:31.018) ~~of~~ (00:53:31.079)  
disobeys our understanding of compute requirements. 


---


#### 00:53:34.980

And   
(00:53:35.119) ~~like~~ (00:53:35.300)  
  
(00:53:36.240) ~~a~~ (00:53:36.280)  
Raleigh-Taylor, let me just give you a specific, so Raleigh-Taylor instability is basically if you have two liquids that are on top of each other with different densities. And that is   
(00:53:47.512) ~~one~~ (00:53:47.592)  
Raleigh-Taylor unstable system. And if you perturb it,   
(00:53:49.855) ~~like~~ (00:53:49.974)  
you get these   
(00:53:50.394) ~~kind~~ (00:53:50.514)  
  
(00:53:50.534) ~~of~~ (00:53:50.594)  
  
(00:53:50.614) ~~like~~ (00:53:50.755)  
just amazing   
(00:53:52.097) ~~kind~~ (00:53:52.237)  
  
(00:53:52.277) ~~of~~ (00:53:52.317)  
chaotic turbulent things that happen. Like we have been looking at this problem for 30 years and we have no idea how to actually predict what will happen. We just know roughly what they will look like, but we don't know   
(00:54:04.465) ~~like~~ (00:54:04.606)  
the specifics. And AI systems are way less efficient than an actual code written for simulation. So to think that it can tackle those types of problems, 


---


#### 00:54:14.942

I just don't see any indication. So maybe I'll just say one last thing, and then we can move on, which is I view that we have these   
(00:54:21.146) ~~kind~~ (00:54:21.266)  
  
(00:54:21.286) ~~of~~ (00:54:21.347)  
modalities of compute that we can throw at problems, right? So the most efficient is basically imperative programming, where you give   
(00:54:28.351) ~~like~~ (00:54:28.530)  
the computer~~,~~(00:54:29.992)  
  
(00:54:30.032) ~~like~~ (00:54:30.152)  
a set of instructions and exactly the rules, right? So that's just normal programming. And then we have another modality, which is declarative programming. And by the way, the first one's the most efficient, right? It just does the rules. The second one is the end state that you want. I know the end state, but you know it very specifically. And then the computer figures out what to do. 


---


#### 00:54:48.126

This is declarative programming. To come to the solution, it has to do all of the logical closure stuff. So that requires more compute. And we don't even know how to bound that compute. That's why databases, when you run queries, are not bounded. So that's declarative programming. Now we're in AI, where you don't even know what the end state is. You're just   
(00:55:06.396) ~~like~~, I have a whole bunch of data, and I want you to find patterns in that data. That requires even more compute, so it's even less efficient. So it's another modality of compute. It's one we've been fighting for a long time. But it doesn't change the nature of computers. They're still systems, and they're still computers. 


---


#### 00:55:21.702

And they still have the same limitations, independent of what distribution that they learn. And listen, if it turns out that these things can simulate systems for a period of time, longer than   
(00:55:34.789) ~~like~~ (00:55:34.929)  
a normal simulation   
(00:55:35.750) ~~that~~ (00:55:35.849)  
I'm with you, I'm like, this is breaking the laws of physics. But until then, it feels to me like simulation where you just don't know all of the rules, but it's learning some of the rules. Yeah,   
(00:55:45.193) ~~I~~ (00:55:45.253)  
  
(00:55:45.293) ~~guess~~ (00:55:45.434)  
  
(00:55:45.894) ~~I~~ (00:55:45.954)  
think about it less in terms of simulation and more in terms of how effective the choice of actions can be at any given time step, right?   
(00:55:55.981) ~~Like~~ (00:55:56.141)  
I'm not simulating the universe. Humanity as a whole is not simulating the universe, but we're all just taking our local conditions and our   
(00:56:05.025) ~~sort~~ (00:56:05.206)  
  
(00:56:05.266) ~~of~~ (00:56:05.326)  
general sense of our own selves and goals and   
(00:56:09.208) ~~like~~ (00:56:09.429)  
trying to do the next step at any given time. 


---


#### 00:56:12.891

And it seems like our overall efficacy through our lives is the integral, if you will, over how good our choices are at each given time step. And that doesn't depend on any huge simulation of anything irreducibly complex. So then if I imagine an AI, it doesn't seem to me like we're too far from...   
(00:56:34.675) ~~I~~ (00:56:34.695)  
  
(00:56:34.715) ~~don't~~ (00:56:36.315)  
  
(00:56:36.335) ~~think~~ (00:56:36.456)  
we're quite there. It seems within reach to imagine an AI that can do something very similar to what I'm doing, which is have a goal, look at its immediate surroundings, look at what it just did, look at whatever other context it may be given and pick a next action and potentially be better than me at it and potentially quite a bit better. 


---


#### 00:56:56.490

And then that to me seems like enough. If it can do that, then I feel like we're in an unprecedented environment where we now have fundamentally pretty alien and not   
(00:57:06.661) ~~super~~ (00:57:06.882)  
well understood things that can take   
(00:57:09.204) ~~like~~ (00:57:09.364)  
more effective actions in many, not necessarily all, but   
(00:57:12.224) ~~like~~ (00:57:12.364)  
many given contexts that I could. And then that to me is   
(00:57:16.648) ~~like~~ (00:57:16.788)  
where I start to turn the conversation toward what sort of safeguards should we have in place? Just again, because this is, these conversations tend to be so mode. If that action requires interacting with the physical world, it has to simulate the physical world. It just does~~,~~(00:57:31.847)  
  
(00:57:32.148) ~~right~~?   
(00:57:32.347) ~~Like~~ (00:57:32.548)  
it has to understand   
(00:57:34.369) ~~like~~ (00:57:34.570)  
dynamics and ballistics. 


---


#### 00:57:36.130

It has to understand what happens if someone throws a rock at it, or if it's   
(00:57:40.391) ~~like~~ (00:57:40.492)  
in water, or if the weather's~~,~~(00:57:42.672)  
  
(00:57:42.693) ~~it~~ (00:57:42.733)  
  
(00:57:42.773) ~~just~~, that's how you navigate the physical. That's really why we created computers, right? It was because   
(00:57:47.215) ~~like~~, these are very hard things to do. And then if it's not that, if it's not interacting with the physical world, then it is interacting in this   
(00:57:55.579) ~~kind~~ (00:57:55.739)  
  
(00:57:55.759) ~~of~~ (00:57:55.838)  
language domain that we've created. And I agree, it'd be very good at some subset of those things. There's zero indication it'd be good at new things. And that's what we're actually very good at. And again, without actually having a model for all of these things that we understand,   
(00:58:10.659) ~~like~~ (00:58:10.800)  
the distributions, we understand the mechanisms, 


---


#### 00:58:12.822

I feel like we just use words and the words all make sense. But   
(00:58:16.585) ~~like~~ (00:58:16.704)  
complex systems, we never know convergence and divergence without actually specifying what~~.~~(00:58:21.369)  
without actually specifying the system. And I feel like for these conversations, we just don't have a system we talk about. And so it's always, we live in the world of   
(00:58:29.132) ~~like~~ (00:58:29.333)  
rectangles and arrows and somebody takes a rectangle and they have an arrow that goes back to the rectangle. They're like, ah, we've got a virtuous cycle without actually specifying that if you have diminishing marginal returns, you don't go anywhere or you're doing the same stuff or whatever. And so I think this is incumbent on all of us to actually understand the systems we're working with and then come up with these basic views and properties to make sure they end, or   
(00:58:57.811) ~~is~~ (00:58:57.891)  
to make sure at least we understand what the convergence properties are. 


---


#### 00:59:00.375

I'm sorry, that was a very muddled thing to say, but I feel that until we talk about specifics, it's very hard to make concrete statements in this.   
(00:59:06.724) ~~Yeah~~, it's tough. We're in a tough environment because things are moving really quick and we are pre-paradigmatic on understanding a lot of these things. Again,   
(00:59:17.873) ~~I~~ (00:59:17.913)  
  
(00:59:17.952) ~~think~~ (00:59:18.072)  
the progress has been incredible, but we're just starting to crack the black box. Okay, so let's change gears because   
(00:59:22.976) ~~I~~ (00:59:23.016)  
  
(00:59:23.056) ~~think~~ (00:59:23.157)  
this probably certainly gives everybody enough to get at least a good intuition for our relative philosophies on this. What do you think we should do right now in practical terms to regulate AI, if anything? The regulation one is sticky   
(00:59:42.447) ~~for~~, to me, for two reasons. 


---


#### 00:59:44.409

The first one is we don't even have a definition of AI. And so   
(00:59:48.132) ~~I~~ (00:59:48.211)  
  
(00:59:48.431) ~~think~~ (00:59:48.632)  
it reduces to regulating software. And then for that,   
(00:59:51.074) ~~I~~ (00:59:51.094)  
  
(00:59:51.134) ~~would~~ (00:59:51.253)  
  
(00:59:51.293) ~~say~~ (00:59:51.393)  
we've been regulating software for a very long time, and there's a broad, robust discourse around that. And   
(00:59:57.798) ~~I~~ (00:59:57.838)  
  
(00:59:57.898) ~~think~~ (00:59:58.059)  
we should make whatever conversations we have part of that broader discussion. So how about   
(01:00:06.005) ~~like~~.   
(01:00:07.275) ~~I~~ (01:00:07.396)  
  
(01:00:07.416) ~~mean~~, liability is a general thing~~,~~(01:00:10.257)  
  
(01:00:10.297) ~~right~~? It's not even software specific. Like if you are a business, you make a product, if it's- Just one second, just one~~,~~(01:00:15.561)  
sorry, one second. Yeah, no worries. Yeah, so   
(01:00:18.222) ~~I~~ (01:00:18.282)  
  
(01:00:18.302) ~~would~~ (01:00:18.442)  
  
(01:00:18.481) ~~love~~ (01:00:18.601)  
  
(01:00:18.641) ~~to~~ (01:00:18.722)  
  
(01:00:18.762) ~~do~~, just do the half hour mark~~,~~(01:00:20.583)  
  
(01:00:20.603) ~~I~~ (01:00:20.663)  
  
(01:00:20.702) ~~told~~ (01:00:20.844)  
  
(01:00:20.923) ~~Michelle~~. Okay, go ahead, sorry. 


---


#### 01:00:24.224

So yeah, just trying to bring a couple different lenses to this from existing rules. We obviously have liability rules for all kinds of products, not just software. It seems   
(01:00:33.811) ~~like~~, There is a general   
(01:00:37.811) ~~kind~~ (01:00:37.971)  
  
(01:00:38.010) ~~of~~ (01:00:38.070)  
trade-off that companies make with the government, where   
(01:00:42.155) ~~it's~~ (01:00:42.275)  
we want to have safe products that we can generally count on as being safe. Companies don't want to be sued every two seconds. And the general trade is you agree to implement some standards. If you implement those standards, you'll be shielded from Regulation and that does vary   
(01:00:57.657) ~~right~~ (01:00:57.818)  
across   
(01:00:58.177) ~~like~~ (01:00:58.318)  
lots of different product types   
(01:01:00.398) ~~cars~~ (01:01:00.739)  
  
(01:01:00.798) ~~are~~ (01:01:00.878)  
  
(01:01:00.918) ~~very~~ (01:01:01.079)  
  
(01:01:01.099) ~~different~~ (01:01:01.340)  
  
(01:01:01.380) ~~from~~ (01:01:01.519)  
  
(01:01:01.659) ~~milk~~ (01:01:01.940)  
  
(01:01:02.019) ~~and~~ (01:01:02.099)  
  
(01:01:02.119) ~~so~~ (01:01:02.159)  
  
(01:01:02.179) ~~on~~ (01:01:02.219)  
  
(01:01:02.380) ~~and~~ (01:01:02.599)  
  
(01:01:02.619) ~~so~~ (01:01:02.719)  
  
(01:01:02.760) ~~forth~~ (01:01:02.940)  
But we don't have anything really like that for AI and not a ton really for software either Software probably is   
(01:01:11.105) ~~you~~ (01:01:11.204)  
  
(01:01:11.224) ~~probably~~ (01:01:11.405)  
  
(01:01:11.425) ~~know~~ (01:01:11.505)  
  
(01:01:11.525) ~~better~~ (01:01:11.684)  
  
(01:01:11.704) ~~than~~ (01:01:11.804)  
  
(01:01:11.844) ~~I~~ (01:01:11.885)  
  
(01:01:11.905) ~~do~~ (01:01:12.005)  
  
(01:01:12.025) ~~in~~ (01:01:12.065)  
  
(01:01:12.085) ~~terms~~ (01:01:12.244)  
  
(01:01:12.304) ~~of~~ (01:01:12.344)  
  
(01:01:12.364) ~~some~~ (01:01:12.505)  
  
(01:01:12.525) ~~like~~ (01:01:12.684)  
niche areas of   
(01:01:14.106) ~~I~~ (01:01:14.146)  
  
(01:01:14.166) ~~mean~~ (01:01:14.326)  
cars have a lot of certain cars are increasing software products and airplanes have a lot of software in them and medical devices have a lot of software in them   
(01:01:21.210) ~~and~~ (01:01:21.269)  
So in all these regimes, there's   
(01:01:23.773) ~~like~~ (01:01:23.893)  
tons of testing, there's tons of verification, there's tons of   
(01:01:26.813) ~~there~~, they are engineered and demonstrated to a standard. 


---


#### 01:01:30.474

We don't have those like standards, really, for these AI systems yet. This is the thing, which is, I don't know what the distinction between AI and software is. I really don't. Like I have seen the definition using these regulations. It's so broad that really could include all non-trivial software. And I don't say this to be   
(01:01:49.063) ~~a~~ (01:01:49.083)  
polemic and I don't say this to be difficult. I'm saying this very clearly. It literally, they literally say a system that can, whatever, navigate and change a virtual or physical system. These are so broad, right? So we're really talking about software. That's what we're really talking about. We have~~.~~(01:02:05.353)  
  
(01:02:05.972) ~~what,~~ (01:02:06.132)  
70 years of history regulating software in many domains. 


---


#### 01:02:12.065

And I think that regulation is very important. I got,   
(01:02:15.905) ~~by~~ (01:02:15.985)  
  
(01:02:16.005) ~~the~~ (01:02:16.065)  
  
(01:02:16.085) ~~way~~,   
(01:02:16.186) ~~I'm~~ (01:02:16.306)  
  
(01:02:16.606) ~~like~~, I'm not a libertarian. I'm a lifelong liberal,   
(01:02:19.807) ~~like~~ (01:02:19.987)  
a very moderate person. I'm just saying we~~,~~(01:02:22.829)  
this discourse has been around for a very long time, and we should continue. And if there's an area that suffers being pushed, an area that~~,~~(01:02:30.132)  
  
(01:02:30.253) ~~you~~ (01:02:30.373)  
  
(01:02:30.413) ~~know~~, we need to have some sort of protections, we should add them to it.   
(01:02:34.195) ~~Right~~. But that's a very different statement than   
(01:02:36.596) ~~a~~ (01:02:36.715)  
saying AI   
(01:02:37.315) ~~somehow~~ (01:02:37.635)  
paradigmatically different. There's just   
(01:02:39.396) ~~like~~ (01:02:39.597)  
literally zero indication that it is. And then try to somehow regulate a computer science premise, like regulating a database. Let me give you   
(01:02:47.543) ~~like~~ (01:02:47.764)  
a specific example. So   
(01:02:49.664) ~~by~~ (01:02:49.764)  
  
(01:02:49.784) ~~the~~ (01:02:49.846)  
  
(01:02:49.865) ~~way~~, we went through this during the rise of the web and the internet. 


---


#### 01:02:52.307

And so a lot of my formal studies were in networking. And   
(01:02:54.509) ~~so~~ (01:02:54.628)  
I was actually pretty close to these discussions.   
(01:02:56.891) ~~And~~ (01:02:56.990)  
At the time, there was this concern that with the rise of the web and the internet, you have actual paradigmatic shift. Like, it's not just software. It's different.   
(01:03:04.657) ~~It's~~ (01:03:04.777)  
  
(01:03:04.817) ~~different~~. And we actually had examples, like the Morris worm had taken out a whole bunch of stuff. We were running critical infrastructure on it. And there was this notion of asymmetry, which actually changed the defense posture of the United States. And the notion of asymmetry is the more that you bought onto the technology, the more vulnerable you were. So as opposed to mutually   
(01:03:21.481) ~~assert~~ (01:03:21.742)  
destruction, which was our doctrine before, we now had this new thing, which is like, oh my goodness, we're the most vulnerable because we've adopted this. 


---


#### 01:03:28.666

So you had two very compelling reasons to want to regulate this stuff and be afraid of it and do closed source and whatever it was. You had like examples of taking out critical infrastructure and you had this change in posture. And yet even that, which we have none of those today with AI. None. We don't have examples of this or like there~~,~~(01:03:46.527)  
like literally it took out like computers and hospitals~~,~~(01:03:48.809)  
  
(01:03:48.889) ~~right~~? We don't have any examples of paradigmatic shift. And even then, the conclusion after very robust conversation for a long time is you regulate the applications, you regulate the industries, you don't regulate the math and the computer science primitives. All that I would ask at this AI discussions is either A, demonstrate it's paradigmatically different, 


---


#### 01:04:10.327

And if you don't have that, then B, let's absolutely regulate and have regulations. Let's do it as part of the broad, robust discourse we've had over the last 20 years. Really smart people in much more dangerous environments. Again,   
(01:04:23.554) ~~I~~ (01:04:23.614)  
  
(01:04:23.655) ~~think~~ (01:04:23.815)  
  
(01:04:23.856) ~~that~~ (01:04:23.976)  
we put all of our fears in AI, but you realize like we're running   
(01:04:27.438) ~~like~~ (01:04:27.577)  
hospitals on the internet for the first time that were being taken out and it was impacting patients. That's what we were dealing with. And now we're dealing with theoretical~~,~~(01:04:36.583)  
threats on bioweapons that don't even exist yet.   
(01:04:39.846) ~~I~~ (01:04:39.885)  
  
(01:04:39.925) ~~just~~ (01:04:40.085)  
  
(01:04:40.126) ~~feel~~ (01:04:40.286)  
  
(01:04:40.306) ~~like~~ (01:04:40.405)  
we've all gotten mad and forgotten where we came from and the discussions that we've had. And all I'm trying to do is saying, we've been here before. 


---


#### 01:04:47.389

It was actually much worse. Here are the conclusions. Let's draft on all of that work and share knowledge.   
(01:04:53.014) ~~I~~ (01:04:53.054)  
  
(01:04:53.074) ~~guess~~ (01:04:53.253)  
to venture a distinction or what makes the technology a paradigm shift, I would probably zero in on the fact that they are trained, not engineered. And   
(01:05:09.969) ~~that~~ (01:05:10.128)  
maybe a better thing even than that would be that the creators of the models generally don't know what they're going to be able to do. And even at deployment time, don't have a very robust account of what the capabilities of the systems are. That does seem~~,~~(01:05:28.521)  
you can point to things in the past and be like,   
(01:05:30.302) ~~Oh~~, you didn't expect this out of whatever, but   
(01:05:32.983) ~~it~~, this does seem to be qualitatively different that they just train~~,~~(01:05:36.923)  
train~~,~~(01:05:37.103)  
train a long time. 


---


#### 01:05:38.224

Especially if you look at base models, right? Base models are totally unpredictable and nobody really knows. Then   
(01:05:44.086) ~~you~~,   
(01:05:44.586) ~~I~~ (01:05:44.626)  
  
(01:05:44.666) ~~think~~ (01:05:44.806)  
one of the reasons that people are putting so much resource into post-training is to try to get control and it's only working, but~~.~~(01:05:51.648)  
  
(01:05:52.677) ~~Yeah~~. So this is the thing is when you're talking to   
(01:05:55.259) ~~a~~ (01:05:55.340)  
internet   
(01:05:55.860) ~~guy~~ (01:05:56.139)  
and   
(01:05:56.300) ~~a~~ (01:05:56.340)  
distributed systems   
(01:05:57.280) ~~guy~~, it's just like none of the systems that we worked on, we understood the implications of. Think about the internet. Like every sociopath becomes your next door neighbor. What does that even mean? What does it mean to put kids on the internet? What does it mean to have your business on the internet? What does it mean to put critical infrastructure in the internet? 


---


#### 01:06:14.318

We had no~~,~~(01:06:14.818)  
there is no~~,~~(01:06:16.219)  
model for how any of this behaves. There is no way to build~~,~~(01:06:21.186)  
make computer systems provably correct.   
(01:06:23.166) ~~Like~~ (01:06:23.266)  
we didn't have any of that. We   
(01:06:24.887) ~~like~~ (01:06:25.027)  
  
(01:06:25.487) ~~a~~ (01:06:25.527)  
lot of the early discussions when we were creating TCP was to prevent congestion collapse.   
(01:06:30.771) ~~Cause~~ (01:06:30.931)  
we thought~~,~~(01:06:31.291)  
  
(01:06:31.451) ~~Oh~~,   
(01:06:31.592) ~~like~~ (01:06:31.711)  
everybody's going to   
(01:06:32.632) ~~like~~ (01:06:32.751)  
the communication protocols. They're going to all decide that there's an issue on the internet and then they're all going to   
(01:06:38.114) ~~like~~ (01:06:38.335)  
back off. And then at the same time, they're going to   
(01:06:40.335) ~~kind~~ (01:06:40.496)  
  
(01:06:40.516) ~~of~~ (01:06:40.556)  
correlate and   
(01:06:41.376) ~~kind~~ (01:06:41.496)  
  
(01:06:41.516) ~~of~~ (01:06:41.556)  
talk again. And who knows if this takes out the entire internet, which run critical infrastructure~~,~~(01:06:45.239)  
  
(01:06:45.278) ~~like~~.   
(01:06:45.958) ~~I~~ (01:06:46.039)  
  
(01:06:46.699) ~~think~~ (01:06:47.000)  
people underestimate how complex and how little we've known for any system that we've built. 


---


#### 01:06:52.061

No idea. And   
(01:06:53.222) ~~I~~ (01:06:53.262)  
  
(01:06:53.282) ~~guess~~ (01:06:53.623)  
having been there during these times, a lot of computer science is building systems, especially distributed systems, that have emergent behaviors that are non-predictable, that are a tool that you use to solve bigger problems. We've got a lot of approaches to doing that we've developed over time. We will develop new ones. And it's just not clear at all to me. And again, the internet is my favorite example to hold up to this. Literally, we changed the social structure of humans.   
(01:07:21.217) ~~They~~ (01:07:21.599)  
  
(01:07:21.639) ~~were~~ (01:07:21.778)  
  
(01:07:21.858) ~~totally~~ (01:07:22.259)  
  
(01:07:22.699) ~~unpredictable~~. We were putting critical infrastructure on it. We had examples of emergent stuff. Worms weren't possible before the internet by definition. So we actually had provable, not demonstrated, new attack vectors. 


---


#### 01:07:36.070

And yet, the conclusion was, listen, we're going to keep this stuff open. It's very important. We're going to regulate the uses of these things. And this was the primary growth driver of the last 20 years. And so either you thought that decision was wrong, but that's a very different discussion. We could have that discussion. We could relitigate those 10 years of discussion. Or you can say~~,~~(01:07:57.847)  
  
(01:07:57.887) ~~you~~ (01:07:57.967)  
  
(01:07:58.007) ~~know~~, this is the same thing, except for a softer form of it, because we have no proof that these things are paradigmatically different, other than these   
(01:08:04.572) ~~kind~~ (01:08:04.713)  
  
(01:08:04.753) ~~of~~,   
(01:08:05.114) ~~I~~ (01:08:05.193)  
  
(01:08:05.233) ~~would~~ (01:08:05.373)  
  
(01:08:05.414) ~~consider~~ (01:08:05.974)  
  
(01:08:05.994) ~~these~~ (01:08:06.193)  
wildly baseless claims. And so why don't we use the same method that we did before? So what the couple of points I wanted to follow up on, but I really want to ask   
(01:08:15.619) ~~your~~ (01:08:15.759)  
  
(01:08:15.798) ~~question~~ (01:08:16.118)  
  
(01:08:16.198) ~~on~~ (01:08:16.498)  
  
(01:08:16.519) ~~or~~ (01:08:16.559)  
really want to ask for your view on what companies should do. 


---


#### 01:08:20.323

Right. There's   
(01:08:21.425) ~~I~~ (01:08:21.505)  
  
(01:08:21.545) ~~think~~ (01:08:21.744)  
  
(01:08:21.904) ~~the~~ (01:08:22.125)  
in terms of the proposed regulations applying to everything, we do have these   
(01:08:26.930) ~~like~~. Compute thresholds, the 10 to the 26, whatever. I don't want to get too bogged down in that   
(01:08:31.493) ~~could~~ (01:08:31.594)  
  
(01:08:31.613) ~~be~~ (01:08:31.673)  
  
(01:08:31.694) ~~a~~ (01:08:31.713)  
slippery slope or they could change that threshold or whatever. I agree. If they all of a sudden drop the threshold dramatically, that would be   
(01:08:37.677) ~~like~~ (01:08:37.837)  
very heavy handed and counterproductive. And the open source of the last couple of years has been,   
(01:08:43.862) ~~I~~ (01:08:43.921)  
  
(01:08:43.962) ~~think~~ (01:08:44.122)  
almost everybody agrees, good for everybody, including even bigger safety hawks that are backing SB 1047,   
(01:08:50.787) ~~I~~ (01:08:50.889)  
  
(01:08:50.929) ~~think~~ (01:08:51.069)  
are quite on record saying the open sourcing of, for example, LLAMA 3 is good even for safety because it gives more people something to study. 


---


#### 01:08:59.537

What do you think   
(01:09:01.018) ~~Forgetting~~ (01:09:01.278)  
  
(01:09:01.318) ~~about~~ (01:09:01.478)  
  
(01:09:01.818) ~~rules~~ (01:09:02.158)  
  
(01:09:02.198) ~~or~~ (01:09:02.278)  
  
(01:09:02.297) ~~people~~ (01:09:02.497)  
  
(01:09:02.538) ~~imposing~~ (01:09:02.958)  
  
(01:09:03.118) ~~on~~ (01:09:03.259)  
  
(01:09:03.399) ~~people,~~ (01:09:03.759)  
  
(01:09:04.679) ~~just~~ (01:09:04.838)  
  
(01:09:04.998) ~~to~~ (01:09:05.099)  
  
(01:09:05.139) ~~be~~ (01:09:05.479)  
  
(01:09:06.260) ~~good~~ (01:09:06.460)  
  
(01:09:06.539) ~~companies,~~ (01:09:06.960)  
  
(01:09:07.060) ~~to~~ (01:09:07.159)  
  
(01:09:07.199) ~~be~~ (01:09:07.319)  
  
(01:09:07.399) ~~good~~ (01:09:07.600)  
  
(01:09:07.659) ~~developers~~ (01:09:08.199)  
  
(01:09:08.279) ~~of~~ (01:09:08.359)  
  
(01:09:08.420) ~~technology,~~ (01:09:09.060)  
what   
(01:09:10.121) ~~do~~ (01:09:10.240)  
  
(01:09:10.301) ~~you~~ (01:09:10.461)  
  
(01:09:10.541) ~~think~~ (01:09:10.740)  
the leading companies should be doing? How much should they invest in? What sort of standards should they have? What sort of testing protocols should they be committing to?   
(01:09:20.484) ~~So~~ (01:09:20.524)  
  
(01:09:21.244) ~~on~~ (01:09:21.324)  
  
(01:09:21.404) ~~and~~ (01:09:21.465)  
  
(01:09:21.484) ~~so~~ (01:09:21.625)  
  
(01:09:21.645) ~~forth.~~ (01:09:21.805)  
Yeah,   
(01:09:23.219) ~~I~~ (01:09:23.279)  
  
(01:09:23.298) ~~think~~ (01:09:23.418)  
this is a great question. So I just have to make the point, like there's no correlation between danger and compute. It's so silly. It's like this made up,   
(01:09:30.921) ~~like~~ (01:09:31.061)  
this made up correlation. And so I don't think   
(01:09:33.402) ~~like~~ (01:09:33.542)  
compute limits make any sense at all.   
(01:09:35.302) ~~Like~~ (01:09:35.462)  
  
(01:09:35.502) ~~I~~ (01:09:35.582)  
  
(01:09:35.662) ~~just,~~ (01:09:35.842)  
  
(01:09:35.922) ~~I~~ (01:09:35.962)  
  
(01:09:35.983) ~~think~~ (01:09:36.203)  
it's in some ways it just sounds good, but there's just, there's zero correlation. 


---


#### 01:09:40.404

We've been building systems for a very long time that have   
(01:09:46.396) ~~like~~ (01:09:46.537)  
social and ethical responsibilities~~,~~(01:09:48.137)  
  
(01:09:48.518) ~~right~~? And these are ones with user-generated content and user interactions. And companies, as a result, either through regulatory action, which I fully support, or through self-policing, have identified behaviors the software should not do. And they've built a lot of systems around those. And I would say that absolutely should not change.   
(01:10:11.604) ~~Like,~~ (01:10:11.864)  
Roblox should protect miners. Social networks should protect people from certain types of content. These are the things that we've evolved as an industry as somewhat of an ethical basis or a principle basis. And some of it is regulatory and some of it's self-enforced. And   
(01:10:31.199) ~~I~~ (01:10:31.260)  
  
(01:10:31.300) ~~think~~ (01:10:31.439)  
that should absolutely continue. 


---


#### 01:10:34.301

But realize that while we were doing that, we weren't putting compute limits on databases. And we weren't regulating computer science primitives. And we weren't inhibiting innovation of startups. And that's what we're doing now. And that is a paradigm shift. And that is a doctrine shift. And it's really scary. And I am not a political person. I'm very happy building systems. The only reason I've been so vocal is because I find this so dangerous. And once this is done, I'll be very happy to   
(01:11:01.875) ~~like~~ (01:11:02.034)  
not say a word about it again. You may have a while to go before that happens. So let's say a couple of just   
(01:11:08.550) ~~kind~~ (01:11:08.670)  
  
(01:11:08.710) ~~of~~ (01:11:08.770)  
concrete things. One of the~~,~~(01:11:11.011)  
I was on the GPT-4 red team back 18 months ago. 


---


#### 01:11:15.614

One of the things that I tested was, could the model be a convincing spear phishing attacker? And surprise to no one, probably at this point, it could. This was the early model that hadn't been through all the RLHF yet.   
(01:11:29.585) ~~If~~ (01:11:29.645)  
Although when they deployed it, it still did that for several iterations. And now finally they've got it to refuse. If for example,   
(01:11:37.761) ~~a~~ (01:11:37.881)  
meta puts out an open source model and they haven't done a thorough job of getting these refusal behaviors in place. And you can just go to it and say, Hey, you are~~,~~(01:11:51.032)  
  
(01:11:51.112) ~~this~~ (01:11:51.231)  
  
(01:11:51.252) ~~was~~ (01:11:51.372)  
  
(01:11:51.391) ~~my~~ (01:11:51.492)  
  
(01:11:51.551) ~~old~~ (01:11:51.652)  
  
(01:11:51.693) ~~prompt~~. You are a spearfisher. Your job is to talk to this person and get their mother's maiden name or whatever. 


---


#### 01:11:57.317

And it'll do it. And then somebody out there does it and people are harmed by that. Do you think Meta should have any~~?~~(01:12:04.921)  
responsibility or anything to answer for in the legal system for their contribution to that problem? Or is that all on the end user? My rule of thumb for general systems is that unless they were purposely trained for malicious behavior,   
(01:12:23.797) ~~their~~ (01:12:23.978)  
  
(01:12:24.037) ~~general~~ (01:12:24.398)  
  
(01:12:24.457) ~~systems,~~ (01:12:24.898)  
would you say the same thing about a database? That's interesting, like a lot of actually computer security started with tools that were like scanners that could be used for malicious purposes. This discussion was enormous back then. So remember back in the early time of the Internet, people would come up with Nmap or any of these scanners that would determine vulnerabilities and some of them would even actually exploit them. 


---


#### 01:12:44.707

Remember Metasploit? And there's all these discussions, all these people are building these things and they're using it for malicious purposes. And   
(01:12:50.012) ~~you~~ (01:12:50.092)  
  
(01:12:50.113) ~~know~~ (01:12:50.233)  
  
(01:12:50.252) ~~what~~? We just decided that it's actually the criminals using them. That's who we want to go after because a lot of good people use them for positive purposes. And   
(01:12:57.238) ~~oh~~,   
(01:12:57.380) ~~by~~ (01:12:57.560)  
  
(01:12:57.600) ~~the~~ (01:12:57.699)  
  
(01:12:57.739) ~~way~~,   
(01:12:57.960) ~~like~~ (01:12:58.100)  
the history of   
(01:12:59.220) ~~like~~ (01:12:59.320)  
computer~~,~~(01:12:59.761)  
  
(01:12:59.801) ~~like~~ (01:12:59.921)  
cybersecurity is built on open source in these tools. Like companies like Tenable came out of these tools. So if history is any indication, You want to regulate or you want to definitely criminalize bad people and bad behavior, but not the tooling because the tooling is very fundamental to defense. So I would say, listen, if, and this is even what I'm saying is even more stringent, it's more regulatory heavy than what we did with the internet. 


---


#### 01:13:26.524

Literally you could build exploit tools 20 years ago and release them open source. And you are not liable if somebody used that. And that was purposely built for exploiting. I'm saying   
(01:13:35.865) ~~like~~, listen, if somebody is perfectly building a model for something illegal. Okay, that's bad, right? So that's even more heavy~~,~~(01:13:42.167)  
  
(01:13:42.188) ~~like~~, I'm more of a regulatory hawk on this case. But I don't think general purpose models that aren't specifically trained for this stuff, the person that created is generally liable,   
(01:13:52.554) ~~like~~ (01:13:52.654)  
that would just basically say software, you're liable for creating software, somebody use it poorly, which I think this would be the biggest kind of inhibitor to innovation and the biggest chill on software we've ever imposed.   
(01:14:05.609) ~~about~~ (01:14:05.909)  
  
(01:14:05.989) ~~a~~ (01:14:06.029)  
  
(01:14:06.090) ~~distinction~~ (01:14:06.890)  
  
(01:14:07.310) ~~or~~ (01:14:07.390)  
  
(01:14:07.490) ~~some~~ (01:14:07.652)  
  
(01:14:07.692) ~~sort~~ (01:14:07.891)  
  
(01:14:07.931) ~~of~~ (01:14:08.011)  
  
(01:14:08.512) ~~gradations~~ (01:14:09.213)  
  
(01:14:10.055) ~~on~~ (01:14:10.175)  
  
(01:14:10.494) ~~the~~ (01:14:10.675)  
  
(01:14:10.835) ~~autonomy~~ (01:14:11.475)  
  
(01:14:11.615) ~~of~~ (01:14:11.716)  
  
(01:14:11.777) ~~the~~ (01:14:11.877)  
  
(01:14:11.936) ~~system~~ (01:14:12.277)  
  
(01:14:12.337) ~~though~~. 


---


#### 01:14:12.738

It does seem like a database on the one hand is inert until it gets a query. And a language model in the raw is inert until it gets a prompt. But I've been testing~~,~~(01:14:27.761)  
  
(01:14:27.820) ~~I~~ (01:14:27.881)  
  
(01:14:28.221) ~~test~~ (01:14:28.381)  
a lot of products. There's a new class of product   
(01:14:30.542) ~~I'm~~ (01:14:30.603)  
  
(01:14:30.622) ~~sure~~ (01:14:30.743)  
you've seen often called calling agent, where you can go in, give it a phone number, tell it what you want it to accomplish. And it will just call the person and have a voice conversation real time with that person. It is   
(01:14:44.518) ~~market~~, they are often marketed as agents, and they have the conversation entirely autonomously until either the goal is achieved or the person hangs up or whatever. Now, if I prompt that agent to scam you or to threaten you or to harass you or whatever. 


---


#### 01:15:02.402

Now, clearly, I'm in the wrong as a user for doing that. But would you put responsibility on the company providing that agentic AI as a service as well? Or would you again shield them and say it's all on the end user? Why not the telephone company? Why not the mining company that mined the ore in the first place? You gotta draw the line somewhere, right? The question is where to draw the line. Yeah, exactly. The illegal behavior. General computer science primitives are very useful for solving problems. They may save the human race.   
(01:15:34.813) ~~to~~ (01:15:34.953)  
  
(01:15:34.972) ~~your~~ (01:15:35.132)  
  
(01:15:35.212) ~~point~~, we're going to go ahead and we're going to solve biology and a bunch of other stuff. And that's just another app   
(01:15:39.375) ~~there~~ (01:15:39.515)  
  
(01:15:39.595) ~~may~~ (01:15:39.716)  
  
(01:15:39.756) ~~be~~ (01:15:39.876)  
  
(01:15:39.935) ~~like~~,   
(01:15:40.456) ~~whatever~~, these things solve grand unified field theory, and physics goes away   
(01:15:43.818) ~~discipline~~. 


---


#### 01:15:44.377

And that's another app.   
(01:15:45.238) ~~And~~ (01:15:45.319)  
We're just going to keep solving problems. And that's amazing~~,~~(01:15:47.579)  
  
(01:15:47.640) ~~right~~.   
(01:15:47.840) ~~And~~ (01:15:47.899)  
  
(01:15:47.939) ~~so~~ (01:15:48.039)  
We want the primitives to be primitives. If people do illegal behavior based on them, then 100% they should be criminalized. We didn't even make guns illegal~~.~~(01:15:56.824)  
  
(01:15:58.506) ~~we~~ (01:15:58.546)  
  
(01:15:58.907) ~~don't~~ (01:15:59.067)  
  
(01:15:59.106) ~~even~~ (01:15:59.287)  
  
(01:15:59.386) ~~hold~~ (01:15:59.587)  
  
(01:15:59.686) ~~gun~~ (01:15:59.907)  
  
(01:15:59.926) ~~makers~~ (01:16:00.287)  
  
(01:16:00.367) ~~liable~~.   
(01:16:01.087) ~~And~~ (01:16:01.148)  
  
(01:16:01.188) ~~you're~~ (01:16:01.328)  
  
(01:16:01.368) ~~talking~~ (01:16:01.707)  
  
(01:16:01.787) ~~about~~ (01:16:02.087)  
  
(01:16:02.368) ~~we~~ (01:16:02.448)  
  
(01:16:02.509) ~~are~~ (01:16:02.609)  
  
(01:16:02.668) ~~outliers~~ (01:16:03.048)  
  
(01:16:03.128) ~~in~~ (01:16:03.208)  
  
(01:16:03.248) ~~that~~. But you're talking about   
(01:16:05.751) ~~in~~ (01:16:05.810)  
  
(01:16:05.831) ~~this~~, you're talking about hypothetical threats for a very useful primitive that's shown far more good than bad.   
(01:16:13.314) ~~I~~ (01:16:13.354)  
  
(01:16:13.395) ~~mean~~, we've all gone a little crazy on this one.   
(01:16:15.976) ~~By~~ (01:16:16.037)  
  
(01:16:16.077) ~~the~~ (01:16:16.136)  
  
(01:16:16.157) ~~way~~, This is a dramatic shift in sentiment.   
(01:16:18.519) ~~And~~ (01:16:18.599)  
  
(01:16:18.639) ~~I'm~~,   
(01:16:19.298) ~~I~~ (01:16:19.359)  
  
(01:16:19.378) ~~think~~ (01:16:19.538)  
  
(01:16:19.578) ~~I~~ (01:16:19.639)  
  
(01:16:19.658) ~~know~~ (01:16:19.738)  
  
(01:16:19.759) ~~where~~ (01:16:19.878)  
  
(01:16:19.918) ~~the~~ (01:16:20.000)  
  
(01:16:20.039) ~~sources~~ (01:16:20.399)  
  
(01:16:20.439) ~~came~~ (01:16:20.659)  
  
(01:16:20.720) ~~from~~. But it's not what people think that we're just a little bit off base, unfortunately. 


---


#### 01:16:27.940

I'm not sure what I think about open source model releases entirely. I do think it's not unreasonable to demand some testing before release. When it comes to these agent calling companies, I actually come down pretty firmly that if you are going to offer an autonomous system as a service, I think it should be on you to make sure that your system refuses egregiously criminal requests. And that doesn't seem like too much to ask.   
(01:16:55.777) ~~I~~ (01:16:55.856)  
  
(01:16:55.957) ~~think~~ (01:16:56.096)  
  
(01:16:56.117) ~~that's~~ (01:16:56.277)  
  
(01:16:56.296) ~~totally~~ (01:16:56.618)  
  
(01:16:56.637) ~~fair~~.   
(01:16:56.837) ~~Fair~~ (01:16:56.938)  
  
(01:16:56.957) ~~enough~~.   
(01:16:58.140) ~~Yeah~~,   
(01:16:58.500) ~~listen~~, I want to be super~~,~~(01:16:59.520)  
  
(01:16:59.560) ~~super~~ (01:16:59.820)  
reasonable.   
(01:17:00.282) ~~Listen~~, if you've done all of the work to do criminal behavior, except for a very modest~~,~~(01:17:06.867)  
anybody can do it~~.~~(01:17:08.007)  
  
(01:17:09.128) ~~Then~~ (01:17:09.309)  
  
(01:17:09.509) ~~I~~, and that's the primary use case I'm all for coming in, but that's a very different statement than I've~~,~~(01:17:14.493)  



---


#### 01:17:14.552

I've created a computer science perimeter. That's very useful to a lot of people. To me, it's miles away.   
(01:17:19.457) ~~I~~ (01:17:19.516)  
  
(01:17:19.537) ~~know~~ (01:17:19.617)  
we're almost out of time. You mentioned   
(01:17:21.078) ~~kind~~ (01:17:21.198)  
  
(01:17:21.238) ~~of~~ (01:17:21.298)  
protocols a couple of times and   
(01:17:24.121) ~~yeah~~, there's also this question of~~.~~(01:17:26.122)  
how much we can trust open source models. Anthropic has had this paper on sleeper agents and the concept there is basically data set poisoning, model poisoning. You can create malicious models that can potentially even attack their own user. I wonder if you've got...   
(01:17:42.274) ~~Sorry~~,   
(01:17:42.456) ~~sorry~~.   
(01:17:42.576) ~~I'm~~ (01:17:42.676)  
  
(01:17:42.695) ~~just~~ (01:17:42.836)  
  
(01:17:42.855) ~~trying~~ (01:17:42.996)  
  
(01:17:43.015) ~~to~~ (01:17:43.095)  
  
(01:17:43.115) ~~like~~,   
(01:17:43.355) ~~I'm~~ (01:17:43.435)  
  
(01:17:43.456) ~~going~~ (01:17:43.555)  
  
(01:17:43.576) ~~to~~ (01:17:43.636)  
  
(01:17:43.655) ~~bump~~ (01:17:43.815)  
  
(01:17:43.836) ~~my~~ (01:17:43.935)  
  
(01:17:43.975) ~~next~~ (01:17:44.095)  
  
(01:17:44.176) ~~one~~ (01:17:44.256)  
  
(01:17:44.275) ~~too~~,   
(01:17:44.376) ~~because~~ (01:17:44.615)  
  
(01:17:44.636) ~~I~~ (01:17:44.655)  
  
(01:17:44.676) ~~don't~~ (01:17:44.775)  
  
(01:17:44.796) ~~want~~ (01:17:44.895)  
  
(01:17:44.916) ~~this~~.   
(01:17:46.056) ~~Okay~~.   
(01:17:46.237) ~~If~~ (01:17:46.277)  
  
(01:17:46.296) ~~we~~ (01:17:46.337)  
  
(01:17:46.377) ~~have~~ (01:17:46.476)  
  
(01:17:46.497) ~~more~~ (01:17:46.617)  
  
(01:17:46.657) ~~time~~ (01:17:46.877)  
  
(01:17:46.917) ~~then~~ (01:17:47.117)  
  
(01:17:47.256) ~~we'll~~ (01:17:47.657)  
  
(01:17:48.118) ~~ask~~ (01:17:48.217)  
  
(01:17:48.278) ~~one~~ (01:17:48.438)  
  
(01:17:48.478) ~~question~~ (01:17:48.677)  
  
(01:17:48.738) ~~at~~ (01:17:48.778)  
  
(01:17:48.818) ~~a~~ (01:17:48.877)  
  
(01:17:48.898) ~~time~~. 


---


#### 01:17:49.637

I think this may be a good pairing.   
(01:17:50.939) ~~So~~ (01:17:51.139)  
  
(01:17:51.498) ~~yeah~~. Open source models.   
(01:17:53.801) ~~Yeah~~. They are great in many respects.   
(01:17:57.067) ~~Yeah~~.   
(01:17:57.887) ~~I~~ (01:17:57.948)  
  
(01:17:57.988) ~~do~~ (01:17:58.148)  
  
(01:17:58.328) ~~see~~ (01:17:58.488)  
a future in which it doesn't seem like we'll be able to just trust any open source models. Certainly like we just can't, you can't just download and execute any binary right off the internet.   
(01:18:09.238) ~~you~~ (01:18:09.377)  
  
(01:18:09.417) ~~have~~, you shouldn't do that. You should probably go through the app store and get the approved version most of the time. It seems like models are headed that direction too, because of these sort of sleeper agent, unpredictable behaviors that could be maliciously coded into them. What technology solutions are you excited about for making sure that this sort of open and free exchange remains healthy and trustable, as opposed to collapsing into a, you can't trust any model you find on the internet equilibrium. 


---


#### 01:18:41.219

Can I point out how silly the whole sleeper agent thing is? How is this different than any arbitrary backdoor in any piece of software that we've been dealing with since forever? One big difference is, in principle, if you read the code of whatever, right?   
(01:18:58.936) ~~Like~~ (01:18:59.097)  
  
(01:18:59.278) ~~you,~~ (01:18:59.438)  
in theory, you would be able to tell, but we don't have any tools that can tell. Wait, that's absolutely not the case. If I give you a binary set of instructions, there's no way to actually determine behavior. This is like a proof. This is computer science. But that's why we don't go around downloading and executing random binaries, right? Right. Can I do the next half? Sorry. I'm just saying we can talk about software and   
(01:19:18.810) ~~I~~ (01:19:18.850)  
  
(01:19:18.871) ~~think~~ (01:19:18.990)  
it's great to talk about software, but we somehow keep making AI these special, unique things. 


---


#### 01:19:23.734

And in some ways they definitely have new properties and other ways~~.~~(01:19:25.935)  
they're actually not that different. Like this kind of notion of a sleeper agent. Binaries have had the potential for backdoors for a very long time. They are not something that you can~~,~~(01:19:36.854)  
you provably can't detect these~~,~~(01:19:38.815)  
  
(01:19:38.876) ~~right~~? It converges on the halting problem~~,~~(01:19:40.658)  
  
(01:19:40.698) ~~right~~?   
(01:19:40.858) ~~Especially~~ (01:19:41.279)  
  
(01:19:41.998) ~~if~~,   
(01:19:42.198) ~~anyways~~. So it's the same thing for models. And I just don't see why you wouldn't just use the same things that we do with models, which is   
(01:19:48.284) ~~like~~,   
(01:19:49.386) ~~I'm~~ (01:19:49.445)  
  
(01:19:49.466) ~~sorry~~, with binaries, which is Apple in the app store will decide to have some sort of criterion. And maybe you just don't download some random model if you don't want to. 


---


#### 01:19:59.994

And personally concerned way less about a model that might tell me something stupid or something offensive than a binary that could like wipe my hard drives.   
(01:20:11.423) ~~Right~~. And so I don't think it changes that game at all. At least maybe I'm missing something. Maybe you can educate me. What could a model do that a binary can't?   
(01:20:23.036) ~~I~~ (01:20:23.076)  
  
(01:20:23.115) ~~don't~~ (01:20:23.256)  
  
(01:20:23.277) ~~know~~. It's a good question. Certainly models can do a lot of things that traditional software can't~~,~~(01:20:29.023)  
  
(01:20:29.103) ~~right~~? When I advise people on how to build AI apps, I usually say, if you're going to have an AI app,   
(01:20:37.141) ~~you're~~ (01:20:37.261)  
  
(01:20:37.282) ~~going~~ (01:20:37.381)  
  
(01:20:37.402) ~~to~~, the reason you're going to use these, at least these new like general purpose models in an app is that there's some cognitive task that for practical purposes, you can't reduce to explicit instructions. 


---


#### 01:20:51.590

Whether that's, is this a cat or a dog or whatever, there is something that you can't reduce to code. So they can do definitely just categorically like a lot more stuff. They can, for example~~.~~(01:21:03.277)  
trick people.   
(01:21:05.137) ~~I~~ (01:21:05.177)  
  
(01:21:05.958) ~~think~~ (01:21:06.099)  
it's hard for, you could imagine a Eliza bot or whatever doing security question extraction from people, but~~.~~(01:21:11.860)  
So your concern is like exploiting the human via social interface versus anything having to do with the machine. Is that your concern?   
(01:21:19.323) ~~I~~ (01:21:19.363)  
  
(01:21:19.404) ~~don't~~ (01:21:19.524)  
  
(01:21:19.564) ~~know.~~ (01:21:19.663)  
  
(01:21:19.724) ~~I'm~~ (01:21:19.844)  
  
(01:21:19.884) ~~not~~ (01:21:19.984)  
  
(01:21:20.024) ~~that~~ (01:21:20.184)  
  
(01:21:20.304) ~~great~~ (01:21:20.503)  
  
(01:21:20.524) ~~of~~ (01:21:20.583)  
  
(01:21:20.623) ~~a~~ (01:21:20.644)  
  
(01:21:20.684) ~~hacker,~~ (01:21:21.003)  
  
(01:21:21.024) ~~but~~ (01:21:21.123)  
  
(01:21:21.144) ~~I~~ (01:21:21.184)  
  
(01:21:21.203) ~~think~~ (01:21:21.323)  
you could also have a sleeper agent model that, oh, great, download and run this thing. And it's going to help you use your computer but maybe it also goes around looking for things in your computer and maybe it's actually better able to find them than traditional software because it can sort of search more effectively and have a better understanding of what it's looking at in any given context. 


---


#### 01:21:43.430

Abstract~~,~~(01:21:43.770)  
  
(01:21:43.789) ~~I~~ (01:21:43.810)  
  
(01:21:43.829) ~~mean~~, people do all sorts of things   
(01:21:45.131) ~~like~~ (01:21:45.291)  
if I want to email myself my credit card number, maybe I   
(01:21:49.030) ~~like~~ (01:21:49.190)  
put a star between all the numbers and then it   
(01:21:51.152) ~~like~~ (01:21:51.311)  
breaks regular expressions, but maybe a model just looks at that and that looks like somebody is trying to trick~~,~~(01:21:56.893)  
trying to trip up a regular expression on a credit card number.   
(01:21:59.722) ~~I~~ (01:21:59.762)  
  
(01:21:59.783) ~~don't~~ (01:21:59.882)  
  
(01:21:59.903) ~~know~~. It just feels like there's a lot~~,~~(01:22:02.400)  
the surface area of these~~,~~(01:22:03.581)  
  
(01:22:03.622) ~~my~~ (01:22:03.761)  
general reasoning is   
(01:22:05.243) ~~like~~ (01:22:05.403)  
the surface area of these things is vast. And there's just a lot of unknown unknowns.   
(01:22:10.926) ~~Yeah~~. Again,   
(01:22:11.606) ~~I~~ (01:22:11.646)  
  
(01:22:11.667) ~~guess~~ (01:22:11.806)  
from my standpoint, listen, if somebody wants to break into your computer,   
(01:22:15.009) ~~like~~ (01:22:15.149)  
probably the best way   
(01:22:16.029) ~~with~~ (01:22:16.170)  
  
(01:22:16.210) ~~a,~~ (01:22:16.229)  
  
(01:22:16.850) ~~with~~ (01:22:17.010)  
some binary~~,~~(01:22:17.650)  
that'll do a backdoor. 


---


#### 01:22:18.490

And then   
(01:22:18.912) ~~like~~ (01:22:19.072)  
almost all computer attacks are humanated anyways.   
(01:22:21.472) ~~Like~~ (01:22:21.594)  
they get a bunch of information, they send it to a group of people who analyze it. And~~,~~(01:22:25.015)  
  
(01:22:25.195) ~~and~~ (01:22:25.275)  
maybe you can put all of that intelligence locally at some point.   
(01:22:27.858) ~~Like~~ (01:22:27.957)  
we're definitely not there yet. And it can do something, but I feel like that's~~.~~(01:22:31.119)  
at this point, science fiction. You can do a lot of automated stuff today with binaries. You can absolutely put humans in the loop. You can do intelligent stuff. You can social engineer. You can do all of those things today. I don't know if there's a paradigmatic shift. I have not seen it. If there is, and you actually have demonstrations of this, then maybe it warrants a discussion, but we're not there yet. 


---


#### 01:22:50.488

I do think we've got a lot of safeguards in place when it comes to binaries. I think we've all learned not to download the wrong ones, and we should. I do want to have a, here's how sensitive we are to this question. No joke. I was in Japan last fall and I got a phone call from my mom and   
(01:23:04.993) ~~my~~ (01:23:05.113)  
mom's~~,~~(01:23:05.373)  
Oh Martine, where are you? And I'm like, Oh, I'm~~,~~(01:23:07.515)  
  
(01:23:07.635) ~~I'm~~ (01:23:07.755)  
in Japan. She said, no, you're not. And I'm like,   
(01:23:09.076) ~~Hey,~~ (01:23:09.216)  
  
(01:23:09.275) ~~I'm~~ (01:23:09.376)  
  
(01:23:09.416) ~~so~~ (01:23:09.716)  
talk to your father. This is very awkward call. And my dad gets on.   
(01:23:12.077) ~~He's~~ (01:23:12.177)  
Oh,   
(01:23:12.537) ~~he's~~ (01:23:12.658)  
  
(01:23:12.677) ~~like~~, where are you? I'm like in Japan. He's like, Oh, I just had somebody call and said~~,~~(01:23:17.420)  
it was you and I could hear you, but you're a little bit muffled and you're in prison. 


---


#### 01:23:22.363

And I'm heading out the door to give $10,000 to get you out of prison. Like literally this happened to me. And it was my voice and they were almost convinced to do this. And so it was funny, I posted this on Twitter and everybody's like, oh my God, deepfakes. Like this is what happens in the age of AI. And all of a sudden there's this indictment on AI. Filed a police report, talked to the police. They actually know the team that does this. It's not~~,~~(01:23:44.408)  
it has nothing to do with AI. It's literally a human being that   
(01:23:47.250) ~~like~~ (01:23:47.449)  
muffled their voice to pull this off. It's been going on for a very long time. And so I just feel like we ascribe these superpowers to AI that have not been demonstrated that are still possible today. 


---


#### 01:23:59.295

And even if they could do the same thing that you can today, which they absolutely can, is not a paradigm   
(01:24:03.997) ~~active~~ (01:24:04.257)  
shift. And maybe if we do see that happen,   
(01:24:07.298) ~~like~~ (01:24:07.439)  
we should say, oh, the safeguards we have in place are not enough. But I don't think that we should do that preemptively, because you're going to inhibit the innovation, the proliferation of these things, which   
(01:24:19.038) ~~is~~, it really is a different way that we will, as a society, view the development and the use of software. And I think that something that has added so much good, we're going to,   
(01:24:30.546) ~~I~~ (01:24:30.565)  
  
(01:24:31.065) ~~think~~ (01:24:31.206)  
it's almost like a moral bet,   
(01:24:32.747) ~~like~~ (01:24:32.886)  
we will reduce the amount of good we do in the fear of something that we just haven't even demonstrated   
(01:24:37.770) ~~that's~~ (01:24:37.970)  
  
(01:24:38.010) ~~bad~~. 


---


#### 01:24:39.230

I do worry about that. The nuclear outcome for AI would definitely be a real tragedy. And I'm a big proponent right now of two projects I'll highlight that I think are like some of the great~~,~~(01:24:52.608)  
maybe the most important things going on right now. One from Google DeepMind, I have an episode coming soon on the med, what was MedPalm, now is MedGemini. And they're just grinding~~,~~(01:25:03.934)  
  
(01:25:03.953) ~~man~~. It's like dialing in the diagnosis, really strong evals, expanding modalities to radiology, all these other things. And I love it because   
(01:25:14.479) ~~I~~ (01:25:14.559)  
  
(01:25:14.618) ~~like~~, I think we can all love it because we all want everybody to have this sort of access to quality expertise and obviously it's   
(01:25:22.006) ~~like~~ (01:25:22.186)  
all too scarce. 


---


#### 01:25:23.628

I also love it because it does demonstrate pretty compellingly, although, as always, we can debate around this, that there is potential for extreme value today, the current level of systems without necessarily having to   
(01:25:38.682) ~~like~~ (01:25:38.863)  
100x or 1000x or 10,000x the scale of the training runs. I asked the guys there, do you   
(01:25:47.188) ~~guys~~ (01:25:47.347)  
think you could create your AI doctor vision if Gemini 1.5 Pro was the   
(01:25:53.310) ~~blasted~~ (01:25:53.810)  
best model you ever got to build on? And they were like, Yeah, I think so. It would take us a little longer. That's awesome. But   
(01:26:02.283) ~~I~~ (01:26:02.344)  
  
(01:26:02.404) ~~think~~, yeah, I think we could probably make it happen. And OpenAI has a similar one. They're working with Harvey, as I'm sure you are familiar. 


---


#### 01:26:08.769

They have a custom model that they're doing for them.   
(01:26:11.470) ~~And~~ (01:26:11.569)  
  
(01:26:11.609) ~~same~~ (01:26:11.791)  
  
(01:26:11.810) ~~deal~~,   
(01:26:12.051) ~~right~~? They're working on GPT-4 base. It's   
(01:26:14.032) ~~like~~ (01:26:14.171)  
incremental compute, not orders of magnitude compute. And they're,   
(01:26:18.715) ~~again~~,   
(01:26:18.975) ~~like~~ (01:26:19.114)  
just achieving huge gains.   
(01:26:21.076) ~~I~~ (01:26:21.115)  
  
(01:26:21.136) ~~think~~ (01:26:21.256)  
it was like 97% preference ratio of the dialed in model to the original base. So   
(01:26:30.619) ~~I~~ (01:26:30.720)  
  
(01:26:30.880) ~~am~~ (01:26:30.979)  
  
(01:26:31.239) ~~very~~ (01:26:31.399)  
  
(01:26:31.420) ~~much~~ (01:26:31.560)  
  
(01:26:31.579) ~~with~~ (01:26:31.680)  
  
(01:26:31.720) ~~you~~ (01:26:31.800)  
  
(01:26:31.819) ~~that~~ (01:26:31.920)  
  
(01:26:32.039) ~~I~~ (01:26:32.100)  
would hate to see us end up in a spot where professional licensing, regulatory capture, rent seeking prevents us from getting the value. And   
(01:26:42.484) ~~I'm~~ (01:26:42.564)  
  
(01:26:42.604) ~~definitely~~ (01:26:43.024)  
vigilant against those sorts of threats. The flip side   
(01:26:48.926) ~~for~~ (01:26:49.046)  
  
(01:26:49.086) ~~me~~ (01:26:49.265)  
is   
(01:26:49.606) ~~I~~ (01:26:49.667)  
would love us to understand this a little bit better before we raced to the proverbial trillion dollar cluster or whatever that sort of AGI superintelligence might be. 


---


#### 01:27:01.592

Sounds like you ultimately just don't think that's going to happen. I take it you guys will not be investing in Ilya's new straight superintelligence venture. I would be delighted to invest in that.   
(01:27:10.279) ~~I~~ (01:27:10.319)  
  
(01:27:10.359) ~~mean~~,   
(01:27:11.881) ~~I~~ (01:27:11.942)  
  
(01:27:11.981) ~~honestly~~ (01:27:12.363)  
  
(01:27:12.403) ~~think~~ (01:27:12.643)  
it reduces to how people wake up in the morning. And some people wake up in the morning and they're really afraid of the future. And they can't really articulate it, but there's something bad that's going to happen. And they want to protect themselves from it. They're not really sure what it   
(01:27:27.492) ~~was~~. And anything that has the spectra of it, they'll find it in there. And listen, that's a constant voice in every technology shift. It's just absolutely constant. 


---


#### 01:27:36.934

And there's other people that are like, listen, we're the ones that created this. We got this stuff. It turns out the universe is very complicated. It turns out that we use tools to protect ourselves, not the opposite. And   
(01:27:45.555) ~~of~~ (01:27:45.615)  
  
(01:27:45.676) ~~course,~~ (01:27:45.895)  
most things are still used, every computer, everything that we create.   
(01:27:48.896) ~~But~~ (01:27:49.077)  
I believe in us as   
(01:27:50.917) ~~kind~~ (01:27:51.078)  
  
(01:27:51.158) ~~of~~ (01:27:51.257)  
meaning creatures to have innovation. It just comes down to the basis of innovation. And I tend to think that, listen, I don't think that there's~~,~~(01:27:58.680)  
I don't think the universe gives up its secrets easily at all. I don't think any single system will ever   
(01:28:04.764) ~~be,~~ (01:28:05.163)  
solve all problems, not even close. I feel like everything is a sigmoid. 


---


#### 01:28:09.546

I feel the universe is heavy tailed. And so the more tools we have, the more we're enabled to help ourselves. That's really what I strongly believe. Every indication of this current AI to me suggests that this is the case here. These are still computer systems. And so   
(01:28:21.216) ~~I~~ (01:28:21.275)  
  
(01:28:21.417) ~~think~~ (01:28:21.636)  
  
(01:28:21.737) ~~that~~ (01:28:21.936)  
any inhibition of innovation on this stuff that's outside of a 30-year discourse. Again, I'm in a very moderate position here. I'm like, we regulate software a whole bunch. We should continue to do that. These are hard-earned regulations. Let's continue to do that. But let's not inhibit innovation unnecessarily based on the precautionary principle. I do want to have just one quick analogy for you. So you're familiar with the whole CRISPR thing, right? 


---


#### 01:28:46.488

Listen, CRISPR~~,~~(01:28:47.149)  
  
(01:28:47.170) ~~like~~, that changes the human genome.   
(01:28:49.073) ~~Like~~ (01:28:49.393)  
it's not just~~,~~(01:28:50.293)  
it doesn't just edit   
(01:28:51.554) ~~like~~ (01:28:51.694)  
one thing.   
(01:28:52.654) ~~Like~~ (01:28:52.795)  
if you   
(01:28:53.295) ~~like~~ (01:28:53.494)  
change a human being with CRISPR   
(01:28:55.935) ~~and~~ (01:28:56.015)  
  
(01:28:56.076) ~~that~~ (01:28:56.296)  
the human genome is different and they have kids that gets passed on. So what happened when CRISPR got invented? There was   
(01:29:01.798) ~~like~~ (01:29:01.899)  
some self-policing. There wasn't really a lot of regulation. A bunch of people went and studied it. And listen, it turns out to be a very useful~~,~~(01:29:08.881)  
  
(01:29:09.242) ~~very~~ (01:29:09.542)  
useful tool for humans to use going forward. And so there's one of two views there. One of the views is   
(01:29:14.904) ~~like~~, Oh, we got it wrong. We should have basically regulated it. But that's not what we did. 


---


#### 01:29:19.172

We did something actually very different. And this is something that edits the human genome. And so to me, it's just so mind-blowing that we're talking about computer systems that don't edit anything that we actually understand the properties of. I could literally sit here and talk about, just like the discussion we had, we can actually talk about the bounds of all of these things, and yet we're talking about constraining this new innovation. It feels like we've just entered this kind of weird, dark period in our emotional state with regards to computers. And   
(01:29:45.150) ~~I~~ (01:29:45.171)  
  
(01:29:45.211) ~~think~~ (01:29:45.331)  
a lot of it is social networking casts a lot of shadow.   
(01:29:48.634) ~~I~~ (01:29:48.673)  
  
(01:29:48.694) ~~think~~ (01:29:48.833)  
a lot of it is Bostrom~~,~~(01:29:49.994)  
  
(01:29:50.034) ~~like~~, red-pilled a bunch of people. 


---


#### 01:29:52.496

And so we're just in this kind of mindset that we just want to protect ourselves from something that's not a threat.   
(01:29:58.199) ~~I~~ (01:29:58.279)  
  
(01:29:58.319) ~~wish~~ (01:29:58.479)  
  
(01:29:58.520) ~~I~~ (01:29:58.579)  
  
(01:29:58.619) ~~was~~ (01:29:58.740)  
  
(01:29:58.780) ~~so~~ (01:29:58.940)  
  
(01:29:59.001) ~~confident~~. Have you ever heard of gene drives, just to raise, to see and raise your CRISPR? So it's a CRISPR elaboration.   
(01:30:06.685) ~~I~~ (01:30:06.706)  
  
(01:30:07.345) ~~don't~~ (01:30:07.627)  
  
(01:30:08.167) ~~know~~ (01:30:08.287)  
  
(01:30:08.327) ~~if~~ (01:30:08.407)  
  
(01:30:08.447) ~~he's~~ (01:30:08.587)  
  
(01:30:08.606) ~~the~~ (01:30:08.686)  
  
(01:30:08.726) ~~inventor~~ (01:30:09.068)  
  
(01:30:09.127) ~~or~~ (01:30:09.207)  
  
(01:30:09.268) ~~one~~ (01:30:09.368)  
  
(01:30:09.387) ~~of~~ (01:30:09.427)  
  
(01:30:09.448) ~~the~~ (01:30:09.528)  
  
(01:30:09.547) ~~inventors,~~ (01:30:09.887)  
  
(01:30:09.927) ~~but~~ (01:30:10.068)  
I have an episode coming up with Kevin Esvelt, who's a biologist. Brilliant guy. The gene drive concept is that it will copy itself into the other chromosome. And thus, it becomes super dominant. So if it gets inserted in one generation, next thing you know, it's in both chromosomes. And then when that generation reproduces, it copies again. 


---


#### 01:30:35.818

And so basically, it's going to take over the entire population with this gene. The only way that they know to combat that is to have another gene drive that comes in and tries to neutralize the first gene drive.   
(01:30:46.134) ~~but~~ (01:30:46.213)  
  
(01:30:46.234) ~~I~~ (01:30:46.274)  
  
(01:30:46.293) ~~guess~~ (01:30:46.453)  
  
(01:30:46.474) ~~the~~ (01:30:47.337)  
  
(01:30:47.377) ~~reason~~ (01:30:47.556)  
  
(01:30:47.596) ~~I~~ (01:30:47.676)  
  
(01:30:47.697) ~~raise~~ (01:30:47.898)  
  
(01:30:47.917) ~~that~~ (01:30:48.037)  
  
(01:30:48.097) ~~is~~ (01:30:48.179)  
it seems like the real   
(01:30:50.503) ~~kind~~ (01:30:50.664)  
  
(01:30:50.685) ~~of~~ (01:30:50.744)  
core. Disagreement between the safetyists and   
(01:30:56.997) ~~the~~, let's not worry about this, or it would be premature to worry about this, is really the expectation of how powerful is it gonna get and how broad would the impact be? Because the CRISPR versus the gene drive,   
(01:31:09.260) ~~it's~~ (01:31:09.359)  
  
(01:31:09.380) ~~like~~, those are very different technologies, right? One makes an edit, one propagates through all future generations, and that differences everything, right? 


---


#### 01:31:17.802

Seems like the same thing basically is true for AI. I totally agree with this, which is   
(01:31:23.125) ~~like~~, I am all for if we've identified one mechanism that has massive destructive power. For example, listen, I've worked in nuclear weapons, right? I totally understand the impact of nuclear weapons. You have one relatively simple mechanism that has massive destructive power. And it could be the case, certainly with something biological, because you do have exponentials when it comes to replication, right? Just like with a nuclear bomb,   
(01:31:48.184) ~~like~~ (01:31:48.305)  
once you hit criticality, you get an exponential. With biology, you also get an exponential, right?   
(01:31:54.113) ~~that~~ (01:31:54.233)  
  
(01:31:54.273) ~~has~~ (01:31:54.372)  
  
(01:31:54.573) ~~not~~ (01:31:54.773)  
  
(01:31:54.792) ~~been~~ (01:31:54.932)  
  
(01:31:54.972) ~~the~~ (01:31:55.052)  
  
(01:31:55.073) ~~case~~ (01:31:55.233)  
  
(01:31:55.252) ~~with~~ (01:31:55.332)  
  
(01:31:55.353) ~~the~~ (01:31:55.412)  
computer systems. There's zero indication that will ever be the case with computer systems. 


---


#### 01:31:58.774

These are not biological systems. They're computer systems. So until somebody actually shows that in a way that's not a platonic thought experiment~~,~~(01:32:05.676)  
  
(01:32:05.895) ~~like~~, there's zero evidence. Actually, in fact, there's   
(01:32:08.237) ~~actually~~ (01:32:08.476)  
counter evidence that we have recursive self-improvement.   
(01:32:11.557) ~~Like~~, there are a number of~~,~~(01:32:12.818)  
  
(01:32:12.877) ~~like~~, good studies   
(01:32:13.457) ~~like~~ (01:32:13.599)  
this will never happen. theoretically. So until we have even a shred of evidence that there's going to be anything that's going to be exponential or that'll behave in ways that computer systems haven't, then we could have the conversation. But we're not there. We're literally taking a thought experiment that is not rooted   
(01:32:31.623) ~~at~~ (01:32:31.685)  
  
(01:32:31.904) ~~all~~ (01:32:32.064)  
in the way that we understand computers and using that to regulate perhaps the most beneficial technology of the last 30 years. 


---


#### 01:32:39.752

Do you like the responsible scaling policy approach or the preparedness framework approach? Each of Anthropic OpenAI and DeepMind now have their own   
(01:32:49.966) ~~kind~~ (01:32:50.105)  
  
(01:32:50.126) ~~of~~ (01:32:50.166)  
version of this, but I'm sure you're familiar, but basically for everybody who may not be, they are defining a set of things that they   
(01:32:58.967) ~~interested~~ (01:32:59.529)  
  
(01:32:59.548) ~~in~~ (01:32:59.588)  
  
(01:32:59.708) ~~looking~~ (01:32:59.988)  
  
(01:33:00.069) ~~at~~ (01:33:00.130)  
  
(01:33:00.189) ~~or~~ (01:33:00.250)  
think could be problematic long-term, defining thresholds of capability, and then in the Anthropx case, committing to either solving it or stopping scaling when they begin to see those   
(01:33:13.527) ~~I~~ (01:33:13.587)  
  
(01:33:13.606) ~~think~~ (01:33:13.726)  
companies should do whatever they want, whether this is for signaling or they're actually genuinely concerned they should do whatever they want. I've spoken to a lot of people, by the way, that are close to these. 


---


#### 01:33:25.694

And a lot of the times they're like, listen, we're worried that if we don't self-police, the regulators will police for us. So it's not because they actually think there's a genuine concern. Not everybody. Some people think there's a genuine concern. Many people   
(01:33:35.659) ~~that~~ (01:33:35.779)  
do this for much more pragmatic reasons. I am fully supportive of that, to be very clear. What I am not supportive of is~~,~~(01:33:42.703)  
regulators   
(01:33:45.847) ~~to~~ (01:33:45.887)  
having arbitrary restrictions that will unevenly impact startups, academics, and researchers. Anthropic, do your thing. OpenAI, do your thing. Google, do your thing. But do not   
(01:34:01.114) ~~voice~~ (01:34:01.475)  
  
(01:34:01.515) ~~this~~ (01:34:01.815)  
  
(01:34:02.716) ~~on~~ (01:34:02.815)  
  
(01:34:02.956) ~~a~~ (01:34:02.996)  
  
(01:34:03.076) ~~broader~~ (01:34:03.436)  
  
(01:34:03.476) ~~framework~~ (01:34:03.917)  
  
(01:34:03.936) ~~that's~~ (01:34:04.117)  
  
(01:34:04.136) ~~going~~ (01:34:04.256)  
  
(01:34:04.277) ~~to~~ (01:34:04.317)  
impact our ability to innovate. I think that's actually like a moral wrong. I really do. 


---


#### 01:34:09.859

What do you think the trajectory of competition in the foundation model space is going to look like? It's a very good question. So   
(01:34:18.466) ~~I~~ (01:34:18.487)  
  
(01:34:18.887) ~~think~~ (01:34:19.047)  
one thing I didn't appreciate, I'm just starting to appreciate is how much there's a perverse economy of scale and like distillation is so effective. And this is why every, so a positive economy of scale, like a network effect is like the leader the marginal cost to add the additional user goes to zero. So leaders get ahead and you have natural monopolies, right? A neutral competitive landscape just fragments because a dollar in is like basically a dollar earned. In this it looks like there's a perverse economy of scale which the leader has to pay more money to stay ahead and because you're a leader you help the followers because they can literally use you to catch up to you. 


---


#### 01:35:02.167

That's a perverse economy of scale. So   
(01:35:04.328) ~~I~~ (01:35:04.349)  
  
(01:35:04.368) ~~think~~ (01:35:04.488)  
there's a very open question on, is this going to just end up fragmenting into a whole bunch of competitive models that are basically the same size? And that seems to be the case. Originally,   
(01:35:17.238) ~~this~~ (01:35:17.319)  
  
(01:35:17.338) ~~is~~ (01:35:17.378)  
  
(01:35:17.418) ~~by~~ (01:35:17.519)  
  
(01:35:17.538) ~~the~~ (01:35:17.658)  
  
(01:35:17.698) ~~way~~,   
(01:35:17.859) ~~this~~ (01:35:17.979)  
  
(01:35:18.020) ~~is~~ (01:35:18.100)  
  
(01:35:18.140) ~~how~~ (01:35:18.239)  
  
(01:35:18.279) ~~much~~ (01:35:18.380)  
  
(01:35:18.399) ~~we're~~ (01:35:18.520)  
  
(01:35:18.560) ~~so~~ (01:35:18.680)  
  
(01:35:18.699) ~~scared~~ (01:35:18.899)  
  
(01:35:18.920) ~~of~~ (01:35:18.960)  
  
(01:35:19.060) ~~a~~ (01:35:19.079)  
  
(01:35:19.100) ~~run~~ (01:35:19.180)  
  
(01:35:19.199) ~~shot~~. Originally, everybody was like, it'll be open AI and   
(01:35:21.141) ~~there'll~~ (01:35:21.322)  
  
(01:35:21.341) ~~be~~ (01:35:21.402)  
ahead of everybody and nobody will catch up. Remember that? I remember it very well.   
(01:35:25.204) ~~This~~ (01:35:25.364)  
  
(01:35:25.425) ~~is~~, you've got data network effects and you've got, they're going to have all the data. Okay. Now that's clearly not the case. So now you have a number of companies that are roughly at the same baseline. 


---


#### 01:35:33.909

But what's surprising is companies that are   
(01:35:35.551) ~~like~~ (01:35:35.650)  
not even software companies, just like   
(01:35:37.332) ~~the~~ (01:35:37.492)  
NVIDIA released recently are building models that are incredibly competitive. There's open source that's incredibly competitive. Databricks released a model that was incredibly competitive. And so my sense is because there's an inherent perverse economy of scale that's built into this~~.~~(01:35:50.837)  
which is because you can use them for distillation. I'm using a very loose phrase of distillation, not the technical phrase, but   
(01:35:56.018) ~~like~~ (01:35:56.137)  
you can use them to build models. I suspect you're going to just have massive, massive fragmentation. And that's absolutely been the case~~,~~(01:36:03.100)  
  
(01:36:03.140) ~~right~~? This is what we've seen is you'd think that it'd be OpenAI and everybody else, maybe OpenAI Anthropic and everybody else. 


---


#### 01:36:08.322

And it's not like that.   
(01:36:10.042) ~~Yeah~~. You look at the LMSS leaderboard, it is like that.   
(01:36:12.823) ~~The~~. It's basically all those guys dominating the top. Certainly there are a lot of people who have done a fine tune based on their outputs and closed the gap. But   
(01:36:27.231) ~~I~~ (01:36:27.252)  
  
(01:36:28.233) ~~think~~ (01:36:28.512)  
this is a pretty different- Listen, if you compare this to a true network effect with a marginal cost of the leader~~,~~(01:36:33.854)  
like for adding a new user goes to zero, it looks nothing like this. This is shockingly fragmented. for any sort of industry that's capital intensive. Shockingly fragmented, just so you know. If you compare this to the rise of social networking, or the rise of the telephone networks, or the rise of the internet, or any of these, this is shockingly fragmented. 


---


#### 01:36:54.729

And so   
(01:36:55.711) ~~I~~ (01:36:55.791)  
  
(01:36:55.810) ~~think~~ (01:36:55.970)  
  
(01:36:56.011) ~~it's-~~ (01:36:56.131)  
On what basis~~,~~(01:36:57.391)  
  
(01:36:57.452) ~~I~~ (01:36:57.492)  
  
(01:36:57.511) ~~don't~~ (01:36:57.612)  
  
(01:36:57.631) ~~know~~, on what measurement would you say that though? Because the leaderboard doesn't~~,~~(01:37:00.453)  
the leaderboard basically shows like three players. The sort of market share   
(01:37:07.056) ~~I~~ (01:37:07.096)  
  
(01:37:07.117) ~~would~~ (01:37:07.256)  
  
(01:37:07.277) ~~think~~ (01:37:07.417)  
is also like very dominated. That's a great point.   
(01:37:10.639) ~~I~~ (01:37:10.698)  
  
(01:37:10.719) ~~think~~ (01:37:10.840)  
  
(01:37:10.859) ~~that's~~ (01:37:10.979)  
  
(01:37:11.020) ~~a~~ (01:37:11.060)  
  
(01:37:11.079) ~~great~~ (01:37:11.199)  
  
(01:37:11.220) ~~point~~.   
(01:37:11.520) ~~I~~ (01:37:11.579)  
  
(01:37:11.600) ~~think~~ (01:37:11.760)  
if you look at the market share, it's breaking what you normally see, which is it breaks Pareto, which is like the top 20% gets 80% of the market and the rest. And it looks like software in that way, but it doesn't look different than software. Yeah, often it's converging on cloud. Yeah, exactly.   
(01:37:26.140) ~~I~~ (01:37:26.201)  
  
(01:37:26.220) ~~think~~ (01:37:26.521)  
this is a great point.   
(01:37:27.603) ~~I~~ (01:37:27.662)  
  
(01:37:27.682) ~~think~~ (01:37:27.842)  
it is converging on what we have seen traditionally with software, which is first mover gets 80% and is able to attract capital. 


---


#### 01:37:35.190

But it does not look like the network. Networks very specifically create monopolies, right? Facebook   
(01:37:42.221) ~~like~~ (01:37:42.381)  
whatever that is   
(01:37:43.842) ~~is~~ (01:37:43.962)  
github is another one   
(01:37:45.043) ~~like~~ (01:37:45.163)  
those look quite different than oligopolies which you tend to see come out of Traditional software where there isn't an inherent network effect, but there are scale effects and first-mover effects and   
(01:37:54.771) ~~I~~ (01:37:55.051)  
  
(01:37:55.131) ~~I~~ (01:37:55.231)  
absolutely grant you that's what it seems to look like now, but   
(01:37:58.154) ~~that~~ (01:37:58.274)  
even then I am surprised by how fast less funded competitors catch up.   
(01:38:04.739) ~~I~~ (01:38:04.819)  
  
(01:38:04.899) ~~do~~ (01:38:05.079)  
  
(01:38:05.119) ~~think~~ (01:38:05.239)  
  
(01:38:05.260) ~~that~~ (01:38:05.359)  
they're diminishing marginal returns all across the board in these models. Like when it comes to collecting data, the existing data that you have, the compute, et cetera. So I expect this to be a lot more fragmented than   
(01:38:16.641) ~~I~~ (01:38:16.662)  
  
(01:38:16.921) ~~think~~ (01:38:17.082)  
many people do. 


---


#### 01:38:18.603

And listen, if you want~~,~~(01:38:19.703)  
  
(01:38:19.783) ~~listen,~~ (01:38:19.962)  
if you want to do a dollar bet, I will bet you a dollar in two years that   
(01:38:24.884) ~~like~~ (01:38:25.184)  
this has turned out to be relatively fragmented.   
(01:38:29.364) ~~Like~~ (01:38:29.484)  
  
(01:38:29.545) ~~it's~~, there's not   
(01:38:30.085) ~~like~~ (01:38:30.225)  
a clear leader that has 80% of the market. I think we could come up with a bet there that I would take. My general expectation is that there are going to be fewer and fewer competing at the top level because it's just going to be, assuming, well, part of my bet would be that something like scaling laws hold. I think the most Unpredictable element of all of this is   
(01:38:54.079) ~~like~~ (01:38:54.279)  
research breakthrough. That's what could really shake the snow globe is   
(01:38:57.742) ~~like~~ (01:38:57.962)  
somebody finds something that doesn't scale quadratically anymore or whatever. 


---


#### 01:39:02.583

There could be all sorts of insights that could change things, but~~.~~(01:39:05.725)  
If scaling laws, people always talk about scaling laws, but   
(01:39:09.188) ~~you~~ (01:39:09.268)  
  
(01:39:09.307) ~~know~~ (01:39:09.448)  
how hard it is to   
(01:39:10.849) ~~like~~ (01:39:11.069)  
deal with a logarithmic scaling law or an exponential or power law or however you want to characterize it.   
(01:39:17.032) ~~Like~~. You need   
(01:39:18.568) ~~like~~ (01:39:18.728)  
an order of magnitude more to get the same improvement. This is really tough and we're starting to see that play out now. And so   
(01:39:27.912) ~~I~~ (01:39:27.953)  
  
(01:39:27.993) ~~think~~ (01:39:28.113)  
when people say scaling laws hold,   
(01:39:30.514) ~~I~~ (01:39:30.554)  
  
(01:39:30.573) ~~think~~ (01:39:30.694)  
that kind of undersells what that   
(01:39:33.836) ~~But~~ (01:39:34.336)  
does   
(01:39:34.596) ~~this~~ (01:39:35.016)  
  
(01:39:35.077) ~~work~~ (01:39:35.317)  
against open source in the sense that if you expect that it is going to be that hard to get those gains? 


---


#### 01:39:42.820

The opposite for two reasons. Let's not even talk about distillation. So let's imagine you have two Xenos paradox runners, right? They're both running, but never getting to the end. It turns out when you have scaling laws like this, the front runner will continue to run slower because the marginal dollar goes down. Put it this way, if OpenAI needs to improve by some fixed amount, it has to pay 10 times more or 100 times more or 1,000 more. And so for the same investment, any challenger is going to catch up by quite a bit. So that gap is always closing. Now, they never catch up because these are   
(01:40:21.980) ~~kind~~ (01:40:22.081)  
  
(01:40:22.121) ~~of~~ (01:40:22.161)  
Zeno's paradox runners, but it's the exact opposite. 


---


#### 01:40:25.604

Is that clear?   
(01:40:26.123) ~~It's~~ (01:40:26.225)  
  
(01:40:26.244) ~~like~~ (01:40:26.364)  
The leader has to now pay more to make the same gains. And then if you add the fact that the followers can learn from the leaders because they can do all sorts of tricks, like create the post-training data or whatever tricks that they do, that's even another headwind. And so it's almost like there's this massive,   
(01:40:44.542) ~~so~~ (01:40:44.662)  
in a normal network effect, the leader gets a tax break because it's easier to connect new users because whatever, you've got super linear value when you have more units or whatever it is,   
(01:40:56.048) ~~right~~? But this one, leaders get taxed a lot more and they get taxed more because the marginal value has dropped so much, but also because the people behind can catch up. 


---


#### 01:41:05.514

And so it's the exact opposite of what you've said. How much does that depend on   
(01:41:11.837) ~~I~~ (01:41:11.877)  
  
(01:41:11.898) ~~guess~~ (01:41:12.078)  
there's no disputing the Zeno's paradox runners analysis, but the question is how apt is that analogy to the actual situation? It seems like you could easily get into a dynamic where, and this is what Anthropic, according to their leaked pitch deck, said, it seems like you could easily imagine a situation where somebody gets far enough along that they are actually really economically valuable, whether that's drop in knowledge worker or whatever. And then they actually have the resources and the others don't. And then that sort of splits. There's like a continental divide there where you're either rolling downhill toward AGI or you're like still coming uphill toward trying to get over the hump. 


---


#### 01:41:52.787

Sure. Listen, if that happens, that's great. There again, there's perverse economies of scales and diminishing margin returns everywhere you look in this space. This is why it seems   
(01:42:02.471) ~~to~~ (01:42:02.572)  
  
(01:42:02.612) ~~me~~ (01:42:02.851)  
that you have one leader, then everybody catches up and then they're edging forward and then everybody catches up. The distillation thing is incredibly real. Having this and being a professional investor for almost a decade and having worked with   
(01:42:13.917) ~~like~~ (01:42:14.077)  
complex systems for three times longer than that, I will say this does not look like a normal first mover scale effect or network effect. It looks the opposite. It looks like a total slugfest to stay   
(01:42:26.362) ~~as~~ (01:42:26.502)  
really~~,~~(01:42:26.962)  
  
(01:42:27.042) ~~really~~ (01:42:27.283)  
hard. And   
(01:42:27.682) ~~I~~ (01:42:27.703)  
  
(01:42:27.743) ~~think~~ (01:42:27.842)  
it's going to get even harder. 


---


#### 01:42:30.425

The marginal value is going down per dollar invested for leaders so quickly. And so to think that somehow gives leaders an advantage, you have to assume there's some step function generality thing, which we've not yet seen. And maybe there will be at some point in time, but we have not seen that.   
(01:42:47.560) ~~Yeah~~, maybe I'll take a... I'll bet you a dollar. Let's think about this. I actually do have to hop off in a few minutes. Let's think about this. I want to make a dollar bet on this one. I think that the tail   
(01:42:57.722) ~~is~~ (01:42:57.783)  
  
(01:42:57.823) ~~going~~ (01:42:57.944)  
  
(01:42:57.963) ~~to~~ (01:42:58.003)  
  
(01:42:58.083) ~~really~~ (01:42:58.423)  
impact the shape of this industry going forward. I think that it'll be different than we've seen in previous   
(01:43:08.027) ~~kind~~ (01:43:08.146)  
  
(01:43:08.167) ~~of~~ (01:43:08.207)  
software industries. 


---


#### 01:43:10.547

Yeah, I'm with that. Definitely. One of my big things is   
(01:43:13.509) ~~like~~, Another difference in our approach is I try to avoid analogies as much as possible, because I feel like I always bring, what am I smuggling in with my analogies? I'm always worried about. So I don't, I'm not trying to pattern match on any social network or anything else, but I do think there's   
(01:43:29.918) ~~a~~, there's probably a couple of bets where I go back and look at the transcript on this. Market concentration is one. Some of these just, I'm not sure quite how we would formalize the extraction. I think we're talking about two~~.~~(01:43:41.581)  
So I think we're talking about two things when we talk about models. 


---


#### 01:43:44.622

One thing is~~,~~(01:43:45.402)  
is language a general reasoning fabric that goes out of distribution? I would say the answer to that is no. I think that converges to Bayesian learning, and so it's very useful, but I don't think the language stuff about it. Then the other one, the one you're talking about, to me converges on simulation, which it can learn the laws of physics in ways so we don't have to do the empiricism, so it can do that, but it converges on simulation. It's like the next step in simulation. These are two very different things. I don't think one that it's good at learning simulation is going to generalize. Just because you're good at biology doesn't mean you're good at playing chess or flying an airplane   
(01:44:22.559) ~~or~~ (01:44:22.639)  
  
(01:44:22.658) ~~something~~ (01:44:22.918)  
  
(01:44:22.939) ~~like~~ (01:44:23.078)  
  
(01:44:23.118) ~~that~~. 


---


#### 01:44:24.039

And so in neither of these cases, the language one nor the biology one, do you have your reputed AGI. You've got something that's good at next token prediction. You've got something that's good at learning distributions. And that's where we're at.   
(01:44:38.597) ~~Yeah~~, there might be a dollar on can those things be put together and still do and do both of those in one system, perhaps as well. The language one can call the other one. I just don't think that changes.   
(01:44:51.327) ~~Yeah~~, I even mean like same weights like genuinely integrated.   
(01:44:55.729) ~~Yeah~~, I literally think this whole problem comes down to simulation. And maybe it's just because my simulation background, like the only way to simulate the universe is to be the universe. 


---


#### 01:45:06.457

Like it literally comes down to the universe is a big computer that's simulating itself. It's basically string theory. And that's the way that it~~,~~(01:45:11.940)  
the reason we can't even simulate like the three body problem. It's just, we don't have the compute resources to do that. You'd actually end up impact~~,~~(01:45:19.005)  
  
(01:45:19.064) ~~like~~ (01:45:19.225)  
you need so many compute sources impact whatever you're trying to impact anyways. And it's like, we can continue to chip away at the problem, but it'll never~~,~~(01:45:26.829)  
it'll never be cracked. I honestly hope that's right because I'm a little bit afraid of what happens if it gets fully cracked. That's probably a great place to leave it today. I appreciate a very collegial, interesting, constructive conversation and I look forward to formalizing one or more $1 bets. 


---


#### 01:45:45.197

All right, I'm going to think about it. I'll send you an email if I have a good idea of what we're going to do. Cool.   
(01:45:50.020) ~~I~~ (01:45:50.041)  
  
(01:45:50.060) ~~think~~ (01:45:50.202)  
these things are great. I would love to do a couple of $1 bets. So to be continued, but for now, Martin Casado, general partner at A16Z, thank you for being part of the Cognitive Revolution. Thanks   
(01:46:00.711) ~~so~~ (01:46:00.831)  
much. This was great. 


---


