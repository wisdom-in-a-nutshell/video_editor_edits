#### 00:00:00.129

Apologies for being so slow there. I had my little studio disassembled to take it to the Google event this week in case I had an opportunity to record there and ended up not recording anything. I had to put it all back together. I would have passed, but for the fact that my brother works at Google, so I had a chance to visit him and stay with him for a couple of days. And also   
(00:00:25.048) ~~Logan, the guy previously from OpenAI, who's now at Google,~~(00:00:28.250)  
invited me and I wanted to say yes. He invited me last time to the OpenAI one as well, and I had to decline for whatever reason. So I wanted to say yes eventually. 


---


#### 00:00:36.790

It definitely would have gone if it was in the New York offices. They put on quite a show. Maybe we'll get to this. It's certainly not the most important topic, so I'll save that question for a little bit later.   
(00:00:47.435) ~~Let's do it.~~(00:00:47.837)  
Zvi Moschwitz, welcome back to the Cognitive Revolution. It's always a busy week when you're here, and I don't think that's a coincidence. We've got a lot to cover. I'm very confident it's not a coincidence. I thought we would maybe organize this   
(00:00:59.801) ~~sort of~~(00:01:00.042)  
as you do your blog posts, start with   
(00:01:02.723) ~~kind of~~(00:01:02.965)  
the mundane utility and what we're excited about, and then get into maybe what we're not so excited about, and then maybe get into what we're worried about. 


---


#### 00:01:10.731

And I think we've got some good topics for each of those sections this week. How's that sound? Oh, yeah. Yeah, that's definitely the right things to do. And there's definitely coverage all around. Cool. Or not cool, depending on the section we're talking about. Let's start off with then, obviously, busy week with OpenAI making some launches, Google making some launches slash announcements. I guess both of them really making launches slash announcements, which is one of the bits of analysis that I want to get into. But   
(00:01:41.167) ~~I guess~~(00:01:41.367)  
for starters, what stood out to you as the new capabilities that are most exciting to you as somebody in search of an incrementally better life? Yeah, so there's always a difference between what they're promising you're going to be able to do in the future and what you can actually touch now, what you can use now. 


---


#### 00:02:01.736

So what you can use now is GPT 4.0, and that is fast, that is half the price if you're using the API, and that is multimodal in various ways. It seems like a great product. It had its weaknesses that are becoming apparent as I use it, but it's already pretty exciting. And then when it becomes fully multimodal, it starts to play into the modalities that they're talking about where you talk to the AI, and it talks back, and it hears your tone of voice, and it hears the cadence of your expressions, and it sees your face, and all that stuff. That looks pretty exciting in the relatively near term. They said a few weeks for that. 


---


#### 00:02:42.252

But I do think this is bearing the lead compared to all of the integrations that both companies are promising. That to me is the big exciting thing if they can be delivered. It's what about these universal assistants? Because the demos they actually gave us were lame as hell on these capabilities. Can you recognize what I'm seeing in this picture? Okay. Yeah. I already know you know how to do that. Why is this exciting? Did you remember what was happening a minute ago? Yeah, you can. Okay. Yeah. Again, I knew that. And it was that   
(00:03:14.765) ~~over and~~(00:03:14.966)  
over again, these demos of educational videos with trivial exercises where the kid is being intentionally dense and already knows the answers and then is doing a good job of play acting being actually lost. 


---


#### 00:03:26.870

And then the AI doesn't pick up on it and just pretends to bumble through. You have a bunch of translations of very trivial statements that I'm not saying I could translate them without knowing the languages, but it's close.   
(00:03:41.611) ~~So on.~~(00:03:41.991)  
  
(00:03:42.853) ~~And~~ (00:03:42.973)  
those are some open AI demos, but Google's demos were similar, basically, in these areas. But then we're talking about the things where it's, okay, search my inbox for all of my receipts, put them into a spreadsheet, organize all of my expenses and all of the things that have been done by category. So do all of this automatically forever. I told you to, on a background continuous basis. All right, now I'm more interested, right? 


---


#### 00:04:09.460

The context of integrating all of this information, right? Open AI thinking it's going to live on your phone, Google thinking it's going to live on your phone. And it's shaping up potentially to be another Android versus iPhone battle, right? Because Google is going to own Android by default, and it's looking as if it can deal with Apple. Yeah, it does. There's a really interesting point there on the demos and just how   
(00:04:31.971) ~~sort of~~(00:04:32.351)  
slapdash some of them were. The OpenAI ones,   
(00:04:35.754) ~~I think they obviously make,~~(00:04:38.115)  
they're trying to make a point on some level of this is, you can tell it's very real and like definitely not prerecorded and staged, although you could always still fake it, but I believe that their demos were real in real time. 


---


#### 00:04:49.543

And there were a couple of things there that the AI came back with where it was like, Yeah, I didn't think that was maybe exactly what they were hoping it was going to say, but they're at least showing off that this is real now and we have the confidence to do it live on stage for you. But across both companies, it did feel like the demos were a little bit of an afterthought and What you make of that, is that   
(00:05:13.584) ~~like~~ (00:05:13.766)  
a, it'd be like, it's a reflection of the sort of ideological motivation behind the development in a sense, where they're like pushing the capabilities and then figuring, we'll figure out what to do with it after, which has obviously been something Sam Altman has said explicitly in various ways over time. 


---


#### 00:05:33.120

But it's still a little striking to me that they were so, okay, we just made this thing now. Hey, can anybody come up with something to do with it on stage? We gotta be on stage in 10 minutes, it felt. Oh yeah,   
(00:05:42.125) ~~well~~ (00:05:42.266)  
Mira speaks Italian, let's translate. That is cool, but it did feel rushed. And   
(00:05:47.829) ~~I guess~~(00:05:47.990)  
the other answer would just be like, they're rushing, they're racing, and so they're taking these things out of the oven and putting them right on stage without much time to really figure out what to do. It's just that these capabilities are new. They haven't had them for very long. It also suggests to their credit that they're not optimizing for the demo. 


---


#### 00:06:06.004

They're not putting lots and lots of effort into putting forward the best demo possible. If these were startups, if these were scrappy little companies trying to use this to raise money, you would have seen dramatically different demos that would have looked dramatically more impressive from both companies. But they deliberately chose not to put in that effort. So you don't have the same effect of, I am very confident this is the best possible thing you could have shown me. I am very confident that if Google or OpenAI had tasked a bunch of people with, okay, take the day, take the week, figure out something much more impressive, try it 20 times, confirm that it always works, get it ready, 


---


#### 00:06:40.925

I think they would have gotten something that would have wowed us a lot more. I have to assume these were hastily assembled, not well thought out, not robustly tested. These were closer to real than you would normally see, but it still speaks to, they didn't try to make this interesting in some important senses. But also that could be a lot of, who are you talking to? doesn't even know what GPT-4 is. They're used to three and a half as what it is, and they barely even spent five minutes on it, and they have no idea what's going on. And the horse can talk at all. And that's actually really impressive, because until a year or two ago, the horses didn't talk. 


---


#### 00:07:22.468

Yeah, I have a few candidates for the things that excite me most. You hit on one already, which is the integration into the whole platform. For Google, that's obviously always been where they're going. And with OpenAI, it's more suddenly it seems where they're going, although I'm sure they've been planning for that for some time as well. But all of a sudden you get the desktop app, now you can connect your Google Drive, or at least again, that's coming soon or in some phase of rollout. That does seem super exciting as our friend or let's say inspirational figure Tyler Cowen says, context is that which is scarce. And that has definitely been a feeling I've had with language models all the time. 


---


#### 00:08:04.740

It's both traditionally that or earlier language models that might've meant you just only have so many tokens in the context window. Obviously, that problem has been largely solved with Google going from now one to 2 million tokens that they're bringing online. That's a lot of tokens. You can fit a lot in that. But now you still have to figure out,   
(00:08:23.211) ~~well,~~ (00:08:23.411)  
how am I going to get those tokens? What tokens am I going to get? If you're copying and pasting a million tokens around to do a single use case, now you've got a lot of friction there as well. Plugging into these different sources, whether it's your Google Drive, or your Gmail, or whatever, that seems like it is going to be a tremendous driver of value. 


---


#### 00:08:43.485

We've always talked about these giant Kinex windows, and they're very useful when you know you need them. You're sitting down to analyze a big bunch of data, or search for a big bunch of data, but it is much slower. It is substantially more expensive for Token to do it. What we're seeing is Gemini Flash, right? We're seeing things that are optimized for speed, things that are optimized for cheap, and things that are optimized to run locally and just do all of these things in a way that the customer wants. And Google is also giving us a 2 million contacts window, which is very useful. But how do you combine these things? And a lot of the key is going to be   
(00:09:25.818) ~~like,~~ (00:09:25.958)  
how do you go through all that context to figure out what the right context is? 


---


#### 00:09:29.200

I've always thought that Tyler Cowen saying context is that which is scarce is saying two things at once. It's saying the whole, you don't have enough context. You're always short on context. But also the definitional thing, right? Whatever it is that is scarce is the context. And so in some sense, once these details of your situation become background common knowledge, that's not context anymore in that sense,   
(00:09:52.129) ~~right? your life, right?~~(00:09:54.871)  
I don't think about that as context when making my decisions. I don't think, well, in context, he likes French toast. No, he just likes French toast. It's just a fact about me that currently these companies don't know, but presumably these companies would know. when I'm destroying absolutely everything and they're taking an input. 


---


#### 00:10:14.541

So other things that I think are candidates for the most impactful aspects of these launches, one is the kind of freeness of it all. OpenAI moving to make the latest model free for everybody is a big deal. And Google seems to be doing something similar. It's not entirely clear exactly what they're going to be rolling out at what price point in what product. They have their Google Advanced, which is not free, which has been required to get you the most advanced models to date. But it's unclear if that will be required as a subscription to get the Gmail assistant that's coming or whatever. So we've got some   
(00:10:53.284) ~~some~~ (00:10:53.424)  
clarification questions, I think, still for them. 


---


#### 00:10:56.206

But This does seem like a moment where, obviously, OpenAI has almost the sort of Kleenex brand in AI thus far. And most people who have tried something and weren't super impressed with it tried ChatGPT with a 3.5 default. Now they are going to get a major step up. How do you think that will change the landscape, the world of work in the immediate term, just on the basis of now everybody can use the best model? Yeah, trivial inconveniences, like you have to pay a bit of money, definitely hold back a lot of people. And it's clear that from all sides, they're realizing that owning the customer is so much more important than the trivial cost of compute for these lightweight customers using a bit of it. 


---


#### 00:11:44.078

Like chat GPT is pure profit for a user like me, right? I'm not actually requiring that many tokens per month, right? I'm paying $20 a month to have it. good as that's part of my job, and I want to make sure that I have a variety of different things to try against each other. But in practice, of course, they pay a dollar in inference in a month, 5% cost to provide the goods. On the margin, that's shockingly high. So it's not very expensive to give these people the free version, except in so far people cancel their subscriptions because they no longer need anything but the free version. I do think it's going to be an issue if they count on that revenue, but I don't think they count on that revenue particularly. 


---


#### 00:12:22.951

And of course they get the data every time someone puts something into there and that's incredibly valuable. So that's another way you own the customer. So yeah, it's the future. I think Google clearly is indicating that in the same way that you've used Bing the entire time to get access to GPT-4, but nobody bothered because it was a weird, different interface and people didn't notice. Google,   
(00:12:43.284) ~~I think,~~(00:12:43.585)  
has realized that for most purposes, Pro 1.5 is in fact the appropriate model for most use cases that people want to do. They don't need what Gemini Advanced is offering. But they will offer the upgrade to Gemini Advanced to people who want to pay it. And most people won't pay for it, and most people won't need it. 


---


#### 00:13:02.035

But the Pro 1.5 is, for most purposes, plenty good enough. When you're trying to compare Pro 1.5 to Gemini Advanced, It's not entirely obvious that it advances all that in some sense, like much better than it in practical purposes for many of the things you want to do.   
(00:13:20.395) ~~Like~~ (00:13:20.535)  
ProFormula5 is very good at searching documents, for example, which is the main thing that I still want Gemini 4 exclusive to other things, but I'm not integrating with other Google services. The voice and the real-time vision also definitely, this is where the demos were flat. I thought this was true, really, in both cases. The OpenAI ones were maybe a little bit better. At Google, I went out to the Google I-O event, and they had quite a production, right? 


---


#### 00:13:50.702

They have the amphitheater, which they don't actually own, I learned. But it felt like they owned it for the couple of days that we were there, because they had set up basically a whole little city on this plot of land with massive, almost permanent, but still temporary structures for all their different sections, web and Android and AI. There was a whole AI building, and you go in there and they've got all these different AI demos. And the one was Project Astra and you go into a little booth and they have a camera mounted and it's looking down at a table and then it's okay, here's a another sort of bin, another table full of junk. 


---


#### 00:14:29.412

Now you can put the, pick a couple pieces of junk, put them under the camera and talk to the AI about it. So you've got like a toy dinosaur and you're like, what's this? And it says, it looks like a toy dinosaur. And you're like, okay. And   
(00:14:42.097) ~~it was,~~(00:14:42.479)  
again, it's going back to this. I'm not sure that they really took this anywhere near how useful it could be. And it does feel like the sort of thing where it's, yeah, we made this and now we've got to show it off somehow. And we're leaving the rest of the work to. the community to do, which I have mixed feelings on that. I don't mind it in some sense. 


---


#### 00:15:03.953

It's an enabling technology and so fine. I do feel like   
(00:15:08.119) ~~that~~ (00:15:08.259)  
those things should have been a little bit better thought through or certainly could have been a little bit better thought through. It does feel a little ideological in some ways. And   
(00:15:20.928) ~~I guess~~(00:15:21.690)  
I just also want them to have a bigger, better idea for   
(00:15:25.735) ~~what it is,~~(00:15:26.155)  
where they're trying to take us. We're all   
(00:15:27.678) ~~kind of, everybody is,~~(00:15:29.581)  
at least everybody in technology, right, is   
(00:15:31.283) ~~like,~~ (00:15:31.403)  
looking forward to these announcements trying to figure out what are they going to do and how's it going to impact me and what can I build on it or is it a threat to me and then they just give you this stuff and it's like the homework isn't really done. 


---


#### 00:15:43.296

I've been saying recently that the scarcest resource is a positive vision for the future and I just I feel like I want more of the like tangible vision for where they are taking us than just, here's a table full of junk, you can put it under the camera and the thing will recognize it and you can talk about it. Even though it was impressive, they had another Pictionary mode too, which was an impressive demonstration. You saw some of these things   
(00:16:07.278) ~~in the Google~~(00:16:07.759)  
in the Gemini 1.5 Pro original announcement where they showed a hand drawing of a thing and said, where did this happen in the movie? And it was able to cross-reference this to a scene. 


---


#### 00:16:18.808

That similar experience was on display. People were drawing little line drawings and saying, what movie does this represent? Somebody drew in the little demo group that I was in a Wilson with the handprint on it and asked what movie this was. And it didn't get it at first. And then they drew a island around it and a tree and put a couple of waves. And it was like, oh, I get it. It's Cast Away. And so it definitely is a very impressive and quite capable technology, but it does feel like the electrical wiring has been created before we have any idea what we're going to plug into it. And it is just a very strange dynamic to me. 


---


#### 00:16:58.910

I should go back and read more. I wonder how much of a vision folks like Edison had for what electricity was going to do. In that era, merely lighting the room at night was a revolution, so probably didn't need to have too much more than that. But I don't know. How do you feel about this sort of, build it and they will come, everybody else can figure it out? I'm a parent, right? And so what this reminds me of more than anything else is the kind of thing you do as your kid. You're like, do you know what this is? It's a toy dinosaur. What's out the window? You ask them basic questions. You check their knowledge. 


---


#### 00:17:31.442

You fill in gaps. You see where they're strong and weak. You're building up muscle memory. You're building up components.   
(00:17:36.846) ~~You know they'll need those components to then do things that are more impressive to synthesize them, right?~~(00:17:41.691)  
The idea is,   
(00:17:42.231) ~~well,~~ (00:17:42.432)  
if you know what everything in the world is, if you can recognize what people are gesturing at in various ways and do other things, This then allows you to do things that actually matter later, and it's also just evidence of intelligence, evidence of comprehension, and that's all they're trying to demonstrate there in some sense. It's not the final exam. It's not the job. The job comes later. Yeah, these are not use cases. These are not things you would actually do. 


---


#### 00:18:11.589

It's a fun little game to have it play Fictionary and try to guess the thing, and it's okay. What is the vision of the future? The vision of the future seems to be the universal assistant, right? The idea is that you tell it what you want it to do or what information you want it to tell you or anything like that, and it does what you want informed by all of this context, right? In the same way as a really smart, knowledgeable assistant who's gotten a lot of experience,   
(00:18:43.104) ~~if you would,~~(00:18:43.544)  
too. go from there, right? What do you do with a person?   
(00:18:48.549) ~~Well,~~ (00:18:48.650)  
whatever you want, right?   
(00:18:50.550) ~~Like~~ (00:18:50.691)  
as far as they got. So how do you, yeah, it feels there was so much talk of her this week and leading up to the events that I found that very striking. 


---


#### 00:19:00.836

I haven't gone back and watched that movie in a while. I don't know if you have or how fresh that that story is in your mind, but would you read that movie as a positive vision of the future? I've heard I've seen takes across the spectrum from people being very excited about it to people saying it's a dystopia. And then others were like, well, it seems like it's more in the middle. How would you read that story, first of all? And then I'm interested,   
(00:19:24.528) ~~too,~~ (00:19:24.708)  
to hear, do you think this is where you personally are going to go? Are we all going to be walking around with her in our ears. Meta's got their, we just heard that they've got an earbud that has a camera in it in development. 


---


#### 00:19:39.577

So that seems like an interesting form factor for the her future. Anyway, start off with how do you read that story? So from what I can tell,   
(00:19:48.705) ~~right, like it's not,~~(00:19:49.906)  
it's good in the sense that it's not a dystopia and it's not a utopia, right? It's not pretending that this is all one thing or another thing. It's saying, here's a scenario, Here are some things that might happen. Here's some speculations as to how some aspects of that might go. And it's up to you, the viewer, to determine, is Phoenix better off in these ways or worse off in these ways? Is this a healthy or unhealthy way to go about doing things? Is it good or bad for his personal growth? 


---


#### 00:20:19.111

What is this alien alternative? What does this do to society? And then ignoring the ending, because as usual, the actual big picture ASI safety takes are terrible. But if you just look at the concrete, mundane questions it's raising,   
(00:20:34.041) ~~I think,~~(00:20:34.902)  
from what I can tell, it does a good job of not judging, in every sense. It's saying, here are some things that we expect to happen. And my reality is something like, if you're treating the AI like it's a person, developing a relationship with it, and getting emotionally invested in it, and especially if you're treating it like you might treat Scarlett Johansson, in some sense. And this is not practice. This is not reps. This is not training. 


---


#### 00:21:02.269

This is not experiments. This is like real for you in that sense. It's not good. That's not good for you. And it's not good for society if everyone else is doing it too. It's going to make it much harder to get real connections with other people in that sense. If you're using it as certain different thing, if you're using it as   
(00:21:20.281) ~~like~~ (00:21:20.582)  
a source of advice, a source of information, a coach, a sounding board,   
(00:21:24.525) ~~like there are,~~(00:21:24.964)  
by testing ground. There are ways you can use this that are clearly good, that are obviously good. Using it as a tutor to teach you new things is obviously just great. How could you possibly think that was bad? 


---


#### 00:21:37.013

But there are obvious ways that you should go, wait a minute, this is not healthy. This is   
(00:21:43.757) ~~like~~ (00:21:43.977)  
taking our evolutionary instincts and then putting something on there that is mimicking the thing that's not supposed to be the thing. And it's bad in the way that pornography is bad. It's not that you would never ever use it, necessarily, or that you would want to ban this, but too much of this is bad for you. You have to touch grass. And the warning of her, again, if you just don't worry about banning too much and look at the concreteness, the warning is that this can substitute for other things. And then if you let that go too far, that can be bad for you. 


---


#### 00:22:20.119

And the promise is that this is actually really cool and exciting in other ways and has a lot of money and utility. And the question is, how do you balance that? And this is not a new problem to AI,   
(00:22:29.453) ~~right?~~ (00:22:29.554)  
We've had to deal with this with a lot of new technologies, like mobile phones, social media, or the latest ones. The one that I keep coming back to more and more is television, actually. Partly because it's now, with social media and phones, we have this current big debate, as it's happening right now, and it hasn't played itself out, we haven't made the full adjustment. With TV, we're at the end of the road, right? We've moved on to the next thing. 


---


#### 00:22:50.422

We've seen what it did. We've gone through generations. We can look back and ask ourselves, what happened? How did we deal with it? What did this do to us? And the more I think about it, I think that's a situation where it's not like they made all these warnings about how horrible this was going to be, and it turns out everybody was being dumb. It's a situation in which everyone made these dire warnings about how horrible this was going to be, and they were just correct. And it didn't destroy our civilization. We're still here. But the things all just happen. The impact on people that we're worried about, they just happen. And we now live in the world that results from that. 


---


#### 00:23:28.541

And it turns out that was like eminently survivable. And we've gotten richer enough and better enough in other ways that it's okay, mostly. But one thing you notice is that basically everywhere that got ubiquitous television is now below replacement level of fertility. So these things can be extremely unhealthy. What do we do about that? So how would you characterize the impact of TV? I obviously heard the fertility point.   
(00:23:52.858) ~~I guess~~(00:23:53.098)  
my general sense would be that it has just given us such a ready solution for our boredom that there's   
(00:24:01.220) ~~like~~ (00:24:01.359)  
just fewer passion projects, fewer people developing new quirky hobbies, fewer people just chasing random stuff because it's just easy to go there for a bit of entertainment. 


---


#### 00:24:12.323

Is that your mainline narrative as well? I think it ate massive numbers of hours in a day and it made activation energy so much more expensive to get. It made gumption much harder to have. It substituted for a lot of other things that led to good places that built up and had increasing marginal returns to devoting your time and energy. And it taught us to be couch potatoes, right? Passive consumers of information just sit there. We consumed massive amount of advertising. We forget now in the internet age where we're pissed off that   
(00:24:43.574) ~~like~~ (00:24:43.733)  
our 30 minute show has three minutes of highly skippable advertising on our podcast. It used to be that   
(00:24:49.361) ~~like~~ (00:24:49.500)  
your 30 minute show was 22 minutes a show. 


---


#### 00:24:51.804

It was a horrible ratio, like 25% ads. Nowadays, I watch a sporting event and I make sure to start late if it's not a huge event because I can't bear the idea there are commercial breaks. This is so bad. But that's a correct reaction if you have the option. Why would you want to endure that when you can just be slightly delayed? It's not the Super Bowl, is it? If it is, then you're stuck. Also, the ads are great. But most of the time, the ads are terrible. built their lives around this. They built their schedules around this. And they stopped planning things with people, like the whole bowling alone phenomenon. I think people underestimate that how much it was just television. 


---


#### 00:25:30.269

It was just, there's a show I can watch. There's a thing I can do. And so I think sounds like a lot of work and requires coordination and requires an activation energy. And I don't have to,   
(00:25:42.654) ~~right?~~ (00:25:43.015)  
I can just stay home. And now it's Netflix, of course, right? or whatever, any number of similar things are. And I watch still today, at least on my television. People think it's over, but it's not over. And am I better off for it? I think so, but I'm not at all convinced. The other thing is you used to sit alone with your thoughts, right? You used to be willing to be bored. And now we have so many different things. 


---


#### 00:26:08.445

It's not just the television. It's not the phone. It's now you can listen to a podcast. You can listen to it. music anytime, anywhere. You can do all these other things. You can just scroll endlessly. And so it's been supercharged. And all these things matter, and all these things change us. And having an AI at our fingertips all the time, again, even in this big sort of mundane utility world,   
(00:26:27.278) ~~right?~~ (00:26:27.479)  
It's not particularly dangerous, doesn't really transform everything. It's going to change a ton of things. And it doesn't necessarily mean we're going to choose to do smart high-level things with it. One thing that's striking about all the character AI style things out there is they're freaking dumb. 


---


#### 00:26:46.596

They spend a ton of time customizing the experience so that you get the type of interaction you want, the kind of thing that's satisfying to people, but they keep not investing in highly capable models to power those interactions.   
(00:27:03.718) ~~They're ridiculously stupid.~~(00:27:05.980)  
So often, if you look at what people were reporting back, or if you tried them out in my experience, you'd be lucky. You're not just not getting free GPT-4, you're often getting below three and a half. You're getting something that's abysmal. Yeah, that was really striking in my testing of Replica. And it's gotten significantly better, but it is amazing how, I'll never forget the line from Eugenia, the CEO there, she said, going back even before language models, they started this project with the idea of addressing loneliness. 


---


#### 00:27:38.527

And she said, we knew we couldn't make a bot that could talk. but we thought maybe we could make one that would listen or could listen and could make you feel heard. And she was pretty open about the fact that they use pretty simple tricks. And she used the term, not my term, her term, parlor tricks in the early days to just make people feel heard. And I'm mixed on that. I feel like that's another one of these things where it's good for some, but it potentially becomes a big problem when it gets to be too good or too broadly adopted because they have research out that shows that, and again, this is done over a year ago, before the current generation of language models, which they haven't even fully implemented, last I checked. 


---


#### 00:28:22.955

But already, their product was helping people reduce suicidal ideation, helping people who need reps get reps. People, by and large, did report that they were more socially engaged as a result of having this outlet or   
(00:28:37.381) ~~kind of~~(00:28:37.621)  
practice ground, training ground, whatever, however you want to conceptualize it. So it seems like it's   
(00:28:41.442) ~~like,~~ (00:28:41.583)  
even in that limited form, it's good for this sort of pocket of people who are really struggling. But then I do wonder, similarly to what you're saying like how does that change life if it becomes really good and compelling to people who are not really struggling but could do other things and maybe now don't do other things so much. I wonder how you think about   
(00:29:02.191) ~~the~~ (00:29:02.411)  
so if TV just happened then maybe this is just going to happen too right we're all going to be walked around with this thing in our ear and it'll be it was striking how responsive the voice interface was they talk about between two and 300 milliseconds latency to respond and that is like   
(00:29:20.684) ~~the~~ (00:29:20.924)  
that is normal conversational interactive response time. 


---


#### 00:29:25.848

When we go edit our podcast into script, like the normal break between words that they, if you have a long silence, they'll take it down to 300 milliseconds as their default for   
(00:29:36.692) ~~kind of,~~(00:29:37.012)  
you had a gap, but you want to just shrink that gap to make it sound like it never happened. 300 milliseconds is what they have. So if it can respond in less than that, it's definitely going to be in this sort of similar zone to normal interactive conversation. I guess I do wonder what forces are going to shape this. It does seem like it probably matters a lot. what the business model is behind it, or who's doing it, and what they're trying to do   
(00:30:05.252) ~~for us,~~(00:30:05.932)  
to us, with us. 


---


#### 00:30:08.232

An advertising model is very different, perhaps, from a subscription model. People obviously have also got sucked into social media. There was a time, which I think it's honestly still there to a significant degree, although maybe it's preceded some, where everybody was optimizing for time on site, and you had this, what can we do to make this person come back? What is the most engaging thing? And it became probably harmfully engaging, not just to individuals, but to society. I do think Zuckerberg, to his credit, has tried to back off of that. I'm not sure if other platforms have, but certainly they've taken some public steps to   
(00:30:45.385) ~~like~~ (00:30:45.566)  
not reward anger emojis, for example, which I think is a nice, clean, low-hanging fruit win, but still got to give credit where it's due for taking that. 


---


#### 00:30:53.609

Do you have any sort of like tree in your mind? What do you see as maybe the forks in the road for how this new kind of ubiquitous technology could develop depending on what our relationship is to it? And   
(00:31:10.641) ~~you can imagine,~~(00:31:11.221)  
I always used to say too, and I think I might have to stop saying this, but ChatGPT, I always appreciated that the branding was not trying to be your buddy. ChatGPT, a name, it doesn't sound like a friend. And the way it talked to you also was not like a friend. It never sends you proactive notifications, which some of these virtual friend things do. Replica will send you notifications on your phone and say it misses you and it wants to talk. 


---


#### 00:31:36.460

ChatGPT was just like, here's your answer. Let me know if there's more I can do to help. But it's not like baiting you into further interaction. Claude, interestingly, does a little more of that where it'll ask you like, what do you think at the end of its responses in a lot of cases, especially if it's   
(00:31:50.307) ~~like~~ (00:31:50.426)  
more philosophical or open ended. So I guess there's related questions there around like, How do you expect this to develop in terms of how much the technology will try to pull us in and try to captivate us versus maybe try to push us out into the world? And how do you think that will depend on who's developing it and with what business models? 


---


#### 00:32:12.699

Yeah, so these are not the branching parts of the universe that I worry about the most. Obviously, I'm worried about bigger things, but the stuff matters too, especially if the bigger things don't materialize. I think you hit a lot of the nails on the head, especially the subscription versus advertising versus time on site optimizations. So what metrics they're aiming for, I think, are a gigantic question. And then the question of will people be able to differentiate and select based on what is good for them, or will they be largely falling into these traps? where they follow paths of least resistance. They get tricked into entering Skinner boxes. They develop these kind of fake emotional attachments. 


---


#### 00:33:01.499

They use it as an excuse not to engage with the world instead of using it as a tool with which to learn how to engage with the world or to better engage with the world. Every time I write a weekly post, I'm constantly filled with ideas either that other people have had or that I have myself, or often both where I'm playing off of them and they're playing off of other people. How could we use this as a tool to make our lives better, not through interacting with it constantly, but for using it to enhance our choices and our actions? solve our trivial inconveniences, solve our knowledge gaps, skill gaps. Again, give us the reps in the ways that are relevant. 


---


#### 00:33:44.739

Let us practice. Let us try stuff out. Let us do all the boring coordination. Let us not get interrupted is,   
(00:33:50.806) ~~I think,~~(00:33:51.026)  
one that's going to become a bigger deal. about how to track all of these logistics and all this paperwork and all this stuff that's expensive to us, where humans effectively can only do so many things, even though the amount of actual stuff going on inside those things is often very small. AI can solve a lot of these problems for us, but we have to want it to, we have to care about that, and it has to be a thing we prioritize. I'm really hoping for a subscription model. I think one of the best things that OpenAI did was go completely advertising-free, completely steer the customer free, as far as I can tell. 


---


#### 00:34:26.976

There's this huge amount of revenue and ability that they've just given up entirely. And instead, all they're trying to do is answer my query, charge a flat rate, or charging the API for the marginal cost providing it is, and that's it. And as a result of that, that has become, for many top people, a standard,   
(00:34:46.458) ~~right?~~ (00:34:46.599)  
Like you've learned, oh, you can't just become a miserable person. It's the difference between a AAA, buy once, carefully crafted experience game that's trying to do something good for you, which I think is a wonderful thing and people should do more of it than they do, if anything. And the gotcha games and the mobile experiences and the free-to-plays, which are designed to hook you and trick you and trap you and exploit the whales that can be exploited and makes everyone else's life miserable to make sure they can exploit the whales. 


---


#### 00:35:18.755

And that stuff you just don't want to touch for the most part. They happen to create an experience that's good enough that if you are bold enough and confident enough in your own self-restraint, you can go into the lion's den. But mostly, you see these mechanics, you should run. You should run as fast as you can. And it's the same thing. Chat GPT is inherently a very healthy product. And I'm not worried that people are going to do something bad for them on a personal level because they use too much Chat GPT. I haven't even heard a single story. Same thing with Claude. Same thing with Gemini. These companies are being very responsible in the ordinary sense. 


---


#### 00:35:56.862

Character AI with Replica, they're not.   
(00:36:00.284) ~~This is the,~~(00:36:00.784)  
Replica is very obviously the predatory free to play, trick you into coming back, like message you when you're away, give you your daily reward or various types.   
(00:36:11.090) ~~Like~~ (00:36:11.231)  
every time, oh, you can get more free messages by waiting and then come back for them.   
(00:36:15.713) ~~It's like,~~(00:36:16.094)  
I know where this comes from. I know about delayed variable rewards. I know daily login bonuses. I know all the tricks. I was in the gaming industry. I know how this stuff works. And when you see that stuff, you've got to run. And so the question is, if we don't regulate, if we don't mandate, because it's a free society, we really shouldn't be regulating and mandating this stuff, right? 


---


#### 00:36:37.371

There's bad incentive to drive out good and there's good to drive out bad. Who wins this fight? And if the bad guys win, in this sense, it's going to be a pretty miserable time for the people who get trapped. But my hope is that the people with the best models are presenting good experiences. And that can continue, and we can scale that. And the other question is, what happens with the advertising model? And where is the advertising? Because right now, advertising is clearly delineated. It's clearly distinct from the experience. But with a large language model, It seems very easy to say,   
(00:37:13.317) ~~well,~~ (00:37:13.476)  
we're going to give you a free experience where you can ask about various different products that you might want to compare. 


---


#### 00:37:20.701

But if you just want a little bit of priority in what's discussed and which features are highlighted, and whether we talk about Coke or Pepsi or we talk about Tide or one of their competitors, you're welcome to pay us. What happens in that future where suddenly this thing is suddenly pushing stuff on you and you can't necessarily tell the difference and the labels are increasingly convoluted?   
(00:37:41.356) ~~And like, you are the target and you are, you know,~~(00:37:45.219)  
if you aren't paying, you're the product, right? In that sense. And   
(00:37:50.181) ~~like,~~ (00:37:50.420)  
we can reach one of those worlds. And then maybe you even have to get your second AI to watch what the first AI is doing. If you're smart, you can notice when this thing is happening. 


---


#### 00:37:59.043

You can double check when they're saying things that are biased and all this gets complicated. I don't want to get too far away from the news because we have so much to get through.   
(00:38:06.947) ~~Well, I do think this is really interesting.~~(00:38:09.688)  
The analysis, many people have covered the news, so I think where we hopefully can add value is with a little more synthetic analysis. I do have one word of defense for Replica in that, having gotten to know Eugenia a little bit, I do think she is really trying And they do have a subscription model, although they do have these add on buy a new outfit for your replica type things going to. I think that research, which I do take as reasonably credible, it came out of a group at Stanford and they weren't directly involved in it other than handing over data, as I understand it, is pretty compelling. 


---


#### 00:38:49.545

I do think they're headed into a very challenging, balancing act of a future. One thing she said to me that was really interesting, and I do see that this is like another dynamic I'm going to ask you to analyze. On the one hand, they started with relationships and basically no mundane utility. And then with ChatGPT, we have pure, isolated, episodic help on whatever you're looking for help with, but no relationship. And I was struck that I heard from her, actually, a couple of months ago, last episode we did, that she is moving Replica toward more of an assistant. And she said something that I thought was really interesting, which was maybe the moat in AI is relationship. 


---


#### 00:39:33.840

That you don't swap out your friends because a new, better friend, potential friend came along. You have some sort of loyalty to your friends, like the history matters. What you've been through together matters. And she was, we're going to become more of an assistant. But we still want to have this notion of relationship. And maybe that is where the moats in AI come from. That's why maybe people don't switch at the drop of a hat long term. Chats GPT, on the other hand, is headed that direction a little bit too now it seems where you've got certainly the voice is like much more given to relationship and it's like developing memory and it's obviously going to have longer context and all these sorts of things. 


---


#### 00:40:13.436

So these maybe seem to be converging. I wonder how you think about the ethics of chat GPT, like going more in the buddy direction, if this is something you think, and we have like a wave of departures from OpenAI to talk about too, I wonder to what degree you might speculate as to how this move toward the like more engaging personal relationship possibly could have been one of the things that the safety team might have objected to. Certainly, I would have been like, hey, do we really want to go this far this fast? Can't we keep the voice just a little flatter? Okay, cool. It's responsive, but does it also have to be so thirsty like in our first release? 


---


#### 00:40:49.405

But then there's another dynamic too that I think I would, if I'm reading into what they're doing correctly, Altman's now saying, well, we want to get into not safe for work content. We want to be able to do some of this stuff, even gore, even like erotica, whatever. But we don't want to be making deep fakes. And I'm like, on the one hand, you might say, well, why would they be doing that? That seems just, why would they even want to touch it? On the other hand, you might think, well, Maybe they think they can do it more ethically than other people can, or maybe if they do 90% of the stuff that people want, it'll take the air out of the sails of the sort of totally unscrupulous actors that would be doing your, if you can get. 


---


#### 00:41:31.972

NSFW AI from OpenAI that's like R-rated, maybe you don't need like Taylor Swift deepfake porn that's X-rated or something. And maybe they think, hey, we can be like the good guys that do this racy stuff, but stay on the right side of the worst ethical lines. There's a lot there. convergence to buddies? And how do you think they're maybe thinking about   
(00:41:54.880) ~~like,~~ (00:41:55.039)  
how they play against like other actors that they may consider to be worse actors? So I'll start with the last part. And   
(00:42:03.266) ~~I will say like, I wrote~~(00:42:04.166)  
I wrote out   
(00:42:04.766) ~~my my model specs analysis~~(00:42:06.768)  
for OpenAI. But because everyone at OpenAI is gonna be hella distracted this week, and everyone else as well, 


---


#### 00:42:12.891

I'm just gonna hold it for the week or two, and refine it and adjust it and then post it later. But One of the things I have in it is, in fact, an endorsement of not safe for work. You have to ask, is this actually harming anyone? Is this bad for people who are not the user? Or is this so obviously just terrible for the user every damn time that as a free society, we need to say no? And as far as erotica or gore, the answer is just obviously not. People have this desire to be thirsty. They have this desire to be horny on Maine. They have this desire to be playing in these ways and getting these experiences. 


---


#### 00:42:56.021

And if you don't let them get it at home, they'll get it on the street. You can't shut this stuff down. It's just part of the human experience. And   
(00:43:06.208) ~~yeah, I think~~(00:43:06.829)  
absolutely if somebody wants to create pornographic images that are not deep fakes of individuals who have not given their permission, then why shouldn't we do that for you. Given we have verified you're an adult, maybe we've charged you extra, because why not, maybe? But why shouldn't you be able to do that? If you want to create a picture with a bunch of blood and gore, why shouldn't you do that? They recognize with Sora that if you can't do sex and you can't do gore, you can't make most of the interesting things that people do with video. 


---


#### 00:43:36.905

If you're going to draw the line at PG-13, a lot of them are just out the window, especially if every individual element has to be very carefully curated and gatekept and so on. It's just not going to work. And   
(00:43:46.650) ~~so,~~ (00:43:46.730)  
yeah, I think that you draw the line specifically at inappropriate portrayals of actual individuals. But you don't want to make an obscene deal with people. That's a problem. You should catch that. You should stop that from happening. And there's a certain level of difficulty you need to impose on that before someone can be allowed to do that. But you have various different tricks to defend yourself against it. But I think if you want to either create a pornographic deep fake, of no specific person that just plays to whatever it is you want to see an image of. 


---


#### 00:44:21.639

Or if you want to create a picture of Taylor Swift wearing a different colored outfit, singing in a different venue that she never visited for her Aeros tour, as long as it's got a watermark, what's the problem? You want to make a picture of her singing at your son's birthday party and have fun pretending that she was at your son's birthday party? and you play her music, right? Because you can just record that and hit play. That's   
(00:44:43.347) ~~like~~ (00:44:43.487)  
kind of the service. What's so bad about that? Why is that a problem? And obviously different people have different opinions and maybe each celebrity should decide like what level of reproduction is okay and not okay. But certainly like the situation where like Barack Obama can't be generated by an image model, like giving a speech at the wrong college. 


---


#### 00:45:01.579

Like it's just like daunting to me. And again, it's going to force me to use a different image model for those purposes. And how long is it going to be before those free image models are just trivial to set up and trivial to use and everyone starts using them? I have to be able to fusion on my computer, and I tried it out for a bunch of stuff. One thing that's very good for is you tell it to create 100 copies of something, and then you come back 30 minutes later, and then you check, just scroll quickly through them and see if any of them have what you wanted, maybe you go image to image on that one, and maybe you figure out   
(00:45:37.045) ~~what,~~ (00:45:37.244)  
maybe you just use this to diagnose, okay, here's what's wrong with my prompt, and try a different prompt. 


---


#### 00:45:42.447

And you can't do that if you're using   
(00:45:44.086) ~~the free,~~(00:45:44.807)  
the internet services, because that would cost them an arm and a leg. Now I'm using my own GPU, so it's fine. Or alternatively, use that to create stuff that they wouldn't let you create, because you want to see what you can do, and you want to create it, maybe you want to enjoy it. So I moved on to a basically, it's become, especially obsolete that I just use the online ones now, but I've been waiting for Stable Fusion 3 to be easier to deploy and I'm going to go back to it and see what the changes can do. In general, yeah, I think they should offer the services that you want. 


---


#### 00:46:13.239

I don't think it should be this thirsty by default. I think this idea that suddenly you have Star Wars Johansson acting all flirty with you, like when you just ask what the weather was outside with no indication you wanted thirstiness, that's weird. I don't think that's ideal. I think we went too far. But if you put in your user preferences, you dial up the thirstiness tone,   
(00:46:33.733) ~~you select,~~(00:46:34.253)  
you want the Scarlett Johansson style voice, I don't kink shame. It's fine. Enjoy yourself if that's the statement you enjoy. And again, you should be very careful with consent and very careful with permission when there are real people involved. But I think a lot of real people will be very happy to give their consent either for free or for a very nominal charge. 


---


#### 00:46:57.266

You ask me if someone wants to use my voice for some ungodly unknown reason on StatGPT, if it's just a handful of people, I'd say go ahead. If it's like 100,000 people, I'm like, I should be paid. But it's about reasonable compensation here. And I'm sure a lot of other people feel the same way, including people whose voices someone might want to hear. Are you telling me I shouldn't just pay an extra dollar and get the Morgan Freeman voice for all of my narrations of my explanations? That sounds cool. Why shouldn't I do that? Or maybe I want Scarlett Johansson. Again, if she gets paid, that's fine too. And then there was some other stuff that you were asking about that I've almost forgotten at this point. 


---


#### 00:47:41.297

What I am thinking about is, and it is, there's a lot of issues that are intertwined here, but there's a couple dimensions of convergence, actually. There's like the convergence to the buddy form factor. And then I also see at a lower level, and we're going to have an episode,   
(00:47:58.628) ~~I think we're going to put this one out ASAP.~~(00:48:01.090)  
And then next week I'm going to have Alex Albert,   
(00:48:03.913) ~~who's the,~~(00:48:04.393)  
he was a prompt engineer and I was leading the developer relations function at Anthropic. And I was asking him about how all the APIs are converging too, and how they think about that in the sense that it may create a winner-take-all dynamic. So on the one hand, 


---


#### 00:48:25.427

I'm like, man, I don't feel great about sort of the trend toward relationship first in general. I do feel pretty good about it for   
(00:48:35.059) ~~like~~ (00:48:35.199)  
the people that are really struggling with loneliness. I'm not sure how I feel about it when it gets to like a super broad based thing. I'm not sure how I feel about OpenAI being so thirsty with their voice. But then if I go down one level, and I look at the API, and I see convergence there, and it's a one-line switch, possibly with a little bit of prompt engineering, definitely with a little bit of prompt engineering, but it's close to a one-line switch just to go from GPT-4 Turbo to Cloud 3 Opus to back to GPT-4.0 to Gemini 1.5 Pro, whatever's next. 


---


#### 00:49:02.983

That, to me, is a worrying dynamic in that If all of the integration points have converged to essentially the same service provided in the same way with a down to the one line change, then it becomes a winner take all dynamic in the   
(00:49:23.663) ~~like~~ (00:49:23.762)  
whoever has the best model in theory can win all the business very quickly.   
(00:49:27.105) ~~Like~~ (00:49:27.244)  
why don't we just switch to the new one? But then maybe one reason that could not be so a winner take all with one release to the next could be this relationship dynamic. So I'm like not sure I like what it does to society if we're all like in these parasocial relationships, but maybe I do. What it does to soften the super intense borderline winner-take-all dynamics. 


---


#### 00:49:55.251

I'm not even sure to what degree those exactly overlap, because one is the app layer and the other is the API layer that powers the app layer. But what do you think?   
(00:50:04.297) ~~It's the opposite.~~(00:50:04.978)  
I think you're thinking about this all wrong. First of all, you have more than one friend, and I have a lot more than one friend. I choose which friend to call based on what we want to talk about and what we might want to go do. Sometimes I have a group of friends, sometimes I have one, sometimes I have a different one. I would get bored if I was hanging out and my best friend's name is Seth. If we hung out every day, all day, and tried to talk about everything, 


---


#### 00:50:31.186

I'd be like, no, I don't just want to hang out with Seth all day. But so I have a lot of people too. And   
(00:50:37.391) ~~this switching,~~(00:50:38.990)  
this means that you are not locked in, right? You're not stuck. So quite the opposite of winner take all, right? It lets you move around. And even Character AI and Rokoka understand this. You can talk to 10 different bots, right, for different purposes. And of course, at minimum, you want to figure out, you have this one for when you want to talk about engineering, this one for when you're thirsty, this one for when you just want to talk about restaurants in the area. blah, blah, blah, which makes perfect sense. 


---


#### 00:51:06.197

And ChatGPT has its GPTs, and now Gemini is going to have its Gems, and so on. And then you have this thing where you can plug into a different LLM. And so for me, I have changed my primary LLM reasonably recently from GPT-4 Turbo to Gemini to Clod, and now back to GPT-4-0. And in many cases, on a dime, it doesn't mean I don't use the other ones. It means that   
(00:51:31.771) ~~Like~~ (00:51:31.952)  
when I want a variety of answers, I'll call all three of my friends and I'll ask them in some sense. And I'll ask them, hey, what do you think? What do you think? What do you think? I'll compare notes. And I know who's good for this, right? 


---


#### 00:51:42.423

I want to analyze a PDF. I want an image. I know who creates this kind of image better. And so I play with portfolio. And then as people advance, you learn, okay, so then he's captured more of my portfolio for now, but then that switches back. And I want this variety. I want these optionality. And maybe I don't necessarily even want these things to know all about me, certain aspects of me when I do different things. Also, data portability is probably going to be a huge thing. If you're at Replica, you build up this relationship. But are they going to deny you access to your records and data? That's a pretty toxic thing for them to do. 


---


#### 00:52:15.358

I assume they won't. What happens when you just download the chat log, upload the chat log into Gemini 3 or whatever it is? and say, you're this guy now. Here's everything that you've said to me, and here's everything I've said to you, and he learns all about you now, and now you have your new assistant that knows what your old assistant did. Often you don't switch employees because you don't want to have to train them to explain all the context with them, but now that context is just copy-paste. That's like a huge, I don't expect Winner to take all from this, unless the winner deserves it. It's like DVD five is out, and everyone else is still at four level. 


---


#### 00:52:50.298

Well, of course, I'm just going to 90% of the time plus be using chat GPT, that's GPT-5. But if it's a marginal change like it's been recently, where just like one of these things is a little bit better, that's completely different. It's particularly the lock-ins and the relationships that I think are the places where you're likely to get a potential winner-take-all scenario. knows the contents of my Gmail, knows the contents of my Sheets, knows the contents of my Docs, is creating workflows where it generates new documents and new memories, sees my photos, analyzes my photos. I start taking pictures of all my food so it knows what I've eaten every day, where I've been, et cetera, et cetera. 


---


#### 00:53:25.068

It has all of this new context. It would be very difficult to transfer this context. Now suddenly I have this huge thing where every time anything is remote and personal, I want to use Gemini. And that becomes a thing. But I don't think that in general I'm seeing this lock-in unless, of course, the people develop this kind of emotional attachment,   
(00:53:44.697) ~~right?~~ (00:53:44.856)  
It may be romantic, maybe it's something else, who knows. But yeah, I'm basically saying that going too far is not particularly healthy,   
(00:53:55.384) ~~right?~~ (00:53:55.623)  
I think that's bad. I think people should strive to avoid that, except under pretty special circumstances, like beyond what it takes to get reputation reps in practice and stuff like that. 


---


#### 00:54:05.833

But I'm optimistic. The convergence seems like a friendly thing, right? How amazing would it be if you could transfer your stuff and your connections and your knowledge between social networks, between Android and iPhone, between all these other things that have converged but are seemingly incompatible? Compatibility is just almost always good. Yeah, it seems good for the consumer. I do wonder about how in a world where Google had their date on the calendar and then OpenAI does their launch one day. in advance, it does seem like there is a potential for exacerbating the general, one of the biggest worries in AI is that the, and I know I'm not telling you anything you don't know as well or probably better than I do, but the arms race dynamic between either countries or even just leading companies within the United States, where hey, we got to get this out. 


---


#### 00:55:01.137

We got to beat them. You have X time to do your testing and we're launching and that's the time you have. And so corners start to get cut, perhaps. This feels like the liquidity with which people can move about the market would enable that dynamic or encourage that dynamic. Because if OpenAI is like, about to launch, whoever, it doesn't matter. I don't need to be super specific with who's going to launch what in a hypothetical, but one party is going to beat us. We got to get ahead of them. Now you have this. And that sort of depends on the idea that if they're out ahead of us, then everybody's going to switch to them and we're going to be screwed for a while. 


---


#### 00:55:37.871

And that could be screwed in revenue. It could be screwed in data. It could be even just pride and reputation, which certainly seems to be a big part of what's happening at the moment. You don't worry about that. To me,   
(00:55:49.802) ~~I'm still,~~(00:55:50.141)  
I'm convinced by your multiple friends argument that as long as there are different things in the same ballpark and they have different characters, you go to different things. I do personally find that as well.   
(00:56:00.570) ~~Like I'll,~~(00:56:00.871)  
I still, if only because ChatGPT has the native coding environment, even if Opus is a little bit better on coding, like the fact that It can execute the code on ChaiGPT's side, keeps me there for coding. 


---


#### 00:56:12.061

Now it actually with O probably is notably better on coding as well. But Opus is a better writer, so I go to it for help with writing. So I do have my different AI friends for different purposes at the retail consumer level. But in my app building, it is more of a, I'm going to make a choice,   
(00:56:30.300) ~~right?~~ (00:56:30.500)  
If it's Waymark and I'm going to have a language model or a vision language model, process all the images of a small business that's just uploaded their library, we're probably pretty much going to just do it with one model. And in practice,   
(00:56:43.664) ~~like,~~ (00:56:43.885)  
you can switch really easily. That dynamic doesn't seem like a big concern to you still? 


---


#### 00:56:51.347

Well, it can go both ways, right?   
(00:56:53.547) ~~If you,~~(00:56:53.907)  
if switching costs are low, Then me announcing a day before you doesn't really matter. So what you get my customers for a day and then I switch back. Like who cares if it's incredibly expensive?   
(00:57:06.668) ~~Well,~~ (00:57:06.847)  
then if you can get ahead enough to convince me to switch, I'm not stuck. And so maybe you get incentivized to try and capture that market a lot more, or maybe you're trying to outdo each other continuously to try and say just a step ahead because we don't take a lot, but. My guess is there'll be enough differentiation between what features these different tools have and in what details you want to use and in how you customize them. 


---


#### 00:57:30.108

Like I've put a decent amount of work at this point into customizing my instructions for chat GPT because they have custom instructions and I haven't done that for Gemini or Cod yet, but like when they offer that and I decide to do this time, I will almost certainly do that. And they might look very different because I'm trying to specialize, right? They might be   
(00:57:47.275) ~~like,~~ (00:57:47.414)  
okay, so when I go here, this is what I want. So it's a different set of instructions. But as an app maker, you're not going to want to switch every time there's a little incremental improvement, right? So it's not the same thing. Look, winner take all, in some sense, from a safety perspective, is ideal. 


---


#### 00:58:04.445

If OpenAI or Anthropic or Google was out there two years ahead of everyone else, and their model was just better, and so for all dangerous purposes, they were the only ones that mattered, then there are advantages to that. I don't think that the interoperability is going to change whether or not that turns out to be true in an important sense. I don't think it substantially changes necessarily the pressure we're putting on people in that way. But also, one of the things about this last week that was why I really enjoyed this week on the release side, even though there's other things that I didn't like as much, obviously, was that everybody now seems to be focused on things that help people have a better experience. and improve the world, but that don't advance the core intelligence of the underlying system. 


---


#### 00:58:57.588

So to me, that's just win, right? I want chat GPT to be better for its users without becoming more intelligent in the ways that make it more existentially dangerous, right? As long as it stays in its current state and just gets all these new cool features, that's just the best in the world,   
(00:59:15.443) ~~right?~~ (00:59:15.704)  
I love this world. And so I was really heartened to see the directions people were taking it and see these new developments. And I can wish everybody the greatest success. Obviously, it built a certain amount of revenue and hype and pressure, but   
(00:59:27.431) ~~I think~~(00:59:27.650)  
mostly that's already built in. except that that's just like the world we have to live in. So why begrudge? 


---


#### 00:59:33.873

Why not just have a good, don't threaten to have a good time at this point, right? Let's just keep going. Yeah, I'm with you on that. I think my standard line on GPT-4 and all these kinds of GPT-4 class models is we're in this sweet spot where it's,   
(00:59:45.981) ~~sorry, I'm just fixing my background.~~(00:59:48.063)  
My standard line with the GPT-4 class models is we're in this sweet spot where it's powerful enough to be really useful, but not so powerful as to be a real concern. And I do think that's great. And I do think all these integration points are great. the enhancements of just, as you of course famously call it, mundane utility are great. It doesn't feel, and I hear the argument too about, yeah, if somebody's way out ahead, then they have the luxury of doing it right, taking their time. 


---


#### 01:00:12.061

It doesn't seem like we're in that world to me. I do expect OpenAI is probably still about half a generation ahead internally, My guess is that they don't make the best model free if they don't have a significantly better one coming before too long that's going to get everybody paying again. That could be wrong, but that would be based on all kind of rumors and Intel and just how much time has elapsed and public comments from Sam Altman. It does seem like they probably still have one additional turn that Google may not have made yet. But overall, it does feel like they're close enough where they're like running generally speaking, neck and neck. And that part does concern me and Meta is like not far behind either. 


---


#### 01:00:57.210

They just put out a paper yesterday where they had trained a new model called Chameleon, which is very natively multimodal. They call it early fusion. meaning text and image tokens are encoded in the same way, basically share all the same weights, trained with 10 trillion tokens, which is not a small amount. And unclear if they're going to open source that yet, but same week as all these other kind of natively multimodal things that they're answering and definitely just showing that you're not going to stay in front of open source for too long, even if you are ahead by a full   
(01:01:33.936) ~~generation,~~ (01:01:34.536)  
generation and a half, you're going to have to keep going.   
(01:01:38.099) ~~I guess I'm just not convincing you or,~~(01:01:39.518)  
but I'm still not, 


---


#### 01:01:40.579

I'm still not convinced the other direction either that I shouldn't be worried about the liquidity at the API layer, the ease of switching and the sort of sense that may create where we got to one up these guys. I do see that dynamic   
(01:01:54.989) ~~kind of~~(01:01:55.269)  
developing and it does seem problematic. It is. Basically, what I'm saying is I'm not sure these things intersect that heavily in terms of the correlation is close to zero, not that it's going the other way or anything. These things seem to me to roughly cancel out in the sense that that same pressure is always going to be there. OpenAI and Google are going to try and one-up each other. They're going to try to advance the models as fast as possible. 


---


#### 01:02:18.494

They're running into a lot of the same constraints. My assumption is indeed that OpenAI has somewhat of a lead here. The fact that OpenAI seems to be quite responsibly taking their time to build the next thing, whether or not they have a choice, we'll see what happens. But it's really hard to tell who's out in front. I think Google has a history of essentially being far out ahead of what Google actually releases. Google has had a significant number of pretty earth shattering for the time. capabilities in the past and it decided that their lawyers should carry the day and they should not release the products. And so it's not obvious to me that Google has to be behind it all. 


---


#### 01:03:02.307

Right. It's very opaque from the outside. You can't tell. six months to a year ahead or something like that. If OpenAI is not six months to a year ahead,   
(01:03:13.476) ~~it would seem to be more likely~~(01:03:15.557)  
because Google also has another half or full generation more than they've shown, not that OpenAI doesn't. My assumption is that OpenAI, things don't take that long to train as such necessarily yet. It's more like they need to get into a position where you're ready to do that, but you have the resources to do that. you have all the data streamlined and you're ready to go. And it's really hard to know, obviously, any of this for sure. It's also possible that there's a big gap between when you have it internally in the base model form and when you're ready to actually release a product. 


---


#### 01:03:49.983

I speculated also that it's possible that there's a GPT-5 style model that either could train or have trained, but that inference is so expensive. that they think it would degrade the user experience to rush it into market. So they'd rather do this instead, because for most purposes, that's not actually what people want in some form, or they need to scale up their infrastructure. Cause if they have to choose, they have to either serve GPT-5 to a small number of people or serve GPT-4-0 to a very large number of people. And maybe they think that it's better for them if they serve the second one. I don't know. We just have a hard time in the long run. 


---


#### 01:04:22.614

Like who has the advantage? Who has the better infrastructure? Who has the better teams? Who has the better capabilities? Again, there are people who know much better than we do, and we'll find out. Yes, in an important sense, I wish it wasn't close. I wish I knew who it was who was in front. And they could, in some senses, take their time, much more so. And I definitely see the pressures that seem to be building of throwing caution to the wind and just doing some stuff. But again, shouldn't caution be thrown to the wind outside of some certain key moments?   
(01:04:55.833) ~~Like,~~ (01:04:55.994)  
when are the moments when you should be cautious, right? So   
(01:05:00.235) ~~like,~~ (01:05:00.414)  
GPT-2, we saw some caution. for OpenAI, even if it wasn't enough for Dario. 


---


#### 01:05:07.077

At least GPT-3, again, we saw some caution. We got some dispute over how much caution was warranted. GPT-4, we saw a lot of caution with the release. It seems pretty reasonable to not particularly be that cautious about 4.0. If you've looked at it, you're like, no, these extra modalities don't really do anything. It's not dangerous. We know that because we've tried a lot of stuff with GPT-4, and it's not like it's close. basically fine, and then here's a normal commercial situation, and they were up against any number of deadlines, but you've got the Google I.O. thing coming the next day. You're potentially asking Ilya and maybe John Leike to stay on until after this presentation, so that doesn't distract. 


---


#### 01:05:47.592

So you've got potentially a bunch of things going on. Let's come back to those departures in a second. I want to talk a little bit also about broader market competition. Our mutual friend Andrew Critch coined this term, the big tech singularity, or I think he calls it the tech company singularity. But the notion here,   
(01:06:07.175) ~~I think~~(01:06:07.335)  
is an interesting one, where he basically says, at some point, we may hit a point where the big tech companies, because of   
(01:06:15.139) ~~their in virtue of~~(01:06:17.594)  
their superior AI and compute capabilities, could reach such a position of dominance where they could effectively enter into any industry that they want to compete in and come to dominate it in relatively short order. 


---


#### 01:07:17.610

And then you can imagine them just plumbing MedGemini into kind of everything, ripping out the sort of nightmarish electronic health records that currently exist, putting something in that would hopefully be a lot better, tapping into all this data. They've been able to do these MedGemini things I would assume with huge barriers to data, this would dramatically reduce their barriers to data, would allow them to... I'm sure, of course, they'd still have to handle compliance in some ways, but they could dramatically streamline their access to data, train the next generation of medical models, provide med GPT as   
(01:07:50.483) ~~kind of~~(01:07:50.664)  
frontline service. And it would seem to me like they would pretty quickly be the best hospital system in the country if they did that. 


---


#### 01:08:01.501

And then I imagine replicating that across other verticals, education, as we've discussed. Obviously, these things hold tremendous promise for education. The key thing seems to be like, actually building it in a sort of first class way, which I'm not sure current institutions are going to do very well. And Google might struggle with it in some ways, too. Again, they could buy   
(01:08:21.280) ~~like~~ (01:08:21.501)  
any number of private education companies for a pittance from their perspective and just run a loss leader experiment to see what they could do in a new vertical like this. Why do you think that's not the case? If you think that's not true? Because I think they're not culturally capable of doing the things you have to do. to make that work. 


---


#### 01:08:43.989

I think that their reputational and legal and regulatory barriers to doing that are very serious. And   
(01:08:52.372) ~~I think that~~(01:08:53.052)  
they're not going to be able to deploy technology in those situations that is that far ahead of what people who are simply paying them to use big tech technologies could get in their place. OpenAI is not going to be able to use GPT-5 for its hospital system substantially before GPT-5 is released. That's not really a thing. The version of, I forget what their alpha med or whatever it is that they're calling it, the version that Google is going to put out there is going to be the same version that they have internally, but it's also going to be available to everybody else if the people want it. 


---


#### 01:09:29.850

Yes, it gives them better data to do this, but they could also get that through a licensing agreement through a sort of cooperatory deal. And there's a reason why, basically, companies like Google do not put themselves into every nick and cranny, try to do everything and every feature. There's a reason why we see these demos and we're like, you have no idea what your products can do? Are you not trying to do anything with your product? Are you just leaving that to the rest of us? And the answer is yes, they really are, in some important sense. They are trying to develop their core systems of products, make their thing available, let someone else build off of it, let someone else take the baton, let someone else handle these things. 


---


#### 01:10:02.087

I saw a few weeks ago a story about AI private equity. The idea is this thing but without being Google. You go out there, you find a company that's obviously in an AI business, doesn't know it's supposed to be an AI business. You buy the company. You then tell all the employees to use AI. You deploy the AI systems. Everyone gets much more productive. The company goes up in value. The company makes a lot more money. You win. You then use the profits to keep doing this. And that seems like a much better model to me. So does being a startup, whose job is to raise a lot of money in order to launch this hospital thing. 


---


#### 01:10:38.801

They either work for existing hospitals or maybe use a private equity thing and buy them out. I don't know. Maybe do both. But these models strike me as more what these people are capable of. But   
(01:10:48.515) ~~like,~~ (01:10:48.676)  
Google is terribly scurrisic in so many different ways. And my understanding is so is Microsoft, so are the other big tech companies in their own ways. You can't simply have them launch an entirely new thing and do an entirely different thing and expect that to just work in this sense. Now, obviously, at some point, if you have GPT-9, You can tell GPT-9, go out and buy a hospital system and deploy your technology to help the patients do better and make more money, and GPT-9 just handles all of it because it's GPT-9. 


---


#### 01:11:21.105

But that's not the most important thing that's going on with the fact that you have GPT-9. You bury the lead if you can do that, right? It's very important senses. So as long as we're still dealing with these mundane levels of AI, my expectation is that The big tech companies, they only have so many imperial focus points. They only have so many different things they can do at once. It's not in their interest to do this. It's the reason why Google is constantly buying startups. And I don't think that's going to change. Let's talk about the startup side for a second as well. I feel like I am hearing less about startup acquisitions lately from the big tech guys. 


---


#### 01:12:01.136

Obviously, we had the Microsoft-Aquahire inflection, but compared to a number of years ago, it does seem like there's a lot less. aqua hiring going on, and certainly there is bureaucracy at these companies. It does seem like they've managed to clear some of that out in the Gemini department, at least. There has been an interesting discussion on that just in the last 24, 48 hours on Twitter, where among other people, Sholto, who was locally famous for his appearance on the Dwarkesh podcast said, yeah, basically nobody cares about levels at Gemini. Elsewhere in Google, yes, but in the Gemini thing, it's a very sort of flat, we're all   
(01:12:38.591) ~~kind of~~(01:12:38.831)  
one team trying to make things happen vibe. 


---


#### 01:12:41.413

If you are a startup, I was going down my list of previous Cognitive Revolution guests, and just asking, like, who is feeling enabled by all these new things and who's feeling threatened by all these new things? And I'm like, I think There's a significant majority of them that seem like they are more likely to be threatened by it, if we're honest. I do think in the short term, probably still everybody grows because, and this is for some of the reasons that you're saying, that Google's not going to productize everything immediately. They're not going to go to market with the same sort of intensity or focus that startups will. So I'm bullish on everybody's next couple   
(01:13:21.815) ~~to~~ (01:13:21.935)  
few quarters still. 


---


#### 01:13:24.198

But I do look at the economies of scale that I see developing in the big tech companies vis-a-vis the startups that are building on the platform. And I'm like, man, I don't like the beyond two to four quarters position for a good chunk of them. because they're always going to be a little bit behind on the models, of course.   
(01:13:47.253) ~~Inside, they're going to know what they have.~~(01:13:48.715)  
  
(01:13:48.814) ~~Inside,~~ (01:13:49.074)  
they're going to be picking off what are the biggest opportunities for us. The outside, of course, you're waiting. Inside, you can also do it for free. Exactly how this is going to be positioned and what you have to pay for is to shake out. But one thing that's definitely clear right now is GPT-4.0 is free for all in chat GPT. 


---


#### 01:14:08.088

It's not free. They've lowered the cost, but it's not free via the API. So right off the bat, as a startup, you're like, how do I exactly compete with that? I have to pay for it, which means I have to charge for it, which means I just have all this additional friction. compared to somebody can just go to chat GPT and use the thing directly there. And again, just the economies of scale just seem, man, who can really compete with these guys? We've had folks on the show who specialize in text-to-speech, who specialize in image generation, and just GPT-4.0, once its full capabilities come online over the next couple of weeks, seems like it's likely to be best in class at both of those potentially. 


---


#### 01:14:51.323

What do you make of the position of the companies that are building on the platforms right now? So Altman,   
(01:14:58.148) ~~I think,~~(01:14:58.488)  
had very wise things to say about this, right? The idea that if you're building your model of your company, you're building your company, and you're building its tech, because GBTN isn't smart enough, isn't good enough, and you're going to build a substitute that's going to, for now, do a better job, then we're going to steamroller you. We're going to curb snob you. it's here to die. But if you're planning, here's an apparatus to use intelligence such that when GBD5 comes in, you switch the four to a five in the function call. And now suddenly your product just works way better. 


---


#### 01:15:35.158

And you're not directly trying to do the thing that the base model can just do. Now you're in amazing shape. And in fact, you have a great company that's going to be happy when OpenAI makes progress, right? So the question is, can you plan for something where you're happy when OpenAI comes up with a new model and not sad, or without loss of generality, obviously. So if you're 11 labs and you have amazing text-to-speech right now, then yeah, you have to worry a lot about whether OpenAI is going to curbsop you because you are fundamentally competing directly with them to offer a technological service that they are going to want. And either you find a niche, say you're willing to replicate Scarlett Johansson's real voice and OpenAI isn't, or something like that. 


---


#### 01:16:18.222

But otherwise, yeah, they're eventually going to have something better and you're going to have to somehow keep up with them and that's going to require being big. That's because that's a generic ability that OpenAI is going to develop anyway because it needs it for its core products. But if you are trying to be a private equity company that buys out hospitals in order to streamline their medical records,   
(01:16:37.435) ~~well,~~ (01:16:37.996)  
now you're like, okay, cool. You have the latest AI to speed into these things to be better. Cool. Hand it over, Google. Hand it over, OpenAI. And you plug it in and your product works better. and you're that much better at the competition than you were last month when they didn't have an AI at all and you had GPT-4 and now you have GPT-5. 


---


#### 01:16:54.341

It's amazing. You're just that much farther ahead now. And so also there's no conflict between there are going to be a lot of amazing companies. We're going to all die before that because it'd be some really cool companies. And the current companies are often not going to be the cool companies. It's very possible that every two years, half the existing companies die. in this realm because they got curb stomped because the thing they were trying to do no longer differentiates them from the core model. The core model was not able to do that, but the other half do well. It's the tech bubble in 1999, right? Where, yeah, a lot of these companies turned out to be doing something that had no underlying value after the internet developed, and it didn't differentiate them. but also do a portfolio of them. 


---


#### 01:17:38.292

Part of that was Amazon. So you made money anyway. So who do you, could you point to specific companies that you think are like well-positioned? Cause I honestly struggle to list too many that I think are going to be like, Oh yeah, this is playing how we, to our advantage, how we hoped and puts us in a long-term winning position. I don't find like a very long list, honestly. Yeah. The problem with, I think you've mentioned this before is that AI is this huge beat. And so you have to choose where you're going to specialize. And I've made a deliberate choice, mostly not to follow the details of various startup projects and small projects that are offering liquidity in this way, because they're not that important to the bigger picture. 


---


#### 01:18:20.579

And it's very hard without investigating carefully to have value-added things to say about them. So it's okay, I'll see what happens. So I can't say which ones I think have great market prospects necessarily, but I also can't say which ones are necessarily going to get curb-stomped either. And a lot of it depends on the internals, right? I have no idea what Perplexity is actually doing under the hood. I don't know to what extent they're training their own models. I don't know to what extent they are working off of GPT-4 or Cloud or whatever it is right now, but then adding custom instructions and then scaffolding to try and allow it to   
(01:18:56.176) ~~much,~~ (01:18:56.315)  
much better analyze the web and come out with very rapid, good answers. 


---


#### 01:19:00.917

And in a sense, there's like a version of Perplexity that gets curb-stomped. in the version of budget that's potentially going to do very well, right? It all depends on a lot of details. But when I look at what are the companies whose tabs I have open, what are the companies whose stuff I use, and I do not use those things because what I want is what the base model is offering. I don't use character AI either,   
(01:19:22.024) ~~right?~~ (01:19:22.225)  
But think about that as an example,   
(01:19:24.386) ~~right?~~ (01:19:24.506)  
If you're a character AI,   
(01:19:25.988) ~~well,~~ (01:19:26.247)  
if you were just counting on the fact that open AI is not going to be thirsty, It's not really a great long-term plan. If you're counting on, we're going to give a bunch of tricks that make this thing feel like human interaction because you're a replica, because the models aren't good enough to simulate real human interaction on their own, that's not going to be sustainable either. 


---


#### 01:19:48.251

But if you're building your AI bot such that when you make it smarter, it provides a better experience, but you know all these things about how to build relationships that open AI doesn't know and is never going to find that. You could be in a great spot. There's nothing stopping you,   
(01:20:03.806) ~~right?~~ (01:20:03.966)  
And in theory, that's actually get much, much better. And it should especially get much better when you start adding video generation and virtual reality,   
(01:20:13.088) ~~right?~~ (01:20:13.248)  
Which is where I assume is the future of that sort of thing, two years from now on the line. And if you're ready for that, great. So it just depends on, again, are you trying to be future proof? 


---


#### 01:20:23.306

Are you getting where the fuck is going? Or are you fitting the fact that like right now there are some flaws that you can better address and you can go to market. But there's also room for people who sign brightly for a year or two and make a bunch of money and then fade away. That is honestly what I've considered any sort of new entrepreneurial endeavor right now. I'm like, I think you're right to cite Sam Altman, and I think that analysis is right. But I'm like, man, that's pretty tough. I can't identify too many things where I'm like, oh, yeah, if I do this, then I'll be like much enabled by the next model, but I won't be threatened by it. the majority do seem right now like they're in this kind of compensating for weaknesses, trying to go to market, and they have to, right? 


---


#### 01:21:07.518

Because if they don't compensate for the weaknesses, they can't really go to market successfully. If they don't go to market successfully, for most of them, aside from a few that can maybe raise enough money to really make a long-term play and not feel rushed to market, but that's obviously not a super common position to be in, it just, it feels tough. So when I've considered entrepreneur,   
(01:21:29.052) ~~I haven't,~~(01:21:29.292)  
I don't think I'm going to start a company in the short term, but when I have considered that I've felt more drawn to this, what would be like a flash in a pan?   
(01:21:37.121) ~~You know, that would be,~~(01:21:37.962)  
that meets a need right now that could grow really fast. 


---


#### 01:21:41.085

That maybe doesn't make sense 18 months, two years from now, but which could be like a good winning short-term sprint. that, sure, inevitably gets burned up in the supernova of the foundation models. Even if your original tech gets burned up in the supernova,   
(01:21:56.494) ~~right,~~ (01:21:57.034)  
by learning a lot about the business and your customers, and by building relationships with your customers, you might be able to entirely turn this over into something that still makes sense. And there's a lot of places where getting it right, making it easy on the user, because we talk about all these custom instructions, all this flipping between different models and different companies. and all of these sophisticated power user things that we're doing. 


---


#### 01:22:21.265

And we've got to remember that most people aren't even using these models at all. And people who are using them at all, most of them are incredibly unsophisticated. They're mostly complete civilians. And most people will never touch a settings button in their life on any of the apps they use. No matter what options you give them, no matter how valuable they are, they will never learn what they are. This is not how people work. So if you can just do some basic good customizations and set up good defaults and handle some regulatory issues and just make life easier for people in a way that slots in the technology, and then you have a bunch of scaffolding to actually have it be better in various ways, 


---


#### 01:22:58.493

I think that's really promising. I was talking to one of my friends who's a lawyer, and his firm is encouraging them to use their own internal customized but like very lightly customized. It's like some basic service. Lawyer GPT that they   
(01:23:12.780) ~~like~~ (01:23:12.940)  
can use internally, but they can't just use the basic thing because of compliance concerns, but also because like they wouldn't necessarily know exactly what to do. But there's enough like annoying individual steps on it that he ends up just not bothering. Because by the time he actually   
(01:23:25.470) ~~like~~ (01:23:25.670)  
imported the proper case law and hooked up the proper documents, he could have just done it by hand. So throughout the time, he loses most of the utility from it. 


---


#### 01:23:33.274

Most of the time he ended up skipping it. It seems you personally could probably within a month, code up something that would supercharge this capability and probably increase the entire firm's production by 10%, maybe 50%, certainly if you had a small team. And then, again, if you built it properly, when GPT-5 comes out, you just switch the 4 to a 5 in the reference call, and your product just gets better. So there's the alternative, but you still can't use the alternative and still doesn't know enough to properly do the alternative. And then, yeah, when it properly integrates into your entire workflow and Google's buddy has fully automated into, okay, now it has access to every document in the world automatically. 


---


#### 01:24:07.225

You're still going to want this kind of specialization and you're going to use the profits you made early on to do the specialization thing. If you buy the hospital, you're going to be able to integrate into the hospital's workflow, you're going to gain the trust and ability and education of these doctors. You're going to make it easy on them because they are not tech people, and they do not want to have to think about it. And they want something that feels right and safe and good to them, and the regulators are breathing down your neck. And there are so many things like this where a professional's time is so incredibly valuable. And getting the answers right a little bit more is so incredibly valuable. 


---


#### 01:24:40.988

But you don't have to be that much better in certain ways. Again, you can be 10 times better in the interface. You can be 10 times better in just the ease of onboarding and usage and regulatory compliance. And then you have some scaffolding, but yeah, sure, you just bargain whatever the last model is. That seems fine. I think there's plenty of things that   
(01:25:01.759) ~~like,~~ (01:25:01.939)  
they're not future proof if you don't change your code, but like they're future proof in terms of if you're planning ahead to make them work. So you said you liked the business model of the private equity AI reboot for companies that don't realize they should be AI powered yet. Do you like venture capital right now? 


---


#### 01:25:19.996

Would you want to be an LP in a generic Silicon Valley venture fund at the moment? Venture is a weird situation because your compensation as an LP in a venture capital firm is not tied to your expected returns that highly. It's a question of, look, if you offered me a big enough fund to go invest in companies and we set aside ethical concerns for the moment, with potentially advancing AI in the wrong ways, or we were confident we could only choose companies we were confident were neutral about, or whatever it is. And you gave me a big enough portfolio to work with, and enough of a share of the profits. And of course I'm excited. 


---


#### 01:25:58.326

Do I expect VC to make money above market returns? I think that's a completely different question. And that is a, now we're talking price, right? We're talking about whether or not there's too much money racing into AI, or there's not enough money racing into AI. And then also the question, of course, of how much are the insiders and the high reputation and well-connected people just getting differential access and positive selection, and you're missing out on all the deals that are worth it. And so you're just left with the dregs. And unless you're really good at picking up from the dregs, even if the industry does well, you're going to lose. But yeah, I'm currently advising Lionheart Ventures. 


---


#### 01:26:38.854

We're trying to do a safety positive fund for AI. And I think they're taking healthy money if you want to invest in their next fund. But that's largely talking about the industry, learn about the startups, learn about what's going on. And partly you have to try and encourage the company to think we'll be better in the world. It's not going to be about the money. I don't have a very big share of it. But sure, of course, if you have a bunch of money, then it comes down to, do you get the good deals? And is your expected return high enough? if you're a certain investor or not. And that's very hard to tell from the outside. 


---


#### 01:27:08.426

My guess, and are the valuations the same? Maybe the valuations have an extra zero on them they're not supposed to have at this point, or maybe everyone's sleeping and they should have an extra zero they don't have. And neither of those seems completely crazy to me. My guess is   
(01:27:22.168) ~~if I could get,~~(01:27:22.788)  
if I could get a portfolio that said I get to buy shares in SV on the stock exchange, in SVUS, was just a equal share of all venture capital investments made by anybody in Silicon Valley, and it was trading at cost of investment plus interest rate or something, I would import a significant portion of my savings into SVUS. But that's not the question. 


---


#### 01:27:48.873

Well, that's a pretty good approximation of what I'm trying to ask. I think I would probably put more into big tech than that. How would, if you had $1 to put in SV, hypothetical SVUS and $1 to put in big tech US, which is whatever your companies. Yeah. It is trading at cost of investment plus interest. And   
(01:28:11.724) ~~it didn't,~~(01:28:12.144)  
the existence of SVUS didn't radically alter the valuation of all the companies, which it would,   
(01:28:17.506) ~~if there was~~(01:28:17.827)  
If this traded, then all the valuations would probably go up 10x or something crazy. We have various reasons to suspect this, but at current valuations in relative terms, I think I'd be pretty excited for the small stuff. 


---


#### 01:28:28.770

And I think that the market would reflect that if other people had similar market opportunities, that you'd be able to turn around and sell it at a profit if it started to go to what actual people would pay. And they would have above size returns if you didn't have that, and you just had to hold to maturity as it were. But I'm very happy with   
(01:28:43.891) ~~my,~~ (01:28:44.072)  
my portfolio of companies essentially is big tech investments that have made me a ton of money and everything else, which is like a wash, right? Like I made some money in first solar and I lost some money in Hasbro. It's all the emerging market stuff is all wash. It's vaguely even except for a significant number of big tech companies that have just   
(01:29:06.529) ~~like~~ (01:29:06.670)  
got returns have been very good. 


---


#### 01:29:09.072

And they, big tech companies are increasing percentage of my portfolio. They don't rebalance. I guess I expect that trend to continue.   
(01:29:16.378) ~~It seems like our,~~(01:29:16.979)  
to summarize our somewhat contrasting viewpoints, I'm seeing it being pretty hard to outrun the big tech companies because they seem to be moving pretty fast and the returns to scale seem to be super high. And it seems like everybody,   
(01:29:34.787) ~~not everybody of course, but you know,~~(01:29:36.068)  
a large majority from what I see seem to be playing this sort of compensating for current weaknesses game and trying to get to market. And meanwhile, the next generation of model is cooking, if it's not already cooked. And the current one is   
(01:29:51.421) ~~free,~~ (01:29:51.842)  
free in retail and not free at the API layer. 


---


#### 01:29:56.109

And it seems like you more see friction and bureaucracy, friction in the market, bureaucracy at the big tech companies, regulatory barriers, such that you think that while maybe everything I'm saying, I don't hear you like disputing any of it, but you're more saying, yeah, that's maybe all true, but it probably still gets contained by these other forces and leaves like a lot of opportunity for other smaller companies anyway. I basically think that the big tech companies just only have so much bandwidth. then attention has to be focused and critical for any given company of whatever size. And that leaves treasure everywhere. And if after we're finished with this, you want to discuss, I have some company, 


---


#### 01:30:43.997

I certainly have at least one company idea that I think would do well and I'm constantly coming up with new ones. And so I'm sure we could find something,   
(01:30:51.181) ~~you know,~~(01:30:51.402)  
wanting to explore the space and start building something over a weekend and see if it does well. But yeah, there's just, think about all the things you could do with current technology, if you had somebody built the app for that. Somebody built a really good, not just a GPT, but a much broader, true context, has gathered all the data, is coordinating a bunch of stuff, is thought out, actually sat down with users, figured out what they actually cared about, and so on. And it's something Google could have built. 


---


#### 01:31:23.043

It's something that a number of people could have built, but they didn't because they're not doing so. So they didn't even try and fail. They didn't even try. They did preview that virtual employee in their keynote. What's your expectation for the Google AI-powered virtual employee? The first thing I noticed right away was like, is this thing in all the chats and reading all the messages and the DMs and   
(01:31:48.782) ~~like~~ (01:31:49.082)  
all of your emails, like for everyone on the team and then talking to other people on the team? It looks like it is. And that doesn't seem great for me. I don't necessarily want all that stuff to be viewable by an AI that people can query. 


---


#### 01:32:01.158

That seems bizarre. People are going to start using the signal at work, just separately on their second phone. What's going to happen here? But. To the extent that if that problem gets solved, it seems really exciting to me that essentially you have this virtual employee, but really what it is is just, it's a bot that scoops up all of the information from all of the contexts in which you're okay with everybody having that context, ideally. And then it can answer questions,   
(01:32:28.721) ~~it can help communicate,~~(01:32:30.203)  
it can help mediate, it can pass things along.   
(01:32:33.182) ~~It can,~~(01:32:33.582)  
I'm especially excited for just things like you ask it when it's happened regarding X or can you keep me posted if something happens with respect to Y or can you contact Cheryl in marketing and have her evaluate which of these three things she wants, she puts priority on and get back to me because I don't want to have to talk to her directly because she's really rude or like any number of things, right? 


---


#### 01:32:55.682

It seems like it's the sky's the limit on just you have this extra tool in your toolbox that in practice is going to be incredibly valuable because it just knows all the things and can do all the things and continuously monitor all the places. And just not having to monitor as many feeds on a continuous basis by itself is a concept. Do you have a sense for what the hourly rate should be for an employee like that? They may or may not charge for it that way, but... So it's always the question, as a consultant, you learn, which hours are people paying? Which parts of your experience are being billed and which parts are not being billed? 


---


#### 01:33:34.173

But if you assume that when we talk about what you're billing while it's being queried by a human, while it is performing a specific task, but not while it's hovering up all the information in the background, I assume a lot if it's good. But it's always the if it's good, right? If it's reliable, if you can count. There's always that threshold effect to me in these situations, right? Is it good enough that I trust it to do this task? Because if   
(01:34:01.252) ~~I've got,~~(01:34:01.573)  
I wanted to check the marketing room and see if marketing has any requests or proposals or craziness that I might have to respond to,   
(01:34:10.475) ~~right?~~ (01:34:10.676)  
And the marketing channels are nuts at my company,   
(01:34:13.716) ~~right?~~ (01:34:13.817)  



---


#### 01:34:13.837

They did like 40 pages of stuff every day. And I don't want to read that. So I have the AI read that junk. Now, can the AI reliably alert me when I actually have to pay attention, such that I trust not to read this stuff? Because if it doesn't, then it's worthless,   
(01:34:31.698) ~~right?~~ (01:34:31.939)  
But if it's good enough that I actually don't read it, then suddenly it's a godsend, even if it has to spam you with four pages out of the 40 per day, just to make sure there's no false negatives. So the question is, can you get it reliable enough? If I ask it to talk to share on marketing, would actually be able to interface with share on marketing and would fail gracefully when it fails such that I can take over from it. 


---


#### 01:34:55.310

If it fails silently, if it hallucinates weird stuff, gets me in trouble, that stuff happens even a little bit, I can't do it. But if we can get there. Suddenly, it's incredibly valuable.   
(01:35:08.140) ~~I think~~(01:35:08.279)  
there's a lot of stuff like that. Or if it's just like ease of use, like with my friend, the lawyer, and the case files. If I can get it such that it's easier to have this thing checked and it's reliable enough, suddenly I win. But until then, I have nothing. So I think you see a lot of this stuff where you have nothing until you have everything. And then once you have everything, you have to learn to have it. And you have to get people to actually use it. 


---


#### 01:35:31.918

So you've got three things you have to do. So these things are hard. but it's Google, we'll get there. The question is   
(01:35:38.319) ~~like~~ (01:35:38.479)  
how long will it take and   
(01:35:39.761) ~~like~~ (01:35:39.881)  
which ways we'll get there, but we'll get there. Yeah. I think you're right. Absolutely right to emphasize the importance of threshold effects. It does seem like we're pretty close to tipping over a couple of big ones and it will be very interesting to see how that affects the broader world of employment. Do you think that, I want to make sure we get, I know you've got to run before too long. I want to get to the safety team departure. a bit as well. But I guess maybe last question in the competitive analysis market dynamics. 


---


#### 01:36:09.610

What's your expectation for employment over, say, the next year? Do you think we hit a point where we do start to see a real impact on either,   
(01:36:20.438) ~~you know,~~(01:36:20.837)  
on the positive side, like measurable productivity, or on the potentially deleterious, whoa, hello, potentially negative side? You could imagine a world in which   
(01:36:32.685) ~~entry,~~ (01:36:33.046)  
people that are entering the labor market are really struggling because entry-level developers just   
(01:36:38.488) ~~like~~ (01:36:38.627)  
can't really beat Devon or entry-level general purpose knowledge workers can't really compete super effectively on an ROI basis with these   
(01:36:47.569) ~~AI assistant,~~(01:36:48.270)  
AI virtual employees from the likes of Google. What's your short-term expectation for that? So you have to obviously separate what's going to happen in specific areas of employment, what's going to happen with employment more broadly. either the sector of information, knowledge workers, or just overall employment. 


---


#### 01:37:06.091

Overall employment, it's just a question of how big the positive number is. Because as you make people more productive, as you enable people to do more things, we are nowhere near the point where the AI just starts doing the next marginal task that a human wasn't bothering to do before. And to me, that's the question. If the AI takes 10% of the tasks in the world, or even 25% of the tasks,   
(01:37:31.291) ~~Well,~~ (01:37:31.490)  
every time there's a new task that we were going to do, 75% of the time, the humans are better off doing it than the AI. So we're wealthier. We have more productivity. We are richer than we thought we were. We are able to commission more things. 


---


#### 01:37:44.319

And people will be in more demand, not less demand. So that's great overall. The question of knowledge work specifically, I don't expect that much diffusion that quickly. As I said, I think that's why the private equity stuff, for example, is so important, slash potentially profitable. So specifically, we're starting to see a few industries where people are hurting. Illustrators, translators, to some extent junior coders, junior engineers. I think the junior engineers are actually mostly a non-zero interest rate phenomenon combined with a flaw in the tax bill. So I think we're seeing this problem where engineers can't get work and everyone   
(01:38:22.077) ~~like,~~ (01:38:22.317)  
but it's not about AI. It's about the fact that you can no longer deduct those expenses properly. 


---


#### 01:38:28.296

And so like every company doesn't have a huge cash set. It's just like getting blown away by this and interest rates are higher. So you can't like invest in the future as profitably. And the combination of these two things is hitting the market really hard because they're happening at once. And hopefully we fix the stupid tax loophole reasonably soon.   
(01:38:45.261) ~~And the anti-loophole, right,~~(01:38:46.922)  
where you just get flammed for no good reason. And then the situation gets a lot better. Because with programming, if you double the effectiveness of every programmer, or if you double the effectiveness of every really good programmer, plus 50% of every bad programmer, and made every person you can't code at all as good as really terrible programmers or something, 


---


#### 01:39:04.180

I think you just get a lot more programming. I think you just get a lot more demand. I'm now much more capable of programming, which has taken me from definitely not doing any programming to might want to do some programming.   
(01:39:15.456) ~~Well,~~ (01:39:15.576)  
it's not going to take someone else's job away. It's just going to mean that we code more things. So I'm pretty optimistic about that aspect of things because I've literally never been at a company   
(01:39:26.498) ~~where I would have thought,~~(01:39:27.658)  
where we had engineers and I was like, what will we even do with another engineer? We have all the engineers we need. That is not a thing. There's always, we need more engineers. We need better engineers. 


---


#### 01:39:38.180

We need to prioritize and we're constantly triaging what has to be done today versus next week versus next month. Because   
(01:39:44.662) ~~like~~ (01:39:44.882)  
good coding is always incredibly valuable to any real company. And I don't think we're anywhere near that changing. So do you think we'll see on the positive side, like measurable productivity improvements in the short term? So they say that the internet showed up everywhere with the productivity statistics,   
(01:40:00.207) ~~right? Like that.~~(01:40:00.908)  
And we know that's bullshit, right? We know that. the world is a much richer place. People are much more productive in important senses because of the internet or because of computers and technology. But the stats don't seem to say that. So I wouldn't be surprised if the stats just act dumb. 


---


#### 01:40:18.712

I don't know how else to put this, and don't reflect the reality of the situation. And also, coding is not that big a percentage of the economy. So we know that certain, even knowledge works of certain types, broadly construed, are not that big a percentage of the economy. So I would say, I don't expect that much in the shape of measurable productivity or economic growth to be apparent in 2024, or even 2025, necessarily, at current pace. Because I expect so low adoption, even amongst the places where people could be productive, and people not adopting it properly, and not getting the most out of what the systems can do for a while they come down. And also, for a lot of the actual productivity not showing up in statistics, including employees just hiding the fact that their product is productive. shirking off a lot because they can, or alternatively doing better jobs of the same thing but looking the same, or just various other techniques. 


---


#### 01:41:52.115

And very few people, almost no one ever suggests that. Yeah, no, we just use the current things we have and we just extend them in a way. And I wonder how many of those economists would watch the presentations from this week and see those demos and rethink what they're saying. Because what always happens is they're always trying to evaluate, okay, if we have exactly the things that we currently have and nothing else ever, what happens to the productivity stats? I think they're still off by an order of magnitude. They're still crazy. But they do have to notice when other things become actually possible. What would be the productivity boost just from a universal translator that actually worked? 


---


#### 01:42:27.838

Let's be very concrete. Everybody on the phone now can get full, real-time, continuous translation, including tone of voice, including affect and associations, really good translation, as good as the Star Trek translator. It's being said as if it was being spoken to you, its original form, with only the minor pause you have to look at your phone. What does that do to productivity? That's huge on its own. And there's so many different things you could put in that slot. They're just huge on their own. So I'm very optimistic. Over time, I just don't want to get too ahead of ourselves.   
(01:43:04.335) ~~Well, you still get,~~(01:43:05.355)  
noon is your hard cut off, right? Sorry, would it be that? 


---


#### 01:43:09.438

Noon is your hard cut off, I just wanted to check. So maybe another time we can talk about the possibility of a biotech revolution, because that's another one where I see an interesting dynamic where it seems like things could be about to get pretty crazy, but much in the way that this new class of weight loss drugs maybe doesn't, makes everybody a lot better off, or it makes those people that are taking it a lot better off, may end up actually shrinking GDP in some ways, because there's like a lot of expensive things that don't have to happen downstream. I'd be very interested to hear your take at some point on how that might play out. zero for a while because of bureaucratic and regulatory issues, and then 10 years from now, watch out. 


---


#### 01:43:48.190

Is it to the point where you change how you live or change your risk, your personal cost benefit or risk analysis at all because you think there might be genuinely revolutionary advances in biomedicine for you? I've already baked in that kind of thing, mostly, and   
(01:44:04.536) ~~I think~~(01:44:04.756)  
mostly there's very little you can do that you shouldn't be doing anyway. If you're a healthy person, You should try to stay healthy. You should invest a lot in your health. And I don't think this changes that. And I don't think that the flip side of that, of course, is the world could end in some sense, right?   
(01:44:20.243) ~~I'm,~~ (01:44:20.384)  
I'm 45 years old. If I think that we're going to hit escape velocity and like all these designer drivers that make everything great. 


---


#### 01:44:27.845

But by the time I hit 60, half the time and half the rest of the time, I'll just be dead. Like I'm not saying this is my model or anything. I'm just saying, if you did believe that. then maybe you just don't care much about your long-term health because the death rate in that range is actually not that large. There's a lot of different ways you can approach it and humans are completely irrational about these things. But the actual observed real preference for me is your health is valuable. You get good returns from being healthy immediately, effectively. You don't need to add the longevity to it. And there are basically no clashes between longevity and current health that are actually like, well, 


---


#### 01:45:03.309

I have to trade one off the other. No, what's good is good for the most part, as far as we can tell. And as far as we can't tell, we just don't know anything. There's probably ways that you do trade off, but we just aren't smart enough to know about that stuff. So yeah, I'm putting a reasonable amount of investment into my health. I do think that it would be dumb with this kind of revolution potentially on the horizon to skydive, to take known large risks of being literal dead, that clearly you will not be safe. If you roll a natural one, we can't help you. Okay.   
(01:45:36.212) ~~Well,~~ (01:45:36.332)  
we can dig deeper into that on another occasion. 


---


#### 01:45:38.675

Got to address the departures from the safety team at OpenAI this week. I think everybody probably has heard the news. Daniel Katalko, I'm not sure if I'm saying that quite right, but he left a couple of weeks ago now and has been posting some cryptic stuff online saying that He declined, left his equity on the table, which would have been a large majority of his personal net worth because he wants to retain the right to criticize the company. But we haven't actually heard what those criticisms are. Now, obviously, we've got Ilya is out. with a pretty conciliatory message saying he thinks everything's in good hands and they're going to build safe HEI. And then Jan, who was his co-lead on the super alignment team, without such a message, simply saying, 


---


#### 01:46:26.011

I resigned and leaving us guessing. It's obviously not good. What more can you say about it beyond its obvious not goodness? I've been writing up the article this morning for this that I'll probably post next week. I have eight safety researchers in the last six months who have left one way or the other. In addition to Ilya and Jan and Daniel, we have Leopold and Pavel who were fired for leaking confidential information supposedly. What little we know about this, it's hard to tell because when you leak confidential information, the last thing they want to do is tell everybody about all the confidential information you just leaked through   
(01:47:02.769) ~~Viserious~~ (01:47:03.229)  
because obviously that's self-defeating. It looked a lot like they technically leave confidential information that was used to fire them, but like from the outside, we can't tell. 


---


#### 01:47:13.712

We also lost William Saunders, Colin O'Keefe, and Ryan Love. So   
(01:47:19.115) ~~that is,~~(01:47:19.657)  
I don't know exactly how many people would count as line letter safety researchers under this criteria, but it feels like a lot, right? We also did lose Diane Yoon, Chris Clark. I'm in Murakawa. We aren't obviously safety related, but I don't know what's going on, guys. The Sherry Lachman head of social impact is gone. It doesn't feel like the kind of pattern of people you're losing when you're like being a socially responsible safety first company, right? It feels like a company loses these people when a lot of people are very dismayed in some fashion. And yeah, obviously we have the messages that are being sent and not sent. 


---


#### 01:47:56.886

We have the fact that people are almost certainly universally under NDA. We have the fact that Daniel gives us very strong evidence that non-disparagement clauses are in place basically everywhere, unless you are willing to pay an extreme financial price to not do so. And even then, you're still under NDA. So what's probably going on with Daniel is that he's reserving the right to criticize them in the future, but the NDA probably severely constrains his ability to reveal the information that would be useful right now. And he's made a very reasonable decision not to break the NDA. So he did say one thing, which was his reason for resigning was loss of confidence that the company OpenAI would act responsibly around the time of AGI. 


---


#### 01:48:36.724

That's almost a verbatim quote, probably not quite exact. The NDA thing seems weird to me, to be honest. I'm like, if you really believe that and you are willing to leave that much money on the table to be able to make some criticisms, What's the NDA doing here? It doesn't seem like they could really sue you super effectively. They maybe could, but like the Streisand effect, you're talking about self-defeating, right? Like suing your safety, departed safety person only increases the attention, the media circus, whatever, to what they're saying. You might not want to proliferate certain technology or certain technological information. There might be ethohazards involved. Yeah, it seems like you could   
(01:49:19.256) ~~kind of~~(01:49:19.457)  
separate that, right? 


---


#### 01:49:20.497

Is it because it wouldn't be like the mere existence or the discovery of something that would lead to a loss of confidence. There's got to be something like social about that, right? And the public is primed for it when it comes to Sam Altman was just the subject of a lot of drama. People, there's definitely like smoke here. It seems like that's the obvious.   
(01:49:39.688) ~~But~~ (01:49:39.887)  
so my experience with this type of thing is that, yeah, if they were to reveal some esoteric detailed reasons why they were concerned, and it was technically leaking confidential information that you were an NDA for. Probably you don't necessarily get sued for that, although you might, but also it could make your life pretty miserable, and it could take away your flexibility to do other things in the future. 


---


#### 01:50:02.220

But the baseline scenario here is that Sam Altman is a rat bastard, right? There's a toxic, and or, there's simply a toxic environment inside OpenAI for people who care about safety and about the safety of right, that it's now transformed into a move fast and bring things start-up mentality, and that they are inherently suspicious after everything that happened of anybody with associations with   
(01:50:25.313) ~~like~~ (01:50:25.453)  
Besseron, or the alignment forum, or PA, or safety of any kind, and that they're making their lives miserable in various ways, and that politically, Altman wants all those people gone, and so is steadily taking every incremental opportunity to get those people gone, and that as those people leave, more of them are now more isolated and under more pressure to leave. 


---


#### 01:50:45.399

And if you were to spell all this out, you'd be breaking your NDA and potentially making yourself vulnerable. And in general, Sam Altman is representing reasonably well that you can be a manipulative person, that he might come after you. If these are personal issues and political internal issues and you spread their dirty laundry and you launch these kinds of attacks, I don't think it's unrealistic that you get sued for it. It's for that effect to be damned. I think that I've dealt with these people. And a lot of these people will absolutely sacrifice the strident effect in the short term to credibly be the type of agent that will retaliate. It's rational. It's highly rational. 


---


#### 01:51:25.595

It's highly correct from a decision theory standpoint to be willing to come down very hard on somebody in the situation who attacks you. The counterpuncher effect. If you are trying to be a modern leader of a company in these situations where there's a lot of secrets, there's a lot of dirty laundry, because there have to be, even if you're doing everything right, Even if no one's doing anything malicious and everyone's trying their best, and everything's basically fine, there's still a bunch of stuff that you wouldn't want aired in the public. And yeah, you credibly threaten that you are going to enforce these things, the full extent of the law, and you're going to make people's lives miserable. 


---


#### 01:52:00.354

You're probably going to do other things to make their lives socially worse in various ways. Try to blackball them from their future jobs and opportunities and blah, blah, blah. And Sam Helton knows a lot of the VCs. He knows a lot of the people who determine whether or not your startup goes well or badly. whether you can get various different spots. I don't think being worried about these things is unreasonable. I do think that we can be confident that there isn't like an imminent,   
(01:52:24.685) ~~like,~~ (01:52:24.925)  
the world is about to end because of OpenAI's technology that is motivating these departures, because then they would be talking. If it was like, no, seriously, they broke QSTAR, and they hooked it up with GVD5. 


---


#### 01:52:35.988

And now this thing is like, clearly on the verge of breaking out into the internet, we're very worried about it, or some crazy scenario that's clearly not happening, let's be very clear, not happening, then I think these people would be willing to talk if only to whistleblowers to a government committee or something. They'd be talking. Because I think potentially they're talking to people who are not in public. If you had serious problems with really deep wrongdoing, would you go public with your concerns or would you call the government? It's not obvious to me that one approach is better than the other. But again, the default explanation for all of this is that the environment at OpenAI became toxic as a result of the events that happened. regardless of the extent to which Sam Altman directly caused that to happen or intentionally wanted that to happen. 


---


#### 01:53:18.618

And as a result of that, and also a lot of these people were effectively his political enemies, and now they're gone.   
(01:53:25.021) ~~Right?~~ (01:53:25.221)  
And if you see someone doing that,   
(01:53:27.643) ~~getting rid of~~(01:53:28.122)  
systematically getting rid of various people in the safety departments, maybe you leave.   
(01:53:32.604) ~~Right?~~ (01:53:32.864)  
Daniel lost faith in OpenAI's ability to navigate the future AGI role responsibly and left. It wasn't too long after they fired Leopold and Pavel. One obvious explanation is that he thought those firings were bullshit, and that he saw that they were cleaning houses of safety people and shoving safety people to the side and working militia with them. Therefore, he lost faith in their ability to be responsible. It doesn't require there to be a big mystery here. 


---


#### 01:53:56.922

I'm still thinking through this stuff, and we have to, unfortunately, go in a minute or two. The obvious explanation is that OpenAI is just not a place that is currently very receptive to something that I think will be very vital to making sure the future goes well. And that's deeply troubling. So any parting advice for either the departed folks from OpenAI or there's been talk also of compensating folks like Daniel for leaving his stock on the table to try to encourage others to do that or to set some sort of a precedent. I guess, what should we want in the short term? I would agree with you, there's probably not like a super emergency or they probably would just be throwing all caution to the wind. 


---


#### 01:54:41.161

What do you think they should be wanting, and what should they be doing to get it, and what can others be helping them? Obviously, it depends on what the underlying situation is. Obviously, they should take their concerns forward, no matter what else they've done. And if they can't do that, then that's,   
(01:54:54.390) ~~like,~~ (01:54:54.630)  
horrible. They should be, if there are things that justify whistleblowing to governments behind the scenes in ways that they're protecting us for, they should be doing that. I think the difference between disparagement and breaking your NDA, obviously, you think about these things. very differently. I do think it would be good to consider compensating Daniel or others who have given up their equity in order to not sign non-disparagement clauses. 


---


#### 01:55:19.141

But I do worry, especially about paying for NDA breaches, about the secondary incentive problem. If you know that people who are working on safety will feel free to violate their NDAs because this ecosystem will compensate them for doing so, then maybe you just can't hire anybody in the ecosystem at all. You can't hire anybody who cares about safety because you're worried they will steal your secrets, including your economically valuable secrets, and therefore you can't do it. I think you should think very carefully about what you are and aren't going to compensate   
(01:55:47.220) ~~to what extent,~~(01:55:47.801)  
and not just blow your one card. I believe in freedom of contract. These people knew what they were signing, and they knew what they were agreeing to. 


---


#### 01:55:56.045

It doesn't mean that it's fully sacrosanct. It doesn't mean that there aren't circumstances where you should break the NDA, and there aren't circumstances where you should go to protect the people who do break the NDA.   
(01:56:06.533) ~~You know,~~(01:56:06.773)  
I think those bars are high. Also, obviously, I am a journalist now, as are you to some extent. And if anyone wants to break news, if any level of confidentiality and privacy, my DMs are open. That might be a good note to leave it on. I'll look forward to your full analysis on this topic next week, and it's always great to break it down with you. Any other closing thoughts? Just to be clear, it's all still, we don't know much about this in particular, and about many of the demos and other things that we've learned about in general. 


---


#### 01:56:37.561

We're speculating, we're trying to process a lot of information very quickly. So I don't think we know what's going on. And to be clear, this could all be as simple as The event's left to that case in people's mouths. They want us to fresh start. The environment is somewhat hostile philosophically to what they're doing, and they're just not having fun anymore in some sense, and then that fact makes people lose confidence. Maybe it's that simple. Maybe it's all a coincidence. We don't know. I don't really believe in coincidence in this sense. Cool.   
(01:57:08.497) ~~Well,~~ (01:57:08.677)  
we'll keep following it in our respective formats. So thank you for taking the time today. Enjoy the conversation as always. And nothing left to say except Zvi Mosvitz, thank you for being part of the Cognitive Revolution. 


---


#### 01:57:20.826

Absolutely. I look forward to next time. All right. Thank you. Let me hit stop and we'll get you. 


---


