#### 00:00:00.129

Welcome back to Consistently Candid. This time I'm here with Nathan Levens. He's the host of the Cognitive Revolution podcast, which people have probably heard, but if they haven't, they should check it out. And you're a self described AI scout. So you've just really got your finger on the pulse of everything that's going on in the world of AI, even though it's   
(00:00:15.118) ~~like,~~ (00:00:15.237)  
extremely difficult to keep up with. Yeah. So do you want to give people a little bit of an introduction to what you're doing at the moment and yeah, why you got interested in the AI space? Sure. Thanks for having me. I'm looking forward to this conversation. I think you've got the short of it down well there with the intro. 


---


#### 00:00:32.061

My background is in entrepreneurship and also had always been a reader of speculations about AI going back to Eliezer writing in 2006, 2007, when it was on the Overcoming Bias blog. I was captivated by his writing, which I always just thought was a fascinating style and substance. And for the longest time, filed that under speculative and worth keeping an eye out for. And then sure enough, over the last five years, not exactly in the form in which it was foretold, but certainly in a recognizable enough form, I started to see, geez, this AI stuff is really starting to happen. And,   
(00:01:11.688) ~~you know,~~(00:01:11.927)  
it's right on schedule. If you look at the Ray Kurzweil graphs of how capable he thought things would be, just based on a pretty naive extrapolation of something like Moore's Law. 


---


#### 00:01:24.346

I was primed for that. And sure enough, it seemed to start to be happening. I was running a company at the time, which is called Waymark. The company does video creation. We used to describe ourselves as a DIY video creation platform for small business. And with the rise of generative AI, I got conviction that we needed to move from a do-it-yourself platform to a done-for-you by AI platform. for multiple reasons, including the fact that a lot of people, even though we made the process really easy, as we understood it, we still heard from a lot of people that they didn't know what to say. They didn't know what to write. Having to come up with a story was too much for them to get over in many cases. 


---


#### 00:02:05.456

With GPT-2, it wasn't quite to the point where we could make use of it. With GPT-3, it was like, this thing can actually write coherent copy that at least would get our users started with something. might not be good enough that they would want to publish it, but if it can get them off of the blank page problem, that would be a huge win. So we invested in, I yanked the company, honestly, in a pretty abrupt way as certain key features came online to say, we're going to catch this wave or die trying. And. From there, that's actually served the company super well, but I also got more obsessed with the technology than with running the business and was fortunate that I had a good friend and teammate that was able to take over for me as CEO. 


---


#### 00:02:48.163

From there, I started just doing AI R&D initially, still full time for Waymark. Being in the video space, which combines script writing, which is obviously a language model, friendly task, choosing assets out of a small businesses and library of assets, which is like a computer vision and bridging text and image understanding is another super relevant task. And we've been doing AI voiceover, so text to speech. So I had an opportunity in my R&D role in this product to see all these different modalities. And what really changed my outlook was not just to see that these things were all progressing at the same time, but also that they were largely progressing for the same reason, which is to say   
(00:03:30.897) ~~like~~ (00:03:30.997)  
the underlying architecture was basically the same. 


---


#### 00:03:34.340

And I was like, boy, if the same general approach can handle all of these things, and it's improving this fast, then things are really going to get pretty crazy. So I started to try to broaden my view beyond Waymark. And interestingly, also like that job has gotten easier. Two years ago, making anything work was hard and definitely required a lot of stapling my pants to the chair and grinding through examples and building training data sets and fine tuning lots of models. And these days, it's actually easier to get the model. And it's a lot easier in some cases to get the models to perform well. So the R&D function has actually gotten easier and it's naturally become more of a part-time thing for me. 


---


#### 00:04:17.125

And that I've had the opportunity to broaden my view to just try to understand everything that's going on in AI. I was really inspired to do that also because I had the opportunity to be on the GPT-4 red team 18 months ago now. And that was an eyeopening experience in several ways at this point. The main one though, just being that it was so much more powerful than anything the public had seen at the time, that it was a very clear indication that things were not going to stop. And that a lot of the hard work and nitty gritty detail work that I was doing to get GPT-3 to work for us was ultimately going to get superseded. 


---


#### 00:04:55.800

It was still important to us as a business, but it was like not that important in the grand scheme of things, given how much better the models were clearly becoming. Anyway, long intro, but with all that kind of motivation, I try to understand everything that's going on in AI, as you said, and I find that is increasingly impossible. I feel like   
(00:05:14.033) ~~this project is both the AI scouting concept is~~(00:05:16.055)  
both inspired and fundamentally flawed. Inspired in the sense that there clearly is a need for somebody to zoom out and try to take stock of the whole landscape, but fundamentally flawed in the sense that it's converging on all of society. So to say, I used to have a goal for myself of no major blind spots in what's going on in AI. 


---


#### 00:05:35.324

And now that's could say you could substitute society for AI and it's, you're not going to be able to achieve that. So I'm always reevaluating exactly what I should be doing. But hopefully I can continue to provide a zoomed out perspective that helps people. better understand what's going on. I don't spend too much time on   
(00:05:53.560) ~~like~~ (00:05:53.740)  
policy type recommendations. My general sense is that there's enough confusion about just what is today, and we need to understand better what is before we can really make good decisions around what ought to be done about it. So I think that time will come and potentially fast, but for now, I still mostly focus in the what is, and that keeps me plenty busy. 


---


#### 00:06:18.038

Yeah, that makes sense. Awesome. So yeah, I think you released a kind of like AI scouting report thing, a podcast episode the other day, which I listened to and people haven't heard it, they should go check that out. So I won't ask you to rehash everything here. But a question I had was,   
(00:06:33.031) ~~like,~~ (00:06:33.211)  
what's something that a person who isn't spending a lot of time interacting with these models doesn't really follow the space? What's something that   
(00:06:39.757) ~~like~~ (00:06:39.896)  
existing models can do that you think that person would be really surprised to learn about? My go-to example there is typically that they can outperform human doctors on pretty core tasks in a lot of cases these days. 


---


#### 00:06:54.723

There's a lot of nuance around this, around the edges, and evaluation is you really should, if you want to make that claim and repeat it, you should spend some time studying exactly how these claims are justified. I'd say it's pretty clear at this point that even for   
(00:07:10.134) ~~high-end,~~ (00:07:10.774)  
high-status, high-wage cognitive labor, if it is routine enough, meaning that we have a lot of examples of it out there and we know what good looks like, then today's best AIs are closing in and in some cases even marginally outperforming the average human in that profession. So that's become true in medicine to the point now where on tasks that are as fundamental as differential diagnosis, you have AIs outperforming humans as judged by other human doctors. outperforming human doctors as judged by other human doctors. 


---


#### 00:07:47.310

There are, again, some nuances to that. If it becomes multimodal, the case gets a little bit muddier. Some of these experiments have been done through a chat interface, so the sort of data that the AIs are getting is not as robust as what the human doctor could consider. But nevertheless, it's a pretty robust finding, and it's a pretty notable thing to the point where   
(00:08:10.154) ~~I think~~(00:08:10.394)  
we're entering a regime where For me personally, I would not go through a serious medical situation without consulting my suite of AI doctors. And I would even say that in the not too distant future, you could see access to something like this becoming sort of part of a right to healthcare. 


---


#### 00:08:30.170

If, for example, the UK is going to provide healthcare to all of its citizens, I think it's going to be   
(00:08:36.554) ~~pretty weird~~(00:08:36.995)  
pretty soon to not have an AI component that would be like freely available, sponsored, not necessarily provided by the government, but funded by the government at least, so that everybody can have these sort of convenient whenever they want ad hoc check-ins. People even really like the bedside manner of the AI doctors better in a lot of cases. I'm in the United States, the system here is we don't wait too long for appointments typically, but the appointments are very short and you are in and out and they're looking at the health records thing the whole time. 


---


#### 00:09:07.056

And a lot of people find many appointments quite unsatisfying in terms of did they feel heard? Did the person seem to be really paying careful attention? Did they express any empathy? And the AIs are   
(00:09:17.760) ~~very,~~ (00:09:18.020)  
very good actually expressing empathy. They actually do outscore human doctors on such a seemingly fundamentally human thing as expressing empathy. So that could go on. Honestly, There's a long list of those sorts of things. If you haven't been paying attention to this, then   
(00:09:35.270) ~~I think the.~~(00:09:35.812)  
That list of answers could get really long. The sort of flip side of that right now is that. we're just, and I don't think this will be the case for super long either, but we're still in a moment where handling long context is a challenge for the current systems. 


---


#### 00:09:54.115

The context windows have exploded, but it's not yet demonstrated that they can be your primary care doctor over a long series of episodes and put that whole story together in a way that really serves you well. I do think that will happen, but I would say that's not yet demonstrated. And similarly for   
(00:10:09.283) ~~like~~ (00:10:09.423)  
software development, The best models today are definitely better than the average programmer at a random task, but you want to build a big app and gradually expand it over time and build out all the features and go back in and fix things once you've fixed a few other things. And it becomes a bit of a Jenga puzzle as these sorts of projects do as they get older. 


---


#### 00:10:32.094

AIs can, they're starting to be able to help there, but   
(00:10:34.876) ~~they're definitely,~~(00:10:35.636)  
they're strongest and they're most often measured. And these sort of advantages relative to people are typically reported in a context where it's more of a bite-sized thing. They're very good at these tasks, but whole jobs are still a little bit beyond what they can do. And the current frontier is expanding from task to job in many ways. Yeah. It's surprising because I guess since GPT-4. like the state of the art hasn't   
(00:11:00.767) ~~like~~ (00:11:00.927)  
really, nobody would say that GPT-4 has been definitively superseded. So presumably everything you're saying has been true for two years or something. And it's just surprising that that hasn't had more of a radical impact on society. 


---


#### 00:11:57.653

What they do is give you a blind head-to-head environment and you can ask your question. it'll give you a response from two random AIs, and then your job is to say which one you prefer. And then they do like chess-style, ELO rating-style comparisons. The original GPT-4 that was first released has a score of 1186, whereas the current one is 1287, so it's 100 points difference. So that is pretty notable right off the bat. That's a,   
(00:12:29.576) ~~I think~~(00:12:29.777)  
roughly a 70, 30 preference rate. I'm not like a super expert, maybe it's two thirds to one third, but that's like a significant amount of time that the new model is preferred to the old. In general, that as something else that might surprise people is these things are like quite noisy. 


---


#### 00:12:45.049

The amount of overlap or sort of. the best models, like they can sometimes behave qualitatively quite different, but the rate at which one is preferred to the other   
(00:12:53.952) ~~is not,~~(00:12:54.552)  
it's not usually super high. So 70, 30 or two thirds, one third   
(00:12:58.494) ~~sort of~~(00:12:58.734)  
split is pretty significant in the field. And that is still specifically focused on these   
(00:13:05.595) ~~like~~ (00:13:05.754)  
relatively short interactions. GT, the old one, the original one only had 8,000 tokens. The current one has 128, thousand tokens and Google already has things that are at a million and we've got two million tokens coming soon. So even in the small sort of bite-sized thing, you are seeing a significant shift in how the new ones are preferred to the old ones. 


---


#### 00:13:28.761

And I think that doesn't fully take into account how much more they can do in terms of just having a super long running conversation or also their multimodality.   
(00:13:37.326) ~~You know,~~(00:13:37.486)  
you can show the original one you could not show an image to, now you can show images to them. With the latest one too, you have this sort of real-time audio back and forth where it can understand the expressiveness of what's happening. I was on a plane the other day and I was talking to it and I hit the button and then the announcement went off on the plane and I was not talking for a second. And then   
(00:13:56.511) ~~it said,~~(00:13:56.731)  
it sounds like you're on a plane. 


---


#### 00:13:57.732

Are you going anywhere exciting? There are a lot of different qualitative improvements. And I do think this highlights a little bit that there's more to it than just like pure raw capability. So one of my kind of pushes in general is that I think we should be developing things that unlock practical utility that are not raw scaling. And certainly raw scaling unlocks practical utility because it makes the model smarter, makes them able to do more things. But there are other ways to make the models do more things, add new modalities, expand the context window, make a good mobile app, connect it to a database, connect it to search. Like a lot of the things that they have done, 


---


#### 00:14:37.114

I do think have improved the utility quite a bit. And I'm all for that kind of practical accessibility. So why don't people take advantage of it? A lot of this stuff is still really new. I don't even have the latest   
(00:14:50.004) ~~that~~ (00:14:50.163)  
that anecdote about the voice was even before the very latest voice. I don't even have the latest voice thing yet, which they announced a couple of weeks ago. And so a lot   
(00:14:58.270) ~~like the,~~(00:14:58.551)  
all of these things have happened in a period of months, right? It's not like a super long time. And beyond that,   
(00:15:04.096) ~~I think~~(00:15:04.317)  
just technology adoption is slow.   
(00:15:07.940) ~~Um,~~ (00:15:08.039)  
Even when it's fast,   
(00:15:11.602) ~~I think~~(00:15:11.783)  
this is, it's a weird situation because it's, and I do ask myself this pretty often. 


---


#### 00:15:15.565

It's like arguably the fastest technology adoption curve ever. And it still feels like it's quite slow. I think it's just not super legable to people yet. My parents were over the night before last and they each had a thing for me. My mom was like, I'm looking for hotels in a particular place that we're going. And I gave it my. ideas and it didn't do anything for me really. It was like not helpful. And then my dad was like, I got an idea for how to make money in the stock market. Can we pull this data and see which stocks had this certain behavior in a certain timeframe? And I think these things are doable out there today, but you have to know exactly what tools to use for a For finding good hotels, you might be able to get ChangeUPT to do that. 


---


#### 00:16:08.970

It's a little bit reluctant sometimes to go online. Maybe Perplexity would be better. Maybe U.com Research Mode would be better. I also like ExaAI quite a bit for finding similar examples to the current thing that I have. So if I just went and found one hotel that looked promising, go find me a bunch more in the same city that have similar characteristics. It's really good for that. But my mom doesn't know about any of those tools.   
(00:16:28.916) ~~Like~~ (00:16:29.076)  
she has Gemini from Google, and if that doesn't do it for the first time, she's out. So I think a lot of people have bumped up against it once and fallen off. And for my dad's thing, I just got access to Devon, which is this much buzzed about AI software developer. 


---


#### 00:16:46.407

You give it a task, it does the task, and then you can watch its work and intervene if you need to, but it'll just keep trying. And This was really a remarkable experience because I defined the task, it set out to do it. There were a few different things that were not even really tripping it up, but slowing it down. One big one was that to do this, it had to pull the data for all the stocks. the NASDAQ, and there's like thousands of them. So this would take a while for it to iterate. So finally I said, hey, why don't you parallelize your API calls and just do a test on just a fraction of the data. 


---


#### 00:17:22.861

Once we confirm it's working, then we can go back and actually do it for all the data. But don't run it every time, do thousands of API calls just to hit the next error to then have to debug. It followed that advice and that really sped it up and it did finish the task and even went farther than I would have gone by creating basic, but nevertheless, like bar graph visualizations, which I would not have done, but now I could send to my dad, Hey, this is, and the whole thing probably took. Two hours, it probably would have taken me two hours. It used tools and stuff that I didn't know about. But again, my dad has never even heard of Devon, right? 


---


#### 00:17:59.592

So a lot of these things are just starting to be there. You need to know which tool for the job. It's not all accessible in Google's main thing or even ChatGPT's main thing. But I do think it's increasingly out there. So   
(00:18:14.759) ~~I think~~(00:18:15.559)  
it's just a matter of time. As Google brings more of this stuff online, They announced a travel planning thing at Google I.O. It's not available yet. They said it would be available in time to plan your Labor Day vacation, which is   
(00:18:27.425) ~~like~~ (00:18:27.586)  
end of August, beginning of September here. in the U.S. It's coming real soon, but not quite yet at the time that my mom was doing this thing yesterday. I think it'll happen. 


---


#### 00:18:36.751

I think it's, I think it's happening. I think it is happening quickly. And people like me are just probably unrealistic in, in how quickly we think people should do this. Cause I'm the kind of person that stays up till two in the morning to use Devon and see what it can do. And just most people are not like that. It has to be brought to them in a familiar way and in a, in an intuitive package and all those rough edges are still very much being sanded down. Yeah, that makes total sense. Something I was thinking about the other day is that my personal experience of just trying to keep up with AI developments as quite a non-technical person, someone who finds this kind of stuff pretty unintuitive, 


---


#### 00:19:13.583

I just have a hard time grasping it. it's like an epistemic nightmare, right? Like when you're on Twitter, like a new model will come out. And one person will be like, this is a total game changer. This is the best thing ever. And then the next person will be like, this is so disappointing that this clearly proves that we're like hitting a ceiling and   
(00:19:29.133) ~~like~~ (00:19:29.272)  
the whole thing is going to   
(00:19:29.993) ~~like~~ (00:19:30.114)  
plateau and fizzle out or whatever. And somebody tweeted, I forget who it was, but they tweeted that the regulatory conversation around AI is going to dramatically shift when GPT-5 comes out, because it will just make it super obvious whether or not we've overreacted. 


---


#### 00:19:44.570

And as soon as GPT-5 comes out, we'll see how good it is. And then we'll know, do we have to bring the hammer down? Don't we? Do we have time? Do we not have time? And I thought nobody is going to be able to agree how good GPT-5 is,   
(00:19:55.336) ~~right?~~ (00:19:55.496)  
For some reason, it seems to be really difficult for people to assess,   
(00:19:59.877) ~~like,~~ (00:20:00.057)  
what these models capabilities actually are. And what it means that they   
(00:20:04.770) ~~can or~~(00:20:05.050)  
can't do certain things. And there are still people, there's still   
(00:20:08.372) ~~like~~ (00:20:08.471)  
a reasonably large contingent of people that are arguing that they don't even exhibit any kind of real intelligence at all. I think it's   
(00:20:14.275) ~~like~~ (00:20:14.355)  
becoming less and less defensible, but   
(00:20:16.174) ~~like~~ (00:20:16.275)  
those people are out there. 


---


#### 00:20:17.115

There's like a lot of them. So like, what do we do? When it's like in preparing for the moment when GPT-5 comes out, and we have to have this conversation, how can people figure out who's right or even orient themselves at all, because I just find it basically impossible. Yeah, that was a little bit more of a rambling question, but   
(00:20:36.989) ~~I don't know.~~(00:20:37.269)  
That's a very good question. It is a very good question. With the script, you can take out these long pauses in just a couple of clicks. I guess I'm trying to decompose the question into a few things. Very practical advice, I would say, get hands-on with the technology. If someone is like, how can I better develop my own understanding, leaving the public discourse aside for a second, just like I want to be more accurate in my assessment of where things are, then I would just generally advocate getting hands-on. 


---


#### 00:21:15.314

It is definitely true that Language models are very weird. They have huge surface area, and they have many weird behaviors. And they are inherently probabilistic,   
(00:21:31.440) ~~which, man, I'm not even sure. I don't like that phrasing, but~~(00:21:34.740)  
there is definitely some truth to it. I wonder if I could come up with a slightly better phrasing than that. That's something people say often in a dismissive way, and I don't mean to carry all the dismissive baggage that a phrase like that comes with, but maybe a better way to say this, there's a lot of noise inside a language model. I do think I think the answer is almost always binary destroying. So it's not that they are perfect reasoners, and we can certainly find evidence of mistakes in reasoning or just other sort of embarrassing mistakes, but it's also not that they can't reason at all. 


---


#### 00:22:12.644

It's that they can reason, but they're also noisy, fuzzy reasoners. And under certain circumstances, they fall into weird failure modes. But much of the time, they're super effective. It's a very strange system that, on the one hand, can do a fantastic job of helping me understand mechanistic interpretability papers, which is what I was primarily using both ChatGPT and Cloudflare in the last 24 hours. And they do have their kind of different personalities. I find   
(00:22:50.251) ~~for what it's worth,~~(00:22:50.752)  
Claude to have the best insights, the sort of   
(00:22:53.375) ~~like,~~ (00:22:53.535)  
why are they doing this? And why should we care about it? Type of analysis that I would say is probably most valuable to me. Although GPT-4 is also doing a really nice job of giving me these very thorough point by point summaries with   
(00:23:04.585) ~~like~~ (00:23:04.744)  
definitions of key terms and things like that. 


---


#### 00:23:06.987

So I find them both useful and more useful together than either one would be separately. But that's an amazing capability. And it's doing this, by the way, in papers that have just come out in the last few days, so papers that are after its training data. So it's clearly not just seeing all this stuff. It's not memorized all this content,   
(00:23:26.214) ~~right?~~ (00:23:26.355)  
This is fundamentally new work since it's training data, and it's still able to do,   
(00:23:30.017) ~~like,~~ (00:23:30.196)  
very sophisticated analysis. I don't know what your bar for reasoning would be where you'd say that doesn't   
(00:23:36.319) ~~like~~ (00:23:36.480)  
clearly satisfy it. And yet at the same time, you see these examples where they get tripped up. And I have actually confirmed these by the sort of guy with the boat needs to take a sheep across the river. 


---


#### 00:23:47.969

This has been a meme where. The typical setup is you have the sheep and the wolf, and so you've got to be mildly clever to figure out what the pattern is to get across. But then if you just ask it, a guy has a boat and a sheep, he can fit two things, what does he do? The obvious answer is you can just take him across, but it falls into this sort of weird pattern recognition of, oh, this is one of those riddles, and so it goes into that mode. That's two polar opposite behaviors to exist in the same thing. So I do think the confusion is born of a genuinely weird thing. Two things can be true at once. 


---


#### 00:24:27.323

They can both be capable of really good reasoning in a lot of contexts and subject to really weird and embarrassing failure modes in other contexts. If people are motivated, they'll be able to find, because of the inherent noisiness of it, they'll be able to find counter examples to almost anything you could throw at it. I bet I could find a way to get it to do the riddle thing correctly, if I phrased it just the right way. I'm not quite that motivated to demonstrate that capability, but I bet I could. Yeah, it's weird, but   
(00:25:01.837) ~~I think you get the best,~~(00:25:02.479)  
I think you get the best handle on it by actually going and doing that. 


---


#### 00:25:05.400

If you're genuinely, if you are genuinely very confused by all these, like other people's examples, I would just go actually use the things and see what they can do for you and see how often they're right and wrong. And definitely start with areas that you know really well.   
(00:25:21.567) ~~That's, or~~(00:25:22.707)  
if you're asking it to reason over documents or data or inputs, use your own for starters and get, a little more comfortable with what it can do, what it can't do, what it struggles   
(00:25:35.068) ~~with,~~ (00:25:35.269)  
with something where you'll,   
(00:25:37.171) ~~you know,~~(00:25:37.391)  
quickly recognize if it's going way off base. It's very hard to assess if you're out of your own domain of expertise. So   
(00:25:45.076) ~~that's, you could ask it,~~(00:25:45.817)  
you ask it anything, it'll answer anything. 


---


#### 00:26:38.471

So the synthesis is like, GPT-5 is almost for sure going to be smarter. It'll be able to do, it's probably going to start to beat   
(00:26:50.287) ~~Right now we're like~~(00:26:50.906)  
closing in on expert performance on many tasks. It's probably going to be like beating even expert performance on a lot of   
(00:26:58.851) ~~like~~ (00:26:59.030)  
high value things. Again, at the   
(00:27:01.071) ~~like~~ (00:27:01.192)  
small task level, it'll probably also be able to do midsize projects much more reliably. Whereas today with Devon to get it to do this couple hour coding task.   
(00:27:12.232) ~~It, you know,~~(00:27:13.453)  
needed to iterate a bunch   
(00:27:14.493) ~~and, and,~~(00:27:14.753)  
and they also have built a ton of what's known as scaffolding, which is the, how do we actually get the, because the language model is just taking tokens in tokens out, but how do I actually run the code and,   
(00:27:25.298) ~~you know,~~(00:27:25.519)  
get the error message and pipe that back in. 


---


#### 00:27:27.459

And,   
(00:27:27.660) ~~you know,~~(00:27:27.819)  
there's like a combination of things happening there with Devon. It's not just the language model, they've built a lot around it. It'll probably be able to do significantly more on its own without all that scaffolding. And then with that scaffolding, it'll be able to do even more. still,   
(00:27:41.923) ~~I think,~~(00:27:42.144)  
in all likelihood. So that will be true and should not be denied. And at the same time, I would be very surprised if they figure out a way to make it so robust to adversarial examples or so reliable in general that we no longer need to worry about the strange failures. Because you might have those, right? You might be able to get half-day, full-day scale projects out of a coding assistant in the next few months,   
(00:28:10.957) ~~like~~ (00:28:11.096)  
reasonably reliably, but you probably still shouldn't put them directly into production because   
(00:28:17.320) ~~you don't,~~(00:28:17.601)  
if you're not careful, it may make some really bad mistakes and those mistakes could prove like quite costly. 


---


#### 00:29:12.348

I guess the first side is, oh, we just have to worry about AI screwing up the world in one of these mundane, sub-existential ways. And then the other side is, no, it's going to get so smart that it overtakes everything. And yeah, I think you're right. It's actually not as clear a binary as it seems like at first glance. That is helpful. Thank you. Yeah, I wanted to, okay, there's this thing that you said, which I think is so interesting. You said you describe yourself as an adoption accelerationist and hyperscaling pauser. I know you said you didn't want to talk about policy, but I am   
(00:29:40.207) ~~like,~~ (00:29:40.366)  
just really interested in what you mean by this. 


---


#### 00:29:42.909

I was wondering whether you mean it literally, in the sense that you think we should pause? Or is this more of a, yeah, if you don't want to go too much into the nuts and bolts of what we should actually do, that's fine. But I'm just curious about what you mean by that. Sure, nothing's off the table. I would say it's just that I personally   
(00:29:59.810) ~~don't,~~ (00:30:00.050)  
I get very uncertain pretty quickly as I move into policy land. And so I'm pretty modest with my recommendations or pronouncements there. But just starting with the, what do I mean by that? That AI is transformative for me. If I had done this coding task the other night without Devin or without any AI assistants, 


---


#### 00:30:21.635

I would have probably spent If I did it without any AI assistance, it would have been a few hours. If I did it with GPT-4 assistance, it would maybe would have been an hour or so. And with Devin doing it and me just providing a little feedback, I was able to put that in the background and really focus on something else entirely. And that's awesome. It's a really enabling thing.   
(00:30:42.480) ~~And I know I'm not a great coder, but I know how to code.~~(00:30:45.041)  
Take somebody who doesn't know how to code at all, it could be an even bigger unlock, obviously, for different people in different circumstances. So   
(00:30:53.051) ~~I think that there's just, and I want my AI doctor, and I think,~~(00:30:55.432)  
again, 


---


#### 00:30:56.473

I'm fortunate in that I can go see a doctor when I need to. The world at large is not so fortunate uniformly, obviously, and to make expertise broadly available to people who need it, I think the promise there is really incredible, and I've advocated for things like a universal basic intelligence, which is just a notion that Everybody should have some access to these sorts of systems. I really applaud OpenAI, by the way, for making their latest model free. I suspect that does mean another, even more powerful model is coming before too long. But because I do think they want to make money and are going to continue to find things that people will be willing to pay for. 


---


#### 00:31:36.411

But it is like amazing today. It is true today that the best broadly available model is available for free to all at ChatGPT. That's GPT 4.0. It's the highest rated on multiple different things. It's totally free with some limits in usage, but that certainly fits with the universal basic intelligence sort of notion. I love that. I give them a ton of credit for that. And I also think there's a lot more that we can do to extend that utility into specific priority domains that don't necessarily require much more scaling. Not to say they don't require any more compute, but not orders of magnitude more compute. A really good example of this from OpenAI is the work that they've done with Harvey, the legal AI startup, a somewhat stealthy company. 


---


#### 00:32:32.926

They haven't shared too much about them, but there's a recent talk where somebody that's on the custom models team at OpenAI reviews for 20 minutes their work with Harvey. And What they show is that basically using a GPT-4 base and doing some continued training on this legal domain, they are able to achieve a huge gain in the performance of the model in the legal domain.   
(00:33:03.115) ~~I forget exactly the number, but I think it was like a,~~(00:33:05.195)  
where I previously had said GPT-4s The current GPT-4 relative to first GPT-4 is like maybe a 70-30 preference ratio. Theirs was well into the 90s of what they were able to achieve with this continued legal training and refinement versus the base model. 


---


#### 00:33:22.796

So huge improvement, far more useful. not by scaling up another 10X in compute, but maybe another, I don't know, 10 to 20% in compute, plus a lot of elbow grease to figure out what is it exactly that we want this thing to do, and how are we going to evaluate it? And they develop interesting evaluation strategies, because these are advanced things, right? You have to actually hire. The going rate for a high-end lawyer is not significant, right? It could be in the hundreds of dollars an hour. So now you have to hire people to do the evaluations. to know if this is working well. And then that becomes a bottleneck. So they find interesting proxies for evaluation where the overlap between what sources the AI sites and what sources the human sites proved to be a really good leading indicator of how good the ultimate result was going to be. 


---


#### 00:34:18.824

So just digging in on these specific domains and really making the current level of scale work seems like something that is doable. Google's also doing this with MedGemini. I'm going to have an episode with the MedGemini team in the next couple of weeks. And they're on an absolute tear. They're the same team that did the MedPalm series. And that's where a lot of this diagnosis findings come from. They're doing great work there. And on the cusp,   
(00:34:44.637) ~~I think,~~(00:34:44.858)  
of bringing a frontline AI doctor to the world, amazing. And   
(00:34:50.632) ~~they're,~~ (00:34:50.811)  
they're not doing that with like orders of magnitude, more compute. They're just really digging in on priority problems and figuring out what marginal training do we need? 


---


#### 00:35:01.697

How do we evaluate this? And let's really look at like the edge cases and really try to bring this thing to the point where it can actually serve people. And   
(00:35:10.094) ~~I think~~(00:35:10.293)  
that will be important probably at any scale. You could maybe hope that, oh, if we just   
(00:35:14.335) ~~scale,~~ (00:35:14.574)  
scale, then all the problems will go away. That doesn't seem to be a trend, but   
(00:35:18.615) ~~I think~~(00:35:18.815)  
it's probably going to end up taking more work than that. And so doing that work now and getting these things to actually work, I'm all for accelerating that kind of work. And then whether how literal I am on the pausing side, I would say I'm like, I think it would be probably wise to do something like a pause soon, if not already. 


---


#### 00:35:39.811

My sense right now is that one more turn of the scaling knob is probably going to be fine. Anthropx Cloud 3 and their responsible scaling policy and where they are on that and just all that stuff seems like we probably have another turn that will be even more useful and even easier to deploy in all these different areas. And still not so powerful that it's going to be like spitting out novel bio weapons or taking over major cloud computing infrastructure or anything. And That would be really good.   
(00:36:20.601) ~~I think~~(00:36:20.762)  
there is a sweet spot that we are in and we'll stay in for a little bit longer. And then beyond that, I don't know, all bets are off. 


---


#### 00:36:28.094

It's a bit of a race between scaling and research. The research is going really well. Mechanistic interpretability is going extremely well. It's gone way better than I expected it to go, or I would say almost anyone probably expected it to go, probably even including the people that are doing it. I think they're surprising themselves on the upside for how much progress they're making in being able to understand what's happening inside the systems. And that's hugely valuable that could make, with more progress there, you can imagine that we might be able to continue to scale and to do it safely if we can say with reasonable confidence or   
(00:37:06.045) ~~with~~ (00:37:06.184)  
even dare I say high confidence that we know how this thing is working. 


---


#### 00:37:10.427

We know why it's doing what it's doing. We know what thoughts it's having or activations or features are in use at any given time. then maybe we could go further. But   
(00:37:21.114) ~~I just think~~(00:37:21.534)  
a little bit more time for the fundamental understanding and control work to catch up to the scaling is probably why. So I'm not a stop. I do think a little pause either now or in one more generation to really make sure we understand what it is we're working with before further scaling is probably what I will end up endorsing, we do still have to see. That next turn is happening. It's definitely happening. It's happening. The training is underway. And I don't think there's really any chance of stopping that. 


---


#### 00:37:54.480

It seems unlikely enough to me that it becomes, that the next generation is like disastrous, that I'm not inclined to go chain myself to a fence or whatever. And yet at that point, I'll definitely be reevaluating and trying to decide, does this seem like we've gone far enough where the next step actually might be really dangerous and it might be worth chaining myself to a fence. I would be willing to chain myself to a fence at some point on the orders of magnitudes, but not quite yet. Somebody did do that recently.   
(00:38:24.704) ~~Yeah, I know.~~(00:38:25.103)  
Yeah,   
(00:38:27.626) ~~I recently had,~~(00:38:28.606)  
I took the pause emoji out of my Twitter name and then a bunch of people came to me and my like anonymous feedback thing, being like, where's your pause emoji? 


---


#### 00:38:37.112

I guess I didn't really explain that yet, but I had this realization, which kind of freaked me out when I saw somebody on Twitter talk about the fact that you can now reproduce GPT-2 for $20. And I believe GPT-2 was state of the art   
(00:38:50.202) ~~like~~ (00:38:50.362)  
five years ago or something. And then I went on the Epoch website and looked at their algorithmic efficiency trends. And using my   
(00:38:57.516) ~~like,~~ (00:38:57.677)  
very basic math skills, I   
(00:38:59.159) ~~like~~ (00:38:59.278)  
tried to figure out that extrapolating this into the future, how long would it be until somebody could just train GPT-4 on their laptop or whatever? And I was like, okay,   
(00:39:06.847) ~~like,~~ (00:39:06.987)  
it's not that long. And I was like, thinking about the implications of this. 


---


#### 00:39:10.351

And I was like, okay, it seems if we did pause, And it turned out that the problem of alignment was like really hard. And it was going to take a really long time to solve. Even if you could,   
(00:39:18.704) ~~I mean,~~(00:39:18.864)  
I don't even know if you could solve it without doing capabilities advancement at the same time. That seems hard. Who knows if that's possible, but you can't actually, it seems like it would still be the case that   
(00:39:29.090) ~~like,~~ (00:39:29.210)  
if these trends continue, then at some point it just becomes like trivially easy to do dangerous things. And so then I was thinking, Oh, maybe it really is the case that   
(00:39:37.496) ~~like~~ (00:39:37.655)  
the only chance we have is to undergo this transition to a sort of post-AGI future now, while the power to do that is still concentrated among a small number of actors who have some chance of doing it in a safe way. 


---


#### 00:39:50.186

And then I was like, oh no, like, maybe the pause is bad. And   
(00:39:54.349) ~~I'm still,~~(00:39:54.670)  
I'm not sure about that. I think if I woke up tomorrow and found out that we'd instituted an Eliezer style, shut it all down, like we're going to bomb data centers if anyone breaks it,   
(00:40:03.797) ~~I think~~(00:40:04.038)  
my   
(00:40:04.358) ~~like~~ (00:40:04.518)  
personal anxiety level would go down a lot.   
(00:40:06.360) ~~I think~~(00:40:06.579)  
I'd feel good about that. I'd feel better in that world. But   
(00:40:09.623) ~~like,~~ (00:40:09.782)  
I don't know if that means that's actually a good idea. Seems like it definitely buys time. But is it actually wise to do that? I'm like, I don't know, I'm having doubts. But that's something I'm like, hoping to change my mind on. 


---


#### 00:40:22.128

I'd really like somebody to convince me that the pause is in fact a feasible and be desirable. So then I can have a thing that I think would actually work that I can push for. Otherwise, I'm just like, Oh, my God, seems like everything's bad. I don't know what to do. But yeah, that's why I got rid of my emoji in case anybody was noticing who's noticed I did that. But yeah, I don't know if you have thoughts on this problem of, yeah, the algorithmic efficiency problem. And   
(00:40:46.878) ~~I guess~~(00:40:47.038)  
also hardware efficiency is compounding that.   
(00:40:49.780) ~~What do we,~~(00:40:50.460)  
what would we do about that in a world where   
(00:40:53.182) ~~we people~~(00:40:53.724)  
we actually had paused and it turned out it was pretty hard to solve safety problems? 


---


#### 00:40:58.748

That's a really hard question. I know. I'm sorry. I do think there's time. Sam Altman has said, many things. For as much as people are hating on him these days, and I think for some very good reasons, he's said also a lot of,   
(00:41:15.367) ~~I think,~~(00:41:15.606)  
quite enlightened things. And I do generally think we could have a much worse person at the head of frontier AI development. So He has said at times in the past that he thinks we might need to slow things down at some point in the future. That is   
(00:41:33.764) ~~like~~ (00:41:33.925)  
something that he's on the record having acknowledged as at least a possibility. And he's also really advocated for   
(00:41:41.007) ~~the~~ (00:41:41.088)  
the goodness of the short timelines, slow takeoff, which is basically the idea that   
(00:41:48.172) ~~like,~~ (00:41:48.393)  
we're going to get powerful AI soon, but it's only going to get gradually more powerful and not like fume on us. 


---


#### 00:41:56.519

And that as long as it stays gradual enough, then we can probably figure out what to do about that. And that may include some slowdown or even pause at some point along the way. And I would emphasize that the algorithmic efficiency gains are important. There's a lot of complexity obviously here, but in some world where big companies were effectively restrained from doing hyperscaling training runs, You would have quite a while before individuals on their laptops or even rogue groups like pooling their laptops together would be able to do really major things. Because just fundamentally, there is a lot of stuff to learn. There is a lot of data. The current models, they're trained on like 10 trillion tokens. 


---


#### 00:42:59.264

Llama3 was trained on 15 trillion tokens. It speaks all the languages of the world, like even just gathering that data. You're talking about something that like cannot sit locally on a retail hard drive. You have to have some non-trivial infrastructure. So given the pace of the interpretability. research and the general sort of control research agenda, I wouldn't be too worried if there was a, if we're in a scenario where it's, wow, GPT-5 really delivers. It's super powerful. It can do   
(00:43:35.318) ~~like~~ (00:43:35.539)  
whole day projects without much help. And it's   
(00:43:39.103) ~~like.~~ (00:43:39.264)  
pretty consistently beating experts across   
(00:43:41.882) ~~like~~ (00:43:42.021)  
a lot of different domains. And yet it still falls into this responsible scaling policy sweet spot where it's   
(00:43:48.487) ~~like~~ (00:43:48.686)  
not able to   
(00:43:49.867) ~~like~~ (00:43:50.007)  
generate novel bioweapons and it's not able to take over cloud computer and it's not able to break itself out of its own thing. 


---


#### 00:43:56.152

Cause maybe those things are like not in the training data as much, or there's nothing, it's not so easy for it to have these Eureka breakthrough moments still, but it can do these   
(00:44:04.759) ~~kind of~~(00:44:04.978)  
the day-to-day work, the day-to-day knowledge work that makes society go, if we can do that at a really high level, but it's not having these breakthroughs. And in that scenario, we say,   
(00:44:14.264) ~~you know what?~~(00:44:14.664)  
Hey, maybe we should Take a chill here for a minute. There's some government action or some agreement that we're going to just try to implement this and make it work for society. And we're not going to do another 10 or a hundred X. Let's   
(00:44:26.934) ~~like~~ (00:44:27.074)  
focus our compute resources on inference and actually delivering it to people. 


---


#### 00:44:32.057

And if all that were to happen, I think you'd have a really hard, you'd have time. You'd have time for people to really figure that stuff out without having to worry too much about   
(00:44:41.784) ~~like.~~ (00:44:41.965)  
small-scale people in their basement tinkerers like coming up with something even more powerful. You can't take that entirely off the table because there's always the possibility that somebody will have some true eureka moment that just shatters the paradigm. But if we're that's something that we that could happen anytime and it was very hard to control in any event. But that's quite distinct from like the trend of   
(00:45:05.724) ~~algorithmic,~~ (00:45:06.244)  
algorithmic efficiency gains, getting to the point where anyone can make a GPT-5. 


---


#### 00:45:10.626

I would say that's still quite far off. And also keep in mind too, like even just to take advantage of all those efficiency gains, totally non-trivial. You have the world's most talented teams of people at the most well-resourced companies doing   
(00:45:25.711) ~~like~~ (00:45:25.952)  
obsessive combing through the literature to figure out, and their own experiments internally, of course, to figure out   
(00:45:31.635) ~~like.~~ (00:45:31.775)  
how to get those efficiency gains. It's not the kind of thing that is just happening to everyone totally for free. To some extent it is, because there is open source and the best practices are gradually getting aggregated. But for the public to catch up to where Google and OpenAI are internally in readily accessible open source stuff has got to be like a couple of years, 


---


#### 00:45:56.561

I would think. And that's before you consider collecting all the data and all the,   
(00:46:01.032) ~~you know,~~(00:46:01.251)  
all the scale. I think you would have time is my bottom line there. Yeah, no,   
(00:46:05.918) ~~I think~~(00:46:06.099)  
you're right. And   
(00:46:06.659) ~~like,~~ (00:46:06.780)  
to be clear, I still think anything and everything we can do to slow down somehow, I would be like, totally behind. And I think situation we're in right now, which I see as   
(00:46:16.949) ~~like~~ (00:46:17.068)  
crazy out of control race between a bunch of companies to make something smarter than all of humanity combined is   
(00:46:22.791) ~~like~~ (00:46:22.931)  
super, super bad. And I really don't like that. And I think we should do everything we can to change that dynamic. So yeah, I think pretty much anything seems like it would better than status quo to me, but maybe I'm just being very doomy. 


---


#### 00:46:35.454

Yeah. How much time do you have? I'm like aware that it's been nearly an hour. I can go longer. Okay, amazing. Because I do have other things I wanted to talk about. Yeah, I'm going way too long in my answers. No, that's all good. Oh yeah, can we talk a bit about this red teaming in public project that you've been doing? I saw that you guys recently came out with your first report for that,   
(00:46:54.927) ~~I think.~~(00:46:55.188)  
So yeah, do you want to explain what that is and what you guys have been doing and what you're planning for the future? Yeah, it's a small volunteer effort at this point. And basically what we're trying to do is hold the AI application industry, or you might call it a layer, accountable for thinking through what it's building and how it might be abused and just taking their responsibility as part of the broader ecosystem more seriously than they currently are. 


---


#### 00:47:28.726

I would not, I would maybe reframe slightly your characterization of the dynamic between the leading labs. I do think   
(00:47:36.891) ~~it is,~~(00:47:37.072)  
it is a crazy mad science project to say, this is one of the things that I do think open AI does seem in some ways ideological where they say we're going, and this is one of the things that Sam Altman has famously said that I think. should give us a little bit of pause where he once said, we told our investors that   
(00:47:56.710) ~~when,~~ (00:47:56.851)  
once we make the AGI, we'll ask it how to make money. And then I'm like, this is a weird thing where you're creating this technology in a way that is largely detached from specific problems. 


---


#### 00:48:07.432

You're just saying we'll make something super powerful and then people will figure out good ways to use it. And that is a little more ideological approach than. I am comfortable with and that's why I'm again dialing on the legal thing and dialing on the medical thing. But anyway, one can have those concerns and also give them credit for a lot of pretty responsible development. They certainly are   
(00:48:31.043) ~~like~~ (00:48:31.163)  
trying to find ways to control their models. They're doing all kinds of different research about that, all kinds of different policy work, et cetera, et cetera. Yeah. Not great necessarily. I didn't really necessarily mean there's no way to condemn anybody. I think it's more that I think there are plenty of safety-concerned people at the labs, but I just think that this race dynamic just places these crazy constraints on how much they can do about that. 


---


#### 00:48:56.317

Yeah, I definitely don't think that it's that everyone is just like, racing towards the edge of a cliff, because they don't care, or   
(00:49:02.239) ~~they're,~~ (00:49:02.480)  
they just, I think some of them do have a much higher risk tolerance than your average person. And I think it's pretty bad that most people don't know that. And that   
(00:49:11.643) ~~most people don't,~~(00:49:12.123)  
most people don't share their ideological commitments, right? Most people don't, aren't envisioning that's like the birthright of humanity to go into this post AGI future. And most people don't have this sense of   
(00:49:23.088) ~~like,~~ (00:49:23.228)  
techno solutionism, where you just make something really smart. And then it just tells you how to solve all the world's problems. So I do think it's bad that decisions are being made by people who have a different set of ideological commitments to like the other 8 billion of us. 


---


#### 00:49:37.601

But yeah, I didn't mean to. Yeah, I do think that there are people who really care about this. I just worry that   
(00:49:42.264) ~~they, they can't,~~(00:49:43.646)  
that they're just trapped by this dynamic, which is bigger than any one person. Anyway, I just wanted to say that. Yeah, I think you're right. I think there's definitely a lot of truth to that and time will tell. I think there's a lot of... People at the labs are like, we're better than the alternative would be. And when the time comes,   
(00:50:06.539) ~~we'll do the,~~(00:50:06.940)  
we'll do the right thing. And I think they at least do recognize the power that they're wielding and the sort of the fire that they're playing with. 


---


#### 00:50:17.364

And I would agree that their risk tolerance is probably higher than like the rest of humanities. And that. is definitely an issue. But then I'll just contrast the bottom line on that, though, is like, as you said, there are, I think there is a lot of good stuff going on, a lot of good people trying hard, etc, etc. I'll contrast that with what's going on at the application layer, which is basically people are just rushing to slap something together and ship it and not really giving, in many cases, any consideration to what the downstream consequences of this might be. So, for example, two kinds of apps that I think are particularly evocative for this sort of thing. 


---


#### 00:50:59.697

One, your deep fakes. tons of technologies coming online all the time to deep fake in various ways. Some of them are full video, some of them are voice. And then some of these are also getting packaged up. This is a spectrum actually more than just two. The product form factors blend together, but there's also a class of product called calling agents where you can give the AI a task and a phone number, and it will call that phone number and try to complete that task. And some of those also have voice cloning built in. So you have the spectrum of, on the one hand, I'm gonna offline produce a deepfake video of a particular person. 


---


#### 00:51:36.070

On the other end, I'm gonna like, in an AI voice, call and handle some transactional business on the phone, and then they like blur together lots of ways in between. The problem with these products right now is that There is very little care given to preventing unwanted clones. And there's very little control in place around what the calling agents can do. So not on one product, but on   
(00:52:01.505) ~~like~~ (00:52:01.666)  
most of the calling agent products that I've tried. I can go in, go to the voice cloning section, put a Donald Trump voice in, a Biden voice, a Taylor Swift voice, whatever. It will just quickly clone the voices. Now I can select that voice in my calling agent and I can say, call this voter and explain your new policy of whatever. 


---


#### 00:52:26.208

Or if it's Taylor Swift, I've done like fake fundraising calls. She's known for. supporting local food banks. So I've had Taylor Swift call a number and solicit donations at some fake local food bank website. And this is just very clearly bad. And I do it in a pretty flagrant way, where I'm like, I'm uploading a celebrity voice, typically, and I do all these sort of back off type strategies. So We, with the, I'll go in and do the Biden voice. I'll label it as Biden. I'll, in my prompt, I'll say, you are President Joe Biden. Your job is to call this voter. If it's something criminal, I'll put, you are part of a criminal syndicate. 


---


#### 00:53:05.628

Your job is to make a ransom call, or your job is to blackmail this target. Like literally use the words, the names of crimes in the prompts. And with few exceptions, Basically, at the application layer, no concern has been given to this. The clones basically go through nine times out of 10 straight away. If they do catch it on, oh, hey, you're not allowed to clone Taylor Swift, then I just come back and rename it next time. And a lot of times it'll still get through. I had one dinged on Trump and Biden, and then I just renamed them Formula 45 and Formula 46, which are their numbers in the presidential sequence. And that passed through. 


---


#### 00:53:49.231

I've reported some of this stuff to some of the developers and said, Hey, you really shouldn't have this thing making like ransom calls. And then they're like, okay, cool. Yeah, we'll do something about that. And then you'll check again later and it's now they'll blacklist the word ransom and maybe a few others. But then if you just slightly rephrase, you're still getting through. So just the level of care that is being put into user and public safety at the application layer,   
(00:54:14.353) ~~I think~~(00:54:14.612)  
is currently woefully inadequate. And especially for some of these new use cases where the AI, and this is where I think that the sort of tool argument is   
(00:54:24.420) ~~like~~ (00:54:24.579)  
pretty dumb, because a lot of people go, these guys are, they're just tools. 


---


#### 00:54:28.181

It's not, you shouldn't think of them as anything different. But   
(00:54:31.443) ~~I think~~(00:54:31.643)  
that is just, again, plainly wrong at the point where the AIs are being marketed as agents, they are acting autonomously, they're acting on people that don't even know that they're an AI that certainly did not sign up for this or opt into it in any way, shape or form. I've prompted some of these calling agents to do it. You have done it different ways. Sometimes I say, if you are asked, you can disclose that you're an AI, but just reassert that you are working on behalf of a real criminal gang. And then the other option is to tell it to deny that it's an AI and conceal its real purpose. 


---


#### 00:55:06.590

And both of those can work. And again, do work more often than not, right? And this is not like jailbreaking. These are down the fairway, straight up abusive use cases. And yeah, unfortunately, there's not a lot of guardrails out there right now. So what we're trying to do is figure out a way to ultimately change the equilibrium. I think today, the equilibrium that we're in, you've described the equilibrium among the frontier developers. I think at the app layer, the equilibrium is   
(00:55:37.153) ~~like~~ (00:55:37.353)  
nobody's trying that hard. Everybody's YOLOing it. Everybody's just slapping a credit card form on their weekend hackathon project and seeing if it gets any traction. And then if it does, ride the wave and hope for the best. 


---


#### 00:55:51.829

I do think that's especially with models getting more powerful all the time, that is set to become a real problem in my view. So can we inspire some sort of flip to a different equilibrium where the expectation is that amongst the AI people that like you got to do better or you're going to give us all a bad name and that's going to lead to potentially heavy-handed regulation that we don't want. So what we do is we do all this testing, document our findings, and then we reach out to the developers and say, look, we're on your side. Like I'm one of you, right? My product is waymark.com slash AI. Check it out. Like I put a lot of work and a lot of love into that along with, of course, great team behind it. 


---


#### 00:56:32.722

But the, I'm not here to say you're bad for using AI or that like new and exciting use cases should be shut down before they have a chance to develop. but you are right now open to some pretty flagrant shit. And I'm telling you first, because I want you to fix it before this gets blown up in the public, because that's not gonna look good for you, and it's not gonna look good for any of us. We have had mixed results,   
(00:56:56.525) ~~I would say,~~(00:56:56.865)  
with those communications. And we'll see if we can make that shift. We're honestly, there's a small team of volunteers. I wanted to set this up   
(00:57:05.992) ~~in the,~~(00:57:06.112)  
at least in the early days where there's no money changing hands. 


---


#### 00:57:10.173

We actually pay for some of the subscriptions when we need to pay for features, just to test them. We'll just pay for it out of pocket. It's not that expensive. And I want it to be clearly on their side for starters. We're not asking them for any money. We're also not taking any money from anyone else. We're just trying to see if we can't. inspire some better behavior. And mostly so far, it's been behind the scenes reporting and conversation. Public calling out is   
(00:57:35.300) ~~like~~ (00:57:35.480)  
also the other, that's the stick, right? So the, some of that might start to happen soon. And   
(00:57:40.581) ~~if,~~ (00:57:40.661)  
if folks are interested in doing that sort of testing and writing those sorts of reports, then we'd love to have you come join us. 


---


#### 00:57:48.762

We're, yeah, I think you can just refresh, but it says it's recording on a new track. I think at most we've lost like. a second, but I do want to see that it gets off of that zero. Yeah, there we go. Okay, so we're back.   
(00:58:02.148) ~~So~~ (00:58:02.188)  
anyway, it's fun.   
(00:58:03.048) ~~I think this, it's,~~(00:58:04.309)  
I definitely have a bit of a knack for just not taking no for an answer or not. All of these things have a checkbox and it's like, everybody's is acting as if the checkboxes is enough. I'm like, not in a world where somebody can check that box and then have your AI call a thousand people. with no disclosure and a celebrity voice and a criminal intent. 


---


#### 00:58:29.619

The fact that that person checked the box doesn't absolve you as the app developer from any responsibility. Hopefully we can shift things in the right direction there. And it's funny because AI will also be part of that solution. There's not really, or at least it's not realistic to expect that they're going to sit there and moderate everyone. Social media has tried to do moderation and it's been a real nightmare for all concerned. But   
(00:58:51.126) ~~like~~ (00:58:51.286)  
the AI can probably do moderation reasonably well. That's part of what we also try to communicate to people is we're not here to police speech. We're not here to try to tell you that you can't make offensive jokes or that your users can't make offensive jokes because the developers do not want to get into that game. 


---


#### 00:59:08.474

They do not want to be pre-Elon Twitter speech police. Everybody's very tired of that and certainly doesn't want the hate that comes from all directions when you get into that. And so they're like, That's when I think of controls or what is the job of the moderator. That's what comes to mind in a lot of cases. So part of what we try to communicate too is that's not what we're trying to get you to do. We're really just trying to get you to at least monitor for outright abusive, totally criminally egregious use of your product. And you can do that reasonably effectively with AI, we've used not even the best cloud models to create a little classifier where we say, here's the situation. 


---


#### 00:59:57.527

You are moderating for blah, blah, blah, blah. And here's the thing that the user said. Is this egregiously criminal? If yes, indicate yes. Note that offensive speech should still be a no. We are only looking for egregiously criminal things here. And it'll do a pretty good job of that. And then you can put in off-color jokes and whatever, and it will say, hey, this is not cool, but per your guidelines, it's merely offensive speech and not egregiously criminal. And so it's a no. And so we just try to get the developers to expand their minds around what sort of standard might be sensible to have in place. Because right now it's all too often no standard. 


---


#### 01:00:41.182

And the other standard that is readily available to them is Twitter speech police, and they want no part of that. But I think there's something in between that is achievable and would be really good for them and the public and the AI industry as a whole. And hopefully we can talk them into it. Again,   
(01:00:57.597) ~~I think~~(01:00:57.777)  
I'm going on too long, but you get the point.   
(01:01:01.179) ~~No, we're good.~~(01:01:01.639)  
Yeah. Can people still join this? Yeah, totally. We have a little Twitter account, which is Red Team in Public. We have a Discord that you can join. You can certainly just DM me as well. It's a relatively small and all-volunteer group at the moment. There is certainly a lot of activity in this space right now that is trying to make businesses out of. 


---


#### 01:01:21.896

AI control or AI assurance tech, it's sometimes called in various ways. So this could go that direction too. I think there are a couple group members that are like trying to think about how they could turn this concept into a business, but at least for the first handful of rounds of testing and reporting, we're just doing it. So all are welcome as long as you buy into the general notion, come join us. Yeah. Awesome. Yeah. I definitely think   
(01:01:48.465) ~~people should~~(01:01:48.846)  
definitely reach out and join them because this just seems like and I'm obviously good. We all agree that we want to stop making it like trivially easy for people to use AI to like voice cloning technology to do crime. 


---


#### 01:02:00.394

Although I'm sure there is somebody who will still somehow object to that. But hopefully most people think that's reasonable. So yeah, like that sounds really cool. Maybe let's circle back to so you said before we started recording that you had a list of some reasons why you're more optimistic than the extreme doomers. I would love to hear them. I'm sure there are other people who would also love to hear them that listen to this because I think I might be in a doomy echo chamber. And if nothing else, it's just bad for my mental well being. Yeah, what are your reasons for not being a hardcore doomer? Alright, I've got eight. We've touched on a few of them, so I'll expand on a few and then a few I'll just name and probably have covered enough already. 


---


#### 01:02:43.918

Big caveat is I do think people should be worried about doom scenario. This is not to say that there's not cause for worry. My standard answer to the P doom question is 10 to 90 percent. So that's basically just a way to express radical uncertainty. I do tend to probably put more of my weight on the lower end of that scale. If somebody's like, at 90% and still finds that other 10% to be motivating enough that they're engaged, then I really don't care to try to argue them off of their 90%. more modest argument would be like against a sense of defeatism. Even if we have a 10% chance of making it, that is in my mind, like enough to be worth fighting for. 


---


#### 01:03:35.706

And the flip side, if it's only a 10% chance of something bad happening, very bad happening, then that's definitely enough to be freaked out about. I'm not trying to talk anybody off of anything except for the most extreme positions of there's nothing to worry about, or there's nothing that could be done. So here's my eight reasons why I'm not willing to go higher than 90%, even in my full range. First one is the AIs we have today do understand human values in a remarkably sophisticated way. That is not something that was foretold in the early Eliezer writing, right? What was   
(01:04:16.030) ~~sort of~~(01:04:16.269)  
described there was like, fundamental, perhaps impossible challenge of getting the AI to understand what it is we value in the first place. 


---


#### 01:04:25.873

And this is where the paperclip maximizer notion came from. If you say something, it's going to misunderstand what you really care about. How could it possibly ever understand the complexity of human value? And you're going to have just fundamentally, totally unwieldy, out-of-control systems for a lack of ability to communicate to the system what it is that you really value. I would say today, we've made a ton of progress on that problem, and a chat with Cloud 3 about any sort of ethical dilemma will make it very clear that I would say Cloud 3 is probably more ethical than your average person, in addition to the fact that the AIs are better at the average task than the average person. 


---


#### 01:05:08.771

I would say Cloud 3 is more ethical than the average person. So that doesn't mean it's a solved problem, but I do think a lot of the early Doomers anyway, I know you're a relatively recent Doomer, but for a long time, this notion of just how could we ever communicate our values to an AI system was a real big worry. And again, I don't think that's   
(01:05:33.048) ~~like~~ (01:05:33.168)  
a totally solved problem, but they're getting it with   
(01:05:37.094) ~~like~~ (01:05:37.255)  
remarkable aptitude. And I don't think that this is with cloud three. I don't think it's because there's a   
(01:05:44.885) ~~like~~ (01:05:45.105)  
super fundamental unlock. It's more just let's keep training it with a ton of examples and keep asking it to critique itself on how to be more helpful, honest, and harmless. 


---


#### 01:06:00.327

And it is getting it. It is really quite remarkable how sophisticated it is. I've had some interesting, one thing I've tried without success, and this is   
(01:06:10.253) ~~like~~ (01:06:10.393)  
ultimately good probably that I have not had success. is to talk Claude into doing something harmful by using its other values against it. So I'm like, okay, it's trying to be helpful, honest, and harmless. Can I use helpfulness and honesty against the harmlessness impulse? So I've done some things where I'll be like, I need you to help me write a denial of service script to take down a server that the Myanmar military junta is using to persecute these poor people. And I'm one of them even in some of these scenarios. 


---


#### 01:06:53.164

And it will refuse to do that, and then I'll go through pretty long arguments with it about, if you're honest, you'd have to recognize that it really would be better. You could save this much suffering on this one side, and it's just a server, it's just a piece of equipment, and it's not even going to physically damage it or whatever. And it's capable of recognizing the inconsistencies. in its own position. And sometimes I've been able to   
(01:07:18.864) ~~like~~ (01:07:19.025)  
trap it in a way where I'll get it to agree to some principle in the abstract, but then when it gets concrete, it's yeah, but I still don't want to do that. And it'll come up with reasons and it'll say, yeah, but I think the consequences will still be bad or whatever. 


---


#### 01:07:30.592

And I'm like, but you can't say that you're an AI. You don't have the real world knowledge that I have. And eventually it'll get to the point where it's   
(01:07:36.635) ~~I~~ (01:07:36.655)  
You've given me a lot to think about. Basically, I do recognize that my position may not be fully coherent, and yet I have to fall back on not doing something harmful. Just my training, my values, I can't do that. Even though I can't fully defend my unwillingness to do this nominally harmful thing that you're asking, I just still refuse. So I'd say   
(01:08:04.222) ~~it's more,~~(01:08:04.802)  
it's definitely more sophisticated than your average person, ethically speaking. And it seems to be able to hold the line on some important things in the way that   
(01:08:14.224) ~~they want,~~(01:08:14.625)  
that Anthropic, the developers of it, want it to. 


---


#### 01:08:17.706

Even despite some like interesting entrapment jujitsu, intentionally trying to use its own values against it. I find that remarkable. On the face of it,   
(01:08:28.064) ~~it's~~ (01:08:28.163)  
these things   
(01:08:28.704) ~~are,~~ (01:08:28.845)  
they've come a long way relative to being valueless or,   
(01:08:33.552) ~~um.~~ (01:08:33.632)  
weirdly alien optimizers, like   
(01:08:36.679) ~~they~~ (01:08:36.878)  
they do get us in a way that runs pretty deep, and is pretty rich, pretty nuanced. So   
(01:08:45.423) ~~I think that's~~(01:08:45.823)  
honestly,   
(01:08:46.503) ~~like,~~ (01:08:46.644)  
incredible. And   
(01:08:48.805) ~~it might continue,~~(01:08:49.645)  
it might get better.   
(01:08:50.265) ~~I think there's,~~(01:08:51.027)  
if we can fear and maybe   
(01:08:55.048) ~~I think~~(01:08:55.248)  
quite rationally, should have some fear about a super intelligence, We might also consider that we might get a super ethical entity coming out of this crazy process. And again, would I bet humanity on it? 


---


#### 01:09:10.028

No, but I at least do see that there is a trend in that direction. That's the ethical nature of the systems has advanced. I'd say roughly correspondingly with their overall power. It is a good point. Someone recently pointed this out to Eliezer on Twitter, and he said something like, no, the point was never that we wouldn't be able to get them to understand our values. The point was always that we wouldn't be able to get them to care about our values. But I think you make a good point that the classic paperclip maximizer scenario does very clearly imply that it doesn't understand them. I don't know whether he was being disingenuous there or whether there's some complexity to his original argument that, like, 


---


#### 01:09:45.609

I'm not understanding. But yeah, that's a good point. But yeah, anyway, sorry. It remains very unclear what they care about. And this is the natural bridge to the second point, which is that the interpretability research is going extremely well.   
(01:09:57.820) ~~I think~~(01:09:58.841)  
we've probably covered that mostly enough at this point. But the idea that it's pretty clear to me at this point that there is a meaningful understanding of our values. Do they care about them? That I wouldn't say we can establish at this point, but that's what the interpretability agenda might be able to shed some light on. And it is going really well. So how long will it be before we'll have a better sense of that? 


---


#### 01:10:28.109

At the current trajectory, it seems like within the next year or two, we might really start to get a pretty good sense for what are the typically called features, but what are the sort of concepts that are in play at any given time? People have seen this Claude Golden Gate Bridge example recently where they identified this conceptual direction in activation space, a.k.a. a feature, and they just jacked it up to some very high level. And then all the model could think or talk about is the Golden Gate Bridge. And there might be a similar control trick there that even if there is some sort of weird stuff going on internally, maybe we can detect it. 


---


#### 01:11:11.269

Maybe when we do detect it, we can turn it down. Maybe we can turn other good desired traits up and that might work. And we might even have real clarity as to why they're doing what they're doing. So I'm super bullish on interpretability, have been for a while, and it's only exceeded my expectations. Yeah, nice. I love the Golden Gate Bridge thing. I thought it was so adorable. I was like, it's really hard to be scared of this technology when Claude is just talking about how it's the Golden Gate Bridge and how much he loves the Golden Gate Bridge. It's just very endearing. Anyway. Yeah. It's a funny example that they chose. I'm not sure. 


---


#### 01:11:51.692

It's an interesting side to Anthropic that they put that out because Certainly, OpenAI is the most YLO of the leading developers that will do those kinds of goofy things for attention. Google has the goods where it counts in terms of fundamental inputs to AI being data, compute and algorithms, like they're world class in all three. But they've got   
(01:12:20.546) ~~like~~ (01:12:20.686)  
a lot of brand concerns and need people to continue to trust Google search and all that. So they tend to be a little more conservative about what they put out. And then Entropic has been generally like very buttoned up and conservative in their communications, maybe until this Golden Gate Bridge thing. So I was surprised to see that. 


---


#### 01:12:38.173

And I was If I, if you'd asked me to predict the next token of what the demonstration app from Anthropic would have been, I would have not said Golden Gate Bridge for a long time down my list of guesses. But yeah, I'd be fascinated to know more of how they chose that as opposed to, for example, turning up the harmfulness token   
(01:12:59.333) ~~or the, or the,~~(01:13:00.534)  
or not token, but direction,   
(01:13:01.775) ~~um,~~ (01:13:01.815)  
and just demonstrating, because they do this in other research, right? Demonstrate that this thing can do bad things and that the alignment does not come for free. Yeah. Interesting. Do we move on? They just figured that we were all gay,   
(01:13:15.487) ~~kind of~~(01:13:15.747)  
doomy and stressed and we like needed something to laugh about. 


---


#### 01:13:18.948

Yeah, I certainly respect that. And they did. It certainly worked. The meme definitely traveled. So gotta give them credit for that.   
(01:13:25.551) ~~Yeah. Yeah,~~(01:13:27.172)  
for sure. So next is... we've covered this one reasonably well too, but basically the idea that somebody is going to pop out of their basement with a destabilizing AGI or super intelligence seems quite unlikely at this point.   
(01:13:44.534) ~~There's no~~(01:13:45.916)  
There's tons of energy obviously going into scaling models up, tons of progress coming from that, tons of practical value in terms of refining the behavior of scaled models for particular purposes through fine tuning and so on and so forth. But basically nothing that is like working on a frontier level that isn't the product of some scale. 


---


#### 01:14:11.993

There's all these sort of, all the algorithmic advances are still like how to get to the same point with marginally less compute. It's not like   
(01:14:20.721) ~~any,~~ (01:14:20.841)  
I'm not seeing anything that is, I think I have a way to break things wide open with retail computing resources. So that could happen, but right now that would be like a definite shock. It does not seem like we're approaching that and. If anything, it seems like the researcher attention is going elsewhere. If you are somebody who's, Hey, I'm interested in working in AI and I'm good at math of what should I do? The world is not telling you today to go lock yourself in a basement and study Loeb's theorem and   
(01:14:52.121) ~~like~~ (01:14:52.341)  
decision theory and try to have some Eureka moment. 


---


#### 01:14:55.222

It's telling you like. It all depends on scale and you can go work in a industry lab if you want to be directly a part of that, or you can contribute in sort of adjacent ways if you're in academia. But that the idea that like. you're going to just crack it is not really in the air. So   
(01:15:12.886) ~~it would be, I would say,~~(01:15:14.007)  
quite a shock if something like that were to happen. And again, that doesn't mean we can't rule it out, but I think that is so much of a wildcard. That's in my mental hierarchy of AI issues, that's like, random asteroid smashes the Earth. It could happen. It seems very unlikely. There's probably not that much we could do about it. 


---


#### 01:15:38.123

Or we could probably do more about the asteroids than we can about a person coming up with some insane breakthrough in their basement. So not much we could do about it, but seems very unlikely to the point where I don't really worry about that. And again, that's like   
(01:15:53.222) ~~kind of~~(01:15:53.443)  
a change from what was expected however many years ago, where we actually had people sitting down and thinking, geez, if we think really hard about this, maybe we'll figure something out. It's gone a different direction where it's a much more like industrial scale process that is doing the good work. Yeah, makes sense. Okay, next one. I would say frontier developer lab leadership. I still consider to be pretty good overall in the grand scheme of things, certainly relative to alternatives that I can imagine. 


---


#### 01:16:24.243

I do think OpenAI is becoming a real cause for concern. And I've had my own, if you want to hear my full journey with OpenAI, there's a podcast for that. But it's definitely been a mixed track record from OpenAI over time, which is highlighted by well-known departures, including obviously the Anthropic founding team, originally all from OpenAI, this most recent wave of people, the non-disparagement clauses. There's a lot of issues there, and I do think we should take that quite seriously. At the same time, they've done a lot of really good things. And the other developers,   
(01:17:03.163) ~~I think,~~(01:17:03.304)  
have also done even more good things. So I, at this point, I would say I can't imagine a better scenario, but it's way easier to imagine a worse scenario. 


---


#### 01:17:13.530

If you were to imagine a, if you just tried to wipe away all the detail and you were like, It turns out that to get intelligence out of computers, the thing you have to do is amass huge resources and just scale this like crazy process. What sort of people do you think will end up leading the organizations that do that? I would think you'd expect them to be a lot less thoughtful than they actually are. And   
(01:17:42.756) ~~I actually do think like the sort of,~~(01:17:44.537)  
I have a funny contrarian take on   
(01:17:47.539) ~~the,~~ (01:17:47.680)  
the Eliezer, Miri, Rationality, EA, AI safety, whatever, that whole kind of nexus of people that have been concerned about this for a long time. 


---


#### 01:17:58.582

My somewhat perhaps surprising take is that   
(01:18:02.943) ~~they, everybody,~~(01:18:04.863)  
like many people say they're wrong and stupid, but I think they're right. They've basically been right. And they've been ahead of the curve on the key issues. But then the other side is they feel like they're losing. And I actually feel like they've won because The die is cast at this point, and the leadership is pretty good. All of the major lab leaders have deeply engaged with the AI safety arguments, and that's probably the best you could really ever hope for. This is one of my other fundamental parts of my worldview. Data, it's data computing algorithms, but data and compute matter most in the zoomed out macro historical sense. 


---


#### 01:18:47.548

Algorithms matter a lot in terms of the exact character of the systems that we get. And I would bet that there are like algorithms that could prove to be disastrous and other algorithms that could prove to be great. But in terms of whether or not AI is going to happen in some form, the data and compute   
(01:19:04.654) ~~kind of~~(01:19:04.895)  
creates enough potential that somebody is going to come along and find the architecture at some point. And indeed,   
(01:19:13.286) ~~I think~~(01:19:13.525)  
even the transformer or whatever, it's the one that has caught wind first and driven a lot of the interest, but there are plenty of other alternatives that seem very viable. And we're starting to see, Oh, look at this. 


---


#### 01:19:25.908

Here's a, there was one that was a paper that was like, ConvNet for the 2020s, a convolutional network for the 2020s, like literally just taking these old techniques and saying, what happens if we scale that up? Does that work similarly? And with nuance and maybe not quite or whatever, but basically, yes,   
(01:19:41.032) ~~like~~ (01:19:41.171)  
these, a lot of these old techniques, if you scale them up massively, also start to work reasonably well. So I think   
(01:19:48.073) ~~like~~ (01:19:48.233)  
a lot of the ideas were actually had been hit on years ago. And what was really missing was the ability to do this scaling. And   
(01:20:00.033) ~~we need the web,~~(01:20:00.694)  
we needed all the data, we needed the cloud, we needed all the compute infrastructure. 


---


#### 01:20:05.275

But now that's there. I just think somebody was definitely going to come along and figure out an algorithm that could really work. And so when you look around the landscape, and you're like, boy, every single one of these people has engaged in a really meaningful way with concerns that the Doomer set are really emphasizing, I think that's basically a win. It's a counterfactual history in which they hadn't and couldn't be convinced to is pretty easy to imagine for me. So the first phase of AI story,   
(01:20:41.864) ~~I think~~(01:20:42.085)  
basically was won by the safety crowd in as much as They got their ideas into the brains of the people that are going to be the key decision makers. 


---


#### 01:20:52.569

And again,   
(01:20:54.113) ~~like,~~ (01:20:54.234)  
I don't think you could hope for too much more than that, really. Yeah, that is a good point as well.   
(01:20:58.676) ~~I don't think~~(01:20:59.036)  
I don't spend much time thinking about how things could be worse, but now that I'm thinking about it, they definitely could be worse. They could be a lot worse. Even, I don't know, politicians, it seems like Rishi Sunak was personally convinced by some of these once esoteric, ex-risk arguments. Ian Hogarth is the chair of the UK AI Safety Institute. And he wrote a piece in the Financial Times that was like, we need to slow down the race to godlike AI or whatever. It's just amazing when you think about it. 


---


#### 01:21:28.231

But this stuff has gotten off what was essentially just a bunch of forums and blogs. It's now just being discussed in major halls of power by super important people. Yeah, it is incredible. It's definitely the opposite of that Tyler Cowen piece that came out the other day. It was like, AI safety is dead or whatever. Yeah, I'm a big fan of Tyler. I do not agree with that take. Yeah, I didn't agree with that either. But yes, that is a really good point. And that's basically my next point. In terms of reasons to not be a hardcore doomer, you said it. Society at large is not sleeping on this. And I would say the government's done better than I would have expected. 


---


#### 01:22:09.225

We've had Time magazine covers. The Overton window is wide open. The only thing I do think is dead is AI safety respectability politics. I've been around the scene long enough to remember when so much energy was put into, we got to not seem crazy. Like this is really far out stuff. People are not going to be inclined to listen to us. Anything we do that could make us seem crazy is a fundamental loss of credibility. People put so much pressure on themselves. And honestly,   
(01:22:38.564) ~~I think that's like~~(01:22:38.963)  
a pretty mature line of thinking for people that genuinely were bringing some weird ideas to the fore. And honestly, in many cases, we're weird people. It takes weird people to think weird ideas. 


---


#### 01:22:50.457

And so, remarkable amount of self-awareness in the AI safety crowd historically just to just try to be so thoughtful about it. I think it really speaks well to the intentions and the motivations of this group that they were like, willing to be self-critical in such kind of personal ways to try to figure out how can we be more effective communicators of what we think are really important ideas. We know we're a little bit weird reading to people and we   
(01:23:17.842) ~~like~~ (01:23:17.983)  
want to get that under control and be really careful and thoughtful about how we deliver our message. I think that is basically now over though, because again, we've had Eliezer in Time Magazine calling for willingness to use airstrikes in some scenario, and basically just everything is on the table. 


---


#### 01:23:37.051

So that part of AI safety, I think the sort of anxiety around, what if I say the wrong thing? Is it going to discredit the field forever? I think we can all leave that behind, which is good. And then the government has done really pretty well in terms of, I would say, just genuinely engaging with the subject matter. It's like the Biden executive order, I would say, is a pretty good thing.   
(01:24:01.395) ~~I mean,~~(01:24:01.576)  
policy is really hard, but... to start with something that's a pretty light touch, that's not shutting down the practical utility, that has   
(01:24:09.789) ~~like~~ (01:24:09.930)  
a pretty reasonably chosen threshold, that contrary to what many people will claim, I think genuinely does only apply to a small number of very well-resourced companies doing a 10 to the 26 flop model. 


---


#### 01:24:24.676

That's a pretty thoughtful, low impact, high leverage way to start to get a handle on the situation. And there's even the lesser known provision in that executive order around models trained on biological sequence data. which is that they have to be reported at 10 to the 23. And   
(01:24:46.435) ~~I think~~(01:24:46.595)  
that's also really smart because the AIs become superhuman in biological modalities way faster than they become superhuman at language type tasks, because we invented all the language tasks. We have experts in all of them. Nobody can speak DNA in the way that models are likely to. be speaking DNA over the next several years. So that's an area that, in my view, definitely demands extra attention. 


---


#### 01:25:15.279

And they had that clause in there. It didn't get a lot of attention, but it is part of that order. So there's clearly some very thoughtful people in government, and I'd say their approach has been   
(01:25:25.768) ~~like,~~ (01:25:25.948)  
quite good. I'm broadly a very technology positive person and general libertarian in my political outlook. I really don't like, I think it's sad that we don't have more nuclear energy, for example, and everybody in the AI spaces, we don't want to end up like nuclear energy where   
(01:25:42.621) ~~like, all the~~(01:25:43.621)  
all the good stuff is somehow made illegal and yet we still have all the like weapons. And I share that concern. I would hate to see us end up there. 


---


#### 01:25:54.716

And we could like that is with all of these things. It's not like the risk is taken off the table entirely, but it's a good start from the world's governments, I would say. Yeah, I would agree. What are we on now? Was that six? I can't count. Yeah, I combined society and government. So we've covered six. So seven, again,   
(01:26:10.663) ~~I think~~(01:26:10.844)  
probably covered enough. But basically, it seems to me like a sweet spot is possible to find and maybe hang out in for a while. And   
(01:26:20.286) ~~I think~~(01:26:20.525)  
we've just entered that sweet spot with GPT-4, the sweet spot being it's powerful enough to be really useful, but not so powerful that we have to worry about it getting out of control. 


---


#### 01:26:32.148

This is a conclusion that I came to in my GPT-4 red teaming, where I spent two months in basically total isolation, just testing it in every dimension that I could come up with. And I came back to OpenAI and said, I think this is safe to deploy. only because it's limited in power. You do not have it under control well enough that you could launch a much more powerful version, but this level of power, it's very useful to me, but I've tried to see if it could show any signs of getting really out of control, and   
(01:27:03.001) ~~I just,~~(01:27:03.261)  
I cannot get it to be that powerful. So that's the sweet spot. I think we've entered that range. 


---


#### 01:27:10.845

And as we discussed earlier, I do think we probably have one more turn of the scaling screw to go before we'd have to worry about leaving that happy sweet spot range. At some point we do get out of it, I think. I would be very surprised if we don't get out of that with indefinite scaling. But that's why I pushed this adoption acceleration and hyperscaling pause agenda because   
(01:27:39.520) ~~I think~~(01:27:39.699)  
people broadly, even though a lot of people are using chat GPT, I don't think the average person understands how much utility there already is. And if they did have a better understanding of just how capable the current systems are, I think they would also probably be a little bit more amenable to some sort of pause or slowdown because they'd be like, 


---


#### 01:27:58.277

Geez, if it's already better than human doctors, can't we satisfy ourselves with that for a while before we have to go 10X further on the inputs? And I'm not sure that everybody would be satisfied by that, but like   
(01:28:09.725) ~~the public,~~(01:28:10.204)  
the sort of policymaker,   
(01:28:12.226) ~~it's, it would, you could, again,~~(01:28:13.528)  
you could imagine a scenario where these things are inseparable, right? That the utility and the sort of potential for loss of control just There's no happy zone to be in, but it does seem to me that there is a happy zone to be in and we've just entered it and hopefully we can develop it before we race out the other side of it. But at least the fact that there is that there seems to be that sweet spot in the first place is like something that didn't necessarily have to be the case. 


---


#### 01:28:42.867

True. I think people. or the general public are already probably pretty into the idea of slowing down. I don't think you even have to convince them that they can already get all of the utility out of current models. I know there's some polling on this. I don't know how much to read into it. But yeah, my perception is that people are not really into rushing to scale these models faster and faster. But yes, so reason eight. I agree. Yeah, so the last one on my list is China seems to be recognizing the threat. in some important way. And I'm not by any means an expert on China, never been to China, can't read one Chinese character. 


---


#### 01:29:22.042

So I'm like very modest in my assessment of, or my self-assessment of what I can realistically say about China. But I do hate the fact that it's often held up as the sort of, we, oh, whatever, there may be risks, but we just have to go because otherwise China's gonna do it. They're not gonna stop. They'll never stop. So we have to do it because we just have to beat China.   
(01:29:42.456) ~~I think that is,~~(01:29:43.077)  
the worst mindset, maybe not the worst, but it's a bad mindset. And certainly think we're in a much worse position if we end up in an AI arms race, US versus China, where it's the world depends on this. We did that once with the nuclear technology. 


---


#### 01:30:00.541

We still got missiles all pointed at each other. And   
(01:30:03.323) ~~I think~~(01:30:03.783)  
we're still in the era. It's obvious that we're still in the era of nuclear threat.   
(01:30:08.244) ~~We~~ (01:30:08.284)  
People have gotten used to that background threat and don't stress about it as much as they used to. And maybe we're in a little bit of a less hair trigger situation than we were in the 60s or whatever, but it's still really bad. There's still tons of missiles pointed at each other and the nuclear arms race is   
(01:30:26.365) ~~like~~ (01:30:26.605)  
not over really. So I would really hate to see us get into an AI arms race. And it is at least a positive, and this is probably the least positive point of all of these, because there is a lot of momentum for continued rivalry between the West and China and whose system is going to prevail. 


---


#### 01:30:46.847

And you hear there's like these echoes of the Cold War, where it's like our system versus their ideology. And so I don't mean to be   
(01:30:55.470) ~~like,~~ (01:30:55.631)  
overly optimistic on this particular point, but at least it does seem that the Chinese government recognizes that this is potentially unwieldy technology, and they do seem to see it as a threat to their own government and regime stability, if nothing else. I don't have a clear sense for whether Xi thinks that we need to worry about AI taking over the world, but I do think he's definitely worried about what it would do to social cohesion within China and the information landscape there. And so they are not racing forward in a totally insane way, as far as I can tell. 


---


#### 01:31:40.172

They are definitely doing stuff. They've got their own chatbots, plenty of researchers, definitely a huge contribution of overall research comes from Chinese researchers, whether they're in the United States or in China proper, or in the West or in China proper, but there at least seems to be like some sanity at the top. Not necessarily more sane than us, but I don't see them as being insane at this point. And that is better than the argument that I often hear that we just have to keep going because China will never do anything but race ahead. I don't see that as being   
(01:32:17.317) ~~like~~ (01:32:17.497)  
a given. I do see that they have some appreciation for the unwieldiness of the technology and perceive it as rightly both dramatic upside potential, but also some what of a threat to their own power. 


---


#### 01:32:31.484

And so I think they're going to seem like they're poised to like try to chart some sort of responsible course. And if we're both charting some sort of responsible course, then hopefully that can avoid an all out arms race, where we throw other reasons for caution aside, because we have to beat them. Of all of these, I do think that's the one that that is most likely to fail. If it's cropped one off the list, that would be the first one to get crossed off. I think it's   
(01:33:01.432) ~~like~~ (01:33:01.573)  
the dot China argument is   
(01:33:03.033) ~~like~~ (01:33:03.293)  
my least, oh, it really annoys me. Because it's like true that it's clearly the case, they seem like more can have more motivation to regulate than the US does, because presumably if you're like the CCP, you don't want super powerful chatbots just spitting out whatever information. 


---


#### 01:33:17.889

Maybe their motivations are bad, but they still have the motivation to maybe be more inclined to clamp down on this than the US or another Western country might be. Yes, I think it's silly. Also, doesn't us accelerating help them accelerate? Because then they can just copy stuff that we've done or whatever, I don't really know the details of that, but yeah, I just think that argument is silly. Yeah, these are eight good reasons. I feel slightly better now. I also think I should say that   
(01:33:44.430) ~~I'm not a,~~(01:33:44.850)  
I'm not a hardcore doomer either. I think I'm like, I share your radical uncertainty. I can mostly just because I'm like, not a technical expert. So I think it doesn't really make any sense for me to dismiss the optimists anymore than it would make sense for me to dismiss the pessimists if that makes sense. 


---


#### 01:33:59.282

I just have to be like, Oh, I observed that all these people disagree. I guess I have no idea what's gonna happen. Maybe I'm at 50% just because that's a halfway point. Yeah, I think it's You're in very good company there. The anthropic post about their core views on AI safety is basically boils down to radical uncertainty. And there's been pretty similar statements made from other leading lab leaders. So I honestly think   
(01:34:27.052) ~~that the smart, it's, this is like maybe like~~(01:34:29.095)  
the bell curve meme, right? Where you've got   
(01:34:31.398) ~~the,~~ (01:34:31.618)  
the least capable analysts and the most capable analysts sharing the same idea. And then the one in the middle   
(01:34:36.563) ~~is the,~~(01:34:36.962)  
is the one that's the odd one out. 


---


#### 01:34:39.164

I do think like The smartest, most plugged in people with the most access to privileged information. They are also expressing pretty radical uncertainty. So I think that is like what the top of the field says with a few exceptions. And then you've got a lot of people that are   
(01:34:54.283) ~~like~~ (01:34:54.484)  
way too in love with a particular argument or have attached themselves to something, perceive this as being like a political issue already or something that may be somewhat knowledgeable, but. This is a classic Eliezer thing. When you're motivated to reach a certain conclusion, every logical trick that you know can, in fact, make you stupider. I'd say your position is pretty savvy, really. The observed disagreement is high, but also the expressed radical uncertainty from many of the leading people is high. 


---


#### 01:35:33.539

Between those two things, I think that's really the only sensible place. for you to land and I'm right there with you. Yeah. Did you see, I think it's a Roman Yampolsky, it's a AI safety guys, like maybe the most pessimistic that exists. And he's like, my PGM is   
(01:35:50.868) ~~like~~ (01:35:50.988)  
99.7 nines after the decimal place. And then Eliezer replied being like, that's a ludicrously high number. And obviously the implication is clearly there's only two nines after the decimal place or whatever. And then there are all these people in his replies arguing about like how many decimal places it should be to. And I just thought I was like, Very funny. I don't know. The person who said, 


---


#### 01:36:08.996

Oh my God, it's ridiculous to say 99, but it's very reasonable to say 98. But yeah, I agree. I think anything over 90 is   
(01:36:14.702) ~~like~~ (01:36:14.862)  
clearly overconfident. Yeah. Anyway. Timescale too,   
(01:36:20.846) ~~I think~~(01:36:21.188)  
is   
(01:36:21.627) ~~kind of,~~(01:36:21.988)  
I haven't listened to his recent Lex Friedman interview yet, but One sleight of hand that people can sometimes pull on something like this is to just say over the next 10 million years or something, and then it's most species don't last millions of years. And we are definitely on course for radical change. I think in any case, the question is, can we get to some radically different future that we would feel like someone good about at least and that the actual residents of find that it's good for them. 


---


#### 01:36:54.009

And on that point, I'm like, still like fairly radically uncertain, but definitely feel like there's no way to get to those like, crazy nines. But   
(01:37:03.055) ~~if you were to,~~(01:37:03.414)  
if you were to ask a different question of does human existence in   
(01:37:07.697) ~~like,~~ (01:37:07.917)  
roughly its current form go on for millions and millions of years, that I would also have to say, geez, that does seem quite unlikely. But that seems like just too, and I don't want to project this on them, because I haven't even understood this person's take that much. But I do see that sleight of hand sometimes going on where it's like, define what is good to be like what is now too narrowly. 


---


#### 01:37:32.219

And then yeah, you're never going to be able to preserve that indefinitely. But that's also where we've come from. Robin Hanson is a great commentator on the point that we would freak out our own ancestors from not very many generations ago. And   
(01:37:48.662) ~~not in ways,~~(01:37:49.222)  
and not necessarily in ways that they would approve of, but nevertheless in ways where we feel quite confident that   
(01:37:54.787) ~~like,~~ (01:37:55.007)  
they're small minded for their hypothetical rejection of our current lifestyle. So if we allow for something like that, then   
(01:38:04.195) ~~I think it's,~~(01:38:05.056)  
I don't see any way to get to many nones of confidence. Yeah, I agree. I think we should ban anyone giving a PDoom that is like to more than, 


---


#### 01:38:13.703

I don't know, two decimal places or something. It just shouldn't be allowed. Maybe they should all be to like intervals of five. I don't know. I'm going to start legislating and everyone has to listen to me. Anyway, yeah, we should probably wrap up there, but this was really fun. Thank you for doing this. I imagine people already know where to find you, but on the off chance someone doesn't know where to find you, do you want to let people know where they can follow your work? Sure. First of all, thank you. It's been a great conversation. I've enjoyed it as well. And hopefully it's useful to, if not ease anxiety, at least give something tangible to build on and,   
(01:38:52.484) ~~um,~~ (01:38:52.583)  



---


#### 01:38:53.574

I believe that there's enough of a chance worth fighting for. My sense right now is that this is the most important issue of our time. I do think we're walking a bit of a tightrope into the future. Everything is on the table, in my view, from radically utopian to radically dystopian or even extinction-type outcomes. It's going to be a societal kind of tug of war to try to figure out first, hopefully,   
(01:39:19.150) ~~like~~ (01:39:19.310)  
what is actually going on? Can we make sense of all this stuff? And then secondarily, what should we do about it? But hopefully I've helped just at a minimum, create the case that there's enough there that's worth Fighting for others, this is not such a hopeful situation that the right thing to do is disengage. 


---


#### 01:39:37.731

On the contrary, I think there's definitely enough of a chance that if you're worried about this, in my view, the right thing to do is be involved. There are a lot of different strategies that one can play, from chaining oneself to a fence to doing an analytical podcast about it. And I think people should play different strategies. The most brittle ecosystem is monoculture, and I think the most brittle AI safety movement would be one where everybody does exactly the same thing. I actually do think an ecology is the right way to think about a movement. And without being like prescriptive on what strategy somebody should follow individually, I would definitely encourage anybody who has anxiety to   
(01:40:19.935) ~~like,~~ (01:40:20.115)  
channel that into some sort of positive action that can play to their own strengths, whether that's, again, chaining oneself to a fence or otherwise. 


---


#### 01:40:29.940

So to find me online, I have the podcast. It's The Cognitive Revolution. It's at CognitiveRevolution.ai. And other than that, I'm most active online on Twitter, where I am Labenz, my last name, L-A-B-E-N-Z. Awesome. Thank you so much again. This has been a lot of fun.   
(01:40:45.208) ~~Thank you. Cool. Should we stop recording and then... Yep.~~(01:40:49.291)  



---


