#### 00:00:00.700

Jim Auschwitz, welcome back to a special bonus session of the Cognitive Revolution. Yep, there's always more to learn. It's happening quickly today. So By the time we got off the recording, Jan Lejka had posted his tweet thread statement about his reasons for leaving OpenAI, in which he puts it pretty plainly that he's had pretty fundamental disagreements with leadership and has had trouble getting the resources that he needed to do the work, including compute resources, and certainly had some nice, fond things to say to his teammates, but basically was like, I don't think we're on the right track. seems to be resigning pretty much in protest. So that doesn't seem like a good thing, doesn't seem like a good situation. 


---


#### 00:00:49.121

What else have we learned and what do we make of it? We also got coverage from Vox, Bloomberg, from TechCrunch. We got Sam Altman's response actually to Leike, which was extremely graceful. Essentially saying, yes, we have a lot of work to do and we're going to do it and expect a longer response later. It's the basically best possible thing you can say there, but then you're on the hook for doing it. We have Kelsey Piper confirming the nature of the draconian non-disparagement clauses, which apparently have lifetime duration and include an NDA that you can't reveal about violating the NDA. And she claims that when employees come on board for the first time and are given equity heavy compensation, they are not told. but they will be required to sign this disparagement clause or have their existing vested equity confiscated upon departure. 


---


#### 00:01:42.626

So that seems like a really bad equilibrium and way to run a company. I'm honestly confused as to why that's legal.   
(00:01:50.292) ~~Well, it maybe shouldn't be.~~(00:01:51.513)  
Yeah,   
(00:01:52.453) ~~I think you can have to like~~(00:01:53.353)  
very much acknowledge the disparagement clause rules very clearly. Initially, if you're going to confiscate something of immense value for someone not signing them, it doesn't seem reasonable at all. It also doesn't obviously speak well of the company in its openness in the good sense, right? If you're forcing every employee to never disparage you for life, no matter what, or else,   
(00:02:16.171) ~~like,~~ (00:02:16.391)  
that's just not a reasonable position to take if you want people to not assume the worst. And   
(00:02:22.456) ~~so,~~ (00:02:22.697)  
yeah, we look at Leckie's statement, essentially saying that for years, they have had the trouble of shiny new products becoming the priority of move away from safety culture. that the culture is not amenable to or compatible with safety. 


---


#### 00:02:39.436

I'm paraphrasing here a bit. I'm not looking at the words precisely. And that he had trouble getting the last few months despite the explicit 20% of existing compute commitment from OpenAI, which should have been sufficient for current purposes. And indeed, TechCrunch confirms that they have not been honoring their commitments. They have asked for a fraction of the 20% commitment and have repeatedly not gotten what they asked for. And that is part and parcel of the whole idea of running anything in AI. In these days, it's compute. You need your compute to do your thing. And he was reporting this has specifically become a substantial barrier to doing their work. So this is not only a philosophical approach, because he said they should be spending vastly more, but not just a little bit more, but they should be spending vastly more of their resources on preparing for our AGI future. 


---


#### 00:03:32.818

But also, if you'll notice what he actually said, for just the next generation, that essentially he doesn't think they're ready for GPT-5. He doesn't think they're on pace to have the tools they need for GPT-5 to be safe in a pedestrian mundane utility sense, like not in an existential sense. And then later on, there's the problem of superalignment, there's the problem of AGI, which many people opening up said they expect within several years, it's a very short timeline. And now the superalignment team has been dissolved. and its people have been dispersed throughout the company, they claim they will still continue to work on that level. But having dishonored their commitment, and having dissolved the team, and having lost the leadership, it doesn't look like the kind of effort they promised us, they said they were going to do. 


---


#### 00:04:24.113

Does the timeline still hold, Sam? We're now nine months into the four years. If he still expects to need superalignment to succeed within four years, then these actions do not reflect somebody who understands that and understands, as he has repeatedly told us he understands, what is at stake with superalignment? And it's clear Mikey's breaking point was Ilya departing, according to the Bloomberg article, that makes perfect sense. But we have a series of departures. We have obvious justifications for that. And this all makes sense. There was all this speculation about what did Ilya see? That was the whole thing. What did Jan see became the thing after Jan quit. And the answer is what they saw was a company that's not committed to safety. 


---


#### 00:05:11.954

It's unwilling to put its money where its mouth is, that has a culture that is hostile to safety efforts and that is pivoting towards the shiny new product. It's a product company. It's a scaling startup. It's devoted to making money. There's nothing wrong with making money. But in this case, they're building smarter than human intelligence as their explicit company mission and goal. And as Leike points out and as Altman has repeatedly acknowledged, this is not a safe thing to do. is not a default thing that will go well by accident,   
(00:05:44.600) ~~right?~~ (00:05:44.821)  
We can debate how likely it is to go badly with good, real efforts to make it go well. But I think any reasonable person can see that there's a very good chance that if we do not put in the effort, we do not do the work, that things would then go badly. 


---


#### 00:06:02.634

And it doesn't have to necessarily involve like some sort of specific AI catastrophe or accidental risk scenario, it simply means This could go very badly for people, as experienced by people on the planet Earth. And we have to think carefully about these questions. But what's clear is that OpenAI's leadership, as embodied by Sam Altman, increasingly is not doing this, and simply in the last few months is not honoring their commitments. Yeah, that compute one for me is   
(00:06:30.505) ~~pretty,~~ (00:06:31.326)  
pretty troubling. I could try to make a, in the absence of that, it would be a lot harder to I feel like obviously people can have all sorts of disagreements and you can imagine the Sam Altman defense being like You're doing your thing over here in the super alignment team. 


---


#### 00:06:46.637

We're doing our thing over here in the product team Why is that inherently a problem? But yeah, if you can't get the compute to do the alignment work It's not just a promise as X and we got Y. It's beyond specifically saying, you promised us X. We needed X over N, where N is a lot more than one, to do our work. And we couldn't get it. And the super alignment commitment, it sounded a lot, 20% of our currently available compute. That's not a lot over four years, because two years from now, OpenAI is going to have 10 times as much compute, for sure. unless something very strange happens. So over the extended period, this is a reasonably small, very modest commitment, as opposed to in certain similar industries where safety is paramount, most of research costs, most of development costs can become safety. 


---


#### 00:07:39.160

And again, that's for mundane level, just make sure the plant doesn't melt down levels of safety. So I don't understand it. I also don't understand why it's not simply good business to give Mikey and people like that their compute. Relative to this outcome, it seems if we are indeed talking about something like 5% of compute, then that could only allow you to move,   
(00:08:03.454) ~~you know,~~(00:08:04.254)  
5% faster, right? Then you could, without that compute, It does seem like a very strange decision to allow people to walk and allow this to become this big of a story over a couple percentage points of compute availability. Yeah, it's almost always the straw that breaks the camel's back, right? 


---


#### 00:08:28.646

It's clearly, Ilya was being shut out of decisions, couldn't play his ambassador role that he had previously played inside the company, if you're reading   
(00:08:35.309) ~~between,~~ (00:08:35.671)  
not even between the lines, just reading the lines of various articles and reports, that people were turning against the very idea, the board battles embodied this struggle, the way that they played out in practice, whether or not this is at all fair or deserved by anyone or any philosophy or any approach or anything. turn people against them. Every person that leaves turns you further against them. The firings, you have a cascade of every person that leaves, you lose more trust. Like why did they leave? What caused them to leave? 


---


#### 00:09:04.823

How was that handled? And yeah, I think presumably it was just like, I'm fed up. We're not getting what we need. You need to wake up call. I can't just sit here and try to pretend that I have the resources I need because I don't. And the way he talks about it, they're just not investing what a company in their position needs to invest in what I call mundane safety either. And in fact, I haven't investigated this, but you see this in their reports, that 4.0 has much less tendency to refuse inappropriate requests, building a bomb, than GPT-4 Turbo or its rivals, that somehow this new model that was sent out is actually just very not robust in the jailbreak slash mundane harm sense. 


---


#### 00:09:48.706

It just is very willing to fulfill your requests. And I haven't tried to stress test it or red team it in this sense, because why would I? I don't think it's particularly a dangerous thing for the most part to have it be jailbroken. It was already jailbroken for those who cared enough. What's very clear is that the 4.0 development process did not involve a robust attempt to make it safe in a conventional sense. They mostly just decided the capabilities that this model possessed were not so dangerous and they just weren't going to give it much care. So I'll see you chain to the OpenAI fence or what do we do? What do we do from here? 


---


#### 00:10:26.986

I think we have to treat going forward until we see who could respond with an amazing set of commitments. He could respond with a new set of hires of people he's bringing in. He could do any number of things.   
(00:10:39.635) ~~So, you know,~~(00:10:40.596)  
we're always hopeful. And as much as AI moves fast, I don't think we need to move this week or anything like that.   
(00:10:47.283) ~~Right.~~ (00:10:47.524)  
And part of it is that it's very clear from Jan's testimony that we don't have to move this week.   
(00:10:51.649) ~~Right.~~ (00:10:51.808)  
that if Yann and Elia and others were concerned about something imminently happening, they would have said so,   
(00:10:58.464) ~~right?~~ (00:10:58.624)  
It is very clear from this statement that these are long-term concerns driven by things that the ships get steered slowly, no matter what you do. 


---


#### 00:11:07.269

So they have time to fix it. But barring evidence to the contrary, given everything that we've learned, I think we just have to assume that OpenAI is functionally a fully for-profit business fully a move fast and break things hockey stick graph startup business run by Sam Altman, who is running it on that basis, and that they are not taking the safety problem seriously. That their culture internally is hostile to the idea of safety, the idea of worrying about things, and that therefore we should expect them to handle this future badly. That we should expect them not to be prepared for what is to come, to be extremely cavalier. And there's the possibility that for a while, cavalier works out. 


---


#### 00:11:56.136

A cavalier GPT-5 might well be the best thing to happen for the world, if it's possible, if they're just not that dangerous. We talked about not safe for work. We talked about sex and gore and other capability. And yeah, it might just be good to let that stuff happen. It might be completely net positive,   
(00:12:15.368) ~~right?~~ (00:12:15.548)  
I actually expect that. Pliny broke all the major models. He broke all of them fully. And he broke GPD 4.0 in about two minutes during the announcement speech,   
(00:12:26.982) ~~right?~~ (00:12:27.123)  
Like the moment he got access to it, because his first hunch just worked, because why wouldn't it? But at some point, that's going to stop being an acceptable situation. At some point, these things are going to be highly capable, and we're going to have to worry about societal implications, and then catastrophic risk, and then existential risk. 


---


#### 00:12:43.292

And all that's probabilistic, because you don't know when it's going to happen, right? Like when GPD 5.0 comes out, you're probably not going to be in an existential situation at all, but how many nines are you going to put on that statement? And if your answer is more than two, you're crazy, right? And I would probably put one, right? So   
(00:13:01.905) ~~like~~ (00:13:02.046)  
you can come crying and say, you said it was only 97% and it turned out not to happen. I'm like, well, right. Chalk it up to a slightly worse collaboration than you on this one and let's keep betting on sporting events. But I don't know what else to say. I assume you have seen the Dwarkesh interview with John Shulman out this week? 


---


#### 00:13:18.910

Fortunately, I haven't because I have a thing with Dwarkesh where I move slowly. I pause. I write notes. I often go up entire posts. I simply have decided I have not had the time to give this my proper attention. This is even more important now. It's an hour and a half podcast. You're going to have to do a close reading. Yeah. I'm getting four hours, right? A couple. Yeah. So as it's reported now that he's taking on the responsibility for safety writ large,   
(00:13:49.797) ~~I guess~~(00:13:50.017)  
he was already responsible as Drakesh presented him in the interview. He was responsible for post-training the models as described in some of these articles today, he's responsible for making today's models safe. 


---


#### 00:14:01.745

Now he's going to have this kind of additional responsibility rolling up to him of big picture long-term safety. In that context, that interview,   
(00:14:11.073) ~~I think,~~(00:14:11.494)  
is going to give you serious pause. Obviously, you can evaluate it for yourself, but there were multiple moments. You're just hearing what he was doing previously, right? You took the mundane safety guy who is doing the job with mundane safety that it's not necessarily better or worse than the rivals, at least until I haven't evaluated for O in a sentence, and there's again, reports that there's potentially a problem. But that's just completely irrelevant problem to the problem he is now being asked to solve. And if he was doing that job properly, then   
(00:14:43.763) ~~like~~ (00:14:43.923)  
he wouldn't necessarily have mentioned that they have these concerns about the next generation of models not being prepared for, that should be part of the job as well. 


---


#### 00:14:53.710

But yeah, the idea that you're going to use post training on your AGI, or even your ASI, right to render it like HHH or like net useful or any of those terms.   
(00:15:06.370) ~~I think~~(00:15:06.590)  
that's basically a pipe dream, right? I need to rely only on that. And in fact, you have to worry during even the training regimen of some of these things, but it's just the kind of strategy, certainly in a way, but the strategies that he's been using so far are the kind of things that   
(00:15:20.336) ~~like~~ (00:15:20.677)  
he himself acknowledged explicitly,   
(00:15:23.077) ~~like~~ (00:15:23.238)  
both online and literally five feet away from me during a talk would not work. He took on the 80,000 Hours podcast. He explained why ROHF is not a solution to this problem, and he has other solutions that he wants to try where I don't think they'll work. 


---


#### 00:15:39.039

I tried to debate him and explain to him why they wouldn't work, and I was unable to make my case sufficiently convincingly, and he didn't buy it. I think it was reasonable for him not to buy it in the sense that I was making some very bold claims that are very different from his worldview, and I didn't back them up specifically enough because it's hard for me to do with that potential alternative. But like he was thinking about the problem,   
(00:16:00.557) ~~right,~~ (00:16:00.697)  
on a different level than someone who's thinking about post-training is thinking about the problem. So I haven't seen the interview yet. Again, I want to be very careful with it. But yeah, if you think it's going to give me pause, 


---


#### 00:16:09.546

I'm confident you're right. It's going to give me pause. Yeah.   
(00:16:12.928) ~~Well,~~ (00:16:13.089)  
just for folks who might be listening to this and might not have time to go through that or inclination to, it is definitely worth it. It's very interesting in multiple ways. One of the things that he says is that he's expecting, quote unquote, AGI on kind of a two to three year timeframe. Dwarkesh asks him, could it be as soon as next year? And he's like, I don't think so. That would be surprising. But two to three, he was pretty much willing to co-sign on. And then Kesh asked a number of questions that were like, pretty fundamental. What happens when this happens? How are we going to deal with it? 


---


#### 00:16:47.539

Or yeah, at one point he said, I'm not sure that's a, doesn't sound like a super robust plan that you're outlining here. And I appreciate the candor, but at multiple points he was, yeah, I don't really have a great account for how that's going to go. Or hopefully we'll be able to work together with the other leading developers in that situation. And yeah, we don't really have a robust plan for that at this point. second best plan, right? The best plan is to actually figure out what you're going to do and how we're going to handle this. And the second best plan is that no, you don't know, right? Just start with a blank beginner mind. 


---


#### 00:17:21.205

So if he comes to this, if he comes day one, and he says, we don't have a plan, we don't have a solution, don't know how to align this thing. We don't know what to do with it. If we did align it, we don't know how society can handle this. We don't know how to make the transition from AGI to ASI, again, even if we handle alignment and even if we handle the interim. We don't know any of these things. We're lost. We need to figure this out. And he starts from scratch. I'm perfectly happy with that as the answer for a person who's highly capable. He founded OpenAI. He's done a lot of impressive things. 


---


#### 00:17:56.250

Like Ilya, I thought, Ilya cares deeply about these issues and appreciates the depths and stakes of the problem. But I always thought, going off of Ilya's publicly stated remarks, I just don't think any of the things he was thinking about are anywhere near what you need to be thinking or what would work. My key stuff wouldn't work, but I thought he was reasonably grounded and trying hard. Ilya's stuff just felt like it often felt like a misconception. It feels like you're thinking about this problem in a kind of fuzzy, not sufficiently geared way often. and in a way that just wouldn't survive an encounter with the enemy. When he and Nike and the other people run their super alignment team, and they build these papers, they run these experiments, and they actually try to make these things work, they'll understand that their plans aren't working, and they'll either find ways to modify them so they work, or throw them out and try new ones. 


---


#### 00:18:55.236

Because one of the things Ilya's famous for is the kind of attitude of, I'm going to try this as many ways as I have to, and throw out everything I think I know. until I make this thing work and I'm committed to making this thing work, right? And that's what you love to see. And that's why I had a lot of faith that Ilya and Jan would find a way, not every time, because I don't think this problem is even necessarily theoretically solvable by humans in a reasonable timeframe with any attitude, but I thought they had a reasonably good shot because with a lot of resources and a lot of time, and four years is somehow a lot of time, 


---


#### 00:19:27.250

They would figure out at least some ways not to align an AI, and then we get to try again. In fact, their first way definitely won't work in my model. That's fine. They were a mix of ideas that were reasonable and I thought promising and they're hopeless, but nobody knows how to solve these problems. So I can't really get that mad at you for being excited by ideas that I think won't solve these problems. What do I have? A better suggestion? You try something, you learn something, you try again. I think there's a decent chance that looking at these generalization questions, looking at these supervision questions will inform your approach to trying to find alternate solutions that might themselves be more promising. 


---


#### 00:20:09.959

If none of those types of solutions, if that entire solution class is entirely hopeless, right? If   
(00:20:15.601) ~~like~~ (00:20:15.740)  
the worst case scenario that like is as bad as it looks to me on first glance and there's no fixing it, then those are pretty bleak worlds in many ways because that cuts off a lot of people's plans. So I'm leaving room for there being places to maneuver there. So with this clarity, and while we await a proper response from Altman and leadership, what else do you think people should be doing? Joking, but maybe I'm not entirely joking about chaining oneself to the open AI fence. For reference, there was a person who's done that this week. There is,   
(00:20:52.288) ~~you know,~~(00:20:52.589)  
like, mundane consumer protest. 


---


#### 00:20:55.230

Now that ChatsGBT is free, it's going to be hard to cancel our accounts, but we could rally app developers to boycott and switch to Claude or something.   
(00:21:05.132) ~~We could,~~(00:21:05.971)  
obviously, on the record on being at least generally positively disposed to SB1047. We could think about supporting that even more forcefully or suggesting possible amendments to that to strengthen that. Something like the non-disparagement clause being made illegal could be an interesting one. When I look at 1047, I wrote it up. I was mostly looking for ways in which it was too strong. But I identified a number of ways in which this bill might go too far in the sense of it has these serious downsides. And unless we're getting a lot in exchange for these downsides, that makes it politically hard to pass, makes it harder to get buy-in, it makes it harder to get cooperation. 


---


#### 00:21:50.088

And nobody actually wants to tank the economy. Nobody wants to actually slow down the mundane utility. Not nobody, but I don't. And so how can we strengthen this bill? The exception being the derivative versus non-derivative definition clause, where I thought it was just a bug. It's literally, there's a major definitional mistake in this bill. Maybe it's just making it worse on every level. We need to fix it because if I can pass off all of my blame to you, that makes your situation terrible, but it also makes all the same to guarantee something I can skirt. I can then ignore anything. And that's not good either, right? Like, we can't land you for really cheap. 


---


#### 00:22:24.890

We have to stop this. So some of the others were about, like, how do we weaken this? How do we, I don't know if weaken is the right word, but how do we clarify this bill? How do we prevent potential overreach or misinterpretation of this bill in a stronger sense? And then other people pointed out ways in which the bill was potentially too weak. everyone's complaining about the criminal liability but it's only under perjury, maybe that's not enough,   
(00:22:46.705) ~~right?~~ (00:22:46.925)  
One could ask potentially and some people, I think for now it isn't, it is where we want to be and I think the reaction to even the perjury showed us that this is just a third rail and people just get so scared of such things that let's not, it just gonna, it might have very bad dynamics for the good people to stop touching things or they panic and they don't want that. 


---


#### 00:23:06.663

But yeah, we have whistleblower, a clause in the bill. And if you don't enforce non-compete agreements, right, this seems so much worse than a non-compete agreement, right? An ordinary non-compete agreement, which now, not only California is not enforcing them forever, but now,   
(00:23:24.013) ~~like they're going to be~~(00:23:24.753)  
illegal for high paid, all but the most prominent employees, the entire country, because the FTC and less than that. Unless that doesn't go through. But yeah, I have a hard time believing that it is in the interests of the public of the United States to allow a company to hold most of somebody's wealth hostage to signing a lifetime full non-disparagement clause on the company they are leaving in an area in which things that are wrong are in the vital national interest. 


---


#### 00:23:54.521

If there is something wrong with the safety at OpenAI, that's something we need to know. If there's anything wrong with the culture at OpenAI in other ways, that's something we need to know. If you have a whistleblower provision, this is like, how do you blow the whistle if you're not legally allowed to blow the whistle? Or the price is millions and millions of dollars in equity that you can't sell, so they can just confiscate it. So they don't even have to sue you, they can just confiscate it in that situation. And who knows what else they might be threatening or holding over people. I just don't know, again, because we can't talk about it. So I can't know these things. 


---


#### 00:24:28.685

So we have to assume the worst, in some senses, because we can't talk about it. And   
(00:24:34.008) ~~yeah,~~ (00:24:34.147)  
I think it'd be very reasonable to say that AI companies should not be able to sign non-disparagement clauses as pertains to certain aspects of the company, certainly, and potentially universally.   
(00:24:46.732) ~~Like,~~ (00:24:46.853)  
I think it's just, why is it good for the two of us to get to an agreement where we agree that no matter what happens, I can never say anything bad about you and you can never say anything bad about me. I understand why it's better for us, in some sense. But,   
(00:24:59.531) ~~you know,~~(00:25:00.432)  
people not being able to talk in that way just doesn't seem great. And if we're in the business of not enforcing contracts that are against the public interest, this seems like a prime place to look, right? 


---


#### 00:25:10.520

Even though I have libertarian instincts that like avoiding contracts is bad. Yeah, this seems to be a reasonable place to consider that, obviously. But beyond that,   
(00:25:19.729) ~~well,~~ (00:25:19.828)  
if you're not going to do the safety work yourself, necessarily, Someone has to make sure that you do, right? Someone has to be doing the check-in. And this is a reason to doubt, right? If I'm going to trust a company, I have to be able to trust their commitments. I have to be able to trust their statements, their attestments. And I need that to be under some sort of punishment, right? Like that's the whole idea, right? If you, if you lie, right? The idea is that you don't have to do anything. 


---


#### 00:25:47.500

You have to tell us what you are doing and you have to be held responsible if you lied your ass off. That is what, key provisions of SB 547 are about. They're about saying what you're doing and being responsible if you didn't do it, right? And saying what your logic is and being responsible if your logic is a lie. It just makes no sense, willful disregard, beyond the pale. That's what it's about. And also just having a mechanism where if you discover that there is actually catastrophic risk in the room, you can get the model shut down. Both that they have the ability to shut it down, at least locally, and that you can order that. 


---


#### 00:26:23.403

These are the fundamental things that this film is about. So these things seem important now more than ever, right, in the light of this information. But fundamentally speaking,   
(00:26:33.730) ~~yeah,~~ (00:26:33.970)  
I think we have to understand that until proven otherwise, open AI is much less of a confusing than it was a week ago or six months ago,   
(00:26:44.875) ~~right?~~ (00:26:45.036)  
There were reasonable arguments to be made when they announced super alignment, when they put out their reasonably good- I made some of them myself. Yeah, they put out a reasonably good preparedness framework,   
(00:26:57.029) ~~right?~~ (00:26:57.190)  
Like they've done some good things. They've hired a bunch of good people. They had a bunch of people who move in circles where like they're credibly spending a lot of their time talking the right ways, asking the right questions, even if I don't agree with their specific beliefs. 


---


#### 00:27:11.080

And a lot of that's just gone now. Their credibility is shot from a safety perspective. And so I think it's a lot less confusing now. And yeah, I think that   
(00:27:19.907) ~~if you have a choice in whose technology to use, in some sense~~(00:27:24.104)  
at this point, and you choose to go with open AIs in a way that matters,   
(00:27:29.506) ~~well,~~ (00:27:29.625)  
this is part of what you are considering, part of what you're doing. And   
(00:27:32.807) ~~I think~~(00:27:32.987)  
it also means that you are taking on a risk, a concrete risk yourself, because I don't think you should necessarily trust their mundane safety in this world going forward.   
(00:27:44.930) ~~I think it's, I'm still just,~~(00:27:46.211)  
again, there's just not much that can go wrong with a TBT 4.0 that I am that scared of, but they don't have a culture of safety. 


---


#### 00:27:54.078

You need a security mindset to build AIs, to make these AIs do the things you want them to do and have it not go up in your face, even in an ordinary, normal, mundane way. You need to be thinking about these problems and working on these problems and giving them the respect they are due. And so in the entropic business case, for we deeply care about this, and we have a culture of this, which they clearly do amongst their employees,   
(00:28:15.905) ~~right,~~ (00:28:16.006)  
where they encourage this concern, where they have this concern, and we're going to make sure that when you use our product, you get what you're trying to get and not something else you did not expect and did not want. 


---


#### 00:28:27.202

It becomes a lot more interesting, right? And then where Google lies on that spectrum, you can evaluate for yourself. I'm not saying there are any angels in this room. I'm not saying there's anybody that I trust, but there are levels. And again, like we'll see what Altman says next week. What do you think about the revisiting the notion of third party testing as well? This was something we chatted a little bit about prior to my interview with State Senator Scott Wiener, and who of course is sponsoring the bill. The current bill just says, including third party auditors as appropriate. And now it does seem okay. Maybe we step that up a little bit, but we got this AI, 


---


#### 00:29:08.479

UK AI Safety Institute, not quite getting the access. Now we have the resignation clearly in protest. You're a good game design guy. How would you think about designing that game so that the right people get the right kind of access to do the right kind of testing and it doesn't collapse into something stupid? The thing that I emphasized when I analyzed both the preparedness framework and the responsible scaling policy for a tropic was these are potentially very good policies. If the spirit of the rules is being honored, if the people at these companies care about safety, they have a culture of safety, And they don't just look at this as a bunch of check boxes to get through so that they can get through compliance and release their thing and satisfy the scolds. 


---


#### 00:30:01.471

But they genuinely care about this result. And when the answer comes back, technically you passed, but that's funny. The response is no, wait, stop. Think about what's going on. Oh,   
(00:30:12.820) ~~well,~~ (00:30:13.102)  
that wasn't okay. If something's going on here, we need to investigate this. We need to stop this,   
(00:30:17.826) ~~right?~~ (00:30:18.105)  
And you react accordingly. And certainly you can't do it if people are potentially gaming to sabotage benchmarks to be like under worse of thresholds. If you're worried about them targeting the task, they know what questions are going to be asked, they know what exactly is going to be the attack surface that is checked and they strengthen that particular attack surface. These things are death, right? If all you have to do is verify the AI doesn't fall or cause a problem in specific ways, 


---


#### 00:30:47.250

I think you're just toast. And I think there's no set of tests that could be at any reasonable cost that could possibly satisfy that. And that's why you need third-party testing, unless you deeply trust the people who are doing this, right? Do you trust them not to teach the test? Do you trust them to look for anything at all, not just things that are specific? Do you trust them to isolate these groups,   
(00:31:09.023) ~~right?~~ (00:31:09.163)  
I can imagine a world in which I would have that trust, but after what we just saw, do you have that trust?   
(00:31:14.107) ~~No, that I do, right?~~(00:31:15.228)  
In the sense that I   
(00:31:15.808) ~~don't.~~ (00:31:15.989)  
definitely don't. But we saw that I trust their benchmarks, right? 


---


#### 00:31:53.488

ChatGBT was just, here is our very sterilely named, very simply presented, very clean. I still give major props to them for not having glammed it up in various ways. But there are things to love about OpenAI. But they just presented this thing. I'm like, hey, here's a cool thing. Let's see what you do with it. We're not going to tell you how awesome it is. We're just going to put it out there. And the GVD forum was the same way, mostly. And now this is the third,   
(00:32:20.132) ~~I think,~~(00:32:20.392)  
time they've gone out there and gone, here are our amazing abilities, some of which we don't have yet. And that's very different. And then Sam Altman went on Twitter. the day after Google's I.O. to gloat about how his hype was so much of a better vibe than Google's hype. 


---


#### 00:32:37.472

But he hasn't addressed, to my knowledge, any of the concrete things that Google announced or any comparisons as to who's building the better product, even to just congratulate them on a great set of offerings or anything like that. He's just, oh, get a hold of the nerd, basically.   
(00:32:52.546) ~~Like,~~ (00:32:52.686)  
they're trying so hard. Isn't that lame? That's not a good sign either. Yeah, it's not great.   
(00:32:58.236) ~~It's not great all the way around.~~(00:32:59.316)  
I would say I definitely strongly noticed the shift from the earlier releases to the current releases. I would say last fall, the demo day was really the first time where it was like, and even that was more buttoned up than this one, but it was like, 


---


#### 00:33:19.298

The GPTs weren't really ready. They didn't really work that well. And it was the first time where it felt like, man, you guys shipped this even though it wasn't really in shape to ship. There's no safety concern, but they felt like vaporware, right? Didn't work. Yeah. Retrieval was not good.   
(00:33:36.972) ~~I think~~(00:33:37.173)  
that has been improved, but it was a few months later that, and I need to do a little more testing with this myself, but what I've, been hearing in my app development circles is that the assistance API has rounded into form now where the retrieval actually does work much better than it originally did. And it's more like what they described in that first release, but that's relatively recent and it's been a number of months. 


---


#### 00:34:00.747

And this one just seems even messier where it's like, they weren't even really clear on what we were supposed to be getting or,   
(00:34:06.849) ~~you know,~~(00:34:07.150)  
people are just all confused at the moment. I was confused I was trying to get Chatsubt to modify a picture of me and my son and it was not doing it right or whatever. And then I was like, what's going on with this thing? And then I went into go on Twitter and I see, oh, okay, I'm still using the old system and it's just not clear to me what I'm even dealing with. They're not updating a lot of the angles and they're not being clear and experienced by a lot of people complaining. 


---


#### 00:35:13.833

So did you hear about so far? I don't think so. So I mentioned this, if you figure out which week, how many weeks ago it is because time flurries, but so the Chinese have proposed a technique called a sofa because let's build a sofa on from the ancient, from the dystopian offering. The, aliens oppress us using sofons. If you read the book, anyone, see the show? But the idea is you can trap a model, open source or closed, but in particular open source, in a local maximum with respect to certain specified topics, such that if you attempt to fine tune it to get it out of the local maximum, it won't work. Ordinary fine tuning techniques to try and escape from a failure world works. 


---


#### 00:36:01.577

So you could actually, the proposal is you could teach,   
(00:36:04.298) ~~you could actually teach, not just not,~~(00:36:06.440)  
because if you don't teach biology to Lama three or Lama four,   
(00:36:11.740) ~~let's say,~~(00:36:12.081)  
Lama four doesn't learn biology.   
(00:36:13.221) ~~Well,~~ (00:36:13.340)  
there's only so much biology that you have to learn, even a bunch of textbooks, suddenly knows biology, right? Even if you somehow managed to not have it learned by implication. But now the second proposal is you can specifically teach it to not understand the biology and be really dense about it. way that certain people were like, I can't do math and just refuse to learn no matter how much you teach them and how many examples you give them, right? 


---


#### 00:36:35.244

Because they had trauma. You have to give the thing trauma, right? And so that is, if you can make sure that the thing can't learn biology, now you've got an open model that can't give up biology, potentially, very early. We haven't run it through its paces. We haven't tried it at scale. We haven't, but it's an idea. It's the beginning of the first proposal I've ever made. In theory, maybe we could do something that raises the cost above epsilon, compared to the training charge of the model, to take somebody's general model and turn it to whatever specific end we have. But maybe this would start to require enough work that we're not just making it easy. 


---


#### 00:37:16.737

And if we can do that, now we still have the problem of you have to enumerate all of the specific things you want it not to know. You have to figure out all the things you want to stop it from knowing and block them. Again, very similar to what we saw in the book, not a spoiler, but the idea being, you see this in sci-fi all the time, right? You see the villain or the oppressors or whatever it is. And they say, oh, all that matters to us is that you don't do X, Y, Z. If you can't do X, Y, Z, we're fine. And someone finds out a way to do W, right? Someone finds out a way to do something that they don't detect. 


---


#### 00:37:51.670

It doesn't count to them. They just, they don't understand what's going on. And their response to that is to ignore it. Constantly happens, right? People start doing weird shit,   
(00:38:00.931) ~~right?~~ (00:38:01.192)  
And the aliens that are trying to take, they've taken over the enterprise or whatever it is. They go, I don't know what weird shit is going on, but eh, whatever. Whereas the correct answer, of course, is I don't know what weird shit is going on. So stop what you're doing until I know that's not okay. But,   
(00:38:18.530) ~~you know,~~(00:38:19.371)  
if you only can have a kind of scripted,   
(00:38:21.213) ~~like,~~ (00:38:21.434)  
I detect you do bioweapons, you can't do nuclear bombs, you can't do chemical weapons, you can't do cyber attacks, blah, blah, blah. that's fine for now. 


---


#### 00:38:32.583

It's incredibly helpful for lot four, but it's not that helpful for lot six, even if it works, because it's no longer going to be the threat that you knew was coming. You're not going to be able to enumerate what a smarter thing than you comes up with, right? So it works up to a point, but it's still incredibly helpful. And it potentially raises the bar quite substantially to the point where maybe we can all reach an agreement if this works great.   
(00:38:55.496) ~~I don't know, but you know,~~(00:38:57.416)  
If you want like a moment of hope or something to end this with, there are at least some proposals. Yeah, that's good. I hadn't heard of that and it definitely sounds like something I need to go do a little more homework on. 


---


#### 00:39:09.079

Anything else? Because you're one for one in terms of new and very interesting pointers there. Yeah, I haven't seen that much in the alignment sphere lately, unfortunately. I haven't seen new evidence that things won't work particularly, but it's just been like relatively quiet, I would say, on that level. something has to be quiet, right? You can't have everything happening all the time. If you were to pitch a movie concept that you think would be most influential right now in the way that   
(00:39:39.036) ~~like~~ (00:39:39.215)  
her seems to be inspiring the current moment of technology, mine might be The social network meets the Lord of the Rings, where the sort of central figure would be the Sam Altman type, who is on this like meteoric rise of technology, but is also being corrupted by it in the way that the ring is corrupting. example just because the ring was a metaphor for the actual AGI for decades. 


---


#### 00:40:09.757

That seems like the story that we might need to all hear. We used to talk about the fellowship taking the ring to Mordor as a sort of metaphor for some of the things that might happen in some scenarios. But yeah, certainly you could tell that story.   
(00:40:24.021) ~~I think,~~(00:40:24.362)  
my instincts tell me that's not   
(00:40:25.882) ~~the~~ (00:40:26.061)  
the most interesting approach to that.   
(00:40:30.023) ~~I think~~(00:40:30.384)  
If I was going to do it,   
(00:40:33.222) ~~I think~~(00:40:33.581)  
I would maybe just do a very kind of straightforward AI takeover scenario with not the smart intelligence anywhere. Just show the humans only giving up control. Show the humans, because it's in everyone's individual interest, no one can stop it. Just show things just spiraling out of control. 


---


#### 00:40:56.592

One thing leads to another. There aren't even any idiots and there are no villains. Things just go wrong. And there's that. You could also have a law and order artificial intelligence set in 2035. That'd be fun and interesting. So the idea being that,   
(00:41:12.625) ~~well,~~ (00:41:12.844)  
not these open models have given everybody these extra capabilities. We have to be very proactive about hunting down people who try to implement catastrophic threats. And then obviously the police have all their AIs and everyone's acting on a high level. But   
(00:41:24.804) ~~it just,~~(00:41:25.045)  
it's one of these things where you notice that   
(00:41:27.065) ~~like~~ (00:41:27.264)  
the world almost blows up every other week. And that- Do you think we can write that? That's one of the challenges that I have with this in general is I feel like leap from here to there is tough. 


---


#### 00:41:39.327

Like what does that actually, what does the procedural look like? Can you imagine that getting concrete enough to be shown on TV in a way that- Procedural, right? Is Star Trek not a procedural? in its own way, right? We explore a strange new world, we find an ethical dilemma, we find a technical, we got a technological problem, and we have our debates, and then we encounter a setback, and then we implement our solution, we solve the dilemma, and we go on our way,   
(00:42:06.621) ~~right?~~ (00:42:06.822)  
It's not that simple, but it also is, right? And so you find a way to do a version of that, potentially. But yeah, like a fun game for watching any sci-fi show is, nope, how often things almost go horribly wrong. 


---


#### 00:42:23.824

I just watch a season of any Star Trek and watch how often the ship almost blows up, right? Or show how the Federation is almost in dire danger, right? And how often it happens because someone was being a complete idiot and how often it happens so naturally, but like they encounter all these different problems. And then ask yourself,   
(00:42:43.619) ~~well,~~ (00:42:43.880)  
if you just looked at the 45 minute mark of every episode and you had to assign probabilities that this wasn't Was it a narrative someone wrote? What's the chance that humanity would have survived from here? Or what's the chance the ship would have survived from here? What's the chance the Enterprise actually makes it through seventh season? 


---


#### 00:42:59.266

The answer is zero, right? It's so challenged in so many different ways. And the Federation is probably challenged too. The Federation is in a lot of trouble reasonably often. Luckily we got out of it because we wrote it. This is not a utopia in the sense that if you actually were there, the safeguards aren't there, right? There's no robustness in this world. This world is fragile. The Star Trek universe is so fragile. And so we get lucky a lot, but   
(00:43:23.853) ~~like,~~ (00:43:24.134)  
why are we getting lucky unless it's you protecting us or the travelers, right? In a way that we don't understand. And so you carry forward. The other game is like, how often does somebody come within five minutes of building an ASI? 


---


#### 00:43:36.920

How often would AI just run completely random here if you didn't have the rule of mysterious intelligence? And that also is just scenarios. So you could also just have an ordinary sci-fi show where it just runs a normal sci-fi world, except that every now and then, and by every now and then, one episode in three or something, someone accident follows through on logic and superintelligence emerges and everybody dies or someone takes over the world or everything gets killed or some new regime happens or there's a recursion. Who knows? I haven't thought this through. I'm brainstorming with you. But the idea being that imagine if you got to just, and then of course the world just, you see the rewind where everything goes back in reverse and then the person just chooses not to do it. 


---


#### 00:44:23.099

But like once, once every episode or two, like somebody almost comes to the world and they just decide not to. And there's really no explanation why they don't probably. Yeah. The sort of garden of forking paths is a pretty interesting idea. I minded of the three body problem too, has part of its story   
(00:44:41.315) ~~kind of~~(00:44:41.534)  
goes that way where the civilization is being restarted and rerun over and over again. And it just ends at various times and then it gets booted up again, but   
(00:44:52.280) ~~it's,~~ (00:44:52.420)  
they last different lengths of time and some of them are short and others are longer, but they all   
(00:44:56.722) ~~kind of.~~(00:44:57.001)  
end and get rerun. And I do think that would be an also a pretty interesting way to present the future that like some branches of this tree are terminal. 


---


#### 00:45:08.907

Yeah. The three-body problem is such a weird, I don't want to spoil anything, but it's such a weird, and I'm going to do my best not to, but it's such a weird mix of this kind of fully cynical, hard realism beyond what I think is even accurate. where,   
(00:45:25.150) ~~like,~~ (00:45:25.411)  
the universe is this cold place that wants to kill you so badly and,   
(00:45:29.014) ~~like,~~ (00:45:29.273)  
you can afford not the slightest bit of kindness and decency if you want to survive. At the same time, we don't just all die. In some sense, it may have been. I'm like, there's a book two, there's a book three, but it's just not the way the book exists. And the book isn't like what happens to Triceratops after we get wiped out. 


---


#### 00:45:45.929

The book is about people in some sense. So. Okay. Last question, I think. Do you find yourself shifting at all in terms of your sense of whether or not we may be in some form of simulation? Simulation on policies has always been essentially all of the value lies in the world, lies in the places where we're not in one. If you make nine simulated copies of me, and then there's me, and you put us in ten copies of the situation, but one is real and the other nine will just be recorded into videotape and viewed back later or something.   
(00:46:22.103) ~~Well,~~ (00:46:22.364)  
shouldn't I just act as if I'm the real one? Isn't that just currently the correct strategy? 


---


#### 00:46:27.867

Even if there's 9,999 of them, maybe I still have the correct strategy. Or rather,   
(00:46:33.902) ~~if you are the ancestor,~~(00:46:34.862)  
if this is an ancestor simulation, and so then 20,000 years later, we try to run a bunch of sims of the ancestors.   
(00:46:40.965) ~~Well,~~ (00:46:41.224)  
the ancestors who decided they were in a simulation, ancestors who, when faced with the ancestral situation, figured out that given the situation, they were probably in an ancestor simulation, and therefore didn't need to actually make sure that the civilization progressed to a point where they could run the future ancestor simulation simulations. Those guys don't get simulated, right? Because those civilizations don't make it. There's a real sense in which your only simulation hypothesis is only valid if you treat it as invalid or something like that, right? 


---


#### 00:47:10.117

You have to take the situation seriously. And also, what's the point of a simulation where the person finds out and acts like it's true? There are a bunch of movies like that, right? No spoilers, even naming them. But in general, the point is to treat it as real. The whole goal is to treat it as real. And   
(00:47:26.608) ~~I don't really see any, I don't know,~~(00:47:28.869)  
right? Obviously there is some probability This is a sim of some kind. I can't rule it out. I will sometimes jokingly refer to things in that kind of way. The writers were a bit on the nose today is one of my things I'll sometimes say, but you can't take it seriously in the sense of changing your being. 


---


#### 00:47:49.123

What about moving to the Caribbean and unplugging? I just saw an interesting tweet from Amanda Eskell from Anthropic the other day where she said, I don't think AI is definitely going to kill us all. I'm not a doomer. If I were, I wouldn't be working on this. And she was also, I do think it's a real risk, but I think it's something that we can shape and hopefully I can have an impact on. But if I really was a doomer, I would just head to the Caribbean and spend the rest of my days there.   
(00:48:15.336) ~~I do know,~~(00:48:15.878)  
I do have a close friend who basically has that attitude. that he said, I just want to enjoy the good times that we have and not worry about it too much. 


---


#### 00:48:23.603

And then she also said the downside of this or flip side of this is if I ever do get burned out and decide to take some time off in the Caribbean, people will take it as a sign of doom. I saw that too. Yeah. I think it was Jeffrey Miller. I'm not sure exactly who it was though, who said it wouldn't work. The food would turn to ash in your mouth. You wouldn't get no joy because you wouldn't know this. And   
(00:48:42.458) ~~I think that that was,~~(00:48:43.137)  
that's largely true for me,   
(00:48:45.139) ~~I think.~~(00:48:45.380)  
knowing I just walked away from this thing and was ignoring it, it just wouldn't sit well with me. And I wouldn't be able to just go, hedonist. 


---


#### 00:48:53.210

It just wouldn't work.   
(00:48:54.871) ~~Right. You can,~~(00:48:55.472)  
you want Mr. Reagan to fuck back into the matrix.   
(00:48:57.893) ~~Well, you need your memory kind of wiped in some sense.~~(00:49:00.597)  
You need to really not notice for some people. Whereas I enjoy,   
(00:49:05.221) ~~I enjoy fighting.~~(00:49:06.021)  
I enjoy struggling. I enjoy the striving to do better and to solve problems. That's what my thing is. I would never go to the Caribbean because what am I doing in the I'd just be bored week anyway. But unless I'm just   
(00:49:18.498) ~~like~~ (00:49:18.657)  
posting, making sports betting, bookmaking decisions again, that's the longest I've ever been to the Caribbean and had a good time. So there you go. But I think   
(00:49:26.398) ~~like~~ (00:49:26.659)  
it highlights, by the way, the fact that the word doomer is just a slur and has been completely misappropriated and misallocated, right? 


---


#### 00:49:35.300

Because who is a real doomer? The doomer is the person who says that there's nothing to be done. that what we do doesn't matter, right? The Doomer is the person whose PDoom is 0.9 bar or Otherwise, it's all over. There's nothing you can do. And when you see those people on climate change, doomers, who think that humanity is doomed and there is nothing you can do about it, your decision doesn't matter. Whereas if you think your decision matters, as Amanda pointed out, that makes you not a doomer. Yeah, so what if we could lose? We can win. And you can help fight this. It's an ancient Judaic idea, right? The universe hangs in the balance, right? 


---


#### 00:50:11.744

The scales oscillate, and it could be up to you. which way they were good and evil and God's judgment. You can decide which way this goes. And obviously, this isn't a God's judgment thing.   
(00:50:22.418) ~~It's not a,~~(00:50:23.039)  
there isn't a moral tone to this. It's about solving a problem. But if you can take it from 34.007% to 34.008% chance of victory, that's a great life, right, in some sense. Look at all the utils, including just for you. Imagine what happens. But yeah, if you don't have any way to interact with the problem, that you just decide to go off and do something else, it makes perfect sense to me. You can't burn out, right? If what Amanda needs to do once a year is sit Mai Tais on a beach in the Caribbean for two weeks so that she can regain her mental health and she can go back and resume, then she should do that. 


---


#### 00:50:56.313

She should do it for an entire year after five years of working because otherwise she won't be able to think clearly or make a good decision. We should do that. There's nothing wrong with understanding your limitations,   
(00:51:07.822) ~~right?~~ (00:51:07.981)  
  
(00:51:08.003) ~~Like we can't,~~(00:51:08.503)  
life is not all. I make an effort to   
(00:51:12.186) ~~like,~~ (00:51:12.286)  
work on other things, think about other things. I need a family. I try to have fun. I'm going to Madison Square Garden tonight with my old magic friend. And we're going to have a   
(00:51:23.085) ~~watch, we're going to do a~~(00:51:23.704)  
watch party where 40,000 of us Knicks fans are going to watch game six on a video screen because they're in Indiana. It's going to be fun as hell. 


---


#### 00:51:32.626

Right. And I don't claim that I'm saving the world here. I'm not. I just want to reflect that. Yeah, I feel you. I think it's probably a good place to end it. Any other closing thoughts? Indeed, many amazing and wonderful things and weird things come to pass. And best of luck to everyone. And I look forward to his response and where all these people land next,   
(00:51:55.922) ~~like who are,~~(00:51:56.561)  
this is some great talent. Someone's gonna snap them up or they're gonna do something. So we'll see what happens. No doubt about that. Well, the saga will continue, but for now, I appreciate the extra time today. Svima Svets, thank you for being part again of the Cognitive Revolution. 


---


#### 00:52:12.432

Absolutely. It's been a pleasure. 


---


