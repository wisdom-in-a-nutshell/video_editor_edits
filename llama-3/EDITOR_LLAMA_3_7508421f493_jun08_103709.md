#### 00:00:00.129

Lowe Crivello, CEO of Lindy and feeler of AGI. Welcome back to the Cognitive Revolution. Thanks, Nick. Yeah, it's like becoming a tradition. Glad to be here as a voice. So you were just saying you are most of the way through the talk of the week, which is the Leopold Eschenbrenner Situational Awareness Manuscript, and apparently you're losing sleep over it. Yeah, if you haven't read it yet, you should. For the record, it's not the first time I literally lose sleep over AGI. But yeah, I slept five hours last night. I'm quite tired. It doesn't make for... This is probably not a good idea to read this right before going to bed. It's pretty freaky. I think the argument is very cogently laid out. 


---


#### 00:00:43.502

At this point, I don't understand how people don't freak out, honestly. I think if you understand what's going on, you should freak out,   
(00:00:48.923) ~~I think.~~(00:00:49.162)  
Yeah. Yeah, I tend to agree and I might circle back to this later, but I think there's a big question of whether or not this, we can assess this thesis on multiple dimensions. One is like how accurate it is, how compelling it is. Another is, is it an idea worth promoting orthogonal to its potential accuracy? And I think before we do that, maybe let's   
(00:01:15.114) ~~work our way,~~(00:01:15.555)  
build up the case and get people to the point where hopefully they're with us in terms of freaking out at least a little bit. 


---


#### 00:01:22.506

I think one of the things that people hear and repeat without being maybe as critical as they should be about it, is the idea that we're probably hitting a wall now because we haven't seen anything better than GPT-4, and that's been, geez, 18 months plus since OpenAI finished training that, so that means we're pretty much stuck at GPT-4. I think we would both agree that is not really an accurate description of the last 18 months, but how would you rebut that claim for starters? I'll start by saying that people have been saying that for 10 to 20 years or something like that, that deep learning was hitting a wall. It's a meme at this point, right? 


---


#### 00:02:04.644

That's the first thing. The second is it's not true that we've not seen anything new since GPT-4. We have seen models that are basically at GPT-4 level of performance, sometimes greater, for literally about a hundredth of the cost. and about 10 to 100x the speed of GPT-4. So I think Gemini 1.5 Flash is greatly under-discussed. It's insanely good of a model. It's very cheap and very fast. And that's a big deal. And I think the cool thesis of the AGI people is the scaling hypothesis, right? It's, hey, if you can make it bigger, faster, if you're going to throw more compute at it, it's like intelligence emerges as a result of that. And I think that Not every inference optimization is also a training optimization, but by and large, when you see this kind of inference optimization of basically 100x, something also happened in the training pipeline. 


---


#### 00:03:07.442

And in parallel to that, you are seeing obviously context windows are becoming infinitely bigger. 18 months ago, we had 4,000 tokens context windows. Today, we have 1 million tokens context windows. That's pretty unreal. We have the member architecture that's starting to be shipped and it's working from what I'm gathering.   
(00:03:24.774) ~~I think~~(00:03:24.993)  
we're on track.   
(00:03:25.935) ~~I think~~(00:03:26.155)  
GPT-5 is going to take people by surprise. I have no insider information, but I feel like GPT-5 is going to come out in Q3, Q4 this year.   
(00:03:33.043) ~~I think~~(00:03:33.242)  
it's going to take people by surprise.   
(00:03:34.604) ~~I think~~(00:03:35.044)  
we're on track. I have no indication that we are not on track. Yeah, I think Flash is a great data point and you hit the key points there. 


---


#### 00:03:43.151

And just to give a use case example of this, which I've maybe talked about in a couple episodes, but I tried to get it to write a character sketch of me based on the last 250 emails that I sent. That turned out to be about 250,000 tokens. which is outside of the context window for any other commercial provider. Anthropic maxing out at 200,000 still doesn't get quite there, but Flash has the million. So it   
(00:04:10.825) ~~literally has, as you said, a hundred times,~~(00:04:14.088)  
more than a hundred times longer context window than the original GPT-4. Now it's only eight times longer than the latest GPT-4, but still over a hundred since when people are referring back to perceived flattening. 


---


#### 00:04:27.255

And it is 1% to 2% the cost, and it is a lot faster. And it did a phenomenal job of writing a character sketch of me based on just this data exhaust, something that I didn't even really prompt, engineer, or optimize. I literally just said, here's a big boatload of emails out of somebody's account. Can you synthesize this into a useful sketch? And it really was super impressive. And that took about 45 seconds and cost under $0.20. so that is the kind of thing that and just for contrast   
(00:04:56.024) ~~right~~ (00:04:56.204)  
to do that back in the original gpt4 would have been chunk it down into   
(00:05:03.271) ~~like~~ (00:05:03.432)  
lots of little pieces a few emails each try to summarize those summarize the summaries and honestly i don't think you could have expected to get anything as good as what I got out of just a single call, just because so much would be lost at each step and you'd have to ladder up to the final summary. 


---


#### 00:05:22.033

And yeah, just the ease of that, the cost, the speed, it really is incredible.   
(00:05:26.899) ~~I also think~~(00:05:27.420)  
maybe we can just riff on a few of these other things. I think GPT-4.0 is also really under-discussed, mostly because people haven't actually had a chance to play with it yet. But I'm sure you've seen the tweet from Greg Brockman where he used the model to generate an image of somebody writing on a chalkboard where the content on the chalkboard is probability of text, pixels, audio. And the obvious implication of that, in addition to just flexing the image generation capability of the model, the obvious implication is that this is a   
(00:06:02.910) ~~single transformer that is handling all of these, or~~(00:06:06.372)  
a single architecture, might be slightly modified transformer at this point, who knows? 


---


#### 00:06:09.934

Could be some state space stuff in there, but it's handling all of these different modalities on par in such a way that we're no longer pipelining data through a transcription and then put that text into the model and then get text back and then maybe generate an audio or create instructions for DALI 3 to then generate the image. But it's literally all happening in the same space. And the generality of that is really,   
(00:06:36.838) ~~I think,~~(00:06:37.939)  
going to be a huge deal as people actually get their hands on it. We don't have it yet, but when I think about what I would build with that, it gets pretty crazy pretty quick. The fact that they solve the interruptions too in such a nice easy way, it's like the speed with which they're able to convert the tokens in is, and it seems that they're modeling, 


---


#### 00:07:03.935

I want to know more about how they do this, I'm sure many people do, but it seems like the fact that it handles this interruption so smoothly suggests that it is putting its own audio and your audio right into the same audio mix. And so it's immediately able to stop when it realizes that there's some other audio in the same space superseding it. That is pretty awesome. Yeah, it's a bitter lesson strikes again, right? Instead of engineering your way around interruptions, what's the threshold and how many seconds and all that, just   
(00:07:36.779) ~~train it.~~(00:07:37.079)  
Train it to figure this out, right? And it's just end-to-end giant-ass model. I agree with you that the multi-modality of GPT-4.0 took me by surprise. 


---


#### 00:07:46.886

I didn't expect to see this kind of heavily multi-modal model come out so soon. By the way, it hasn't come out yet, indeed. Now, I actually find its reasoning abilities to be very underwhelming. I've actually moved away from GPT-4.0 for most of my workflows, but I think that's fine. I think the scaling hypothesis is going to take it from here. But yeah, multi-modality of GPT-4.0 is awesome.   
(00:08:08.822) ~~So~~ (00:08:08.923)  
yeah,   
(00:08:09.406) ~~I think,~~(00:08:09.788)  
I don't know what to tell you, man, like a GI is coming. I will say though, one thing, I've updated in two ways, actually, in recent months. One of them is, and both of them are in the book, right? One of them is, if we don't get AGI by 2030, then probably we won't get it by 2040. 


---


#### 00:08:28.593

It's like, there's like this window right now where we've got like the next five years are going to be very critical. And the reason for that is because there's a lot of one-time optimizations that we're going to grab during the next five years. So the data, obviously, is going to run out. It may not be that big of an issue, but we're going to have to figure this one out. Most important to me, we're going to run out of dollars. There's a lot of scaling that's been happening just because we've been investing so much more. But at some point, once you hit a $1 trillion training run, you can't really grow much bigger than that. 


---


#### 00:09:01.140

So basically, there's these orders, and then there's what he calls the unhuddling of the model, where it's like, the low-hanging fruits of the cognitive architecture. There's a lot of stuff around the model that we need to figure out. All of those are like one-time gains. So we have these three to six orders of magnitude of improvement ahead of us. Once we've grabbed them, if that doesn't lead us to a GI, then we are left with Moore's law, which is pretty slow. It's half an order of magnitude a year or something. and we're left with architectural improvements which are also not super fast. So that's the first way I've updated. If AGI doesn't happen by 2030, 2032, 


---


#### 00:09:43.918

I think then we're left with 10 more years of no AGI. The second way I've updated is on the speed of the takeoff. I used to be very undecided about take-off. Is it going to take a day? Is it going to take 10 years? And I'm increasingly convinced that it's   
(00:09:58.543) ~~like~~ (00:09:58.703)  
one to four years or something like that. But I no longer decouple AGI and ASI as strongly as I used to. And so   
(00:10:07.206) ~~ASI, super intelligence,~~(00:10:08.547)  
I used to be like, AGI for sure, ASI, I don't know. Now I'm like, if you've got AGI, you've got ASI, period. because you can automate AI research and all of a sudden you literally 1,000 extra bandwidth of AI research. 


---


#### 00:10:19.890

It seems to me like a foregone conclusion that leads you to exponential improvement very quickly. Yeah, I've been trying to fight that notion in my own head and this is previewing or tipping my cards on how I feel about the value of promoting some of these memes because I'm like, I don't want to see us get into a AI arms race with China. That seems like a very bad scenario, and I would almost do anything to avoid it. And so I do find myself being motivated to say maybe, because I think the path to some sort of AGI seems quite clear. And I honestly think it's not even really that much more than what we have today. 


---


#### 00:11:01.431

I think if they wanted to In terms of when AGI will be declared, it seems to me almost more a function of how and when OpenAI wants to renegotiate its deal with Microsoft than anything else because they have the contractual clause where when the OpenAI board declares AGI to have been achieved, then they don't have to give Microsoft the IP. Obviously, they're running around trying to court other infrastructure providers and diversify their PowerBase away from purely being dependent on Microsoft. Microsoft, of course, I think is well aware of this too and investing in their own internal research. But at what point would they want to make that move and be able to renegotiate? I think if they wanted to declare that in 2025, they probably could. 


---


#### 00:11:55.033

It seems like you start to think about how many unhoblings are really necessary from where they are today. The reasoning and the coding ability isn't superhuman, but it does seem to be on par with   
(00:12:09.798) ~~your, or~~(00:12:10.239)  
maybe even a bit better than your typical knowledge worker. And   
(00:12:13.716) ~~if it's,~~(00:12:13.995)  
if you imagine the sort of investment in post-training of the sort that John Shulman, Dwarkesh is really on an unbelievable run here in terms of creating historical artifact podcasts. The Shulman one I thought was, it was noted, but it wasn't as noted as this last Leopold one, but it was quite striking when he was like, when do you think this will happen? Well, next year, maybe two to three years, but it was just like, yeah, we're going to go collect a lot of kind of mid and longer   
(00:12:42.931) ~~uh~~ (00:12:42.990)  
project execution episode data and teach the pattern of like how you run into obstacles and backtrack and come up with some other strategies and go around those obstacles and that seems like it's probably well underway in terms of the collection of that data. 


---


#### 00:13:01.220

I would be shocked if they don't already have scale and potentially several other partners already tracking the work of knowledge workers in a variety of different verticals and annotating what they're doing, explaining what they're doing out loud. We don't have chain of thought in the sort of raw internet data set, but it's not that hard to collect if you're just   
(00:13:19.418) ~~like,~~ (00:13:19.559)  
prompted every few minutes to explain what you're doing as you're working to just dictate it via microphone. So it seems like they're going to have that. It seems like the next generation of   
(00:13:29.446) ~~the~~ (00:13:29.625)  
next big upgrade, in addition to being smarter, it should have these   
(00:13:33.908) ~~like~~ (00:13:34.067)  
mid-length at least project execution capabilities with the ability to get stuck and restart and come up with other ways and gradually get there. 


---


#### 00:13:44.875

And I think you could probably call that AGI, if you just stuck to the very literal textual definition of something that can outperform the majority, outperform humans at the majority of knowledge work. That seems like it really can't be that far. Have you had a chance to use anything like Devon or,   
(00:14:04.024) ~~I guess to close my loop on my thought,~~(00:14:05.806)  
Let me ask you first, have you had a chance to use anything like Devon or the new GitHub workspace? I have not, no. I usually do all day, so I see agents doing real stuff all day, yeah. I think they're quite impressive. I think Devon definitely has an interesting paradigm where it's, especially for someone who is not a full-time coder themselves, with the GitHub workspaces, my feeling was like it's very much a product for coders. 


---


#### 00:14:37.821

It starts with an issue. You have to have a GitHub issue to start with. So for anybody who's not a coder, right off the bat, they're like, what's a GitHub issue? I don't really, I'm lost. Devon, on the other hand, you just go to the website and you say, here's what I want you to do. And it assumes nothing in terms of any infrastructure that you have, whatever, it handles all that on its side. What's really interesting about it and feels like the future is that it just goes to work. You don't have to really do anything. It may get stuck, or you may observe that it's doing something a little weird, and you can just chat with it even while it's still working, and then it will interrupt itself, absorb your message, rework the plan if it needs to, and then keep going. 


---


#### 00:15:19.172

But you can just drop in a message anytime you want, and it just keeps going, incorporating your messages when they show up. But otherwise, if it runs into issues, it'll just try to work through them. I wouldn't say it is drop-in. Honestly, it does feel like coding intern. If you had a young person who hasn't run into a lot of things before and needs that sort of basic coaching, it probably feels like that. It honestly is getting to that level. And with one more turn, it feels like it would be likely competitive with a not elite, but employable coder. I agree. And   
(00:15:54.259) ~~so,~~ (00:15:54.499)  
again, I think people are asleep. No one is really realizing what is for sure happening, which is, yeah, like AI code devils are like 100% happening in the next few years. 


---


#### 00:16:07.144

And I was actually just having this conversation with a teammate And I'm realizing that   
(00:16:12.865) ~~I think~~(00:16:13.326)  
sometimes there's this real psychological thing that's happening, and I don't know what to call that logical fallacy, but it's like, suppose I came to you and I was like, Nathan, I'm going to tell you two things. I'm going to tell you, number one, here's proof of very compelling evidence that someone's going to come to your house and kill your family tomorrow. And number two, a shark tornado. A shark tornado, Nathan, is going to come to your house. And there's this weird thing that happens psychologically, which is when you hear that, all of a sudden, you're like, ah, nonsense. 


---


#### 00:16:43.644

Shark tornadoes, bullshit. Even though I just showed you evidence that even on its own, it's very, very telling that someone's going to come to your house tomorrow, right? So I think what's going on with the whole AGI discourse is there's so many claims that are bundled into the same bundle. Because realistically, we're basically speedrunning 100 years of history, if not more. We're going to speedrun them in the next 5 to 10 years,   
(00:17:06.236) ~~right?~~ (00:17:06.455)  
And so lots of shit's going to happen,   
(00:17:08.597) ~~right?~~ (00:17:08.817)  
And you've got folks like Elijah Yudkowsky, who are making outlandish claims to some. I happen to find them very credible, but that the whole sci-fi thing is going to compel matter and it's going to take over the universe and all of that stuff. 


---


#### 00:17:25.394

And so people are like, okay. And then I think there's like very straightforward claims that are also being made about, look, the AI engineer is coming, right? We are going to automate all knowledge work, period, right? That's coming in the next five or 10 years, right? So we're going to have to deal with that. And even when you talk about the risks, so this is merely disruptive,   
(00:17:47.846) ~~right?~~ (00:17:48.047)  
But heavily disruptive,   
(00:17:49.707) ~~right?~~ (00:17:49.906)  
Hey, we're automating all knowledge work. FYI, this is happening. No practitioner disputes that, period. I find it insane that we've moved on from the debates, oh yeah, of course, let's not talk about that, let's talk about the sci-fi stuff. But hey, FYI, people don't know that. 


---


#### 00:18:05.372

People don't know that thing that we all agree about and don't even talk about anymore because it's just so boring. We're automating all knowledge work in the next five or ten years. And then also, as far as the risks are concerned, you don't have to worry about the sci-fi misalignment risks to be deadly worried. I think there's two broad categories of risks. There's   
(00:18:25.722) ~~like~~ (00:18:25.883)  
the AGI goes rogue, and I understand why people are skeptical about this. I disagree, but I understand. But then there is another form of risk that I understand a lot less that you could disagree about, which is the misuse kind of risk. I do not understand how there can be any doubt whatsoever that people are going to do evil shit with this extremely powerful technology that we are putting up to everyone and gleefully open sourcing. 


---


#### 00:18:53.601

I don't understand the case. And I don't think there is a case, period. I think it is extremely telling that the majority of the case that's being made about this is not a case at all. It's just slogans. It's all my dead body. You will take my second amendment, first amendment. Hey, can we just talk on the objective all about what we're talking about here, right? I believe it's Dario Amodei from Anthropic who made this excellent point. He said, We've been benefiting from an invisible protection in the world today, because there is a set of evil psychopaths out there. That's just a fact, that's indisputable, who want to do evil shit and kill people. But then the overlap between the evil psychopaths and the capable people is very small, because as you become capable, as you educate yourself, two things happen. 


---


#### 00:19:37.292

One is you become socialized, right? And so you don't want to kill people. But also, your opportunity cost rises because now you can do things with that education. You can become a trader, a software engineer, an entrepreneur, an investor, whatever. And so you can make money, and so you've got better things to do with your time than go around and kill people. And when there is an exception to that, and when someone very educated goes rogue, it's notable. It's like the Unabomber thing. People remember it for decades. This is a freak thing that just happened. But I think that protection is disappearing. I always talk about the upside of that. Oh, look, we're going to empower everyone with AGI. 


---


#### 00:20:13.905

A 17-year-old is going to have the same go-to-market power as the Coca-Cola company because he's going to have 10,000 employees in his bedroom. Mr. Beast built a media empire from his bedroom. Same thing. You're going to be able to build an industrial empire as a 17-year-old in your bedroom. That's awesome. The corollary of that, though, is that we are just giving capabilities to everyone. And so some people are going to do evil shit with these capabilities. It's just obvious. And so I think really like the two most obvious classes of risks here are going to be bio and cyber. And   
(00:20:49.232) ~~so,~~ (00:20:49.413)  
for example, the cyber thing is, look, our entire civilization relies on computer systems that we barely understand, by the way, at this point, and   
(00:20:56.820) ~~many of which,~~(00:20:57.401)  
arguably the most important of which, are extremely antiquated, right? 


---


#### 00:21:42.317

People starve to death. And by the time people fix the grid and the banking systems and all of that stuff and unhack it, the hackers bring it back down. And the hackers could be, pick your flavor of AGI, depending on how much it will hack. It could be   
(00:21:57.690) ~~like~~ (00:21:57.769)  
a script kiddie on the most extreme side of the spectrum. It could be China, it could be North Korea, it could be Russia, it could be whoever,   
(00:22:03.938) ~~right?~~ (00:22:04.097)  
Or just   
(00:22:05.019) ~~like~~ (00:22:05.118)  
an organized group with a few hundred thousand dollars. It could be Al-Qaeda, I don't know. And by the time you fuck the Greeds, they fuck it again, right? And how long does that last? A month, two months, three months? 


---


#### 00:22:15.704

People always reply, oh, but AGI can protect us against AGI. We could also have AGI cyber defenders, to which I retort two things. One is that   
(00:22:25.131) ~~A~~ (00:22:25.191)  
technology does not necessarily symmetrically increase defense at the same pace at which it increases offense. That's the first thing. The second thing is that even if it was symmetrically increasing defense and offense, even if indeed it was improving defense faster than offense, I think these institutions are going to adopt this new technology much slower than the hackers will. Who do you think will adopt AGI first between Bank of America and PG&E and the hackers in Russia, right? Of course, it's going to be the hackers in Russia, but maybe by as many as a few years, right? 


---


#### 00:23:02.773

And so that delta is going to be super dangerous and super scary. And I've just not heard a single quality rebuttal to this simple reasoning. Yeah, there was just a new paper out in the last day or two from a former Cognitive Revolution guest, Daniel Kang,   
(00:23:21.637) ~~who~~ (00:23:21.857)  
the new paper is that they've got a little team of LLM agents finding and exploiting day zero, zero day exploits. And   
(00:23:32.366) ~~I don't,~~(00:23:32.567)  
I haven't quite got deep enough into that to understand exactly how they, what the data set is that they're creating. Cause obviously that's a tricky one as this stuff gets known, then it gets on the web and whatnot, but presumably using cutoff date as a safe way to delineate what was known at the time and what's new. 


---


#### 00:24:38.375

But if they do believe that there's going to be this decisive advantage, then it seems like they probably will race for it. So I'm like, okay, can I come up with a reason to believe that there's not going to be, or it's at least not obvious that there would be some sort of decisive advantage. And so I asked myself,   
(00:24:54.981) ~~is there any,~~(00:24:55.422)  
what's the story whereby either We don't get to a very high-end human drop-in knowledge worker by the late 2020s, or conditional on getting there, we stall out before some kind of intelligence explosion to move toward something genuinely very superhuman. And honestly, I don't have great answers for either. When I think about the AGI case, it really just seems to me like you think about all the kind of current weaknesses, and it seems like there's a pretty clear path to unhobble on a few different key dimensions. 


---


#### 00:25:37.401

And then you imagine, OK, now we're there. Is there any way that we'd stay there and don't get into a superintelligence-type dynamic? Certainly the uncertainty increases, but when I look at all these other neural network tools that have been created recently, whether it's AlphaFold3 or one that I just saw the other day that is learning from quantum mechanical simulations, how to model particle level dynamics at   
(00:26:08.221) ~~like~~ (00:26:08.461)  
orders of magnitude faster than the raw simulations can happen, and is starting to show this emergent behavior. There's this incredible graphic the other day from somebody studying ions in solution, and they trained it on just this solution data set, but then observed crystallization predicted by their trained model. 


---


#### 00:26:31.027

So they're seeing these phase changes as emergent properties coming out of this learning on pure simulation data. And all that stuff's being developed in parallel, right? We've got the bio ones happening at the same times as the material science ones, as the go players and whatever else. And it's, damn, at the time when there is a drop in high-end knowledge worker, there will also be incredibly powerful tools that we're just starting to see the beginning of, but that will presumably   
(00:27:04.599) ~~like~~ (00:27:04.720)  
those loops will be pretty well established, right? To say, oh, run this sort of simulation, see what happens. Run that 10,000 times, by the way, see what happens. Now we're also seeing the development of automated labs, cloud labs, 


---


#### 00:27:18.644

Emerald Cloud Lab and similar things where via APIs, you can actually run physical experiments to verify the sort of candidate theory hypotheses that have come out of the simulation. And it just feels, man, those things will be able to run pretty fast. They're going to be pretty parallelizable. And even if the thing isn't like beyond Einstein, it still feels like it's going to have tools and the ability to use this   
(00:27:47.940) ~~like~~ (00:27:48.119)  
insane array of tools. And   
(00:27:50.522) ~~that's not,~~(00:27:50.863)  
that's even assuming that those things don't merge. I could also easily imagine in the quest to scale up to something super powerful, that bio data and quantum mechanical simulation data just gets folded into the core dataset. 


---


#### 00:28:04.118

And it's like P of text and pixels and audio and quantum mechanical data and biological sequence data and proteomics and everything else. But even if they don't all get merged, it just seems like the tools are going to be so powerful that it is hard for me to imagine that it stalls out at that point. And I've been wrestling with this for the last couple of days, trying to come up with if I was going to try to talk somebody down from this notion that they should be concerned that there might be some decisive advantage to be had.   
(00:28:35.143) ~~I'm like,~~(00:28:35.482)  
I honestly can't put together a great argument for it.   
(00:28:39.185) ~~Could you put together, what would be your, if it doesn't happen, if we don't get any AGI and if we don't get,~~(00:28:45.351)  
and or if we don't get super intelligence relatively quickly after some early AGI, could you put together a coherent story for how that might not happen? 


---


#### 00:28:56.461

I'm sure I could. I think probably, I think the data headwinds would be my first reflex here. This is where I would jump because we truly are going to run out of data. I think it's tractable, but I could be wrong. But I also think it's the wrong question to ask. I think this is not the question that we ask when it comes to any other risk. Like when it comes to risks, for example, about climate or about nuclear war, about pandemic preparation, We're not asking like, hey, we really need to get in a room here and think about everything and establish beyond the shadow of a doubt that this is for sure going to happen. 


---


#### 00:29:29.345

It's, no, hey, we've got a bunch of really smart experts around the room and there's like an emerging consensus. So climate, there's like a consensus. And   
(00:29:36.712) ~~it's,~~ (00:29:36.792)  
look, it's something's happening. Not fun. And so we probably should invest something and be prepared, like just from a pure EV standpoint. Look, you have some likelihood of something really bad happening. It's worth throwing   
(00:29:47.800) ~~like~~ (00:29:47.961)  
a few billion bucks at this. I think we're   
(00:29:51.261) ~~like~~ (00:29:51.382)  
way, way beyond that in AI. I think the experts all saying there is a consensus. If you look at   
(00:29:58.945) ~~like~~ (00:29:59.046)  
the top scientific experts, like the top three or five, every single one of them is really ringing the alarm bell right now and being like, hey, this is really concerning. 


---


#### 00:30:08.351

There is a consensus, which wasn't always the case. I will note that it's interesting that the AI risk skeptics or deniers, I should say, the AI risk deniers used to hide behind the lack of consensus in the research community about AI risk. They were like, I don't know what it is because I'm not an AI researcher, you're not an AI researcher. But when you talk to the AI researchers, they said eight years ago, they say there's nothing to worry about. I'll just find it interesting. That's completely changed since then. And these people haven't updated the slightest. They just don't mention the AI experts anymore. So look, at this point, it's very clear. It's like the trend line is up and to the right, smooth, not flattening, or many orders of magnitude, and the experts are all in alignment. 


---


#### 00:31:00.313

I think at this point we need to start treating the denials the same way we treat the climate change denials, which is politely ignore them. which by the way, climate change, I really don't want to get into this thing, but sure, politics gets in the way and it gets co-opted by all sorts of political agendas and that's pretty shitty and dangerous. There's no race, there's no doubt that climate change is happening and it's bad and it's worth investing deeply into. That's it. So you ignore the folks who say, no, actually it's not true.   
(00:31:29.855) ~~I think~~(00:31:30.016)  
that's what we need to do.   
(00:31:30.915) ~~I think~~(00:31:31.175)  
we need to ignore these people. I think we need to move the conversation onto, okay, what now? 


---


#### 00:31:35.659

There is a consensus amongst experts. It's very clear what's happening. What now? And   
(00:31:40.039) ~~I think~~(00:31:40.421)  
sort of policy conversation that's happening in California, for example, the bill that's being introduced, whatever you think of the specifics of the bill, I think that's exactly the kind of conversation that needs to happen right now. And we need to start talking about HCI preparedness right now, because if you don't treat the problem now, sooner or later, you're going to have to worry about it. And it's much better to be prepared and have to be taken by surprise five years from now. Yeah, I agree with you that my question was an inversion of the right question, at least on the first analysis of, is this something we should be worried about? 


---


#### 00:32:17.674

Is this something we should be doing something about? I do think that's, I always say   
(00:32:23.118) ~~my,~~ (00:32:23.339)  
in response to the P. Doom question, I always say something like 10 to 90% or five to 95%. And the key point there is that on both ends, there's enough to be, that we're fighting for, right? If it's only 10%, or even if it's only 5%, that's a big enough problem to motivate me. And on the other end, if it's only 10% chance of survival, then that seems like it's enough to try to achieve. The next, but the sort of, I'm playing out this game, I don't know if it's game theory or just going down this branching scenario analysis and thinking, the note that I was at on that question, is there any way that Is there any credible case that we should be thinking about that we don't end up in this race for decisive advantage? 


---


#### 00:33:09.987

And I can't come up with a good one, but I do think that the utility of that, if there were something, would be like, hey, we don't need to be racing China to create the trillion dollar cluster. Maybe we can take a little bit more chill attitude toward scaling.   
(00:33:28.868) ~~I do think,~~(00:33:29.190)  
one thing I think is, yeah, if anything, I think that the trillion dollar cluster to me feels like an upper bound on what it's likely to take. There's just not, it seems like that does not assume that there's anything clever happening between now and then. That's just like a raw just take what we have and just keep doing it more and more. And that probably will work for a while yet, certainly when you think about all these different data modalities and whatnot. 


---


#### 00:33:55.407

But my best guess is that there'll be plenty of efficiency things and plenty of ways to break this out across data centers. If there is a scenario where it's like, hey, we need a trillion dollars worth of data centers for security reasons alone, you would probably want to diversify that location-wise away from one single site. So there's going to be incredible incentive to both just reduce the resource requirements and also to figure out how to not have such a concentrated physical capital plant required to do this all in one place. Yeah, I don't know. I just don't like it because it seems man the there's been   
(00:34:37.583) ~~like~~ (00:34:37.722)  
a few different big worries of the AI safety community over time and runaway unfriendly AI is one the paperclip maximizer the thing that doesn't understand our values can't understand our values. 


---


#### 00:34:51.836

That was like It remains, I think, not off the table, but that's not exactly looking like the AIs that we're getting today. But then the AI arms race with China was like another one that I've been hearing about for years. And it seems like despite everybody recognizing that would be a terrible scenario to enter into, the gravity of it is hard to fight, right? I don't know. It just seems like, man, I'm trying to come up with some way to not fall into that trap and seems really tough. I think we don't need China though to be in an AI arms race. I think even if you took China off the map, like you still have Google and Anthropic and OpenAI. 


---


#### 00:35:33.800

And so these folks have been in an arms race for a long time now. But the thing is that if you don't play the arms race for geopolitical reasons, you're going to play it for economic reasons. It's just that the economic incentives are just too strong for corporations to resist. It's their job to pursue this kind of thing. It's just, look, even if it costs you $10 trillion to automate all knowledge work, that's cheap. It's very cheap to automate all knowledge work. Are you kidding me? The world finds the money, no problem. Markets will love to take that up. And I agree with you also that,   
(00:36:06.242) ~~look,~~ (00:36:06.422)  
it is funny that the community seems like it's disagreeing about, hey, is the risk 10% or 90% or is it even 1% or 90%? 


---


#### 00:36:15.284

And is the timeline, it's funny because I remember before being as strongly AGI peeled as I was having dinner with a friend. And he was like, I was like, I'm really worried about AGI. And he was like, oh my God, how worried are you? And I guess he was more up to date than I was about the timelines. And I was like, he was like, what's your timeline? And I was like, I don't know, like 15, 20 years. he was like dude people are freaking out right now they're saying five years and   
(00:36:39.751) ~~now~~ (00:36:39.952)  
now that i'm more up to date   
(00:36:41.032) ~~i am~~(00:36:41.192)  
also saying five years   
(00:36:42.713) ~~and~~ (00:36:42.853)  
but i just found his reactions so strange they're like man like 15 or 20 years is nothing so i really think there's this weird anchoring effect or framing effect that's going on it's like the shark tornado thing that i was mentioning where it's like because people are saying five years and that sounds so outlandishly incredible to these people. 


---


#### 00:37:01.264

They're totally dismissing the fact that they would agree about that it is maybe 15 or 20 years away, which is tomorrow. It's coming so quickly, right? We all today as a civilization spending plenty of money on risks that are not supposed to materialize before 15 or 20 years, like corporations on a routine basis plan on that kind of timeframe. So even if you just think it's 15 or 20 years from now, which to me is an upper bound, we need to be planning for this. And I view way too little planning and way too little conversation about this topic. So   
(00:37:33.197) ~~I do think I might challenge the notion that we,~~(00:37:39.179)  
I do think if we took China off the game board, things might look significantly better. 


---


#### 00:37:43.940

Maybe you'll talk me out of it. But my thinking there is that you look at how many live players are there today, right? There's three to five, maybe three to seven, depending on exactly who you want to count. And   
(00:37:55.224) ~~like~~ (00:37:55.364)  
for the most part, Aside from the Chinese dimension, they all know each other. A lot of them live very close by to one another. A lot of them have directly worked together in the past. Even the Mistral team is largely ex-Deep Mind, I believe. So there is a collegiality there, and there's also the sense that if there was no external force, I would think that the US government might be a lot more likely to create some governing environment that could slow things down. 


---


#### 00:38:31.800

And that could look a lot of different ways. But my concern is that the government will move from a moderating force, in a scenario where there's no China, I think the government would at least have some chance of playing a moderating force. And if the government instead is obsessed as it seems to be with competing with China in every dimension, then it seems like the Leopold scenario of national security state getting involved and the sort of likely militarization of technology is like the top priority and the race for decisive advantage. The idea that we're going to get just far enough ahead that we'll be able to solve alignment in months and then offer China a deal. 


---


#### 00:39:58.206

And the role that I see government playing in this whole thing is once we have AGI, at some point, the DOD is like, oh, fuck. okay, this thing is really powerful. And also Baidu's also got it, or Tencent, or whoever the case may be. We need to do something about this, and we need to ask the NILS to please give us the keys ASAP before shit gets out of hand. That's how I conceptualize the role of the government here. So what do you think Canon should be done today. You alluded to a policy like an SB 1047. I've had a couple episodes on that. If anybody is not up to speed on it, they can go back and check those out. 


---


#### 00:40:34.822

It's just been amended and the amendments probably seem to have been considered to be positive updates. It's pretty much across the board, although I haven't seen a lot of people actually going from not supporting to supporting. It seems like   
(00:40:48.735) ~~the~~ (00:40:48.875)  
Among those that were not inclined to support it previously, the response has largely been, this is a positive update, but I still don't support it. I do find that to be weird in as much as it really doesn't feel like there is a huge ask there. It's try to make sure your models aren't going to cause mass casualties or hundreds of millions of dollars in damage. And if you can't be reasonably confident about that, then can only, you should deploy them with safeguards, which would mean not open sourcing. 


---


#### 00:42:19.878

And within the restricted domain, they did a fine tuning technique to basically create like a denial of service type of response where in the denoising step of an image generator, it would just guess zero and basically not change the image if it's in the restricted domain and then outside of the restricted domain they had this kind of normal training reinforcement that would make sure that the standard behavior was still accessible. What this does is it creates a local minima that's hard to get out of And so they call it fine-tuning suppression because even when you come in and fine-tune it, it still doesn't really work. There's no gradient. They construct the loss function of the fine-tuning suppression in such a way that the gradient is basically zero. 


---


#### 00:43:02.396

It converges. And so there is no gradient at that point. So when you try to go fine-tune it and pursue like your usual gradient descent technique, the gradient starts at zero. You can't really get out of it very easily. And their goal there was to create something that is harder to fine-tune to get those capabilities than it is to just train from scratch to get those capabilities. So that was a really interesting and novel thing. But overall, it just feels,   
(00:43:33.114) ~~man,~~ (00:43:33.235)  
we've got a few things here and there, but I don't know. Maybe it's all of the above. What do you think we should be doing now? What should we be advocating for now I think for sure. 


---


#### 00:44:28.237

GPT-4 was not fine-tuned on these use cases, but GPT-4 can beat these specialized models just with a specially engineered prompt, that it takes them a while to engineer this prompt. These models are just such huge objects that we have no idea what's in there. It's like a dense jungle and it takes us years to hack our way through the jungle and we still don't know what's inside GPT-4, honestly.   
(00:44:49.365) ~~So,~~ (00:44:49.565)  
we're open sourcing models and we have no idea what's in them, and we know that certain little tweaks can make them jump in capabilities tremendously. And   
(00:45:00.427) ~~so,~~ (00:45:00.628)  
the cat may be out of the bag. You could reach a point where it's already too late, you've already killed yourself basically as a civilization and you have released a model that is AGI capable out there in the wild, or like super hacker capable out there in the wild, and you just don't know it yet. 


---


#### 00:45:15.170

The weights are already out there, there's nothing you can do at this point, and you're just six months away from some guy figuring out how to prompt the model the right way. That's entirely plausible. And   
(00:45:23.876) ~~like I said,~~(00:45:24.315)  
there is no advantage in open sourcing these models. So what are the typical advantages to open sourcing something? People can edit it,   
(00:45:34.762) ~~people can edit the code, and people can edit the code.~~(00:45:38.344)  
So, editing the code, aka fine-tuning the model, you can offer that through an API. OpenAI offers fine-tuning through an API. That's perfectly fine. So why don't you do that? Auditing the code in order to increase its security, I will say, not only is that not possible, 


---


#### 00:45:54.757

I will say the only thing that is possible is the exact opposite. So you cannot audit weights of a model. Unfortunately, there is a field of very active research. You can't look into the weights of a model and be like, ah, look, there's a problem here. So you can't do that. So really, there's no point in open sourcing. What you can do, however, is whatever security measures were put in place during the training of the model, you can train out the model unless the trainers   
(00:46:17.735) ~~just~~ (00:46:17.896)  
did this thing you just mentioned, which I hadn't thought about. It's trivial today, at least, to remove the safeguards from a model. And I forgot what they call it. 


---


#### 00:46:26.420

It's like an unshackled model. If you go to Hanging Face, it's like a wizard, uncensored. Look for the uncensored families of models on Hanging Face. It's trivial to remove these things. I just don't think there is a good reason to open source this model. I think that's step one.   
(00:46:41.826) ~~I think~~(00:46:42.106)  
step two is we need a Manhattan Project for alignment. We need to spend dozens of billions of dollars. Now, I don't know how sensitive to spending alignment is. I suspect not very sensitive, but at this point, just throw the kitchen sink as the problem and perhaps even mandate private companies for, hey, for every dollar that you spend on training, for every flop that you spend on training, you must spend a dollar and you must spend a flop on alignment. 


---


#### 00:47:09.431

I'd start there. I don't expect any of that to happen because if the kind of California bill that you just mentioned cannot pass, then I don't expect these bills to pass. I foresee what will need to happen is that there will need to be a catastrophe.   
(00:47:22.561) ~~I think we need, it sounds awful to say that,~~(00:47:24.523)  
there's going to be like a 9-11 of AI. I just pray that it's not too bad. But   
(00:47:29.266) ~~I think~~(00:47:29.547)  
after we've had that, then   
(00:47:31.309) ~~we, I think~~(00:47:32.108)  
then we'll get very serious about regulation. Broke up on me a little bit there, but I think you're back now. I was saying basically, and I certainly don't wish for that to happen, but I think what's going to happen is that no regulation is going to happen until she did connect. 


---


#### 00:47:48.094

I think there's going to be a 9-11 of AI happening somewhere, and then regulation is going to arrive swiftly. Yeah, I think that on your first point, and you can maybe square it with the second one a little bit, but I'd be interested to hear how you would do that. One common argument, I think there's a lot of arguments obviously for open sourcing, including just people can do what they want and so on. But from a purely safety focused standpoint, the LLAMA2, LLAMA3 so far has been good for interpretability research. It's been good for a lot of technique development. We've got not just anthropic and open AI doing sparse code or auto coders, but like research groups are getting into it and they can study circuits and they need at least somewhat advanced models to be able to do the, because you can't study emergent properties before they emerge, right? 


---


#### 00:48:45.773

So you've got to have at least somewhat advanced models. You couldn't do this stuff on GPT-2. It doesn't seem like for many of the most advanced things that you'd want to study. So if you had a Manhattan project, I guess that would all just be done via structured access somehow. You'd imagine like the weights never leave secure servers, but somehow you create a scheme by which academic researchers can do the things that they want to do. And maybe they just have to be vetted to some degree and they can only get maybe, it still seems like it would be hard if you're going to try to, there could be some sort of cryptographic you know, way to do this sort of thing. 


---


#### 00:49:25.119

But if you want to both not let the weights out and allow people to do the, like, development of all these inspection, interpretability, editing techniques, that does seem like a still bit of a hard thing to square. I think it's like the Manhattan Project, right? It's like you had all these researchers who had access to a bunch of classified secrets doing very active work on them, and somehow it worked out.   
(00:49:49.960) ~~Sort of,~~(00:49:50.221)  
in as much as they did manage to make the thing. Yeah, I do worry about the drift, the mission drift potential of something like that. The government gets involved and it's   
(00:50:00.905) ~~like,~~ (00:50:01.005)  
all right, we're racing. We got to get ahead of China. 


---


#### 00:50:03.985

We got to stay ahead of China. We got to create this decisive strategic advantage. Does that, is that the right structure to get the sort of deliberative or deliberate science that we need to figure out how to make sure these things are actually going to do what we want them to do? Maybe I could definitely see it veering off in another direction. Yeah. And I will say, by the way, that,   
(00:50:25.934) ~~like,~~ (00:50:26.094)  
I don't think these positions lately,   
(00:50:28.117) ~~like~~ (00:50:28.257)  
people who've known me for a long time know,   
(00:50:29.978) ~~like,~~ (00:50:30.117)  
I'm a libertarian.   
(00:50:31.679) ~~Like,~~ (00:50:31.858)  
I hate this thing. I hate regulation. This is the most evil institution we have out there. It will destroy everything it touches. So I'm, 


---


#### 00:50:39.623

I, and I, by the way, also my understanding   
(00:50:42.405) ~~of that,~~(00:50:42.726)  
of like Eliezer and Mayrai and all of that stuff is like for the longest time, they were like, Hey, Let's find a way to solve the alignment that does not entail asking that regulation. They did that for 10 or 15 years and they gave up. They're like, we can't do it. There is no, we need regulation ASAP right now, right? This is like the last resort for a lot of people, including me. So I agree. I think the state is going to get involved. Shit's going to get ugly. Politics are going to do what politics do. And I think it's worth it because the situation is that dire, 


---


#### 00:51:11.304

I think. Yeah. I'm certainly with you in terms of being a very reluctant advocate for regulation or state intervention, it's bizarre to find myself even entertaining recommending that because I certainly always have resisted it in the past. Wheeling this back in a little bit just to the present day, I want to ask a couple of questions about just   
(00:51:35.186) ~~like~~ (00:51:35.306)  
the current state of play. What are you building right now? How are you thinking about the challenge of You only have access to the best models today, but you know that we've all heard this from Sam Altman and others, right? You got to be building for GPT-5. What does building for GPT-5 look like for you? 


---


#### 00:51:53.661

How do you conceptualize that? Yeah.   
(00:51:56.262) ~~Me, I am~~(00:51:56.722)  
but a humble founder   
(00:51:58.043) ~~building,~~ (00:51:58.385)  
building B2B SaaS.   
(00:51:59.865) ~~No, what we're building.~~(00:52:00.867)  
So,   
(00:52:01.146) ~~you know,~~(00:52:01.387)  
in that Leopold book, he calls it basically unhubbling. So that's what we're doing, right? We're building the layers around the model. We're not building the model itself. And so we try to focus our work on things that are simultaneously useful and commercially useful and valuable. over the short term and aligned with the march of AGI and these models. So we try to build cognitive architectures and layers around these models that will maintain their utility regardless of the model. You can basically think of it as layers that make GPT-3 roughly as capable as GPT-4, and GPT-4 roughly as capable as GPT-5. 


---


#### 00:52:45.369

Regardless of the model you've got, these layers are going to give you a job. What are these layers? These layers are like long-term memory and RAG, or retrieving information selectively depending on the situation it's at. Continuous learning that's leveraging this model's ability to learn in context and in a way that is sealed by the user. So for example, Lindy, you can give it a task and then as it performs these tasks, you can give it feedback or it can ask you for feedback when it's uncertain. and then it will keep getting better and better. And that's just going to be universally useful regardless of the model, right? Tool use, obviously through API, but also computer use, right? 


---


#### 00:53:23.717

And UI use and all of that stuff. I think of what we're doing as basically, there's like three buckets, right? Like you can think of, there's the agent, and the agent's got an LLM at the center, but it's got these other layers, which I just mentioned, like the memory, the planning, the   
(00:53:38.143) ~~criter layer of the,~~(00:53:39.423)  
There's a verification layer, there's a recursive goal decomposition. You can do all of these things outside of the model that's still in the core of the agent. We do that number one thing. Then you get that black box that's really useful, and you have inputs to that black box, and then you have outputs. That's the three things we're working on, which is the black box itself, minus the LLM, the inputs and the outputs. 


---


#### 00:54:04.621

So the inputs are going to be, sure, text, right? It's like the most straightforward one. Images, audio. Lindy can join your meetings. You can listen into your meetings. That's just   
(00:54:13.710) ~~like~~ (00:54:13.829)  
a very useful kind of input. And then the outputs are going to be, again, tool use, which is like today, like the garden variety is like API tool use. It's like the most simple one. Again, eventually giving the ability to not just sit in a Zoom meeting, but talk to you in a Zoom meeting, make phone calls, talk to you on the phone, use a computer. That's what we're working on, broadly speaking, as a company. I find myself increasingly, in some ways, the more I know, the more confused I am. 


---


#### 00:54:44.568

I often go around calling myself an adoption accelerationist hyperscaling pauser. And somebody recently challenged me as to whether or not that is even a coherent position. And I'm like, I think it is. It certainly seems like there's a lot of value from fine tuning.   
(00:55:03.413) ~~Like,~~ (00:55:03.572)  
I've definitely experienced that firsthand. And there definitely are significant unlocks that you get from this sort of scaffolding. There's somewhat of a different question there between, on the one hand, I'm like, man, I don't want to see us just rush into six more orders of scaling. That seems dangerous. And I also believe that,   
(00:55:29.358) ~~like,~~ (00:55:29.518)  
GPT-4 is enough in some ways, or GPT-5, certainly. I think we do have,   
(00:55:33.561) ~~like,~~ (00:55:33.681)  
probably one more half turn to turn before we get to something that is,   
(00:55:38.967) ~~like,~~ (00:55:39.186)  
generally smart enough to be the core of that drop-in. knowledge worker in the next not too distant future. 


---


#### 00:55:48.489

But then I think about things like the, I don't know if you saw the talk from somebody at OpenAI that has been working with Harvey on their custom model. And they basically use GPT-4 as a base and do some continued pre-training. They didn't disclose what percentage of flops the continued pre-training is. And then they do post-training after that to try to get it dialed in on exactly how they want it to behave. And they have massive preference for the custom model as opposed to GPT-4. And I infer from just the general vibe that   
(00:56:21.753) ~~like~~ (00:56:22.032)  
the incremental compute there is no more than 10%. It's not, and probably significantly less than that. So there I'm like, okay, that does seem like we can get a lot more utility without rushing through orders of magnitude. 


---


#### 00:56:37.054

And also I like the sound of that custom model, because it sounds like it's really good at what it does, but it's probably not very good at a lot of other things. It's probably not like, it's probably worse than GPT-4 at a lot of other things. And,   
(00:56:47.936) ~~you know,~~(00:56:48.137)  
that narrowness in some ways sounds really good, right? If we had something that was just really good at handling legal briefs, that's a big thing. There's lots of value there. if we had a hundred of those things for a hundred different areas,   
(00:56:59.961) ~~you know,~~(00:57:00.181)  
we could really create a ton of consumer surplus, which I'd be super excited about. And yet I don't think those things would be like posing the sort of risk that I think we're both worried about with more orders of magnitude. 


---


#### 00:57:14.054

So that's one kind of line of thought here. I'd be interested in your reaction to that. Then I'm still also wrestling with which of these things are not,   
(00:57:20.599) ~~you know,~~(00:57:20.780)  
listen to the Zuckerberg comments with Dwarkesh too, and he, we have this sort of TikTok cycle where we're always building the scaffolding and   
(00:57:28.302) ~~kind of~~(00:57:28.503)  
in doing that, we're realizing what it is that we want to build into the next generation of model. And then we're doing that. And then a lot of that scaffolding is no longer needed. So like how much of the scaffolding that you're building, do you think actually goes away, how much of it stays relevant. It's fine to throw some of it away, right? 


---


#### 00:57:42.608

But you got to have enough that stays relevant. And that seems very tough to predict. So I gave you a run-on prompt there, but you can give me your answer in as many parts as you like. Yeah, we think about that a lot, like how much of what we're building is future proof. And   
(00:58:00.514) ~~I think probably 100% of it is sort of horizon,~~(00:58:04.235)  
maybe not literally 100%, but 80%. And I insist upon like the five years time horizon here, because look, suppose that you're building something right now, that you can get for free five years from now. Or rather, what you're building now, you can't have it at all today. So what you're building goes from zero to one. 


---


#### 00:58:24.121

But five years from now, you would have had it even if you hadn't built it today. But the fact that you built it today gives you 100x cost and speed improvement.   
(00:58:32.525) ~~And~~ (00:58:32.585)  
so I'll give you one very concrete example of that, which is our continuous learning system, the one I just mentioned, where it's like Lindy continuously learns from the feedback. The way it works, it's like it's RAG. You give feedback to Lindy and then before she performs any step, she looks into her feedback database and she's like, as this happened before, what kind of feedback did I receive? I'm going to take it into account before doing it now. It's unreasonably effective. It's pretty awesome. 


---


#### 00:58:56.288

On paper, you could think that this is made totally obsolete by infinite context windows. Because you just have one agent, you talk to it all the time, it just keeps reusing and growing the same context window. It doesn't need to do this whole RAG thing because it just checks its context. I received feedback a few days ago about this very thing, I'm just going to do it differently this time. But that's going to cost you a fortune. I think the death of RAG has been greatly exaggerated because it's literally 100x more expensive. to kill RAG via context windows. I don't think this is a sort of   
(00:59:24.371) ~~like~~ (00:59:24.510)  
just difference that you can ignore. I am reminded of Android versus iOS. 


---


#### 00:59:29.715

Like Android, they built it on top of Java, which is a notoriously inefficient language because it's garbage collected. And so it's just very slow. It's just very memory inefficient. And I assume the people who built it were like, oh, who cares? Moore's law, compute is infinite. Just build it, ship it. And the reality of it is, for the longest time, until very recently, the performance difference was enough to make Android super sluggish and iOS super fast. It's like when you smooth on iOS, it's always been better smooth. And Android, it's not until very recently that it is better smooth now. And so I think that this sort of literal 2 to 5x difference in performance that it gave you, sure, it eventually became moot, because it's like, ah, who cares? about the difference between one millisecond and five milliseconds. 


---


#### 01:00:19.846

But for the longest time, it wasn't one or five, it was 100 or 500. And people do care about the difference between 100 and 500. So I think it's going to be the same thing here. And so we're working on a lot of things that give you this sort of 2x improvement, at least 2,000x improvement at most, for the kind of example system that I just mentioned. We're more comfortable with this being useful over the very long   
(01:00:39.650) ~~tail.~~ (01:00:39.889)  
I didn't quite understand your first question. You're grappling with what? What was your question? Just how much more mundane utility can we get? It's the adoption, acceleration, hyperscaling, pauser. So is it coherent? So the challenge that I got back online from no less than Tamai from Epoch was basically like more scale makes the models easier to use that makes them more useful. 


---


#### 01:01:11.190

So it's hard to does that position really have any sort of natural center. And so I've been trying to find one and I think fine tuning both does unlock a lot of value. And it also does have a lot of the same sort of utility that you're describing where it's like, if you have to 10 shot the thing to make it work versus fine-tuned and then you can just do it on a one-shot or a zero-shot basis post fine-tuning, then that also saves you a lot of time and money. But yeah, clearly there's a lot of utility that's gonna come from continued scaling. I'm just thinking, I guess at some point I expect to begin to advocate for a pause. 


---


#### 01:01:52.528

I'm not, my, my GPT-4 red team final report was, I think that you can and should deploy this and it won't be, it's not too risky to do so because the, it's in this sweet spot where it's like powerful enough to be really useful, but not so powerful enough as to be likely to get out of control and cause real problems. And then   
(01:02:14.273) ~~like~~ (01:02:14.414)  
the corollary to that was, but I'm not sure how long that lasts. And I, it does not seem that you have any. control systems in place or in development that I've seen that would suggest you have a solution to this problem on the horizon. That was before the super alignment team was even announced, now we're in the post super alignment team era. 


---


#### 01:02:32.480

And I think we're still in that sweet spot. I think we probably stay in that sweet spot for one more generation, if I had to guess. And then   
(01:02:39.523) ~~I feel like~~(01:02:39.885)  
after GPT-5,   
(01:02:41.266) ~~I maybe personally,~~(01:02:42.806)  
while I continue to love what AI can do for me,   
(01:02:45.748) ~~I think~~(01:02:46.548)  
I maybe start to advocate for Let's dial this thing in for actual practical use cases. GPT-5 should be enough to be our AI doctor. It should be enough to be our AI lawyer. It should be enough to be our AI many things. Let's bring that to reality. And we probably don't need to continue to go to race through orders of magnitude to get those sort of gains. 


---


#### 01:03:10.264

And so maybe we shouldn't, maybe we should question why we would be doing that. And a race against China would be one reason and I'd love to find some solution to that. But yeah, that's my.   
(01:03:22.648) ~~So I guess the question is like, what do you think?~~(01:03:25.632)  
Do you think that there is a coherent center to this? adoption, acceleration, hyperscaling, pause concept or does it slip away? I think it's a game of brinksmanship, right? Because it's, I really get the lighter image here, which is imagine if laundry detergent could produce gold. And at some point, and the more laundry detergent you pile up, the more gold it produces, the faster it produces it. And at some point it blows up the entire earth. 


---


#### 01:03:50.606

So you're very tempted to keep piling on laundry detergent and you never know at what point it's going to explode. And I would share your bias. I think one more generation is fine. I think I'm comfortable with two or three orders of magnitude. I wouldn't be comfortable with six or ten orders of magnitude. I would dispute, however, that there's no point adding more after that point. I think that's what makes it so dangerous. It is always tempting to add more. Because, hey, cool, now you've got your AI doctor, your AI lawyer, hey, we could just add a little bit more and it cures cancer. And it solves global warming and it solves biology for us, right? 


---


#### 01:04:25.014

It's like the memes, just one more order of magnitude, bro, I promise, just one more order of magnitude and I'll be okay, bro, just one. And it's never going to run out of temptation. And I think if anything, the closer we get to that point, the stronger the temptation will be. And also the gears of the regulatory state are so slow to put themselves in motion that, again, I think roughly now is the time to really, because the next generation is coming regardless, like GPT-5 is coming regardless, like roughly now is the time to really have this conversation so we have a chance to be much more thoughtful about the generation after next. How do you feel about OpenAI's leadership today? 


---


#### 01:05:02.478

Obviously, there's been a lot of, I would say, there've been a lot of good things that they've done, although some of those have turned to sand already, such as the super alignment team.   
(01:05:12.260) ~~I have to say, I've had a winding,~~(01:05:14.342)  
I've been on a winding road with them and have gone from, oh my God, this seems like they have no idea what they're doing, to, oh, actually they have a much better plan, to now I'm again, oh man, I don't know. The culture there, the sort of exodus and repeated exoduses, the apparent   
(01:05:35.800) ~~something like~~(01:05:36.260)  
a purge seems to be going on. That's a little bit of an inference. Not a crazy one. All these bully tactics. 


---


#### 01:05:43.527

And yet I do think they're like still fairly enlightened. I don't think like Sam Altman   
(01:05:46.610) ~~is,~~ (01:05:46.710)  
he has definitely engaged with all the ideas that I would want him to engage with. I don't think he's ignorant of anything that I'm concerned about. So yeah,   
(01:05:54.679) ~~what do you think?~~(01:05:55.179)  
What's your take on OpenAI? Are you keen out about them in particular? I'm reminded of my time at Uber, which is like, we were hyperscaling, and the chaos is impossible to describe. You can't imagine what it's like to be at a place that hires literally about 800 people per month. OpenAI isn't there, but it's hyped in its own ways. And so I think people underestimate the chaos that happens when you're like they've been. 


---


#### 01:06:22.074

They turn themselves from a little like quasi obscure research lab into a corporation generating billions of dollars of revenue in the span of 18 months. And of course, Sam Altman is a ruthless businessman. Of course, he's going to play hardball and have a bunch of   
(01:06:39.827) ~~like~~ (01:06:40.007)  
ruthless tactics. I'd be surprised if he wasn't to get into this position and to build the kind of company he's got. Yeah.   
(01:06:46.157) ~~It's like when Travis, people are like, Oh my God, he's so aggressive. What do you think? Of course it's business. Welcome to.~~(01:06:51.726)  
And some of that, I'm like, Oh, it's just business. Some of that I'm like, It's chaos, the chaos of hyperscaling. Like when the whole Susan Fowler incident broke out at Uber, 


---


#### 01:07:01.105

I was like, what do you think? We're like 35,000 people. Of course, it's going to be like a bad apple. I'm sure Travis didn't even know none of these people's names. He's never heard of it until now. And now all of a sudden, he's like the devil for doing all of that stuff, right? Even the super alignment stuff,   
(01:07:16.358) ~~I think,~~(01:07:16.639)  
I'm sure they meant it when they said they would invest 20% of compute, otherwise they wouldn't have announced it. I just think then chaos got in the way that it's just business, especially when you scale that fast. So I think that's what's going on. And I just think that it shows that you need a forcing function for these businesses. 


---


#### 01:07:35.096

They're not going to self-regulate. They can't self-regulate. They're structurally incapable of self-regulating. So you need to have an external forcing function for them to regulate. And by the way, I think Sama has been welcoming, inviting that regulation, if anything. So that's my read on the whole OpenAI situation. Yeah, for sure. Right now, they're going through a negative part of their price cycle. I think that's natural. I think it will pass. Just every company goes through that and it'll pass. Again,   
(01:08:05.559) ~~I think~~(01:08:05.800)  
to me, it just highlights the need of external regulation. I feel like I can definitely tell myself the story around like the contractual terms. It didn't shock me that there would be a non-disparagement clause in a separance agreement, even though it was funny, because then he came out and said, this is one of the few times I've been genuinely embarrassed. 


---


#### 01:08:29.154

And I was like, I don't really think you didn't know about it. And it almost seems like a overstated apology in some ways to me. Because I hear you, like things are   
(01:08:40.909) ~~going,~~ (01:08:41.109)  
they're going fast, they're going hard. They got a lot of intellectual property. Obviously they're trying to protect. They have an argument, which I don't think is a bad one, that leaks are potentially not in the long-term public interest, just in as much as they tip other people off to what is possible. And yet I'm still like, okay, but the safety team all leaving, that does seem concerning. The regulation could maybe step in and solve that problem. Yeah, I don't think the safety team was purged. 


---


#### 01:09:12.271

Perhaps Ilya was because of the coup, right? But I think they left. I think they left because OpenAI didn't make due on their promise. And I don't think that was ill-intentioned. I obviously think OpenAI's leadership is worried about safety. I just think when you're in that kind of game that they're in,   
(01:09:29.640) ~~it's just,~~(01:09:30.000)  
it's too hard. You can't, they're acting like the organization is acting rationally in dedicating their flaps to capability and not safety.   
(01:09:40.490) ~~So, yeah. Yeah. That's,~~(01:09:43.373)  
that's tough. We've been at it a while. I know we got to go in just a minute. Quick question on the sort of future of live players or the, you could frame it another way as like the fate of the second tier of foundation model developers. 


---


#### 01:10:39.278

I don't put them in the same category as these players, neither do I think they put themselves in that category. I think the one that surprised me recently was the member folks. Like these guys I could see better, but I also think   
(01:10:53.900) ~~like~~ (01:10:54.020)  
the other players I just mentioned can replicate that kind of research and I am certain have a bunch of models and training like looking into this. Yeah, I think at this point the battlefield is set. That's my expectation as well. Do you think that in the sort of next few years, do you think that the big tech companies are going to metastasize and take over everything that I've been teasing around this concept of the big tech singularity. 


---


#### 01:11:21.061

And I see things like what just came out from Google about optimizing shipping. They have some new shipping planning API that they say can double the profits of a container shipping company delivering like however many percent more containers with however many percent fewer boats. And I'm like, man, if I'm a shipping company, am I happy about this or am I not happy about this? It seems like the big tech singularity idea is just that anything that they turn their attention to, they're going to have the data, the compute, and the research advantage to figure out how to AI-ify it and dominate it if they want to. And then the counter argument is there's too much friction, yada, yada, yada. 


---


#### 01:12:00.481

What's your intuition on that? Yeah, I think big tech is very deliberate about where they expand. They're very good at that part of their strategy. And it makes perfect sense for Google to spend as much as they are on AI. From the beginning, it's made a lot of sense because search is basically an AI problem. Always has been,   
(01:12:19.413) ~~right?~~ (01:12:19.634)  
Android,   
(01:12:20.734) ~~like~~ (01:12:20.854)  
the reason why Google did Android is, it sounds insane, but they literally built an operating system just so that they could be the default search engine on it. That's basically, there's more to it than that strategically, but that's basically the strategy of Android, right? There's no chance that Google turns themselves into a logistics company. 


---


#### 01:12:35.309

They don't have a bone of that in their body. And yeah, I think they're going to offer that as an API. I wouldn't be worried if I was like the logistics companies about Google entering logistics. By the way, if they were to ever enter that game, like Google Fiber was the time when they would have done that and it didn't work out and they pulled out. Yeah, I don't expect that to happen. But yeah, I do view it more as an example of the speed at which OpenAI is penetrating every little nook and cranny of the economy. I wonder if they can even   
(01:13:03.828) ~~just~~ (01:13:03.969)  
do it by API though. Going back a few years to when I was doing a lot of Facebook advertising, which I am grateful now that I'm doing much more interesting things, but there was this general phenomenon of if you wanted to reach a new audience, 


---


#### 01:13:20.673

Facebook was the way to do it. And they were so good, and everybody was doing it, and they were so good at pitting you against your competitors, that they seemed to be taking everybody's margin. But if you wanted to go advertise a bicycle on Facebook or whatever, they'd somehow make your cost of acquisition just less than your profit margin. And everybody was like, I can sell a lot of shit on Facebook, but I can't make any margin doing it. And it does suggest that if Google can offer this API,   
(01:13:49.118) ~~and maybe they won't, maybe they'll just be magnanimous about it,~~(01:13:51.020)  
if they can offer this API that doubles your profits and then it would stand to reason that they would be able to price that at like current profits. 


---


#### 01:14:02.399

Do they then get half of the resulting profits out of the industry? And do they have everybody over a barrel where it's, if you cross Google, they can, in theory, cut you off from their like profit doubling API. Again, they would take some political hits for that, or they could be accused of being anti-competitive, but you can get a long way just on the sort of implied threat of those sorts of things, right? I think if a player gains a monopoly over one of your required inputs, they can basically drain all of the margin out of you and turns you into their serf, right? But I don't think Google is going to gain the monopoly over AI, right? 


---


#### 01:14:38.688

I think in the case of Facebook, the network effects are such that you can actually be in a very dominant position. But AI, you can't really. There's going to be a bunch of players. And by the way, they hate each other. And so that's good for us, those customers, because there's a price pool raging. which leads to these models declining in price as rapidly as they are. So yeah, I wouldn't worry about that. I don't worry about it myself. I used to. I was like, I have the required input and OpenAI is really the only one that's giving me this required input. Am I just going to become like an OpenAI itself? And now I have Gemini and I have honestly Lama and I have Cloud and Mistroll is in the running. 


---


#### 01:15:17.653

So I'm comfortable with that. Yeah, if you're a shipper, you're hoping that Microsoft has a similar project launching real soon, because then you could imagine, yeah. It's funny how that dynamic is really fascinating. If there's one shipping API that doubles profits, it takes half of the industry profits. If there's two, then they probably race down to the bottom and the shippers get more of the benefit, which is fascinating. I think   
(01:15:44.470) ~~Flexport is going to do it,~~(01:15:45.631)  
obviously, yeah. Okay, cool. Any other thoughts before we break? This has been great and I appreciate the opportunity to catch up and work through some of this stuff with you. Anything else top of mind? That's all, AGI is coming and it is time to freak out. 


---


#### 01:16:01.231

That's my message. Yeah, let's hope against hope that we can somehow avoid the AGI or ASI arms race, but it does seem to be a natural attractor. Flo Crivello, CEO of Lindy, feeler of AGI. Thanks again for being part of the Cognitive Revolution. Thank you so much. 


---


