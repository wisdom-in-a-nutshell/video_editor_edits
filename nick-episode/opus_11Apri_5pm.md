HOST: Nick Gannon, welcome to the Cognitive Revolution. ~~**Thank**~~ ~~**you.**~~ ~~**Thank**~~ ~~**you.**~~ So we are here to talk about data. And I've been really intrigued by some of the analysis that you've brought to bear on this question of what data exists, what's out there. Are we going to run out of it? What are the different modalities? So this is a scouting report episode that really just tries to get our arms around the scope, the scale and the nature of data to the best of our ability.

And I really appreciate all the work that you've put into trying to answer these questions. I'm excited to learn a lot from your analysis.

GUEST_00: ~~**Yeah**~~, ~~**absolutely**~~. ~~**Yeah**~~. Thanks for letting me share. ~~**Yeah**~~. ~~**So**~~ ~~**I**~~ ~~**guess**~~ diving in here, the general premises, what are the data requirements to brute force your way to systems that are as generally intelligent as humans across essentially all cognitive tasks? And ~~**I**~~ ~~**think**~~ oftentimes people's conceptions of AI progress seem to be more so derived from aggregating the sentiments of the crowd than any core ground up framework.

And I think this is something often I do as well, but I think we want to avoid reducing AI as a concept to an index that we're longer, short, bearish, ~~**and**~~ bullish, overpriced, underpriced. Because doing so makes our models of the AI space grounded in other people's opinions of AI, rather than in any facts of the case. So instead of this AI perspective of crowd sentiment analysis aggregation, I think it makes more sense to live within an explanatory paradigm. Paradigms being heliocentrism, geocentrism, Newtonian or general relativity for physics, caring about explanations that yield a lot of predictive power. An analogy essentially being if the year was 1690 and we ~~**want**~~ ~~**us**~~ to get a grasp on why things fall, we could ask everybody's opinion on why things fall, but ultimately it would make more sense to just subscribe to Newton's explanation because he has the most robust explanation of the space. And ~~**like**~~ in the long run, ~~**it**~~ ~~**was**~~ less correct than general relativity. It's not the ultimate paradigm. ~~**It's**~~

In AI, the current paradigm that sits at the core of the hyperscales being OpenAI, DeepMind, and Anthropic is the scaling hypothesis of intelligence. GUEST_00: In AI, the current paradigm that sits at the core of the hyperscales being OpenAI, DeepMind, and Anthropic is the scaling hypothesis of intelligence. And ~~**I**~~ ~~**think**~~ Ilya puts it pretty well outlining two fairly simple premises for the scaling hypothesis that sort of underlies the AI strategy of these three firms. Premise one being there are things a dog can do that a mouse cannot. There are things that a human can do that a dog cannot. And it's really just this, if brain bigger, then brain smarter premise. And the second one being there is functional parity between biological and artificial neurons. And this is ~~**like**~~ a substrate independence of intelligence.

There's nothing necessarily unique about fleshy meat wires or anything along these lines. It doesn't need to be carbon based, it can be silicon. The syllogism ultimately being, if premise one and premise two hold, then human level AI systems are not only tractable, but it's a very doable engineering problem. It is examining data through this reference frame of scale that ~~**I**~~ ~~**think**~~ gives it the most grounding in how we should go about approaching this. ~~**that**~~ ~~**allows**~~ ~~**us**~~ ~~**to**~~ ~~**take**~~ ~~**ATI**~~ ~~**like**~~ ~~**extremely**~~ ~~**literally**~~, ~~**even**~~ ~~**more**~~ ~~**so**~~ ~~**than**~~ ~~**taking**~~ ~~**it**~~ ~~**seriously**~~, ~~**where**~~ ~~**we**~~ ~~**start**~~ ~~**planning**~~ ~~**for**~~ ~~**the**~~ ~~**world**~~ ~~**that**~~ ~~**we**~~ ~~**are**~~ ~~**walking**~~ ~~**into**~~, ~~**where**~~ ~~**we've**~~ ~~**got**~~ ~~systems~

He's got a quote, says, it's becoming awfully clear to me that these models are truly approximating their data sets to an incredible degree. What this manifests is trained on the same data set for long enough, pretty much every model with enough weights ~~**in**~~ ~~**training**~~ ~~**time**~~ converges to the same point. And ~~**I**~~ ~~**think**~~ this is a very extreme interpretation of the scaling hypothesis where we could have something, an algorithmic architecture as naive as ~~**like**~~ character level bigram neural network. And we could, in theory, build something as generally intelligent as humans. We might need billions of suns worth of energy to fuel a pre-training run of a biogram model, but you could still do it. And in this sense, improvements in data quality and improvements in algorithmic architectures can be viewed as reducing the scale requirements to reach this human level performance in generality~~**.**~~ across a large range of tasks. HOST: Yeah.

Let me interject here for a second. ~~**I**~~ ~~**guess**~~ ~~**just**~~ to zoom out for a second, ~~**like**~~ why does this matter? ~~**The**~~ why is it worth spending so much time to try to characterize the data landscape? One of the key questions seems to be, is there enough high quality data? The leaders at the labs seem to be pretty clear on their belief that there will be enough data or that they can figure it out one way or another, create it if they have to. But ~~**I'd**~~ ~~**say**~~ across the board, nobody has shown much concern for, ~~**of**~~ the sort of SAMe, GDB, ILIA, DEMIS, Shane Legg, Dario set. Nobody there that ~~**I'm**~~ ~~**aware**~~ ~~**of**~~ has shown much concern about lack of data being a fundamental barrier. And on the contrary, ~~**I'd**~~ ~~**say**~~ pretty much all of them ~~**I**~~ ~~**can**~~ ~~**recall**~~ ~~**quotes**~~ ~~**for**~~ where they're like, yeah, ~~**I**~~ 

We'll be able to get over that. So that's the paradigm. How far does it hold? They seem to think it's going to hold. Now we can actually go out and look at how much data there really is and take it back to the mainline thread of what is the data we actually have. ~~**GUEST_00:**~~ ~~**Yeah,**~~ ~~**absolutely.**~~ ~~**GUEST_00:**~~ And so going back to D3, GPT-3 was trained on roughly 1 trillion tokens or 600 gigabytes of text. And a lot of these numbers are not going to be to the 15th decimal point.

As far as there's just a lot of ambiguity and uncertainty. And our ultimate goal is to be in the correct order of magnitude. ~~**Honestly,**~~ our confidence interval, we would like to be smaller than an order of magnitude ultimately. ~~**And**~~ along these lines, the GPT-4 training is estimated to be somewhere around 10 trillion words or 10 trillion tokens. And this 10 trillion data point number is something that we're going to anchor against going forward. And it comes from many places. ~~**I**~~ ~~**think**~~ ~~**probably**~~ the most influential is Semianalysis's research, where he estimates GPT-4 to be 16 head, MOE 111 billion parameters per head. 1.8 trillion parameters total, with also an additional vision encoder with cross-attention to the text encoder, trained on another 2 trillion data tokens, or image tokens.

And so, sorry, giving these GPT-3 and GPT-4 estimates, ~~**I**~~ ~~**think**~~ current scaling trends suggest that GPT-5's dataset is going to be somewhere in the 100 trillion byte ~~**Or**~~ ~~**we**~~ ~~**can**~~ ~~**think**~~ hundred trillion word territory. And if we look at chinchilla scaling laws, this would be if we're going up about nine, 10 X on the increase in data set size, that would be akin to about two orders of magnitude increase in the amount of compute that we're going to be using, which does fall in line with the compute trends that we're seeing as well ~~**in**~~. It is also worth noting that Chinchilla optimality is no longer in fashion and everybody essentially trains significantly past Chinchilla optimality to minimize under some constraints about inference latency and inference cost with the goal of having really high token per parameter counts. To add a couple more caveats to these GPT-5 training set estimates, it's worth diving into some of the algorithms ~~**that**~~ ~~**are**~~ ~~**released**~~ that are likely to be central features. So on the sort of pre

And to illustrate the computational savings that you get from ring attention, because it's not ~~**the**~~ ~~**tension**~~ approximate, so you still do have to pay this quadratic compute. If you have a one terabyte model, we can say this ~~**about**~~ roughly a trillion parameters, If you were to pre-train it on a fixed dataset size at 4k token context window, and then you were going to pre-train it on a million token context window, it would actually be 5.6 times more compute for the million token context window. The intuition for why this would be the case is attention's flop contribution is being offset by the dramatic increase in tokens per batch. So we can think we have this 4k squared value in the numerator, and then you've got ~~**like**~~ this divided by 4k. And then you've got, we're changing that to a million squared in ~~**a**~~ numerator. along with a bunch of other variables, number of layers, things along these lines, but you're dividing by a million. So it takes the byte out of that 256X. And intuitively, if you're batching at a million tokens, you just

And so we can look at this ~~**sort**~~ ~~**of**~~ variable as being a downward force on the data requirements that we would need to train something like GPT-5. ~~**I**~~ ~~**think**~~ You've actually spoken to this in ~~**sort**~~ ~~**of**~~ your Mamba work, where some of these algorithmic developments, it's pretty astonishing that anybody is publishing them at all. This feels like a secret that if somebody went to DeepMind and asked for $10 million, they could likely be given $10 million just for DeepMind to have the option to have a secret a couple months before everybody else figures out an algorithmic secret. So it's definitely~~**,**~~ ~~**it's**~~ shocking that any of this is publicly disclosed, and bless the hearts of all ~~**of**~~ the academics ~~**of**~~ ~~**the**~~ ~~**world**~~ for doing that. And further caveats to these GBT-5 data requirement estimates, the scaling laws for MOE are different than Chinchilla optimality. MOEs ~~**were**~~ ~~**dealing**~~ ~~**with**~~ highly sparse

But on the other side of that, there is no free lunch out here. And so while the MOE still follows scaling laws, meaning cross-entropy log-loss and log-perplexity decreases on a range of test sets linearly as a function of log-data and log-parameters and log-compute, the slope is a little less slopey. HOST: Okay, let's pause. Let's do a quick recap of that section because there's a lot there. I'll just try to maybe summarize because ~~**I**~~ ~~**think**~~ it's super information dense and having these sorts of heuristics ~~**I**~~ ~~**think**~~ is really helpful for just general intuition and ability to navigate research and just new stuff that's coming at us all the time. So starting off with the amount of training data That is going to be potentially thrown into a GPT-5. 1 trillion was the number for GPT-3 estimated, and that is 10 to the 12th. So, especially as we get past trillions, ~~**I**~~ ~~**start**~~ ~~**to**~~ ~~**need**~~ scientific notation.

So we get 10 to the 12th tokens for GPT-3, 10 to the 13th or 10 trillion tokens for GPT-4. And you're estimating that would go up another 10X with GPT-5 to a hundred trillion tokens of training data or 10 to the 14th tokens. And ~~**I**~~ ~~**guess**~~ to get an intuition for why it would grow by a factor of 10x, it seems ~~**like**~~ that's based off the budgetary observation that the compute budget is going up 100x per generation, per ~~**like**~~ significant step. That basically tracks, certainly Nvidia's revenue seems to be on the order of magnitude, suggesting that people are doing that. Meta's announced purchases would allow them to go in that direction pretty soon if they wanted to. So, okay. Billion dollar training run. 100x compute, 10x the data size, and then why does that 100x compute translate to 10x the data size?

I guess the way to think about that is, The Chinchilla training laws say that for a given compute budget, you're going to expand both the parameter count and the token count. And when you expand both, the compute required is the multiplication of that. So if you make the parameters 10 X bigger and the data 10 X bigger, then your compute budget gets a hundred X bigger. And something like that roughly is ~~**like**~~ what the chinchilla laws would suggest. You would essentially expand both of them in parallel. It's also ~~**really**~~ good ~~**note**~~ ~~**to**~~ ~~**say**~~ ~~**that**~~. ~~**the**~~, ~~**it's**~~ LLAMA, right? That's ~~**the**~~ ~~**sort**~~ ~~**of**~~ was the next big advance in scaling laws, or at least the ~~**sort**~~ ~~**of**~~ shift in overtraining seemed to be really associated with LLAMA, because they trained LLAMA 2, ~~**I**~~ ~~**believe**~~, or LLAMA 2.7b, even the small one, they trained up to 2 trillion tokens, which was like

because the models can learn past the point of optimality. And ~~**if**~~, especially if you're going to run them, and especially if you want to make something that people can run on their laptops or whatever, which obviously was a big part of that whole program. We now basically, with all that said, we have to see our way to a hundred trillion high quality tokens.

GUEST_00: Yeah, absolutely.

GUEST_00: So diving into how much data we currently have at our disposal, in the year 2020, estimates were provided that we created about 64 zettabytes of data, which is roughly the equivalent, if we converted everything to text. This would be somewhere between 60 and a hundred sextillion words, which is an almost meaninglessly large number in some sense. It clocks in at about 13,000 times more words than all ~~**of**~~ the humans have ever said collectively. Through the history of humanity, there have been about a hundred billion humans that have lived ~~**thus**~~ ~~**far**~~.

And this data just includes everything.  It's the continual monitoring logs of edge computers such as electric toothbrushes. It's the 333 billion emails that are sent each day, the vast majority of which are ~~**like**~~ the same 100,000 emails being sent to all of us and all just immediately going to our spam folders. And really, ~~**I**~~ ~~**think**~~ almost all of this is just copies and copies of copies. You can think of a Netflix video or a YouTube video as being pre-stored segments or chunks of video that are streamed from some CDN node that happens to be closest to you. We have orders and orders of magnitude more data than we actually need. And the current trends seem to suggest that the amount of data being generated by the world doubles roughly every three years, meaning about half of the data ~~**that**~~ has been generated in the last three years. And it seems that this is going to continue, if not accelerate.

To get a little bit more of a grasp on what this looks like with a little bit more granularity, there's a great research paper called Big Data Astronomical or Genomical that dives into a couple domains, including astronomy, genomics, YouTube, Twitter. And it also touches on other things like particle physics data accumulation. Apparently the Large Hadron Collider creates quite a bit of data. So Twitter~~**,**~~ ~~**I**~~ ~~**think**~~, can serve as a great microcosm of ~~**sort**~~ ~~**of**~~ mediocre quality text data on the internet. And estimates suggest that we're getting about 33 terabytes of texts that are being generated per year. ~~**And**~~ ~~**this**~~ ~~**is**~~ about a billion tweets a day, most of which ~~**is**~~ ~~**probably**~~ ~~**executed**~~ ~~**by**~~ ~~**bots**~~ ~~**spamming**~~ ~~**people**~~. ~~**And**~~ ~~**this**~~ ~~**is**~~ ~~**all**~~ ~~**from**~~ ~~**sort**~~ ~~**of**~~ 500 million, which 

HOST: Okay. Let me re-summarize the text bit for a second and just make sure I have this right. So ~~**the,**~~ that headline super high level number is all data that's ~~**like**~~ global, how much~~**.**~~ ~~**basically**~~ storage. I imagine that's probably estimated by the size of the storage market and then just assuming people are not greatly overbuying against what they need to store. So the 2020 number there is zettabytes. ~~**I'm**~~ ~~**going**~~ ~~**to**~~ ~~**need**~~ ~~**a**~~ ~~**heuristic**~~ ~~**or**~~ ~~**a**~~ ~~**mnemonic**~~ ~~**here**~~ ~~**for**~~ ~~**this,**~~ ~~**but**~~ ~~**I**~~ ~~**did**~~ ~~**look**~~ ~~**that**~~ ~~**up.**~~ That is 10 to the 22.

So our hundred trillion words by comparison is 10 to the 14. So we're talking one in 10 to the eight, AKA one in 100 million parts of this raw data would have to be high quality text for us to see our way to the~~**,**~~ theoretical GPT-5 training data scale. So it seems like we probably, ~~**I**~~ ~~**would**~~ ~~**say**~~ that's feel safe as much as there's a lot of log data and a lot of garbage out there. It does feel like, okay, if only one in 100 million parts of total data has to be good, ~~**like**~~ surely that much is good. ~~**Like**~~ ~~**it'd**~~ ~~**be**~~ ~~**hard**~~ ~~**to**~~, it's almost hard to imagine how you would design a world where it would be less than that. That also goes to show OpenAI has a data acquisition team that they've been fairly public about where they're just ~~**like**~~ looking for all kinds of~~**.**~~ data and trying to partner with, whether it's governments that have their own data sets of their particular language, or really, ~~

All this stuff is locked up in other people's systems. And so the question is, who knows where it is, how to tap into it, how much do they need to be compensated to potentially share it? But one in a hundred million parts definitely suggests that we'll get there. And then just for a couple other comparisons to put them in scientific notation as well, the emails per day are three times 10 to the 11th, which means that ~~**yeah,**~~ all the email for a year. ~~**So**~~ ~~**to**~~ ~~**take**~~ ~~**that**~~ ~~**times**~~ ~~**365,**~~ The email for a year basically gets to the same. ~~**I**~~ ~~**haven't**~~ ~~**even**~~ ~~**multiplied**~~ ~~**by**~~ ~~**the**~~ ~~**tokens**~~ ~~**yet.**~~ ~~**So**~~ There's 10 to the 13 emails sent per day. Assume 100 tokens, you're already up to 10 to the 15.

Assume 100 days of the year,you're up to 10 to the 17. We need to get to 10 to the 14 of high quality. So if one in 1000 emails sent is high enough quality, then email traffic would contain enough data to do the job. Obviously you've got major access questions on something like email, but ~~**yeah,**~~ ~~**again,**~~ ~~**I**~~ ~~**mean,**~~ it seems like you can start to wrap your head around why people would not be too worried because there's just a ton flying around. HOST: Okay. Let's do the Twitter one also. Let's see. The Twitter one was a billion tweets a day.

So 10 to the nine tweets per Day 10 to the 11, 10 to the 13. ~~**So**~~ ~~**do**~~ ~~**I**~~ ~~**have**~~ ~~**it**~~ ~~**right?**~~ ~~**That**~~ ~~**basically**~~ ~~**sounds**~~ ~~**like**~~ ~~**maybe**~~ ~~**I'm**~~ ~~**missing**~~ ~~**something**~~ ~~**there.**~~ Billion tweets a day, ~~**say**~~ a hundred tokens per tweet. We're at 10 to the 11, 300 days, three times 10 to the 13 would be the total volume of Twitter for a year. ~~**Yeah.**~~ ~~**So,**~~ ~~**and**~~ ~~**you,**~~ ~~**yeah,**~~ ~~**so**~~ ~~**you**~~ ~~**said**~~ that's three times GPT-4 and would be about a third of the way toward this hypothetical GPT-5. Wow.

That's interesting. They give the relative scale of email versus Twitter is shocking. The idea that one~~**,**~~ one thousandth of email over a year would be enough for GPT-5, but you need three times Twitter to get to GPT-5. Okay, cool. Let's~~**,**~~ ~~**let's**~~ get back to the story. GUEST_00: ~~**Tweet.**~~ ~~**For**~~ ~~**dealing**~~ ~~**with**~~ ~~**Twitter,**~~ ~~**we**~~ ~~**were**~~ ~~**33**~~ ~~**terabytes.**~~ ~~**We**~~ ~~**skipped**~~ ~~**petascale**~~ ~~**and**~~ ~~**we**~~ ~~**went**~~ ~~**straight**~~ ~~**to**~~ ~~**exascale.**~~

So we skipped essentially four orders of magnitude in there. And for YouTube, this essentially means we've got 500 million hours of uploaded content every year. And remarkably, we have about that equivalent in images of space. Hubble and James Webb are certainly pulling their weight here. And that gives us about ~~**a**~~ 200X the training data required for GPT-4, 3.3 million times the amount of data that we need for GPT-3. So if you did just want to go straight YouTube, ~~**like**~~ it does seem like that's a honey well that will just keep giving and giving for a good long while. HOST: ~~**Here's**~~ ~~**the**~~, ~~**I'll**~~ ~~**trust**~~ ~~**folks**~~ ~~**are**~~ ~~**familiar**~~ ~~**up**~~ ~~**through**~~ ~~**giga**~~, ~~**which**~~ ~~**is**~~ ~~**a**~~ ~~**billion**~~. ~~**So**~~ ~~**a**~~ ~~**gigabyte**~~ ~~**is**~~ ~~**a**~~ ~~**billion**~~ ~~**bytes**~~.

A terabyte, tera for 12, right?  A tera is 10 to the 12. A peta is 10 to the 15. And an exa is 10 to the 18. So the scale of both YouTube and ~~**as**~~ ~~**it**~~ ~~**turns**~~ ~~**out,**~~ astronomy is~~**,**~~ and that's a fascinating thought to consider. Like what is, talk about your universal function approximators. Here's all the stars, figure out what's going on over the course of 10 to the 18 bytes worth of data that is your theory of everything's what happens when the function is everything ~~**that's**~~ ~~**probably**~~ ~~**just**~~ gets to the point of computationally if we were to think that way for a second we are talking about scaling up ~~**the**~~ if we were to talk about trying to use all the astronomy data it is at 10 to the 18 that is four orders of magnitude bigger than our 10 to the 14 target 10 to the 14 target corresponded to a billion dollars in compute. But now we can start to be maybe a little fast and loose on which scaling

If we're going up an order of magnitude of data, we're also presumably going up an order of magnitude in params. Therefore, we're going up two orders of magnitude in compute. So if you actually said, what would it take to scale out to all this raw astronomy data? You'd be talking about taking the budget up 10 to the eight, which would be off of 10 to the eight, billion dollars. So you're talking basically a hundred quadrillion dollars, which is a thousand years of global GDP at present. ~~**So**~~ That's probably a bit out of reach. We're ~~**gonna**~~ ~~**need**~~ some filtering techniques to do that. But it is also really interesting just from a YouTube standpoint.

Those are bytes and there is definitely ~~**a**~~... I wonder what the deflation is between bytes and tokens. ~~**Definitely**~~ ~~**can**~~ ~~**get**~~... You can definitely get a pretty good discount from pixels to tokens. ~~**Yeah,**~~ ~~**and**~~ ~~**obviously**~~ it depends on how well the model works and the nature of your images that you're looking at. But I've been doing a bunch of stuff recently with GPT-4V and the new Cloud multimodal and just looking at ~~**like**~~. how, ~~**which**~~ by the way, the new Claude Haiku is insane. It's so much cheaper and faster. And for my use cases, it largely seems to work roughly as well.

If I was looking for a tumor in an X-ray ~~**or**~~ ~~**whatever**~~, I don't think I would take it to Claude Haiku. But for ~~**the**~~ ~~**sort**~~ ~~**of**~~, is this an appropriate image to be used in a certain context ~~**or**~~ ~~**whatever**~~? It's ~~**like**~~ totally handling that stuff fine. ~~**But**~~ ~~**I**~~ ~~**guess**~~ I know GPT-4V best. There, a low-res image is billed at 85 tokens. And a low-res image is, ~~**what**~~ ~~**did**~~ ~~**they**~~ ~~**take**~~ ~~**it**~~ ~~**down**~~ ~~**to?**~~ 250 by 250, ~~**I**~~ ~~**think**~~, something along those lines, maybe 256 by 256. So you're looking at, ~~**let's**~~ ~~**call**~~ ~~**it**~~ 50,000 pixels, each of which is basically a byte of information.

That's why I love order of magnitude computations. They don't have to worry too much about ~~**these**~~ ~~**little**~~, ~~**this**~~, the nuance details of these conversions, but that means you are going down a thousand X pixel to token compression. So now let's do that again on the YouTube side. And by the way, if anybody's listening to this and feels I'm really butchering these, please let us know. But YouTube at 10 to the 18 bytes, if we were to try to compress that to tokens at that thousand to one ratio, we'd be at 10 to the 15 tokens, which would be 10 X the tokens of our 10 to the 14 GPT-5 target. And that's the annual upload to YouTube. ~~**Yeah.**~~ ~~**Okay.**~~

So that would be only, if you wanted to do all those YouTube tokens following a chinchilla scaling law, you would be hundred X-ing past our hypothetical GPT-5 budget. So then we'd be talking hundreds of billions of dollars of compute which ~~**it's**~~ ~~**funny**~~ that's ~~**like**~~ on the balance sheet for the world's biggest tech companies that's ~~**like**~~ the size of cash and cash equivalents google microsoft apple meta balance sheet it's crazy to think that it is in scope for those guys there's ~~**really**~~ ~~**not**~~ obviously a lot of engineering ~~**he's**~~ ~~**gonna**~~ ~~**go**~~ ~~**into**~~ they're gonna need to buy a lot of chips but they could just ~~**rock**~~ crunch all of YouTube. If it really came to that. ~~**Okay,**~~ ~~**cool.**~~ So we got 10 to the 14 target. We got 10 to the 18 from astronomy. We got 10 to the 18 from YouTube. Apply a token deflator.

Remember we get 10 to the 15 tokens from YouTube. All of YouTube on a chinchilla scaling law basis would be estimated at a hundred billion dollars to train on that scale. But ~~**again,**~~ ~~**that**~~ ~~**would**~~ ~~**end**~~ another way to think about that is 10% of the tokens of YouTube would have to be good in some sense to get that same level. ~~**Okay.**~~ Useful~~**,**~~ ~~**useful**~~ data points to have in mind. ~~**I**~~ ~~**will,**~~ ~~**I'll**~~ ~~**be**~~ ~~**referring**~~ ~~**back**~~ ~~**to**~~ ~~**this**~~ ~~**often.**~~ Let's go into one, ~~**just**~~ maybe started over at the top of the genomic section. ~~**Oh,**~~ ~~**sounds**~~ ~~**great.**~~

GUEST_00: Yeah, so moving on from astronomy and YouTube data, we move up in order of magnitude for genomics data. And the rate of growth in the genomics data space certainly outpaces ~~**a**~~ ~~**lot**~~ ~~**of**~~ these other spaces where we're seeing about a doubling in the amount of genomics data generated every two years. ~~**And**~~ ~~**so**~~ right now we're generating about 40 exabytes of genomics data a year. Which definitely gets us to this point where we're a million X the amount of data required to train GPT-4. So moving on to areas outside of data, genomics and astronomical, this ~~**like**~~ planet is really dripping in data ~~**to**~~ ~~**a**~~ ~~**large**~~ ~~**degree**~~ where there's 600 GPT-4s worth of pre-training data. in the World Data Center for Climate. There's another several hundred GPT-4s worth of training data in the U.S. Census Bureau. Apple Vision Pro has ~~**just**~~ a remarkable mechanism to accrue data ~~**in**~~ ~~**there**~~.

There's blockchain data, really just like the whole financial sector has a remarkable amount of data. ~~**In**~~ ~~**sort**~~ ~~**of**~~ ~~**a**~~ ~~**digression**~~ ~~**on**~~ ~~**the**~~ ~~**financial**~~ ~~**sector**~~, One of Jeffrey Hidden's ~~**lost**~~ proteges, Peter Brown, who ~~**is**~~ ~~**actually**~~ ~~**Hidden's**~~ ~~**first**~~ ~~**advisee,**~~ has allegedly been recruiting a really large number of H100s. And he currently runs Jim Simons' hedge fund, Renaissance Technologies. Renaissance Technologies, having had their premier fund return about 37% ~~**a**~~ ~~**year,**~~ every year for decades. And there's definitely an interesting element where we are starting to have people, to a remarkable degree, using AI to essentially win the financial markets. And they appear to be increasingly using larger amounts of compute as they're doing it. ~~**And**~~ ~~**then**~~ ~~**moving**~~ ~~**on**~~ ~~**to**~~ ~~**the**~~ ~~**size**~~ ~~**of**~~ ~~**the**~~ ~~**internet,**~~

When examining what is the unindexed internet look like, try to find an old tweet and you'll find it. It's just not indexed on the internet and it's not going to pop up. And this is the sort of stuff that's just lost in the ether where it is accessible in theory. It's just not searchable. And it's also worth noting the massive error bars here ~~**or**~~ 25X to 2000X larger than the ~~**index**~~ ~~**internet**~~ indexed internet. And it's ~~**asking**~~ ~~**like**~~, how big is the unobservable universe? So it's at least a little bit bigger than the observable universe~~**.**~~ ~~**And**~~ ~~**it**~~ ~~**could**~~ ~~**be**~~ ~~**a**~~ ~~**lot**~~ ~~**a**~~ ~~**bit**~~ ~~**bigger**~~.

It's really, it's tough to calculate.

GUEST_00: And then even in the synthetic data space, we're seeing a remarkable growth rate in the synthetically generated data from GPT and Clod in Gemini, where Quick market sizing, we could estimate GPT to have maybe 25 million daily active users, maybe 15 queries per day per user, which is potentially an overestimate, but I'd be interested to see what you think. I'd probably query GPT 40 to 60 times a day on the average weekday. And if we estimate about a thousand tokens per response, we again get to this sort of ~~**team**~~ ~~**to**~~ 45 trillion generated words by Chad GPT, which again is enough to train a couple GPT-4s. And so you were speaking about this earlier, the degree to which nobody seems to be particularly concerned about the amount of data that we have. It comes up and then there's a sly smile that goes across the face of the AI researcher. And they say, don't worry about it. We've got it figured out.

And it might be synthetic data.  It could be these huge Honeywell's data elsewhere. But it definitely does seem ~~**like**~~ at first glance, there is just a lot at our disposal. HOST: Just a couple other little anchor points, or maybe do the same. order of magnitude thing again, and then a couple other anchor points. So we switched to genomics. So on genomic data, genomics projects are expected to hit 10 exabyte scale, 10 to the 19. all healthcare data, including ~~**like**~~ imaging, ~~**whatever's**~~ ~~**even**~~ ~~**like**~~ a lot bigger than that, that gets up to 10 to the 21, just raw bytes of data. Presumably a lot of that is imagery.

But on the, to get to 10 to the 19 worth of genomics data, that's like a billion human sequences. ~~**which**~~ ~~**seems**~~ ~~**high,**~~ ~~**but**~~ ~~**isn't**~~ ~~**like**~~ ~~**crazy**~~ ~~**high**~~ compared to what is probably happening today. And certainly with all the other things being sequenced, it seems much more achievable. ~~**And**~~ ~~**then,**~~ ~~**so**~~ that's ~~**like**~~ high end ~~**kind**~~ ~~**of**~~ what genomics data look like. Low end from the Evo paper, 300 billion tokens, all prokaryotic and phage. is used to train that 7 billion parameter model, which is ostensibly developing some sort of ~~**like**~~ life or cell model, which is something I really am eager to understand better. And if you're listening at this point and you're somebody who knows a lot about that, then definitely ping me because I want to do one of these for that as well. I've been actively pinging people and DMing to try to find the right, ~~**the**~~ 

But okay, again, 10 to the 18 on the high end, 10 to the 19 raw, 300 billion tokens curated just from prokaryotic organisms. Two trillion was the amount of image data added to GPT-4 to get to that vision understanding. So it seems like ~~**it's**~~, there's ~~**a**~~, ~~**again,**~~ a pretty clear path to the scale of data that would be needed to certainly add a DNA modality. And this is where this stuff starts to get ~~**like**~~ really trippy ~~**to**~~ ~~**me**~~ is when you start to think about adding modalities on ~~**the**~~ ~~**native**~~ ~~**on**~~ ~~**par**~~ basis that humans just have no way~~**,**~~ have no intuition for ~~**it**~~. We have no~~**,**~~ for all the tools and understanding we've developed with DNA, we cannot natively speak DNA. Nobody can do that. And there's ~~**like**~~ now both a proof point to believe that models are starting to do that, even at 300 billion tokens. And then ~~**like**~~ a path to easily scaling that 10 X

some sort of native sense for DNA into a language model. ~~**I**~~ ~~**imagine**~~ the curriculum learning aspect of that would be quite important. ~~**I**~~ ~~**know**~~ ~~**that**~~ a big part of how OpenAI has been so successful with their multimodal stuff in general has been recaptioning. There's this process of refinement, ~~**it**~~ ~~**seems**~~ ~~**like,**~~ where they are creating better and better captions for the images and much more tightly aligning the vision and language spaces that way. Think of that ~~**like**~~ ~~**almost**~~ a centrifuge process. And ~~**I**~~ ~~**imagine**~~ there would be definite need for tricks there as well on the DNA side, ~~**like**~~ to really make use of that, you would presumably need to annotate it in all sorts of ways or ~~**kind**~~ ~~**of**~~ figure out how ~~**it**~~ ~~**I**~~ ~~**mean,**~~ you'd want proteomic data or regulatory RNA data as well. But if you started to interleave all those together, especially with some sense of health outcomes, it sure

And we're not even talking actually in this~~**,**~~ ~~**the**~~ last two minutes, I've not even been talking at the level of a theoretical GPT-5. This is the level of vision that's under, that already exists in ~~**a**~~ GPT-4 ~~**was**~~ ~~**the**~~ 2 trillion. ~~**I**~~ ~~**guess**~~ ~~**you**~~ ~~**might**~~ ~~**also**~~ ~~**have**~~ ~~**a**~~ ~~**tokenization**~~ ~~**thing**~~ ~~**that**~~ ~~**could**~~ ~~**really**~~ ~~**cut**~~ ~~**into**~~ ~~**that**~~ ~~**the**~~ ~~**same**~~ ~~**way**~~ ~~**that**~~ ~~**we**~~ ~~**did**~~ ~~**that**~~ ~~**for**~~ ~~**the**~~ ~~**YouTube.**~~ ~~**I**~~ ~~**don't**~~ ~~**really**~~ ~~**even**~~ ~~**know**~~ ~~**enough**~~ ~~**about**~~ ~~**DNA**~~ ~~**to**~~ ~~**have**~~ ~~**a**~~ ~~

It's like there's not too much stuff that's not doing anything. If you think it's not doing anything, it might be more of a you problem. But ~~**yeah,**~~ ~~**probably**~~ ~~**some,**~~ you could still definitely imagine some Compression. Okay, great. HOST: So this is the bear case or the ~~**bulk,**~~ ~~**the**~~ bull case we're on the bull case. So let's just try it. Let's do a quick recap of the bull case. We'll do the bear case.

Then it might have to call it there for today. And we can definitely do a part two. ~~**The**~~, ~~**I**~~ ~~**find**~~ ~~**the**~~ ~~**bull**~~ ~~**case**~~ ~~**pretty**~~ ~~**compelling.**~~ ~~**The**~~ ~~**bull**~~ ~~**case**~~ ~~**is**~~ GPT five would need 10 X more data than GPT four, which would correspond to~~**.**~~ a hundred X bigger training budget, which is like a billion dollar or a couple billion dollar training run. And 10 to the 14 or a hundred trillion tokens is the target. And what we've seen is basically that's one in a hundred million parts of all the data that we're creating. That would be one in about a thousand of all the email volume.

It's roughly on the same scale as all of Twitter for a year. Other modalities seem to ~~**also**~~ ~~**be**~~ ~~**like**~~ ~~**pretty**~~ accessible. YouTube tokenized, even with a thousand to one pixel to token compression ratio is 10 times that target with just one year's worth of data uploaded. So 10% of YouTube would have to be good for that to hit some sort of parody. And then when it comes to~~**.**~~ Astronomical data, it's just another ~~**kind**~~ ~~**of**~~ ridiculous amount. Genomic data, another ~~**kind**~~ ~~**of**~~ ridiculous amount. And we already see that not huge amounts are starting to work.

300 billion tokens uncompressed in the Evo paper as compared to 10 to the 18 or whatever, just total data that's being generated. ~~**And**~~ as compared to 10 to the~~**.**~~ 12, which was the scale of image tokens in GBT four. So the benchmark of 2 trillion tokens to add a modality ~~**that**~~ seems like there's ~~**like**~~ ~~**kind**~~ ~~**of**~~ all the usual suspect modalities would seem to have plenty of data available. ~~**And**~~ ~~**so**~~ we see the path to~~**,**~~ ~~**and**~~ ~~**then**~~ synthetic data was the last thing that we mentioned where~~**.**~~ ~~**basically**~~ chat GPT is ~~**like**~~ generating ~~**sort**~~ ~~**of**~~ its own, ~~**is**~~ generating more data than it was trained on. It's already generating more than it was trained on and it's approaching generating as much as GPT-5 would need to be trained on. Obviously quality there becomes the question, right?

So I think that's probably the good, ~~**how**~~ ~~**is**~~ ~~**it**~~, because it can only, the bull case is that just the scale is so big that surely these ~~**like**~~ small fractions of these holes are ~~**like**~~ good enough to work. ~~**I**~~ ~~**think**~~ that's honestly most compellingly obvious on these other modalities where we just can't interpret them natively very well. And any sort of being able to ~~**like**~~ see DNA, ~~**so**~~ ~~**to**~~ ~~**speak**~~, would be potentially game changing. But the bear case presumably has to start with quality. OK, sure, there's all that stuff, but How confident are you really that the fraction that you need is actually going to be of the quality that you need to get there? ~~**So**~~ ~~**let's**~~ ~~**tackle**~~ ~~**that.**~~ HOST: ~~**So**~~ ~~**let's**~~ ~~**tackle**~~ ~~**that.**~~ Time for the bear case.

GUEST_00: Yeah, absolutely. So a research paper from Epic AI called Will We Run Out of Data does outline a little bit more of ~~**like**~~ the taking us back to reality, ~~**like**~~ how much of this data is usable, at least insofar as it's high quality text data. And Epic AI suggests that we're actually going to run out of high quality text data~~**.**~~ between 2024 and 2026. And they claim the amount of high quality data to be roughly on the order of 10 trillion words. The amount of high quality text data is roughly akin to the amount of data used to train GPT-4. They define this as books, news articles, scientific papers, Wikipedia, filtered web content. And just to sniff test this number, the amount of~~**,**~~ ~~**you**~~ ~~**know,**~~ words of text in the Library of Congress is about 10 trillion words or~~**,**~~ ~~**you**~~ ~~**know,**~~ 10 terabytes of data.

So it does seem that if getting into the Library of Congress's storage is some sort of litmus mechanism for, are you a high quality text token? It does seem fairly reasonable that we might actually be quickly falling off a cliff here. On ~~**the**~~ one hand, we've got plenty of data. And on the other hand, most of it seems to not necessarily be of high quality. And certainly here we are over-indexing on the idea of text tokens, where building human level systems could very much, ~~**even**~~ ~~**if**~~ ~~**not**~~ ~~**more**~~, be a function of vision or other modalities. But it also makes the estimates a little bit more consumable to reduce it down to a single modality. ~~**And**~~ ~~**yeah,**~~ ~~**so**~~ ~~**I**~~ ~~**guess**~~ moving on to if we were to brute force our way to the largest training run we could feasibly do. ~~**And**~~ this is a little bit of the Karl Shulman perspective where Karl Shulman informs that ~~**sort**~~ ~~**of**~~ the max scale of a training run that we could get is something on the order of

And investments like this are. They're ~~**precedented**~~, ~~**they're**~~ very uncommon. The primary anchor point would be the U.S. during the Cold War Apollo program was spending about 2.2% of GDP on the Apollo program, meaning it's not completely out of the discussion to be spending 1% of global world ~~**products**~~ ~~**on**~~ ~~**just**~~ a very large training run. But if we moved up an order of magnitude, it would fall out of feasibility. ~~**And**~~ ~~**furthermore**~~, Apple's revenue is closing in on 400 billion. This could be a degree to which we say the numbers are there for this to be ~~**just**~~ like the final attempts of brute forcing it. If there was no major algorithmic development, we can just throw like 30 aircraft carriers of money at it or~~**.**~~ 10 international space stations of money and see if we can't build this thing that's as generally intelligent as the human brain across all cognitive labor tasks.

I think more than likely, if you were going to go about doing something like this, it would probably be The defense department's dollar, their current budget is about $800 billion. If that was siphoned into chip fab production over the course of a half a decade to a whole decade, it does seem very feasible. And certainly plenty of headlines recently about Sam Altman, ~~**allegedly**~~ speaking to investors in the Middle East about $7 trillion. Diving into it, I didn't see any quote that was like, and then Sam said, I would like $7 trillion. It seemed ~~**like**~~ ~~**it**~~ ~~**was**~~ a lot more rumor mill. But there is evidence of Sam Altman, even as far back as 2015, having discussions with the Secretary of Defense about the role of AI and the role of compute in a national security context. It does seem if you were going to do a trillion dollar training run and you were going to have a cluster that was going to be able to run this large pre-training, it would likely have to be in the United States. And really it does seem like the only two parties that could feasibly execute a training run at this scale would be Xi

So if Sam Altman were to get this $7 trillion and was able to do a trillion dollar ~~**rating**~~ training run, and maybe it's Google, maybe it's another player. Looking into the investment side, ~~**like**~~ when would it happen and how much data would be needed for a training run of this scale? So GPT-3 was roughly on the order of ~~**about**~~ $5 million. And most estimates for GPT-4's pre-training are somewhere between 50 and 100 million. ~~**All**~~ ~~**the**~~ ~~**bells**~~ ~~**and**~~ ~~**whistles,**~~ ~~**I'm**~~ ~~**sure**~~ ~~**it's**~~ ~~**far**~~ ~~**more**~~ ~~**than**~~ ~~**that.**~~ There's plenty of R&D, plenty of side experiments going on, but this is just ~~**like**~~ straight GPU costs for the singular pre-training run. In following these investment trends out, where you're moving up an order of magnitude every two years or so, we see this trillion-dollar training run coming somewhere in the early 2030s, if we're following this investment

And one is this investment trend where the most expensive training run The cost of that is doubling about every two years. And then we also have Moore's law, the compute trend that we're seeing this doubling of transistor density every two and a half years. And in some sense, Moore's law is dying, but in the less parochial sense, it's very much still alive, where we're interpreting the spirit of Moore's law to be something closer to the ~~**sort**~~ ~~**of**~~ Kurtzwellian law of accelerating compute, where we don't really care about transistor density. We care about the amount of operations ~~**a**~~ ~~**second**~~ that we can do. And then third, there's algorithmic developments where since 2012, we're seeing a doubling of effective compute about every nine months. And what we mean by compute efficiency or effective compute is essentially the amount of floating point operations or the amount of operations per second required to get a 90% on test set X, Y, and Z. That is coming down, cutting in half every nine months. And this is just a testament to better strategies, like all ~~**of**~~ the algorithmic developments.

We have more dollars going to compute. We have more compute per dollar, and we have more effective compute per compute. So in this sense, we can expect our trillion dollar training run to be somewhere between 2029 and 2033. requiring about 85 quadrillion data points. Which is a lot of data, but if we're going to take the non-tokenized version of YouTube, it's not even a single year of YouTube. And just to give us a couple of anchoring points here, most estimates put the processing of the human brain at about 11 million bits per second. implying that the human brain processes about 2 to 3 quadrillion bytes over a 70-year time frame. And in this sense, the data processed by the human brain is about 2 to 300x the scale of GPT-4's training data. And what we would need for this trillion dollar training run would be about 8,500x the scale of GPT-4's training data.

And so from the scale perspective,~~**I**~~ ~~**think**~~ this human level performance on cognitive tasks~~**,**~~ ~~**it**~~ does seem quite reasonable if it's truly a story of data, that it would be as we're ~~**like**~~ crossing through this nexus where we are approaching the amount of data that the human brain processes, that that's when we would start to get performance levels akin to that of the human brain on cognitive labor tasks. ~~**And**~~ ~~**yeah,**~~ We've got 85 quadrillion data points that we need to come up with for our trillion dollar training run. And that would be about 0.00013% of the data that was generated or replicated in the year 2020. ~~**And**~~ ~~**then,**~~ ~~**embarrassingly,**~~ We've got this epic estimate that suggests that we need 8,500x more quality tokens of text data to build this full corpus if we were going to do it exclusively in text. ~~**And**~~ to determine whether we actually can come up with these quality tokens and to determine the viability or the quality of the tokens that are currently out there, ~~**I**~~ ~~think~

with the ultimate goal of trying to epistemically ground the answers to these questions in some sort of provably correct mathematical formalism, such that we had a good explanation for the state of affairs, and hopefully an explanation that yields some sort of predictive power about what future system capabilities might be. Diving in next, it goes into information theory and then ~~**like**~~ Bayesian statistics and KL divergence and Kolmogorov complexity and ~~**sort**~~ ~~**of**~~ those ~~**sort**~~ ~~**of**~~ more technical elements. We cater to those ~~**that**~~ ~~**are**~~. HOST: Let's do ~~**the**~~ ~~**kind**~~ ~~**of**~~ recap of the bear case again. ~~**And**~~ ~~**again,**~~ You're coming at it from ~~**like**~~ a top down and a bottom up. The top down idea is largest conceivable training run would be a trillion dollar training run. That would be a thousand times the compute budget of the 1 billion that we've estimated for GPT-5. That would mean, based on what we've been saying previously, you can divide ~~**the**~~, you can split the compute budget

Yeah, okay. Which certainly seems like a reasonable prospect. Now, it's also important to keep in mind~~**,**~~ ~~**that**~~ ~~**is**~~, how did we pick the starting point here? We basically picked the biggest training run that seems ~~**like**~~ economically somewhat plausible, and then said ~~**we**~~ ~~**would**~~ Assuming that compute continues to get cheaper and we're maximizing the value of that budget. Then to really do that, you would need something like 10 to the 17 tokens, which is 10,000 times GPT four and a thousand times our target for GPT five. But ~~**yeah,**~~ ~~**I**~~ ~~**mean,**~~ if it does take a trillion dollar worth of compute, it does seem like we may struggle to get to that quality of data. Then you'd have to either be synthesizing a lot of it, which may work. Cloud3 certainly has changed my thinking about how viable the ~~**sort**~~ ~~**of**~~ refinement process really can be.

I still am like somewhat uncomfortable with the notion of just having AIs self critique to the singularity, but the quality of cloud three definitely suggests that they've got that top spinning pretty tightly. It's not totally crazy to think that you could generate data on the scale that we're talking about here. The other thing that we didn't cover is code. And that's probably one that we should at least look at for a second. Just pulling up a dataset from ~~**From**~~ HuggingFace, the big code, the stack data is six terabytes raw. And that is three terabytes, ~~**like**~~ what they say near fully deduped. So there you have hanging out on HuggingFace as code deduped. And that would be 30% of GPT-4, 3% of GPT-5.

That's an area where you can really generate an unbelievable amount of stuff. ~~**I**~~ ~~**think**~~ ~~**this**~~, the interaction of this with curriculum learning, which is getting back to a really natural segue to part two is a super key question, ~~**right**~~? It's okay. We have this insane amount of data. We know that ~~**we**~~ ~~**have**~~, we can say, Oh, we only need these ~~**like**~~ tiny percentages. But then we look at, okay, what's the library of Congress contained. And that doesn't suggest it's going to be super easy to get to a GPT-5 scale, let alone ~~**like**~~ orders of magnitude past that. Obviously one question is at what point does the system become ~~**like**~~ superhuman?

It may not at all require a trillion dollars worth of compute. It may be GPT-5, GPT-6 type scale could start to tip into that zone. Obviously~~**,**~~ ~~**I**~~ ~~**think**~~ for me, the analysis stops where the system is meaningfully superhuman, because then presumably you're in a different data regime as well. That's definitely a new source of data. I wouldn't be training on 3.5 outputs and expecting to get to the singularity, but the GPT-5 quality outputs It's hard to really anticipate what that might look like, but definitely in this code regime, there is the opportunity for this kind of self-play, create, modify, and validate on the fly in ways that actually run it. What's the runtime algorithmic feedback on the quality of code such that you could really generate an unbelievable amount of~~**,**~~ ~~**of**~~ code. ~~**But.**~~ ~~**I**~~ ~~**do**~~ ~~**think**~~ ~~**this**~~ ~~**is**~~ ~~**a,**~~ ~~**it's**~~ ~~**becoming**~~ ~~**journey**~~ ~~**that**~~ 

I think because from the, the bull case of there's so much data that only this small amount should work. We've got all these different modalities. These things should be able to do all these different things in an integrated way. But on the other hand, wait, we don't have a clear path to 10 X what we currently have. We don't have ~~**a.**~~ ~~**Or**~~ we have a pretty clear path, but not necessarily an easy path. It would be ~~**like**~~ a lot of work, but ~~**it**~~ ~~**was**~~ ~~**like,**~~ seems pretty clearly doable, but then we definitely don't have an obvious path to a thousand X that if that's what it were to take. And so then the question becomes, can we actually create a curriculum that gets the right grokking going on early enough in the cycle that you get the~~**.**~~

superhuman capabilities that you either hope for or fear. ~~**And**~~ ~~**yeah,**~~ ~~**I**~~ ~~**guess**~~ that's the motivation behind ~~**now**~~ getting into information theory and what~~**,**~~ ~~**what**~~ actually counts as a good data point. GUEST_00: ~~**Yeah,**~~ ~~**totally.**~~ ~~**All**~~ ~~**of**~~ ~~**it.**~~ ~~**So**~~ all ~~**of**~~ the research focuses on ~~**like**~~ AlphaGo and AlphaZero and AlphaGR, the whole Alpha series, the mechanisms that they've created to determine how to do Monte Carlo tree search at inference time and how to do synthetic data generation and then self-play and how those work together to create the only datasets that we have that are actually capable of generating superhuman performance in systems, if the goal was to create systems with superhuman performance. We have to throw most of the human generated data in the trash. We start over with just alpha geometry like systems, but we scale horizontally where you've got an alpha geometry ~~**in**~~ an alpha fold for every game that could be played. So really anything

could be constructed into an algorithmic architecture that's value network, Monte Carlo tree search, focusing on self-play and then updating, and then self-play and then updating. And then if you would like to do reasoning, something that looks like reasoning, and ~~**I**~~ ~~**think**~~ the closest thing that looks like reasoning right now is Monte Carlo tree search at inference time~~**.**~~ which is sort of Noam Brown's work associated with~~**.**~~ If we let the model think for a while, can it essentially role play or cause play as a neural network that has 10,000x more parameters? That's really a way to hack your scale by just thinking more about the problem~~**.**~~ ~~**It's,**~~ those are a lot of the elements that are going to be zeroed in on and other things going along with the initial premises. ~~**What**~~, ~~**what**~~ ~~**is**~~ ~~**the**~~ ~~**math**~~ ~~**look**~~ ~~**like**~~ ~~**that**~~ ~~**allows**~~ ~~**us**~~ ~~**to**~~ ~~**make**~~ ~~**claims**~~ ~~**like**~~ ~~**universal**~~ ~~function

And then if we're going to look at how circuit search and Kolmogorov ~~**compress**~~ compressors ~~**could**~~ you can look at stochastic gradient descent and back propagation and neural networks and see how those are functionally equivalent to the degree that it's not a big leap to say this data compression perspective and intelligence is very reasonable. ~~**And**~~ ~~**this**~~ ~~**universal**~~ ~~**function**~~ ~~**approximator**~~ ~~**perspective**~~ ~~**is**~~ ~~**very**~~ ~~**reasonable.**~~ GUEST_00: ~~**And**~~ ~~**this**~~ ~~**universal**~~ ~~**function**~~ ~~**approximator**~~ ~~**perspective**~~ ~~**is**~~ ~~**very**~~ ~~**reasonable.**~~ HOST: Yeah, that self play stuff, man, that was ~~**the**~~ my kind of final thought on that was just like, Can GPT-5 create a coding problem that even GPT-5 can't solve? Like you can really start to see ~~**the**~~, ~~**I**~~ ~~**think**~~ the answer is ~~**like**~~ probably yes, but also in a

pretty demanding coding problems at a pretty high rate that itself then could then try to solve and have ~~**like**~~ only a sort of okay hit rate and then train on the ones where it's working and whatever. ~~**Like**~~ that formula seems ~~**like**~~ it is almost sure to work. And we're getting there now to where I would even bet you could start to generate that data even with just GPT-4, even without ~~**really**~~ doing a ton more on ~~**kind**~~ ~~**of**~~ core reasoning, core capabilities, just run GPT-4, have it generate its own problem sets and attack them. And if it can be validated, ~~**man**~~, it seems ~~**like**~~ you're, it seems ~~**like**~~ you get there pretty quick. GUEST_00: Yeah, ~~**no**~~ recursive self-improvement. Yeah, ~~**I'm**~~ ~~**sure**~~. ~~**Yeah**~~. It just seems, ~~**it**~~ ~~**seems**~~ ~~**like**~~ a tipping point where at some point RSI, it just starts working and it was clear it wasn't even close to working the GPT-3 and the GPT

You see the glimpses, but ultimately ~~**I**~~ ~~**don't**~~ ~~**think**~~ it would~~**n't**~~ work. HOST: I've seen a number of papers where there has been this phase change between 3.5. A lot of times the self-critique and attempt at self-improvement loop makes it worse. And then with QPT-4, it'll get better, but it'll plateau after three to five rounds. That's been the~~**,**~~ ~~**the**~~ thing in program improvement and also ~~**like**~~ chemical reaction improvement. It levels off in both ~~**of**~~ those domains ~~**on**~~ ~~**three**~~ ~~**to**~~ ~~**five**~~ ~~**cycles**~~. But notably that's not involving any retraining. So it seems that's the real~~**,**~~ that's when the loop really could start to be closed in a true.

take off sort of way. All right, let's come back another time and do the what makes good data and what's a good curriculum look like. And this has been cool for now. Nick Gannon, thank you for being part of the Cognitive Revolution. ~~**Yeah**~~.

