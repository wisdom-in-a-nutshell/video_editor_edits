00:00:00 
 SPEAKER_00: Lin Chao and Dmitro Ivchenko,~~**co-founders**~~ of Fireworks AI. Welcome to the Cognitive Revolution. SPEAKER_01: Thanks for having us. SPEAKER_00: Thank you. Yeah, I'm excited for this conversation. You guys are in a super interesting business, which ~~**I**~~ ~~**will**~~ ~~**confess**~~ ~~**to**~~ ~~**not**~~ ~~**knowing**~~ a ton about. ~~**you**~~ provide primarily inference compute. And ~~**I**~~ ~~**think**~~ people broadly are well aware of the fact that compute is one of the hottest commodities in the world today and ~~**don't**~~ ~~**need**~~ ~~**to**~~ ~~**look**~~ ~~**any**~~ ~~**farther**~~ ~~**than**~~ Nvidia's stock price to get a sense for how high the demand is for compute.

00:00:32 
 I also hear speculation that it's ~~**like**~~ a tough business to be in because commodity businesses long term can be tough. And then ~~**I**~~ ~~**also**~~ ~~**know**~~ ~~**that**~~ there's a lot of low level execution details that really matter ~~**in**~~ ~~**businesses**~~ ~~**like**~~ ~~**these**~~. And I'm super interested to learn more about some of the close to the metal work that you're doing. So maybe to start off, want to just give us a ~~**kind**~~ ~~**of**~~ quick intro to Fireworks AI, ~~**like**~~ how you guys got the idea to start the business and what your big picture vision is for it? SPEAKER_01: Yeah, definitely. So ~~**I**~~ ~~**think**~~ this is a very interesting question about, hey, is influence support, ~~**like**~~ provider a low margin commodity? So ~~**I**~~ ~~**think**~~ there are ~~**a**~~ different way to answer these questions.

00:01:11 
 So first of all, reselling hardware is a low margin business. ~~**I**~~ ~~**still**~~ ~~**remember**~~ when we ~~**just**~~ started, we were brainstorming all ~~**kind**~~ ~~**of**~~ different ideas. And we ~~**did**~~ notice a demand that because of GPU shortage, GPU arbitration could be something interesting, an interesting problem to solve. And we decided to stay away from that because it's indeed, as you said, it's a low margin and it's not a sustainable business. ~~**I**~~ ~~**have**~~ ~~**also**~~ ~~**seen**~~ artificially manufactured pricing from ~~**highly**~~ competitive landscape. And there's no way those providers can build a sustainable long-term business. And ~~**I**~~ ~~**think**~~ at ~~**least**~~ ~~**at**~~ the minimum, people need to take caution~~**,**~~ building on top of such ~~**that**~~, as when these companies or startups run out of funding, they will disappear. One of the important ~~**The**~~ thing we want to build fireworks on top of is ~~**like**~~ focus on our specialty based on our experience.

00:02:04 
 And here, when we think about ~~**Jenny**~~, so it is going to empower a whole slew of application and product disruption, particularly being consumer, prosumer, and developer facing. The fundamental reason is this is a new revolutionary technology that didn't exist before that can generate content emulating what ~~**human**~~ can generate. And by definition, the receiving part of those content are by and large ~~**human**~~. And for those B2C application, latency is ~~**very**~~ important part of product experience because it has to be hyper interactive. Without that interactiveness, it's not a viable product. So many of our customers come to us for extremely low latency requirements. At the same time, the content generated ~~**founders**~~ ~~**have**~~ to be high quality. And we provide high quality through automated loop across fine tuning and inference.

00:03:00 
 And last but not least, we provide low TCO in a highly sustainable way. And here, low TCO is very important because it's a different business from traditional application development. We're built on top of commodity CPUs. The growth margin is very high because CPU is so cheap. And ~~**now**~~ shift towards GPUs. And GPU~~**,**~~ ~~**not**~~ ~~**just**~~ ~~**the**~~ ~~**hardware**~~ ~~**expensive**~~, it is very power-hungry. It consumes a lot of power and generally a lot of heat. And traditional air cooling doesn't work.

00:03:31 
 People have to use liquid cooling or immersion cooling. That means put the GPU inside oil and fully immersed. And those are all costs of operating ~~**GNI-I**~~. inference. So that very high cost makes it very challenging to justify business impact. And we specialize in reducing total cost ownership. So if you have a viable product, it can turn that into a viable business. So that's the value ~~**ad**~~ ~~**we**~~ ~~**have**~~ ~~**been**~~ focused on delivering from ~~**fireworks**~~ ~~**site**~~.

00:04:05 
 In terms of a product, quick overview, we provide the general AI platform to do fast experiment ~~**attuation**~~ iteration and inference production scaling. So here, there are two development loops we're optimizing for. ~~**In**~~ ~~**the**~~ ~~**loop**~~, ~~**product**~~ experimentation, we optimize for iteration speed, and ~~**auto**~~ ~~**loop**~~ production. We optimize for hard system metrics, including latency, TCO, scalability, reliability, ~~**I**~~ ~~**just**~~ ~~**mentioned**~~. The specific product ~~**feature**~~ ~~**On**~~ ~~**these**~~ ~~**two**~~ ~~**loops**~~ ~~**are**~~ fine tuning and inference with on-demand serving and automation. This is for ~~**inner**~~ ~~**loop**~~ fast iteration ~~**of**~~ experimentation. And we provide ~~**a**~~ faster inference at scale for ~~**the**~~ ~~**auto**~~ ~~**loop**~~. When you have product market fit, you want to scale your business.

00:04:52 
 And this auto loop of production,~~**we**~~ help you to get the best seed and best TCO. So that's a very high level summary of fireworks product. SPEAKER_00: Cool. Okay, several threads there that I want to follow up on. Interestingly, it sounds like you're saying that a lot of products on the market today are in an Uber moment where... You think they're essentially being offered below cost and in a fundamentally unsustainable way. And by contrast, ~~**I**~~ ~~**understand**~~ ~~**that**~~ you are ~~**like**~~ operating your business without doing that. You're not ~~**like**~~ radically subsidizing the customer. Do ~~**I**~~ have that correct?

00:05:31 
 SPEAKER_01: That's correct. But also I just want to call out our value ~~**ad**~~ is not extremely low cost. Our value ~~**add**~~ is low latency, high quality. and low TCO. So I'm almost saying low TCO is a byproduct of our high performance. SPEAKER_00: Yeah, I think this is a real, I've experienced this in my own application development where TCO is not the natural way that a lot of engineers think of things and they specifically don't tend to factor in the cost of their own salaries as they start to build out infrastructure. I am a big believer in certainly the specialization of dedicated infrastructure providers. just because I've seen how hard it is to create a reasonably reliable stack when you're doing it on your own.

00:06:17 
 But it's just on a purely,where are we in this market development cycle and where are things headed? It's an interesting observation off the bat that Some of these bargain, ~~**the**~~ cheapest options that are out there are subsidized in an unsustainable way. That is actually not something ~~**I**~~ ~~**had**~~, ~~**I**~~ ~~**know**~~ that's happening a lot at the application layer. ~~**I**~~ ~~**had**~~ ~~**not**~~, because certainly people are offering free demos. That's obviously subsidized, ~~**right**~~? Because all the tokens are costing money. And people are certainly giving away a lot of free accounts and a lot of free inference to their end users. But ~~**I**~~ ~~**hadn't**~~ ~~**really**~~ considered how much that might be happening at the inference compute layer.

00:06:55 
 So it's really interesting to consider that as also a reality in today's world. SPEAKER_01: Yeah, and a lot of our customers, whether they are developers or enterprises, come to us, not because we are the lowest pricing provider at all. They come to us because they~~**,**~~ ~~**again**~~, they're building consumer-presumer developer-facing applications. That requires very low latency, and they cannot get it by themselves. They cannot get it from any other providers. Even from open AI and topic, they didn't get the right latency or latency is not stable. ~~**and**~~ They are seeking ~~**solution**~~ ~~**from**~~ ~~**our**~~ ~~**side**~~. And getting to low latency is actually not easy, and Dimitro can speak a lot to it today.

00:07:37 
 But the high-level challenging is Gen.I model is among the largest model~~**.**~~ ~~**sizing**~~ and complexity in the whole spectrum of machine learning. In ~~**early**~~ days of machine learning, the algorithm ~~**is**~~ ~~**extremely**~~ simple. It's tens of megabytes. And now we're talking about ~~**the**~~ tens of billions of parameters. Because of that complexity, ~~**but**~~ with that complexity, it doesn't change the nature of all these B2C applications, whether ~~**it**~~ build on traditional machine learning or built on top ~~**or**~~ genii. It doesn't change the latency requirements. It has to ~~**be**~~ stay extremely interactive.

00:08:13 
 And that put a lot of back pressure on the inference serving tier to do even more aggressive optimization. And that's the challenge ~~**you**~~ ~~**are**~~. We are really good at addressing. And ~~**I**~~ ~~**think**~~ that's our biggest value add. In addition to pricing, ~~**I**~~ ~~**don't**~~ ~~**think**~~ pricing in the long run is not a sustainable value add. SPEAKER_00: Yeah, makes sense. Okay, cool. So I want to get into then how you're doing it to the greatest degree that you can share and that I can comprehend because ~~**I**~~ ~~**do**~~ ~~**think**~~ this stuff is going to get very into the weeds as we get close to the metal.

00:08:46 
 Do I understand correctly that you are managing your own servers on racks? You're talking about cooling and all this sort of stuff. So are you vertically integrated to that level? 

SPEAKER_01: ~~**Yeah**~~, ~~**so**~~ that's a good ~~**segue**~~ ~~**way**~~ to talk about our compute stack. ~~**And**~~ we currently~~**,**~~ we build on top of CSPs. We're running on top of AWS, GCP, and ~~**the**~~ Oracle, OCI. The reason is~~**,**~~ ~~**I**~~ ~~**think**~~, when we grow bigger, there ~~**are**~~ a very different way to be more efficient. And right now, we are optimizing for velocity of our product development.

00:09:18 
 And those CSPs has been battle tested. So that's why we build on top of them. And we also, as a company, we aspire to run on top of the best hardware across the whole entire industry. And of course, it's ~~**the**~~ easiest to build on top of Nvidia GPUs, and that's how we started. But at the same time, we see a lot of emerging hardwares coming to this hardware landscape for ~~**JDI**~~, including ~~**MD**~~, including Intel, including Customs, so there are ~~**a**~~ very different interesting trade-off~~**,**~~ ~~**I**~~ ~~**will**~~ ~~**say**~~. We'd love to pass on to Dimitri to talk about the trade-offs across those different hardware providers. Sure, yeah. SPEAKER_02: So as Lynn mentioned, historically, we've been primarily dominated by Nvidia in the overall AI landscape.

00:10:07 
 And the reason for Nvidia domination is, ~~**I**~~ ~~**would**~~ ~~**say**~~, two ~~**folks**~~. First is they have pretty good hardware, and if you look at the latest H-100 has two petaflops, this is pretty high. And they ~~**have**~~ pushing with a new Blackwell to double that, and then you can go to 10 petaflaps for their super chips. And they also are doubling, quadrupling, memory bandwidth along the way. They're also improving on their cross-host interconnect. And ~~**I**~~ ~~**think**~~ the most ~~**like**~~ welcome development there is the actual latencies, which will allow us to reach of interconnect, which will, in turn, will allow us to reduce the latencies of our generation speed. That's one, but the second thing is, NVIDIA historically, has had a very robust developer stack. And because of these two reasons, ~~**I**~~ ~~**think**~~ we ~~**have**~~ ~~**been**~~ dominating.

00:10:59 
 But lately, the... say, Nvidia's arrival AMD is coming back. And now with the new MI-300, which has a better hardware spec than Nvidia. And they are also working, ~~**I'm**~~ ~~**sure**~~, day and night on their software stack. And with the new Welcome Open Source development on the Rackamas stack, they are catching up ~~**basically**~~ in all aspects to NVIDIA as well. We also see other newcomers where there is development from Intel with the Gaudi 2, a company the Goudi 3 release, which is actually even more powerful than ~~**MIT**~~ ~~**300**~~. So it ~~**says**~~ ~~**in**~~ ~~**between**~~ ~~**H100**~~ ~~**and**~~ ~~**B**~~, ~~**100**~~ ~~**B**~~ ~~**to**~~ ~~**100**~~ ~~**views**~~. So that's like on the more programmable side of things.

00:11:41 
 And now if you look at the different spectrum, the less programmable, more specialized hardware, namely A6. So here ~~**I**~~ ~~**would**~~ ~~**probably**~~ ~~**just**~~ select one is rock, which has been making a bit of ways lately on Reddit because of their nice demo, the high generation speed. But the thing is that ~~**X**~~ is always compromising something. So ~~**in**~~ ~~**here**~~, ~~**that**~~ if you look at the rock, and surprisingly, there is very little ~~**kind**~~ ~~**of**~~ detail, and sometimes you need to guess, ~~**like**~~, what these guys do, because they're not as open as ~~**in**~~ Nvidia. But what they're fundamentally doing, they're compromising on the fact that They want to remove the shared memory, ~~**this**~~ HBM in GPUs, and they want to put it as close ~~**units**~~ as possible. They want to put ~~**on**~~ ~~**the**~~ ~~**dye**~~. But then they are compromising on the processing units, so they have ~~**achievable**~~ ~~**flow-ups**~~ are

00:12:37 
 Now, what did the results is that they have much lower achievable flaps, ~~**what**~~ they can do faster, they can run the generations faster because they don't need to copy memory from the shared memory to the memory which is on ~~**time**~~. So this is like a fundamental compromise. They're also way more expensive to operate because if you want to host a single model, you need to have a ~~**human**~~ guest setup which costs millions of dollars. So ~~**I**~~ ~~**think**~~ these are ~~**kind**~~ ~~**of**~~ compromise~~**s**~~ in there. Although it's very interesting and creates cool demos, but ~~**we**~~ practically ~~**can**~~ ~~**talk**~~ ~~**about**~~ ~~**this**~~, ~~**practically**~~ ~~**see**~~ limited application of this in the real world. ~~**and**~~ ~~**sort**~~ ~~**of**~~ limited disruption. Although it does solve some interesting cases, but there seem to be more ~~**of**~~ edge cases right now where it wins over traditional programmable GPU hardware. There's also TPUs, ~~**of**~~ ~~**course**~~.

00:13:33 
 There was been a case. They've always been around and they're setting out from programmability between ~~**A6**~~ ~~**and**~~ GPUs. ~~**Although**~~, ~~**technically**~~, ~~**PITOR**~~ ~~**supports**~~ ~~**TPUs**~~, but to get the best performance, you have to go to Jax and Google ~~**Google's**~~ ~~**World**~~ ~~**and**~~ ~~**Google**~~ ~~**South**~~ ~~**West**~~ ~~**Tech**~~. So it becomes ~~**like**~~ quite different operational ~~**and**~~ ~~**a**~~ quite different business ~~**too**~~. So this is ~~**in**~~ ~~**the**~~ ~~**nutshell**~~. SPEAKER_01: ~~**Yeah**~~. So speaking about that, as you can see, it's a spectrum. It's a spectrum from most programmable to ~~**like**~~ hyper-specialized.

00:14:04 
 And I want to talk a little bit about our meta experience, but because both of us and actually a large portion of the funding team came from meta. And we all worked on PyTorch extensively over the past five years. So it's ~~**very**~~ interesting experience there. ~~**Of**~~ ~~**course**~~, as you can see, ~~**a**~~ ~~**matter**~~ ~~**like**~~ other ~~**hypers**~~ ~~**running**~~ on top of CPU, GPU, ~~**A6**~~, this is ~~**a**~~ public information, ~~**a**~~ variety of hardware. And recently they mentioned they deployed more than 600,000 ~~**H-100,000**~~ ~~**in-house**~~. And that, what does that mean? Put that in perspective, that's equal to the power needed for 600,000 average American ~~**household**~~. That's roughly twice the size of San Francisco.

00:14:43 
 A huge power consumption, but matter didn't get there overnight. It took us more than five years to go from basic machine learning on CPU to complex deep learning on GPU and ~~**A6**~~. So there are a lot of interesting ~~**lesson**~~ ~~**learned**~~. First is because we work on ~~**software**~~ ~~**stack**~~, ~~**right**~~? So the first ~~**lesson**~~ ~~**learning**~~ is a good product cannot compromise. And ~~**need**~~ ~~**to**~~ ~~**be**~~ ~~**opinionated**~~ ~~**about**~~ ~~**the**~~ what's the part of philosophy and ~~**the**~~ what's the target audience. So as a matter when we ~~**start**~~ ~~**to**~~ ~~**work**~~ ~~**on**~~ ~~**dab**~~ ~~**into**~~ AI infrastructure, we had at least three different AI frameworks ~~**that's**~~ ~~**in**~~ 2018. We have coffee two for mobile, ~~**on**~~ ~~**export**~~ ~~**server**~~, ~~**production**~~.

00:15:22 
 So our charter is to unify them into one and cover both server, mobile research and production. It's called PyTorch 1. This feels like mission impossible at the moment because it has so many different goals. So we ~~**merge**~~ ~~**all**~~ ~~**these**~~ ~~**teams**~~ ~~**together**~~, but there's no consensus ~~**how**~~ ~~**to**~~ ~~**build**~~ ~~**this**~~ ~~**one**~~ ~~**framework**~~. So we came up with ~~**a**~~ ~~**idealistic**~~ ~~**zipper**~~ ~~**approach**~~. And the project was actually literally called zipper. Taking the ~~**pie**~~ ~~**torch**~~ ~~**front**~~ ~~**end**~~, which is very easy, simple to use, and ~~**zip**~~ ~~**it**~~ ~~**with**~~ ~~**coffee**~~ ~~**two**~~ ~~**backhand**~~, which is highly performant. It failed miserably because these two ~~**framework**~~ ~~**was**~~ ~~**never**~~ ~~**designed**~~ ~~**to**~~ ~~**work**~~ ~~**together**~~.

00:15:55 
 The integration overhead was more than writing a new framework from scratch. So we kept~~**,**~~ ~~**we**~~ end up ditch that plan, and we kept the pie torch front end, and re-roll this backend into a torts script, and that's an interpreter to execute pithwich efficiently. So notice that the design decision, ~~**the**~~ product choice we made is to hold ~~**a**~~ line for ease of use, hold ~~**the**~~ line for ease of AI innovation for developers built on Pytorch. And then we take all the complexity of building the backend. So that is the design decision. However, TOR script for PITH1.O, it requires the user, ~~**the**~~ developers, to annotate which part of Pytouch code is scriptable. That's not a very good UX because people need to know how to annotate. So two years ago, we also started Torch Compile to fully automate the process.

00:16:52 
 People don't need to annotate at all. So that's called ~~**Pytouch2.0**~~PytTorch2.0. We left before the project fully shipped, but I'm thrilled to see that there's great progress made from the ~~**PytTorch**~~ PyTorch team. So another interesting takeaway we had in that process is we thought it should be easy to bring ~~**PytTor**~~ PyTorch to production. We thought the process is just swap the libraries. So because both PyTorch and ~~**Cafe**~~ Caffe2 and ~~**Ones**~~, ~~**they're**~~ just ~~**library**~~libraries, ~~**they're**~~ not services. Swap those libraries in the service. However, that's a wrong assumption.

00:17:23 
 A framework innovation. A framework or library innovation requires a completely new infrastructure service to build, to support it. We end up ~~**have**~~ ~~**to**~~ build new data loaders, new data transformer, transformation layer for training for PyTorch. We need to build a new training loop for PyTorch. We need to build new distributed inference ~~**that**~~ for PyTorch. So after five years, we wrote the whole entire PyTorch platform from ~~**Guangda**~~. It was serving more than 50 trillion inference per day across 50 data centers. So as you can see, this journey took us five years.

00:17:56 
 But we start fireworks. We want to reduce. We want to shrink these five years journey into five weeks or even just five days for ~~**current**~~ broader set of developers in the industry. We have built better tested large-scale AI systems. So we're confident that we can out-innovate the incumbents ~~**in**~~ ~~**significant**~~ shorten time to market. For everyone out there, ~~**want**~~ ~~**to**~~ build disruptive applications and products on top of JNI. SPEAKER_00: If I try to summarize that and make the translation from the meta experience to the fireworks product direction, it seems like the key principles are never compromise on the product, user experience above all. The users hate to wait.

00:18:43 
 I certainly know that from personal experience. ~~**there's**~~ ~~**like**~~ an echo of that at the developer layer ~~**two**~~. Like developers hate complexity and don't want to have to deal with all this mess. So internal focus on the end user experience above all, focus on the developer experience second, simplify everything, bring all the complexity in-house, abstract away from all the different kinds of hardware. And it sounds like the place that you want to carve out for yourselves with fireworks is, a sort of layer that sits between the developers and all the different hardware providers so that folks can easily develop their applications but ~~**not**~~ and ~~**not**~~ really ~~**have**~~ to ~~**worry**~~ about what hardware they're running on ~~**the**~~ relative strengths and weaknesses of all that ~~**what**~~ bottlenecks they're going to hit at all these different ~~**depending**~~ on all the different stacks And ~~**I**~~ ~~**guess**~~ ~~**where**~~ ~~**I**~~ tell me if that's right. First of all, and then I'm very curious to get into a little bit more detail on maybe what some of those bottlenec

00:19:56 
 And I think the audience will be pretty varied in terms of how much command they have of this stuff. First of all, do I have that general story and ~~**kind**~~ ~~**of**~~ market position and vision~~**,**~~ right? SPEAKER_01: ~~**Yeah**~~. ~~**So**~~ ~~**I**~~ ~~**think**~~ it's mostly right. ~~**I**~~ ~~**think**~~ a little bit more nuance is what is getting ~~**the**~~ ~~**way**~~ ~~**of**~~ ~~**the**~~ fast iteration loop? Here, we're talking about current set of application product developers. They haven't jumped onto AI yet. And they have a slew of AI tools they have to learn and pick and choose different models for their use cases.

00:20:30 
 And I also try to optimize for latency. ~~**and**~~ ~~**try**~~ ~~**to**~~ justify business impacts through low TCO, all of those are on their plate right now. And our goal is to take away those concerns, let them focus on where they should be focused on, is application and product development. And we give them proper tools and higher level ~~**obstruction**~~ abstraction. So they get low latency, low TCO, high quality, extremely easily. Of course, it sounds really nice, and we can dive a lot deeper into how we get there. But at the high level, I will break it ~~**at**~~ ~~**least**~~ into two buckets. One is low latency, low TCO.

00:21:06 
 That's a bucket. We should dive deeper into ~~**that's**~~ driven by our high system optimization, ~~**a**~~ performance optimization. The other bucket is quality. And we have to talk about quality because at the end, we think we are~~**,**~~ ~~**our**~~ competition ~~**are**~~ not other inference ~~**provider**~~. Our real competition is actually ~~**open**~~ AI and ~~**the**~~ ~~**topic**~~. So solving and addressing people's quality ~~**issue**~~ is ~~**very**~~ high priority ~~**of**~~ ~~**our**~~ ~~**company**~~ ~~**goals**~~. SPEAKER_00: Yeah. Okay.

00:21:34 
 Cool. Those are both really interesting.  ~~**Mel**~~, let's maybe start ~~**the**~~ quality one because ~~**I**~~ ~~**think**~~ that is, that's probably the more intuitive one. And ~~**I**~~ ~~**was**~~ ~~**noticing**~~ in trying the product and ~~**I**~~ always do go in and try products in my preparation for these conversations. ~~**I**~~ ~~**would**~~ ~~**say**~~ the experience is super easy to get started, just create an account. Next thing you know, you're in a playground. You can choose from all these models. You've got what you call serverless models, which basically is like on, you also have ~~**been**~~ on demand, but that's like setting up dedicated compute paying by the hour.

00:22:09 
 But the serverless is literally just a very similar experience to what you get with open AI and Anthropic where you're making an API call. You're purely paying for tokens. And that's all super simple. You can test stuff out, try all the different models in the playground, and then hit the give me the code button and it pops up the code and you can go copy that code and drop it in your application. So all that is~~**,**~~ ~~**I**~~ ~~**would**~~ ~~**say**~~, probably reasonably familiar to folks in our audience. They've done that kind of thing, at least with an open AI or an anthropic, if not another ~~**.**~~ inference provider. Quality~~**,**~~ ~~**though**~~, is a tough challenge~~**,**~~ ~~**right**~~?

00:22:41 
 Because the best models, certainly in today's world, with some notable exceptions that are starting to crack into the top 10, certainly still the very best models are proprietary to the open AIs and Anthropics and Google deep mines of the world. And notably~~**,**~~ ~~**too,**~~ ~~**like**~~ the price is also starting to get pretty low from those guys. The~~**,**~~ ~~**I**~~ ~~**think**~~ haiku for Anthropic is ~~**like**~~ a really interesting point of comparison. The price that you guys have for the small tier of models up to 16 billion parameters. I'm interested to hear how that cutoff was selected, but up to that 16 billion threshold is 20 cents. per million tokens, which is crazy to think. Not that long ago, it was six cents per thousand tokens from OpenAI with the original GPT3. So we've gone from six cents per thousand to 20 per million for better models than the original GPT3 in just two years.

00:23:41 
 I always like to stand back and just marvel at that. ~~**how**~~ fast that cost has come down. But the leaders are not too far behind, ~~**right**~~ ~~**there**~~. With Haiku, Anthropic is at 25 cents per million, at least on input, they charge a little more for the output. But with the long context, presumably a lot of the time, it is pretty input-heavy workload ~~**there**~~. So ~~**yeah**~~, let's talk about quality. How do you win in a world of Haiku? ~~**I**~~ ~~**would**~~ ~~**assume**~~ that's ~~**like**~~ the number one direct competitor for those smallest models.

00:24:10 
 Curious to hear how you're thinking about the quality challenge. SPEAKER_01: ~~**Yeah**~~, ~~**definitely**~~. ~~**Probably**~~ we can take a big step back, ~~**right**~~? So it's more like between closed source vendor and open source vendor, who is going to win? It's clear that ~~**at**~~ ~~**least**~~ we can agree that we are currently in a very intense race. between open source and closed source models. This week is an interesting week. ~~**Mistral**~~ ~~**team**~~ just dropped a MOE, a new MOE model with eight experts of 22 billion parameters, and we just ~~**enable**~~ ~~**it**~~.

00:24:41 
 A matter will open source Lama 3 in a few weeks. And Google has been ~~**continuous**~~ continuously open sourcing newer ~~**gamma**~~ models. And ~~**Q1**~~ ~~**and**~~ ~~**E**~~ models ~~**founders**~~ from Chinese institutions are getting better and better also. At the same time, Open AI and Anthropic keep improving their model quality, including at the smaller model size scale. So here's my thinking, ~~**right**~~? ~~**So**~~ ~~**I**~~ ~~**think**~~ all these models in the same size bucket~~**,**~~ will converge in quality eventually. The reason is for smaller models, because of the size, it ~~**just,**~~ ~~**it**~~ has ~~**the**~~ upper bound on how much knowledge it can absorb. And it determines capability.

00:25:20 
 And as we are also converged on how much data, ~~**we**~~ ~~**can**~~ ~~**train**~~ a model. So ~~**I**~~ ~~**think**~~ just inevitable over time, whether open source or closed source, the model quality will be similar to each other. If that's true, and ~~**I**~~ ~~**will**~~ ~~**argue**~~ open source models have a much stronger ecosystem potential because it has a lot more active people engagement, developers are engaging in tuning open source models, and then they open source them. And this keeps going. It has compounding effects. So more people can build on top of each other's work. So this is how ~~**I'm**~~ ~~**thinking**~~. So that's why fireworks fundamentally build on top of ~~**those**~~ open source ecosystem.

00:25:59 
 We also just launch our fine-tuning service. It's relatively new. So in terms of revenue, it's not dominating any ~~**of**~~ ~~**other**~~ product features, but it's one of our fast-growing product ~~**feature**~~. With that said, ~~**I**~~ ~~**will**~~ ~~**still**~~ ~~**say**~~ between closed-sourced, right now, we are not there yet. All open-source, close-source models, small models are converging. Right now, the open-source model still ~~**need**~~ fine-tuning. The challenge of fine-tuning is it's a much longer development process than prompt engineering. And we fully acknowledge that.

00:26:34 
 And that's why we as a company, ~~**we**~~ want to solve those problems to make fine-tuning much faster, easier, and actionable~~**,**~~ ~~**right**~~? Because once you see the result, hey, what is not working, it's actionable. And also, we will~~**,**~~ ~~**by**~~ ~~**and**~~ ~~**large**~~, automate a lot of this pain in fine-tuning away. So our developers will have ~~**much**~~ simpler experience as close to prompt engineering as possible. So that's one aspect. The other aspect is~~**,**~~ ~~**I**~~ ~~**think**~~ eventually ~~**I**~~ ~~**will**~~ ~~**say**~~ open AI and Anthropic, their goal, including matter automation, their goal is to get to AGI~~**,**~~ ~~**right**~~? What is AGI? It's basically building a system that's smarter than human.

00:27:18 
 It means this system can solve very complex problems and hundreds of them at the same time. But if we look at the problems developers and enterprises~~**,**~~ ~~**they**~~ are solving. They don't solve hundreds of very complex logical reasoning ~~**problem**~~ at the same time. They probably have a handful of very specific problems. For example, they want to solve ~~**classification**~~ ~~**problem**~~ from intent routing based on intent ~~**route**~~ to different agents. ~~**to**~~ ~~**classify**~~, categorize product catalog~~**,**~~ nicely ~~**to**~~ re-ranking, retrieve results, get top K of the best answers, or ~~**to**~~ do structured data extraction from images, like extract information from receipts, ~~**for**~~ medical bills, ~~**for**~~ insurance policies. ~~**to**~~ paraphrasing emails for better sales follow-up or for ~~**a**~~ better marketing lead generation. And the list can go very long.

00:28:14 
 But as you can see, every single examples ~~**I**~~ ~~**mentioned**~~ here, they're very specific. So that's created an interesting opportunity for us, especially for smaller open source models. Think about model training as a process of aligning the model to optimize for ~~**set**~~ ~~**our**~~ objectives. So this is very important framing because what the model is good at is being decided at the beginning of the training process. How you form the training ~~**data**~~ ~~**set**~~, what ~~**is**~~ ~~**a**~~ proportion of what kind of training ~~**data**~~ ~~**set**~~ you mix together, is basically ~~**a**~~ product ~~**opinion**~~ you put in what is the capability of this end model. So no model today is going to be good at solving all kinds of problems. You have to pick and choose by ~~**set**~~ ~~**the**~~ objective at the beginning. So that's a general framing, but you can also think about ~~**it's**~~ much easier to align ~~**model**~~ to solve a specific problem very well, instead of ~~**us**~~ to solve hundreds of problems very well.

00:29:13 
 Solving narrow down the objective to one problem. It's a much easier alignment process. So that's why in practice, all the practical problems we are seeing today is amenable because it's amenable to smaller models because the problems are very narrow and well defined. And we have seen a lot of success using fine-tuning to solve those problems. And second, almost every single enterprise we talk with, they have data. ~~**they**~~ ~~**have**~~ ~~**data**~~ to align a model better, and sometimes not just ~~**unparall**~~, even better than GPD4. So based on those observations, we're pretty bullish on the direction of ~~**continue**~~ ~~**down**~~ to make the feedback loop of quality between fine-tuning, inference, data collection, cleansing, ~~**this**~~ ~~**view**~~ ~~**by-loop**~~ very efficient for our users. And we will, again, we will try to automate as much as possible in this process.

00:30:10 
 SPEAKER_00: Okay, cool. I have a number of follow-ups there as well. So just speaking to my own experience, briefly, ~~**I**~~ ~~**think**~~ the general notion that fine-tuning is, that seems like a really good answer to how do we compete with haiku? Because ~~**I**~~ ~~**would**~~ ~~**agree**~~ ~~**that**~~, and in fact, at Weymark, my company... We use a fine-tuned GPT 3.5 currently to power our core script writing task. The number one most important AI function in the product, we could use GPT for it. It's not really ~~**like**~~ a budget thing for us. We have a pretty high value use case. So ~~**we**~~ ~~**are**~~ our strategy is use whatever gives us the best results.

00:30:51 
 Whatever gives us the best. not just the best overall user experience, ~~**I'd**~~ ~~**say**~~. And the quality of the output is probably the number one factor there. Latency also is important. As we've discussed, people don't want to wait. And so at the moment, we're on a 3.5 fine-tuned model. GPT4 could probably do the task ~~**.**~~ pretty well.

00:31:10 
 And we wouldn't be scared off by the cost, but ~~**it**~~ ~~**is**~~ a little bit slow sometimes. And ~~**also**~~ it's a little bit unwieldy. You're trying to prompt engineer your way. You have all these different caveats and rules and whatever. And it's ~~**like**~~ hard to represent all that stuff in the prompt. And you could ~~**also**~~ think, maybe we could use haiku and maybe we could even put ~~**like**~~, 10 examples into Haiku, and that could, in theory, get it to do the in context learning. But now we're back to a situation ~~**also**~~ where the order, it's now we've gone up in order of magnitude in price because we're doing 10 examples at every prompt. Fine tuning, ~~**I**~~ ~~**do**~~ ~~**think**~~, is a really good answer.

00:31:44 
 And certainly, even if companies don't have, as you said, companies have data, but even if they don't, a lot of companies would probably be very surprised with how little data it really takes to do a reasonable fine tuning. Our data set is in the three figures, hundreds, but not even thousands of data points. And that works quite well, actually. We don't have to have a huge thing for a narrow task. If you're trying to maintain generality, then you have a much bigger challenge on your hands. But for this one task of write a script for this business, for this video, hundreds of data points we have found ~~**pretty**~~ ~~**well**~~ suffices. SPEAKER_01: That's an excellent point. ~~**Yeah**~~.

00:32:21 
 SPEAKER_00: So I guess, tell me about that fine-tuning product. And this also can maybe get into some of the bottlenecks. It feels very~~**,**~~ ~~**I**~~ looked at the documentation. ~~**I**~~ ~~**didn't**~~ ~~**actually**~~ ~~**do**~~ a fine-tuning myself, but maybe I'll put that on my to-do list. But it is very similar to the open AI product where you can set up your ~~**sort**~~ ~~**of**~~ chat records and there are JSON L lines and you upload a bunch of JSONL lines and run it. You build that by token. Is it accurate to say that it's basically ~~**a**~~ ~~**kind**~~ ~~**of**~~ at the ~~**sort**~~ ~~**of**~~ spec level~~**,**~~ ~~**same**~~ ~~**thing**~~ as the Open AI experience? SPEAKER_01: Yeah, this is a great way to summarize it because we are one of the product design philosophy is to be open-air compatible because open-air has a lot of draw for the initial set of developers to try out ideas.

00:33:10 
 And the benefit of being open-air compatible from fine-tuning to inference is it's really easy to migrate. So even if ~~**our**~~ developer ~~**don't**~~ start from us, but once they get to ~~**stage**~~, they need more interesting fine-tuning ~~**.**~~ ~~**that**~~ OpenNet doesn't provide and they need low latency and low TCO for production, ~~**and**~~ they can move to us. So you're absolutely right, and the API will feel very familiar. But on top of that, we're adding also new APIs that ~~**doesn't**~~ exist in OpenNet. So that's also our unique advantage. But coming back to fine-tuning, ~~**yeah**~~, so here we started offering a special fine tuning called Paft, performance efficient fine tuning, actually ~~**found**~~ a long time ago. The most popular technique in that bucket is called ~~**Laura**~~.

00:33:57 
 Laura stands for low-rank adaptation. So the idea is you freeze the pre-trained model weights and inject trainable rank decomposition matrices. ~~**into**~~ ~~**each**~~ ~~**layer**~~ ~~**of**~~ ~~**the**~~ ~~**transformer**~~ ~~**architecture.**~~ And the end result is ~~**greatly**~~ ~~**reduce**~~ ~~**the**~~ ~~**number**~~ ~~**of**~~ ~~**trainable**~~ ~~**parameters**~~ ~~**for**~~ ~~**downstream**~~ ~~**tasks.**~~ So usually, ~~**like**~~, fine training, you can think about it as short training, ~~**right**~~? So you need to go forward backwards and do all this stuff. With Laura, then your base model is only forward, and then your additional adapter, you go through the full training steps. So that's how it saves compute and ~~**be**~~ ~~**very**~~ ~~**efficient**~~.

00:34:34 
 And also, you just mentioned, you actually don't~~**,**~~ ~~**you**~~ figure out many people with this consistent feedback. You don't need a lot of training samples or tuning samples. Typically~~**,**~~ ~~**like**~~, around 1,000. In your example, just hundreds of them will be sufficient. ~~**A**~~ ~~**compound**~~ ~~**with**~~ ~~**very**~~ ~~**efficient**~~ ~~**lower-fine**~~ ~~**tuning.**~~ That means your feedback loop is going to be really fast. And that makes the whole fine-tuning process ~~**even**~~ more appealing ~~**alternative**~~. So let me spend a little bit ~~**time**~~ explaining what ~~**Laura**~~ means.

00:35:05 
 So under the hood, it just leveraged a concept again called rank decomposition. And rank decomposition is basically ~~**allowed**~~ ~~**us**~~ to represent a high dimensional matrix with a product of two lower dimensional matrices. So if the pre-chained weight matrix, let's say ~~**of**~~ dimension n by k, and then it can be represented with ~~**a**~~ product ~~**two**~~ matrices, let's call it n by r for the first one, and the second one is r by k. So if we do a multiplication across these two matrices, you will still get N by K, ~~**right**~~? So that's ~~**a**~~ high-level idea. And let's make the ~~**saving**~~ more concrete. N is 2000. K is 5,000. And R is one ~~**for**~~ ~~**extreme**~~ ~~**case**~~.

00:35:50 
 Just make the point here. So the total ~~**of**~~ parameters in the original matrix is 2000 by 5,000. That's 10 million. And then if we decompose~~**,**~~ ~~**into**~~ lower rank. Then the first matrix is 2000 by 1~~**,**~~ ~~**it's**~~ 2000, and ~~**second**~~ ~~**is**~~ 1 by 5,000 ~~**to**~~ 5,000. So the total ~~**normal**~~ parameters across these two lower rank, lower dimensional ~~**matrix**~~ ~~**is**~~ 2,000 plus 5,000~~**,**~~ 7,000. So 7 ~~**is**~~ ~~**a**~~ ~~**significant**~~ lower number ~~**of**~~ parameters ~~**than**~~ 10 million.

00:36:23 
 That's the original reaches. So this is more than 1,000 times reduction. That's the idea behind why using ~~**Laura**~~ to do fine-tuning is so much faster. It's not just for ~~**a**~~ faster fine-tuning. At inference, we can also do something very interesting. So we actually have many ~~**customer**~~ ~~**account**~~ to us. They need to deploy many ~~**lower**~~ adapters, which share ~~**against**~~ the same base model. So the naive way to deploy those ~~**lower**~~ adapter is you merge it with the base model, and then you deploy ~~**this**~~ 100.

00:37:00 
 If they have 100 lower adapters,  you have to deploy models. And all these 100 models sit in memory. We found early on, we ~~**are**~~ ~~**one**~~ ~~**of**~~ ~~**the**~~ ~~**first**~~ to deploy many ~~**lauras**~~ by sharing the same base model. You can imagine it looks like a tree. There's a trunk that's a base model, and each ~~**lair**~~ adapters are hanging on the trunk. So by doing this sharing, we can save a lot of costs in production for inference. Because for 100 ~~**lair**~~ adapters, you basically originally, without this saving, you have to deploy 100 times, and then you just need to deploy the one base model that's ~~**dominated**~~ ~~**the**~~ ~~**cost**~~. So by that, you save 100 times ~~**off**~~ ~~**the**~~ ~~**cost**~~.

00:37:39 
 So we kind of use Laura pretty extensively across both fine-tuning and inference, and that's a significant increase ~~**the**~~ velocity of isolation. If you haven't used~~**,**~~ ~~**try**~~ ~~**that**~~. I strongly encourage you to try our fine-tuning service. SPEAKER_00: Yeah, ~~**I**~~ ~~**think**~~ this is, let's just flesh that out a little bit more for people. Maybe I'll take a stab at it. You tell me if I'm going wrong anywhere. The high level situation is you want to have a model deployed that you can get inference from quickly. If you... Have all your computers turned off, you have the cold start problem of~~**,**~~ ~~**okay**~~, I got to boot up a Docker container and load this stuff in.

00:38:18 
 And then these like billions of parameters take time to move in. So typically most services are going to try to have some mechanism of not making you wait for a full cold start. ~~**Right**~~. So instead, what you have when something is deployed is you have a GPU sitting there with the model loaded into the high bandwidth memory, which is to say the second tier memory~~**,**~~ ~~**right**~~? ~~**I**~~ ~~**think**~~ ~~**I've**~~ ~~**gone**~~ ~~**into**~~ ~~**this**~~ ~~**a**~~ ~~**bit**~~ ~~**on**~~ ~~**a**~~ ~~**previous**~~ ~~**episode**~~ ~~**where**~~ ~~**I**~~ ~~**dug**~~ ~~**into**~~ ~~**the**~~ ~~**Mamba**~~ ~~**architecture**~~. But basically you've got ~~**your**~~ on your GPUs, you've got your many computation cores. There is the S RAM that is the highest ~~**or**~~ let's say the lowest latency closest to the computation core RAM, but it's small. You can't fit the whole model in there.

00:39:07 
 So you have to have this second tier RAM, the high bandwidth memory, where the actual billions ~~**of**~~ ~~**billions**~~ ~~**of**~~ parameters of the model sit. And then when you're actually doing inference, this is where I get a little bit fuzzy and it probably depends on different setups, ~~**right**~~? But there you're paging in parameters from high bandwidth memory into the S-RAM for the actual computation, paging them in and out to do stuff. And what you're saying with the Laura, ~~**like**~~ the many Laura's configuration is that you could, the naive approach ~~**as**~~ you described it would be to say, okay, I'm going to have a hundred different servers. Each one will have its base model and ~~**it's**~~ Laura. And the Laura is whatever one to three percent or something, ~~**I**~~ ~~**think**~~, as many parameters. ~~**again**~~, ~~**depending**~~ ~~**on**~~ ~~**the**~~ ~~**setup**~~ ~~**and**~~ ~~**exactly**~~ ~~**how**~~ ~~**you**~~ ~~**do**~~ ~~**it**~~. So you could have 100 of

00:40:15 
 That kind of stuff gets a little bit more complicated, but it allows you to save on having to set up all these servers and have them~~**.**~~ waiting there. What's the next~~**,**~~ ~~**you**~~ ~~**know**~~, level of sophistication in terms of the analysis there? That could be like, what are the bottlenecks? What are the tradeoffs that you're facing? That's literally just about everything that I know. Tell me, what should I, what do I need to know next to get smarter beyond that base level description? SPEAKER_01: ~~**Yeah**~~, ~~**so**~~ ~~**I**~~ ~~**think**~~ we did a lot of very complicated inference stack optimization to bring down latency.

00:40:48 
 As we have discussed in this episode, we are hyper-focus on latency, but also while we're hyper-focus on very low latency, we ~~**also**~~ hold the latency bar ~~**we**~~ can pump up throughput to very high, and that is the result of low TCO. I will pass it to Dimitio to talk about all the trade-offs, nuances, we have put into the inference optimization stack. SPEAKER_02: So I want to hear, yeah, I'll really take a step back and try to go over some very basic steps of how to build your own GNI inference service and what are the key points you should pay attention to because ~~**a**~~ ~~**sense**~~ ~~**that**~~ ~~**we**~~ ~~**have**~~ ~~**right**~~ ~~**now**~~ ~~**a**~~ ~~**huge**~~ ~~**information**~~ ~~**over**~~ ~~**a**~~ ~~**lot**~~, there's so much information, ~~**oh**~~, this attention information is the best, this mom architecture is the next big thing. ~~**or**~~ we need to run ~~**a**~~ ~~**chart**~~ model differently

00:41:51 
 And right now, let's always focus maybe on one or two cases. One is text generation and another is image generation. Luckily, the very welcome development is that the new model from stability ~~**is**~~ based on the image generation is based on the transform architecture. So the old architecture based on the Resnet is gone. And with that, all the convolutions are not as important anymore. So now ~~**we**~~ ~~**all**~~ ~~**be**~~ ~~**talking**~~ ~~**about**~~ ~~**are**~~ transformers. So ~~**all**~~ ~~**we**~~ ~~**have**~~ ~~**to**~~ ~~**worry**~~ ~~**about**~~ is a transformer now~~**.**~~ ~~**which**~~ ~~**is,**~~ ~~**in**~~ ~~**some**~~ ~~**extent,**~~ ~~**is**~~ ~~**great,**~~ but it has its own challenges.

00:42:24 
 Now, what does matter for Transformer?  So if a transformer, ~~**I**~~ ~~**would**~~ ~~**say**~~ there are only two operations which matter. One is the most ubiquitous matrix multiplication, and that's ~~**all**~~ ~~**the**~~ ~~**only**~~ ~~**I**~~ ~~**more**~~ ~~**close**~~ ~~**to**~~ ~~**any**~~ ~~**I**~~ ~~**or**~~ ~~**not**~~, but ~~**Mool**~~ is the most important one. And the second most important one is, ~~**of**~~ ~~**course**~~, attention. And attention is a bit special, ~~**kind**~~ ~~**of**~~ ~~**metamool**~~. It's ~~**a**~~, ~~**I**~~ ~~**would**~~ ~~**say**~~, ~~**kind**~~ ~~**of**~~ it's a back-to-back batched ~~**metamil**~~. And optimizing it is quite critical. ~~**I**~~ ~~**can**~~ ~~**relate**~~ ~~**a**~~ ~~**bit**~~ ~~**more**~~ ~~**in**~~ ~~**detail**~~ ~~**like**~~

00:42:59 
 Okay, so now let's say you optimize these two operations, but ~~**guess**~~ ~~**what**~~? There are different flavors you need to opt out of these operations. And if you~~**,**~~ ~~**these**~~ ~~**famous**~~ are coming from the text generation workflow. Because text generation is a bit special. There is input tokens and output ~~**pockets**~~. ~~**and**~~ processing them is vastly different. The input tokens is mostly compute bound, and generating tokens is mostly memory bandwidth bound. ~~**Now**~~ you have to have these two ~~**favors**~~ ~~**of**~~ these two operations.

00:43:31 
 One is Flaps optimized, compute optimized,  and the second is the memory bandwidth optimized to speed up this page loading of model weights from this HBM to the S-RAM and then to registers. So that's what you need to implement, not how to implement it. So let's take Nvidia stack for example. SPEAKER_00: Can ~~**I**~~ ~~**just**~~ interject for one second? ~~**I**~~ ~~**just**~~ want to make sure ~~**I**~~ understand and that it's clear to the audience also that the two, we're all within inference here, ~~**right**~~? But the ~~**kind**~~ ~~**of**~~ two forms of latency that matter are time to first token and then speed of token generation from there. And these are essentially two different ~~**.**~~ phases of the process.

00:44:18 
 because, and here's where I'm learning from you and putting two and two together in real time, you said that the input tokens, ~~**aka**~~ the processing that happens for time to first token is going to be compute bound. And I understand that to be because in that process, we're doing all the attention, all the MLPs for all the tokens, but we ~~**can...**~~ do that in such a way where we don't have to ~~**like**~~ page in and out the parameters for each token. We can page them in, process all the tokens with those parameters, page new parameters in, process all the tokens with those parameters. And so that's why it's compute bound, ~~**right**~~? Because we're not paging in and out as much on a per token basis versus then when that's done and we get into token by token generation, now we need all the parameters in and out for every single token. Is that right? SPEAKER_02: It's actually much simpler than that. So it's actually much simpler than that.

00:45:16 
 So the thing that if you look at typical use cases, ~~**is**~~ they have, I would say roughly 10 to 1 ratio of input ~~**or**~~ output. So ~~**now**~~ when you have your input and the input also typically ~~**like**~~ 1,000 and more tokens goes to 10,000, practically it's very common to see this kind of workload. ~~**Now**~~ that's for the input. For the generation, when you run the generation, you first of all, you need to~~**,**~~ ~~**you**~~ generate one token at ~~**a**~~ time, unless you do some speculating generation, which is ~~**like**~~ separate, but it's mostly one token at ~~**the**~~ time. And ~~**of**~~ ~~**course**~~, ~~**yes**~~, you want to also batch multiple generations together, but there is also ~~**like**~~ a limit there. ~~**because**~~ ~~**you're**~~ ~~**a**~~ big pront length, you need to allocate a lot of memory to host your intermediate activations. ~~**Okay**~~? So ~~**in**~~ practically what we see is

00:46:12 
 I would say it goes from ~~**like**~~ 16 to 2 ~~**to**~~ and the batch time for prefil can go to tens of thousands. So that's why here you see this more ~~**order**~~ ~~**of**~~ ~~**magnitude**~~ ~~**even**~~ ~~**more**~~, two orders of magnitude difference. And that's why ~~**the**~~ here, the compute versus the memory bound thing comes in. So for the pre-fill, you have so many tokens to process. So your bottleneck is actually ~~**a**~~ ~~**metmills**~~, your ~~**bottlenecks**~~, ~~**it**~~ flaps on the ~~**computer**~~ ~~**on**~~ ~~**the**~~ GPU. For the generation, you don't have that many. ~~**and**~~ your bottleneck is actually, ~~**but**~~ ~~**in**~~ ~~**the**~~ ~~**board**~~ ~~**situation**~~, you need to load the weights from ~~**HBA**~~ ~~**all**~~ ~~**the**~~ ~~**way**~~ ~~**to**~~ ~~**register**~~. If you do it in both situations, but

00:46:56 
 It's not the flaps. So it's quite simple, actually, if you think about it. SPEAKER_00: Okay, so ~~**I**~~ ~~**think**~~ ~~**I'm**~~ with you. ~~**I'm**~~ not fully sure ~~**I**~~ understood the distinction between what ~~**I**~~ was trying to say and what you said. It seemed like the big idea as ~~**I**~~ understood it was that when you do the input tokens, like to create an attention matrix, for example, you're going to do Every you have to do ~~**every**~~ ~~**every**~~ token, ~~**right**~~? And all the attention heads throughout the thing. And so there's a lot of compute there. And so just on a relative basis, there's more compute relative to the moving of parameters in and out.

00:47:31 
 Whereas in the generation side, now you're generating one at a time. ~~**Is**~~ ~~**it?**~~ There's not really a way to get around the fact. You have to run the whole transformer, ~~**right**~~, to generate one token. So you ~~**do**~~ have to ~~**like**~~, please, go ahead. SPEAKER_02: Right. So you ~~**do**~~ have to run the four transformers, but the fundamental thing here is that you use fundamental optimization, which is called kV caching. So in this case, with a nutshell, it is, as you mentioned, for the~~**,**~~ ~~**for**~~ the profile initial stage, you need to attend to all, so all tokens have to attend to all previous tokens, ~~**right**~~?

00:48:07 
 So it's basically a quadratic divided by two. So that's generation. You need to do the same, but you can~~**,**~~ ~~**you**~~ don't have~~**,**~~ for the previous tokens, you can cache this attention. You don't have to run it again and again and again. SPEAKER_00: This is the KV cache. SPEAKER_02: Yeah~~**,**~~ ~~**exactly**~~. Results into only the token you're trying to generate attending to the prior ones. So this is~~**,**~~ ~~**as**~~ ~~**you**~~ ~~**see**~~, is much cheaper because it doesn't have this quadratic nature, it's ~~**on**~~ ~~**linear**~~ ~~**nature**~~.

00:48:33 
 SPEAKER_00: Yeah. How big does the KV cache get and does it have to come out of the S-Ram and go on to the high bandwidth memory? Or does it just stay in the S-Ram the whole time? SPEAKER_02: ~~**Yeah**~~, ~~**so**~~ it gets really big, ~~**right**~~? And it varies because of multiple factors. One is the ratio of the Q heads to KV heads. ~~**So**~~ that's one ~~**of**~~ ~~**the**~~ ~~**MQA**~~ ~~**paper**~~ ~~**popularize**~~ ~~**this**~~, and now all the big models they have this. ~~**So**~~ they reduce ~~**amount**~~ ~~**of**~~ KV heads.

00:49:04 
 Usually, order of around 4 to 8, sometimes even more. But 8 has been very common there. So there's one thing. Another thing, ~~**you**~~ ~~**can**~~ ~~**shut**~~ the model. Now ~~**you're**~~ reducing ~~**you**~~ as well. So what we see ~~**while**~~ ~~**in**~~ ~~**game**~~ models, Qe Cash is not typically a big ~~**of**~~ ~~**a**~~ problem. But in the end of the day, you have to keep it in the HBM for fast access. In some case, you can actually put in CPU memory and it's going to be permissible.

00:49:29 
 But that introduced a lot of complexities as well. ~~**Yeah,**~~ ~~**but**~~ ~~**most**~~ ~~**typical**~~ ~~**case**~~, you'll keep it in HBM. SPEAKER_00: Okay, cool. So you were just about to say when I interjected, okay, so let's do that on NVIDIA. SPEAKER_02: ~~**Yeah,**~~ ~~**so**~~ ~~**I**~~ ~~**think**~~, ~~**yeah,**~~ going through the, how do you implement these ~~**met**~~ moves and attentions very efficiently? So on the Nvidia's tech has a lot of APIs, and they go from very low level to very high level. And let's start ~~**the**~~ ~~**other**~~ ~~**direction**~~. So from the high level, Nvidia has now this ready service, TRCLLM.

00:50:00 
 So it's based on Tritin for a service. It's already ready to go. But ~~**it's**~~ easy to get started, although ~~**it's**~~ not as easy as some open source offering~~**,**~~ but Compared to the rest, ~~**it's**~~ much easier. But ~~**it's**~~ the most rigid, as always. So if you want to change anything, ~~**it's**~~ C++ code. Basically, good luck. And then when you go down the stack, if ~~**this**~~ ~~**is**~~, ~~**you**~~ ~~**not**~~ ~~**have**~~ enough flexibility from this setup, you can go and take a step down and you can try to write, you want to use your auto-customized metmool using, for example, kublas library or QDNN, QDN. Then if these are pre-compiled versions, there's a configs.

00:50:36 
 You can tweak, but you can't fundamentally change them. Then you want to take ~~**even**~~ step down, you want to be more flexibility. Now we're looking at the Cutlass Library and with the new sub-library, which is called Cute. It's pretty cute~~**,**~~ ~~**that**~~ ~~**would**~~ ~~**say**~~, much better than the older API. This is ~~**very**~~ program, way more programmable. It's basically ~~**a**~~ C++ code library. You can perform a lot of customization there. But ~~**even**~~ ~~**if**~~ that is not enough, you can program yourself in Kudac C++.

00:51:05 
 Even sometimes since that is not enough, ~~**then**~~ you can go all the way to the hardware instructions and you can program in PtX. So all these options are possible. The interesting things that ~~**I**~~ ~~**would**~~ ~~**say**~~ ~~**it's**~~ interesting how GPU programming evolved, ~~**right**~~? So it all started with ~~**the**~~ programming Kuda cores. And if you use ~~**the**~~ GPU, that's what used to be. So ~~**yeah**~~, you had CPU core, which is very programmable, but not as parallelizable. Now we go into ~~**the**~~ GPU, and GPU has ~~**the**~~ SCUDA ~~**course**~~. It's also programmable, not ~~**a**~~ CPU.

00:51:37 
 They are fast. Now the time passes by, and that is also not enough, because as ~~**I**~~ ~~**mentioned**~~, we're only optimizing really for METMOOs and this attention, which is also sort of METMOO. So how can we do that? Now we need to do~~**,**~~ the next best thing is embed ASIC into your GP. And that's where the tensor course is coming. That's basically ASIC embedded in GPU. But guess what? It complicates the programming ~~**big**~~ ~~**time**~~.

00:52:01 
 And with the, especially on the newest hardware on H, the tensor cores are asynchronously programmable from the Kuda ~~**course**~~. Now you have two levels ~~**of**~~ ~~**asyncrasity**~~. You code in CPU to launch CUDA kernels, and then from CUDA, you'll also asynchronously launch tensor core ~~**corner**~~. So this becomes really complicated. And what we see as a result of it, ~~**I**~~ ~~**see**~~ practically very few good kernels which are geared towards H-100. V-100 and A-100, they had a lot of good ~~**Kuda**~~ kernels geared towards them. People wrote it and enjoyed GPU programming.

00:52:37 
 Comes Hopper with the sync programming nature, and the amount of those public codes just drops~~**,**~~ ~~**crazy**~~. And there is a very good explanation to it because ~~**program**~~ ~~**becomes**~~ ~~**even**~~ ~~**more**~~ ~~**complicated**~~. So now the question is, how do we solve this problem? ~~**I**~~ ~~**think**~~ one good step ~~**on**~~ ~~**it**~~ is a welcome development is Triton from OpenEI. So it actually~~**,**~~ ~~**it's**~~ a different compiler. ~~**I**~~ ~~**would**~~ ~~**say**~~ ~~**that**~~ C++ was a good choice probably back in the day when Nvidia chose it for Cuda. But now it's really getting in the way. So ~~**like**~~ ~~**I**~~ ~~**really**~~, in some cases, it's much easier to program in ~~**a**~~ ~~**sample**~~ for CUDA than in ~~**that**~~ CUDA C++, to be honest.

00:53:15 
 Because of that, we see a lot of experimentation and this Triton is a very neat idea. And so there ~~**you**~~, literally ~~**a**~~ programming in Python, and there is a lot of magic happening behind ~~**these**~~. ~~**Of**~~ ~~**course**~~, there is ~~**never**~~ ~~**free**~~ ~~**lunch**~~, ~~**right**~~? So we basically introduce some kind of structure to your GPU programming, and it splits it in different levels. So first is like a very simple layer where you program just in Python. Then it translates in what's so-called MLIR, intermediate representation, and then even that one is ~~**a**~~ low-level MLI~~**.**~~ ~~**which**~~ is closer to hardware and then gets translated to PTF. So they go to the lowest level right away.

00:53:52 
 The nice thing about this is this structure. ~~**Structure**~~ because of this very complex GPU architecture, we talked about this structure actually makes it much easier to, ~~**I**~~ ~~**would**~~ ~~**say**~~, to understand the existing new kernels, and ~~**I**~~ ~~**would**~~ ~~**say**~~ longer term maintenance is much better. Because if you compare it to the C++-passing implementation based on C-QA to Triton, those are really hard to understand. There are thousands of lines of code. This provides a very nice clean separation. But again, no free ~~**launch**~~. And ~~**I**~~ ~~**would**~~ ~~**say**~~ if you hope that, oh, I'm just going to write a ~~**Kuda**~~ kernel in Triton, and it's going to be, I just going to write Python, and it's going to be super fast. I'm most likely, unless you're doing point-wise, probably not.

00:54:34 
 You have to still look at what is getting generated, what instructions are they used in GPUs, are they pipeline, are they not, what tweaks ~~**I**~~ ~~**need**~~ ~~**to**~~ ~~**do**~~, and so forth. You have to still look ~~**in**~~ ~~**the**~~ ~~**output**~~ ~~**in**~~ ~~**that**~~. There is no way around it ~~**to**~~ ~~**achieve**~~ ~~**the**~~ ~~**cutting**~~ ~~**edge**~~ ~~**performance**~~. But the nice thing is that it has this compiler, because ~~**I**~~ ~~**don't**~~ ~~**really**~~ ~~**want**~~ ~~**to**~~ ~~**write**~~ ~~**the**~~ ~~**assembly**~~, ~~**right**~~? Because they need to keep track ~~**all**~~ ~~**the**~~ ~~**registers**~~ and there's 255 registers, and it's just that code is unreadable, ~~**right**~~? So here, by introducing this structure, the hope is, and ~~**I**~~ ~~**think**~~ it does work in many cases, although not a silver bullet, it does

00:55:26 
 SPEAKER_00: Okay, so let me ask a couple ~~**basic**~~ questions. ~~**Or**~~ ~~**again**~~, let me try ~~**basic**~~ summary and then ask a couple ~~**basic**~~ questions. There's a lot of, obviously, and this is true of all computing, there's a lot of layers between electrons moving around at the very lowest level to Python, or now we could even say~~**,**~~ ~~**you**~~ ~~**know**~~, prompting GPT4 to write me ~~**to**~~ Python to do the things that I wanted to do. ~~**I**~~, and ~~**I**~~ ~~**think**~~ ~~**I**~~, and certainly ~~**I**~~ ~~**would**~~ ~~**expect**~~ most of the audience is pretty familiar with the fact that the higher level at which you're working, the more you are at the mercy of all the other layers to determine what your ultimate performance is going to be. ~~**I**~~ ~~**have**~~ ~~**lived**~~ a pretty privileged life where most of the things ~~**I've**~~ ~~**done**~~ ~~**have**~~ ~~**been**~~ fine at the Python layer. And ~~I

00:56:17 
 What is not super intuitive to me is ~~**like**~~, what is it, what is happening that is causing the need for ~~**like**~~ lots of ongoing optimization today? One might naively think, hey, if it's all transformers, there's presumably ~~**like**~~ only so much optimization that would need to be done across these ~~**like**~~ handful of operations within the transformer. And yet, it sounds ~~**like**~~ that's not really the case. So what are the things that are driving the need for continual optimization or why isn't ~~**just**~~ ~~**like**~~ a finite set of problems that have already all been solved by the community? SPEAKER_02: I would say number one, reason is ~~**a**~~ new hardware. ~~**and**~~ new hardware trying to get more optimal. And it's not just getting better with the existing precision. So it used to be we do everything in ~~**a**~~ single precision.

00:57:09 
 It's flawed 32 bit. And now it's shifted to a few years ago ~~**we**~~ ~~**doing**~~ half precision. FB16 for inference, FD6 for training. ~~**I**~~ ~~**would**~~ ~~**say**~~ new standards ~~**of**~~ ~~**some**~~ new precision, go to new precision. But guess what? That is not ~~**also**~~ good enough. And VIA needs to have an MD. They need to have new step functions.

00:57:32 
 And they, what they do, they're lowering the precision. So now, our go-to-precision inference, and to some extent, ~~**in**~~ e-training, is FDA. It's only eight bits, ~~**right**~~? We use ~~**for**~~ a single parameter. And we also do computing eight bits. Then look at the B100, ~~**right**~~? Guess what we'll introduce? Of course, they go to four bits, ~~**right**~~?

00:57:54 
 And look at the recent papers,~~**and**~~ one of the best papers, which ~~**I**~~ ~~**like**~~ is so-called 1.5 bit. It's~~**,**~~ ~~**we**~~ ~~**call**~~ 1 bits, not 1 bit. But fundamentally, the theory is that to train LLM, you just need 0-1 ~~**minus**~~ 1. So basically, go left, go right, or stay put. So this is like a fundamental... property. And if you can have that, you can train a model. And it looks like this is enough, you can basically replace the larger parameters model with these smaller parameters. ~~**Of**~~ ~~**course**~~, ~~**you**~~ ~~**have**~~ ~~**to**~~ ~~**retrain**~~.

00:58:29 
 This is a crazy reduction in precision. But it looks like these models perform on par with ~~**the**~~ half ~~**of**~~ full precision models. So this is a very welcome development. You will see even more hybrid optimizations. Because if you do this 0-1 ~~**minus**~~ 1, guess what? You don't need ~~**multi-deplication**~~. It's all ~~**the**~~ addition now. You will see even more optimizations from, ~~**I**~~ ~~**would**~~ ~~**say**~~, Nvidia and AMD in ~~**coming**~~ years.

00:58:54 
 Once we train bigger models because they only want three billion models for this. ~~**Once**~~ ~~**you**~~ ~~**train**~~ ~~**bigger**~~ ~~**models**~~, this optimization will become even more critical. So now these new precisions, they require very different instructions. And because you see here, the ratio of generation ~~**of**~~ GPU is really optimized for this specific ratio of ~~**computer**~~ and memory ~~**back**~~. So here, when the ratio changes, they change the nature of the APIs. And you need to basically code from scratch, to be honest, ~~**like**~~ for the GPU generation. So all the API ~~**stay**~~ ~~**works**~~, but it's much slower when ~~**you**~~ ~~**couldn't**~~. Like a good example could be, I would say, a flash attention implementation.

00:59:35 
 Flash attention two was called for A1, for 16 precision. And then you run H100, you have just half of what you can get. It's ~~**same**~~ kernel, but it just uses all the operations, all the instructions from Imperial, while the Hopper instructions are different. So ~~**I**~~ ~~**think**~~ this is ~~**like**~~ why fundamentally it's happening and we still have a lot of work ahead. But for a few years ahead, until we reach these three bits, let's see what's next. But there's still a long way to get there. SPEAKER_00: Yeah, that three-bit paper is, or the 1.58 bit paper is super interesting. SPEAKER_01: ~~**I**~~ ~~**will**~~ ~~**add**~~ ~~**to**~~ ~~**that**~~, ~~**right**~~?

01:00:12 
 Dimitius just mentioned, hey, hardware keeps moving forward, and the basic instruction set keeps changing. But ~~**also,**~~ there's other lines of product~~**,**~~ ~~**right**~~? There's ~~**MD**~~, there's Intel, there's customistics, and so on. So they're very broad. And we want to simplify that for our user. But ~~**also**~~, if we look at the upper stack, so we ~~**are**~~ looking ~~**down**~~. If we look up, at the application level~~**,**~~ ~~**right**~~? ~~**So**~~ we mentioned latency.

01:00:37 
 You mentioned time to first token,~~**right**~~? So sometimes latency also means time to ~~**the**~~ first 30 tokens because they are voice streaming out and they have to get ~~**the**~~ 30 tokens before they can ~~**streaming**~~. Or they care about ~~**the**~~ end-to-end latency, ~~**right**~~? So latency also means different things. And people always want to get ~~**the**~~ spectrum of latency cost trade ~~**law**~~. They want to see a curve. ~~**and**~~ they want to pick a point in that curve ~~**and**~~ best for their business. And then on top of that, ~~**the**~~ input-output ratio is different per application.

01:01:09 
 We see a lot of rag usage where it pushed the input to be ~~**like**~~ ratio, to output ratio to be very high. It can be 10 to 1 or even much higher. Or sometimes people just generate one token for classification. Or sometimes they generate a lot, ~~**like**~~ they're generating code. So the ratio keeps changing. And the best deployment to find depends on what kind of latency you care, which ~~**dot**~~ ~~**in**~~ ~~**this**~~ latency cost curve. What is your input output ratio? What ~~**are**~~ ~~**generating**~~?

01:01:42 
 What kind of is your input?  Your ~~**contest**~~ ~~**window**~~ length? It all matters. It's very complicated to optimize upwards towards where your application looks like. Does it have repetitive prompts, for example, as another application vertical, down to every moving hardware landscape? That is a complexity ~~**all**~~ application product developers as they are doing things, fun stuff themselves or in enterprises, they're all facing this challenge. So that's where we come in and we say, you ~~**don't**~~ worry about it.

01:02:20 
 We handle it all for you.  So you just focus on your product application development. So ~~**I**~~ ~~**just**~~ ~~**want**~~ ~~**to**~~ double click back to what is our roles in this complex ecosystem. SPEAKER_00: Yeah, it's funny that you mentioned the curve and picking the point on that curve. I'm putting together a little talk for business leaders and developing ~~**a**~~ ~~**like**~~ top 10 or whatever things to know for business leaders. One of my tips for them is to~~**.**~~ Learn to think in Pareto curves. And this is something that ~~**I**~~ see at literally every level of the stack, including ~~**like**~~ just false positives, false negatives.

01:03:00 
 Truly like every level seems to have this. So do you envision a future of ~~**like**~~ your product experience being literally just showing people these curves? ~~**I'm**~~ ~~**like**~~, should ~~**I**~~ ~~**expect**~~ ~~**to**~~ ~~**see?**~~ a number of Pareto curves where ~~**I**~~ ~~**can**~~ ~~**pick**~~, okay, ~~**I**~~ ~~**want**~~ for this application, ~~**I**~~ ~~**want**~~ absolute minimum time to first token. ~~**I'll**~~ ~~**take**~~ the highest cost. For this one, ~~**I'll**~~ ~~**take**~~ the happy medium. For this one, ~~**I**~~ ~~**want**~~ the lowest cost, but it's a background process or whatever. So it's okay to wait.

01:03:26 
 Is that the kind of experience?  That's the kind of choice that you want to expose ultimately to developers. SPEAKER_01: So ~~**I**~~ ~~**will**~~ ~~**draw**~~ analogy here. ~~**So**~~ ~~**I**~~ ~~**think**~~ 20, 25 years ago, database ~~**is**~~ ~~**a**~~ new domain, ~~**right**~~? And database management systems ~~**or**~~ start to come into ~~**pictures**~~. And it's very~~**,**~~ ~~**actually**~~, ~~**it's**~~ very interesting analogy, ~~**I**~~ ~~**think**~~. ~~**Now**~~ ~~**I'm**~~ ~~**thinking**~~ ~~**about**~~ ~~**it**~~ ~~**on**~~ ~~**the**~~ ~~**fly**~~. So database queries have strict patterns.

01:03:53 
 It's called SQL, right? You have your select clause. You have your... ~~**Work**~~ ~~**clause**~~, ~~**group**~~ ~~**by**~~, ~~**doing**~~ ~~**aggregation**~~, ~~**right**~~? So this is a very clear pattern. If you think about that, ~~**Jenny**~~ ~~**I**~~, the models ~~**has**~~ very clear pattern ~~**too**~~. You have your operator layers. It's more or less stable right now. Although the pattern is simple, but ~~**depends**~~ ~~**on**~~ ~~**which**~~ ~~**the**~~, ~~**you**~~ ~~**know**~~, how many columns you have, which column you're going to put filter on which column you're going to do aggregation, which ~~**color**~~ ~~**you're**~~ ~~**going**~~ ~~**to**~~ ~~**do**~~ ~~**group**~~ ~~**by**~~.

01:04:26 
 There are all different ways to optimize your query. Does that make sense? And at ~~**early**~~ days, none of these database management system are smart, ~~**right**~~? That's why it created a slew of, it created a whole new entire career. It's called DBA, ~~**right**~~? So basically, all database has knobs for ~~**human**~~ to tune to try to optimize things. ~~**Right**~~. And those people make a lot of money because once they optimize, it's a much better experience.

01:04:55 
 It save a lot of money too. It's very similar to what we just talked about, latency and TCOs. But over time, all these database management has become smarter and smarter because they all have a layer called optimizer~~**,**~~ ~~**right**~~? Those optimizer observe the workload. And it started to create, oh, you're doing a lot of filter on this particular column. So I'm going to create index. I'm going to partition those columns based on your ~~**future**~~ criteria. So it's much better search, much faster search.

01:05:21 
 You can skip a lot of things. You have to do sequential scanning. ~~**And**~~ ~~**so**~~ ~~**on.**~~ So then the optimizer becomes smarter and they start to, you don't need to hire DBAs at all. ~~**fully**~~ automated. So as long as you observe the workload, you have the workload defined, it's self-tuned towards the best outcome. So that's our vision. Today we have multiple configurations based on what you tell us, your workload pattern, what you care about, we deploy for you, which is really good.

01:05:47 
 But over time, we want to automate ourselves away from that process. Basically, we'll learn from what you are running, and the system becomes smarter and smarter. ~~**become**~~ lower latency over time. It can be higher quality over time. So that's what we ~~**are**~~ aspired to build. SPEAKER_00: So ~~**are**~~ these options, this would be in the product line today, would be in the dedicated deployments product, ~~**right**~~, is where I get to set that. If I'm doing serverless, Presumably, you're already handling your own optimization challenges there, and I don't have to worry about that at all, although I maybe get less choice in terms of what tradeoffs I get to make. But in the dedicated deployments, can you run through some of the...

01:06:29 
 I scanned these documents, but I wouldn't say I conceived of it in the way that you're describing it now. So what are some of the choices that ~~**I**~~ ~~**get**~~ to make today, depending on what my application needs are and... How does that evolve in the short and medium term? SPEAKER_01: ~~**Yeah**~~. For example, ~~**I**~~ ~~**think**~~ ~~**we**~~, if you look at our offering, product offering, we have three tiers. The developer tier, the business tier, the enterprise tier. So the business tier is more like surplus pay as you go. ~~**In**~~ ~~**enterprise**~~ ~~**tier**~~, ~~**we**~~ ~~**will**~~, usually they have clear workload definition. And they know where the latency cost curve where they want to be because they have budget.

01:07:11 
 They also have product requirements. And they know their workload, ~~**like**~~ in terms of input, how much ~~**rag**~~ they're using and how much generation and so on. And then we come in and pick a perfect spot for them. And then we deploy a specific configuration ~~**towards**~~ their workload. But over time, this can be more automated. And because their workload can evolve, they don't always have to get us in the loop. ~~**and**~~ ~~**we**~~ ~~**were**~~ ~~**just**~~, our system would just self-adjust~~**ing**~~ ~~**towards**~~ ever-changing workload. So that's ~~**how**~~ the direction ~~**I**~~ ~~**want**~~ heading ~~**towards**~~.

01:07:43 
 SPEAKER_00: So could you give me a little bit more ~~**like**~~, Intuition for, let's say I have three, let's say I'm an enterprise customer and I have three applications. I have one that's user facing and it's going to generate tokens for a voice app. And those first 30 tokens, as you said, are ~~**like**~~ super critical to get fast because I want to have conversational fluency. So that's ~~**like**~~ one. Then the other thing is ~~**like**~~ a background job where I'm just ~~**like**~~, I just give it to me as cheap as I can. And then there's maybe a chat bot one that's in the middle. I don't want to pay top dollar for that, but ~~**you**~~ ~~**know**~~, I don't want it to be slow. How does that trickle down into what you are actually deploying for those three different scenarios?

01:08:22 
 SPEAKER_01: So usually in your enterprise,they have, ~~**as**~~ ~~**exactly**~~ ~~**as**~~ ~~**I**~~ ~~**said**~~, they have many applications. And they are, usually the distribution of traffic is always ~~**skilled**~~, ~~**right**~~? They're heavy hitters, they're very high volume, and they're long tails. They ~~**each**~~ ~~**doesn't**~~ ~~**have**~~ ~~**high**~~ ~~**volume**~~, but they add up. So we recommend, ~~**like**~~ they basically think about ~~**longtail**~~ ~~**as**~~ one bucket of deployment, and we give them one configuration, and we help them optimize for the heavy ~~**heaters**~~. ~~**Each**~~ ~~**of**~~ ~~**the**~~ ~~**heavy**~~ ~~**hitter**~~, ~~**as**~~ ~~**you**~~ ~~**said**~~, they care about different ~~**kind**~~ ~~**of**~~ latency. And those heavy hitters probably are coming from their product team. We also work with ML Infra team coming from their product team and they have a certain product budget.

01:09:01 
 So then we work together to figure out, hey, which ~~**dot**~~ you want to pick in this curve? And the product will say, ~~**you**~~ ~~**know**~~, when it comes to quality, then the prompt length ~~**coming**~~ ~~**into**~~ ~~**picture,**~~ ~~**right**~~? How much instruction tuning you put there. ~~**whether**~~ you fine-tune and take away the instruction prompt, or you put a lot of ~~**a**~~ ~~**rag**~~ information as ~~**a**~~ context to constrain the model to spit out, do less hallucination. So all these kind of product ~~**contacts**~~ ~~**start**~~ ~~**to**~~ ~~**come**~~ ~~**into**~~ ~~**picture**~~, and then we have different kind of deployment to hyper-optimized for very long context window processing versus they do a lot of generation. So those are also different configurations. SPEAKER_02: Yeah, I can give ~~**it**~~ ~~**to**~~ some concrete example where you have to have different deployments for specific use cases. So let's say you really want to get a very low time to first ~~

01:09:55 
 And say your model is small.  ~~**Smallish**~~ ~~**meaning**~~ ~~**that**~~ ~~**like**~~, say, seven billion parameters, you don't have to ~~**shut**~~ shard it ~~**really**~~ to put ~~**in**~~ ~~**different**~~ GPUs. But the thing is that it's still not fast enough to run on a single GPU because ~~**I**~~ ~~**promise**~~ so long. So now for this specific use case, you want to shard it. But there is a cost for ~~**shard**~~. ~~**So**~~ ~~**they're**~~ ~~**not**~~ ~~**like**~~ ~~**fundamentally**~~, ~~**I**~~ ~~**would**~~ ~~**think**~~ ~~**of**~~ three sharding techniques. One is to shard your model weights. Another one is to shard the activation ~~**of**~~ the input.

01:10:24 
 The third one is to chop modeling pieces that is called pipeline ~~**parallels**~~. So the last one doesn't really help you with the latency. It just helps when a model is too big. The case when you shard model weights, also called model parallelism, ~~**aka**~~ ~~**tensile**~~ ~~**paralysis**~~, this does help also with the model size and does help with the latencies, although there is a cost for sharding. So basically your throughput overall declines. And the first one, the input sharding is usually, it doesn't help you with the model size. Although it does, and it's pretty cheap, but there's a caveat ~~**attention**~~. So you need to take care of ~~**attention**~~ because you ~~**attempt**~~ ~~**to**~~ ~~**add**~~ ~~**a**~~ ~~**token**~~.

01:11:07 
 So now you have to have a bit more complexity in your setup. So the bottom line here is that for the ~~**pre-fail**~~, if you, for example, most common ~~**shining**~~ ~~**right**~~ ~~**now**~~ is the tensor parallelism. If you choose it, you don't want to always ~~**shut**~~. You really want to ~~**.**~~ ~~**don't**~~ ~~**charge**~~ as much as possible up to the point where you have to, because there is a cost. Because your throughput will be tanking the more you ~~**shut**~~. But ~~**so**~~ much you have to ~~**shut**~~ because you cannot meet the latency budget. So this is a fundamental thing, and I don't think this problem goes away any time soon.

01:11:36 
 SPEAKER_00: Yeah, okay. So I think,~~**again,**~~ everybody will be conceptually familiar with the idea ~~**of**~~ ~~**that**~~ There is a latency throughput trade-off. That seems to be a ~~**very**~~ recurring pattern. But if I understand what you're saying correctly here, it is that if you have a really long prompt, even if you have a small model that can fit onto one GPU, you may want to split ~~**across**~~ GPUs. I'm not quite clear on the nature of the splitting. Are you splitting ~~**like**~~ layer-wise? Like you'd have early layers on one and then later layers on another? SPEAKER_02: So every layer you split.

01:12:10 
 So basically, you'll be running. It's more complicated, but it's ~~**like**~~ very in nutshell, because it goes to Megatron sharding. When you try to ~~**met**~~ ~~**mules**~~, ~~**even**~~ do one column-wise ~~**another**~~ ~~**roll-wise**~~ and minimize the amount. Because in the end, once you do this, you need to reshuffle, it will reduce. So you want to minimize that and this Megatron style ~~**Shaddy**~~ ~~**helps**~~ ~~**you**~~. But fundamentally, ~~**yeah**~~, you're splitting actually the weights across the GPUs and then you need to all reduce the activation ~~**sensor**~~. SPEAKER_00: Can you give us a little bit better intuition for that? Because ~~**I**~~ ~~**think**~~ that's something that is...

01:12:42 
 It's not super clear to me,  ~~**and**~~ ~~**I**~~ ~~**suspect**~~ a lot of people are not going to be very clear on it either. I've seen examples of this in my study of the Mamba and Mamba-related things recently. I've been trying to get a better handle on this. ~~**And**~~ ~~**I**~~ ~~**guess**~~ one thing that's~~**,**~~ ~~**like,**~~ ~~**kind**~~ ~~**of**~~ intuitive~~**,**~~ surprising or counterintuitive is that there are ways to take advantage of basically the associative property where you can do these ~~**like**~~ quite counterintuitive algorithms, even for ~~**just**~~ ~~**like**~~ adding up numbers that ~~**kind**~~ ~~**of**~~ look pretty weird, but end up being faster, especially because it allows for this ~~**kind**~~ ~~**of**~~ parallelism. Am I on the right track with that? SPEAKER_02: ~~**Yeah,**~~ ~~**so**~~ ~~**I**~~ ~~**think**~~, ~~**so**~~ here it applies not necessarily. ~~**I**~~ 

01:13:33 
 They've been there like before even Gen.EI kicked in. But they're fundamental to the machine learning. And then ~~**you**~~, but then which one you want to apply totally depends on the architecture because the one architecture is much better. For example, if you don't have~~**,**~~ ~~**you**~~ ~~**don't**~~ ~~**have**~~ a sequential nature, you're using pipeline ~~**imperilism**~~ is actually ~~**no**~~ ~~**goal**~~. It's not going to ~~**say**~~ anything because you cannot even ~~**jump**~~ the model. And it used to be a model like that. All transformed models tend to have layers, so it's much better. But something like if you look back on the image, older image model based on Resnet, they have the same connections, it's becoming harder to do pipeline ~~**paralysis**~~.

01:14:11 
 Yeah, and then the tensor parallelism actually conceptually is harder to understand. ~~**I**~~ ~~**think**~~ the data input ~~**parallels**~~ is easier because you just split data and data is independent, so you could just duplicate the models, split the inputs, ~~**right**~~? And so you make all process and then join them ~~**in**~~ ~~**there**~~. That is easy. The model parallel is, which actually ~~**do**~~ use right now, hardest to understand, because once the way it starts interacting with each other, you need to make sure your math is correct. Because you ~~**kind**~~ ~~**of**~~ just shard across, if you need to, for example, two values, if you need to add them ~~**in**~~ ~~**there**~~. You need to, if you want to shard them, but you have to still do the addition. If you need to multiply, you still have to do the multiplication.

01:14:51 
 So when you do that, you need to probably gather them on one rank, another rank, or send them across ~~**them**~~ ~~**and**~~ ~~**then**~~ split these operations across the ranks. Of course, sending across GPUs is way more expensive than just sending from a single GPU to the registers, from ~~**magnitude**~~ ~~**worse,**~~ ~~**even**~~ ~~**more**~~. So it becomes quite complicated, but these are the kind of compromises you have to go through. So basically, in the end ~~**of**~~ ~~**this**~~, you need to make sure that your math is still correct once you do the shard. So this is the ~~**other**~~ ~~**of**~~ ~~**this**~~, and there are different techniques. And basically, not every operation can be easily split. ~~**This**~~ ~~**is**~~ you need to look at specific operations, specific matrix multiplication, how you can split. ~~**Yeah**~~, so if you want to research about this, yes, Megatron is a good study point.

01:15:38 
 It has, like, fundamental descriptions, how you do the Metmoo shot. SPEAKER_00: Yeah, okay, cool. I'll check that out. That's a good pointer. Let me just try to summarize the three kinds and make sure I ~~**at**~~ ~~**least**~~ have the conceptual framework right. Simplest one is data parallelism. That is just make multiple instantiations of the model. You can split the data across them.

01:15:56 
 That's essentially like a load balancing setup. ~~**the**~~ ~~**next**~~ ~~**level**~~ ~~**up,**~~ ~~**which**~~ ~~**maybe**~~ ~~**is**~~ ~~**the**~~ ~~**most**~~ ~~**complicated,**~~ ~~**it**~~ ~~**sounds**~~ ~~**like,**~~ ~~**is**~~ actually splitting the weights. This is the kind of thing that you would do because you have a super long prompt, for example, and you want to have a fast time to first token. So you need to have even more parallelism of the computation. And it can be worth it, but it comes at this cost of now you've got GPU to GPU communication plus the complexity of~~**,**~~ ~~**What**~~ ~~**the**~~ ~~**hell**~~ ~~**am**~~ ~~**I**~~ ~~**doing**~~ ~~**across**~~ ~~**all**~~ ~~**these**~~ ~~**different?**~~ Now that I'm splitting weights~~**,**~~ ~~**okay,**~~ that's a lot to keep track of. And ~~**then**~~ the third one is the pipeline. And that is where you actually split early layers from

01:16:41 
 And you're feeding through multiple things in a pipeline in a sequential way. ~~**Cool.**~~ That's helpful. Lots to learn. I've been doing this around the clock, ~~**I**~~ ~~**feel**~~ ~~**like**~~, for a few years now. And ~~**I**~~ ~~**still**~~ ~~**am**~~ ~~**still**~~ a target rich environment for new things to learn. SPEAKER_02: Yeah, and then you can combine them in many different ways and go to the really complicated setups. So it gets hairy.

01:17:04 
 SPEAKER_00: Yeah, all independent dimensions as well. Okay, so we've covered a lot of ground. One other question I wanted to ask about and then maybe just give you guys a chance to touch on anything that we didn't touch on that you want to is with all this complexity of... everything we've talked about, ~~**right**~~? The one thing that was surprising to me is that you are also training your own models. You have your own branded fireworks function calling models. And especially given your earlier comment that ~~**like**~~ you think the~~**,**~~ ~~**at**~~ ~~**least**~~ the small models are converging. How does that fit into the overall strategy? ~~**Like**~~, why bother training your own models as opposed to just focusing on the insane complexity of the inference stack and letting that convergence happen independently of your efforts?

01:17:52 
 SPEAKER_01: That's a great question. And ~~**I**~~ ~~**think**~~ ~~**like**~~ one of the biggest~~**,**~~ ~~**like**~~ the North Star of the product ~~**well**~~ building is not just serving individual models, ~~**right**~~? So we want to build the inference ~~**that**~~ for knowledge extraction. It's an inference ~~**that**~~ for knowledge extraction. And when we talk about knowledge, we can first look at the knowledge provided by large language models. It's very capable now. It can answer many questions. It surprises us.

01:18:22 
 But at the end, all large language models ~~**has**~~ ~~**limited**~~ knowledge. Why? The knowledge is limited by the training data. It's training ~~**that**~~ is limited in terms of time range. It doesn't have the most latest information, and it doesn't have ~~**the**~~ information ~~**.**~~ People cannot crawl from ~~**Internet**~~, those public information. So they are just limited to the corpus of information people can crawl directly from. And many of those knowledge also live outside of large language models because there are foundation models.

01:18:55 
 They generate images, generate videos, general audios. They also do information extraction from images. So that's, ~~**I**~~ ~~**want**~~ ~~**to**~~ create a framework to think about the knowledge distribution is beyond just large language model. And even for Given a large language model, it's limited. And there are other modalities that carry a lot of information. Beyond that, there are a lot of knowledge hidden behind APIs that we cannot extract from. And we use API very extensively~~**,**~~ ~~**day**~~ ~~**to**~~ ~~**day**~~. For example, we do search, ~~**right**~~?

01:19:29 
 So there's Google search. There's weather API. There's ~~**dock**~~ doc extraction API. There's map API. For personal productivity, there are APIs for ~~**dogs**~~, ~~**for**~~ spreadsheets, ~~**for**~~ calendars, and so on ~~**so**~~ forth. So APIs are everywhere. Within enterprise, one enterprise internally can have hundreds of API~~**,**~~ surface areas ~~**too**~~. There are a lot of key value stores, document stores, production logs, ~~**pop**~~ subsystems, internal search.

01:19:53 
 So API itself contains a large corpus of knowledge. ~~**So**~~ ~~**think**~~ ~~**about**~~ function calling. What is function calling? ~~**So**~~ ~~**our**~~ ~~**file**~~ ~~**function**~~, The model we build ourselves is serving the function calling area. And function calling is the layer to tie knowledges together behind all different modalities of foundation models and APIs together. ~~**So**~~ ~~**to**~~ ~~**us**~~ ~~**strategically**~~, this is a critical layer for us to build the best inference here for knowledge. And with that, you can build a lot of ~~**an**~~ application. For example, you can build a personal ~~**adminem**~~ ~~**to**~~ ~~**knock**~~ ~~**down**~~ ~~**and**~~ ~~**to**~~ ~~**everyone**~~ ~~**on**~~ ~~**the**~~ ~~**planet**~~ ~~**that**~~ ~~**can**~~ ~~**sort**~~ ~~**out**~~ ~~**tedious**~~ ~~**work**~~.

01:20:36 
 This application can learn what you like over time and can ~~**preemptily**~~ ~~**finish**~~ work before you ask and put out the reminders or even suggest things ~~**out**~~ ~~**of**~~ ~~**time**~~. I wish I could have a person ~~**on**~~ ~~**me**~~ like that. But these~~**,**~~ ~~**those**~~ ~~**kind**~~ ~~**of**~~ new ~~**application**~~ ~~**has**~~ ~~**to**~~ ~~**depend**~~ ~~**on**~~ tools and ways to extract knowledge across the board from foundation models and from APIs. That's a fundamental reason why we built our own function calling model. We have released two versions~~**.**~~ ~~**open**~~ ~~**weights**~~, so everyone can use it. You can download ~~**your**~~ ~~**phone**~~ ~~**Hugging**~~ ~~**Face**~~. We are currently working on our third version.

01:21:12 
 That's going to come out soon,and we'll have a great quality bomb. So stay tuned. SPEAKER_00: Cool. ~~**I**~~ ~~**can**~~ ~~**look**~~ ~~**forward**~~ ~~**to**~~ ~~**that.**~~ ~~**I**~~ ~~**know**~~ we're just about at the limit of our time together today. And you guys have been very generous with your time and knowledge. Is there anything that we haven't managed to touch on yet that you want to make sure you mentioned? SPEAKER_01: ~~**I**~~ ~~**think**~~ we'll touch on a lot.

01:21:31 
 I'm curious about the final production coming out. It must be very interesting. SPEAKER_00: You mean in terms of ~~**like**~~ our editing process? SPEAKER_01: Yeah. SPEAKER_00: ~~**Yeah**~~, we'll edit. It'll be fine. It'll be pretty straightforward, honestly. SPEAKER_01: ~~**Yeah**~~, ~~**I**~~ ~~**think**~~ you have one question about SSM.

01:21:45 
 SPEAKER_00: Yeah, that's a hobby horse of mine. Is that on your customer's radar? Is it on your radar? Is it on your roadmap? SPEAKER_01: So actually, we haven't heard ~~**too**~~ much about the requirements for our customers. ~~**I**~~ ~~**think**~~ it's a very cool technology. So~~**,**~~ ~~**of**~~ ~~**course**~~, when we talk about SSM, it's in the context of Mamba. ~~**I**~~ ~~**think**~~ it's a really cool technology.

01:22:03 
 The main barrier to practical adoption is mainly quality in my mind, compared with transformer models. So we haven't seen a pure Mamba SSM-based model emerge as a ~~**company**~~, highly competitive in quality to ~~**Mistral**~~, ~~**Cloud**~~, ~~**GBT**~~, 3.5, or 4 yet. ~~**I**~~ ~~**think**~~ AI21 Labs just open-source~~**d**~~ jamba. It's a hybrid of SSM and transformer models. But the real quality, benchmark is one thing, real quality in practice, another thing. So real quality using practical use cases ~~**yet**~~ to be verified. So that's why ~~**I**~~ ~~**think**~~ we haven't seen huge demand there. And ~~**I**~~ ~~**guess**~~ people are more cautious.

01:22:46 
 It's hard to say what will be the technical challenge. ~~**I**~~ ~~**think**~~ Dimitro is pretty confident. We can support it in no time. But ~~**I**~~ ~~**think**~~, again, coming back to quality, it's a little bit hard to assess. So those kind of architecture, Mamba and SSM is really good for long context by removing the ~~**cogedic**~~ ~~**nature**~~ of Transformer. But for long context, it's hard to assess the quality. Needle in ~~**this**~~ haystack is the most common used benchmark, but it's not very comprehensive. ~~**I**~~ ~~**don't**~~ ~~**think**~~ as an industry, we have ~~**not**~~ ~~**yet**~~ standardized the benchmark measuring long context quality ~~**yet**~~.

01:23:21 
 But this is another area. I think the whole industry needs to move forward. SPEAKER_02: And this ~~**team**~~ ~~**of**~~ ~~**the**~~ benchmarks still show that there is room to go for ~~**Mamba**~~ architecture to meet the current transformer-based models. SPEAKER_00: Yeah, my money's on the hybrids, for what it's worth. I think there's been some really interesting ~~**kind**~~ ~~**of**~~ very fine-grained studies of the comparative strengths and weaknesses. And I've seen some really interesting, right. SPEAKER_02: It's a compromise. So the thing is that ~~**I**~~ ~~**think**~~ if you don't care, if you don't want to get into this compromise, you just want to ~~**get**~~ ~~**with**~~ a transform attention.

01:23:57 
 And once you, the compromising is really hard sometimes~~**,**~~ ~~**right**~~? There's a compromise in the quality~~**,**~~ ~~**right**~~? This is hard. But once you ~~**know**~~ what you can compromise ~~**Zana**~~ or can tailor this for specific use cases. But ~~**like**~~, for example~~**,**~~ ~~**like**~~ if you're looking to summarize. So maybe ~~**Mamba**~~ is maybe okay~~**,**~~ ~~**I**~~ ~~**think**~~. Specific stuff in your document, you probably don't want to compromise. So ~~**I**~~ ~~**think**~~ this~~**,**~~ so these kind of examples, maybe if you can classify your use case in one of these categories, then you can say, oh, maybe this~~**,**~~ ~~**I**~~ ~~**would**~~ ~~**say**~~, imprecise attention implementation or attention proxy is good enough for me.

01:24:32 
 SPEAKER_00: That is definitely something that we can watch and there will be~~**,**~~ ~~**I'm**~~ ~~**sure**~~, many more developments. ~~**Guys**~~, ~~**I**~~ ~~**know**~~ ~~**you've**~~ ~~**got**~~ ~~**to**~~ ~~**go**~~. This has been a great conversation. Lin Chow and Dmitro, Dmitro Ivchenko, co-founders of Fireworks AI. Thank you for being part of the Cognitive Revolution. SPEAKER_01: Awesome. Thanks. SPEAKER_00: Appreciate it~~**,**~~ ~~**guys**~~.

01:24:49 
 I'm going to hit stop. Just give it one second to...

