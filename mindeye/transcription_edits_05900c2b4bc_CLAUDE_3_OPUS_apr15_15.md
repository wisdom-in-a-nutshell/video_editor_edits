GUEST_02: One thing I would like to talk about is the ~~**kind**~~ ~~**of**~~ unique setup that we've had as a team, basically an open lab concept that stability supported. This is not a very typical setup. ~~**like**~~ engaging the public and publishing everything as it's being developed. ~~**So**~~ ~~**I**~~ ~~**think**~~ ~~**that**~~ ~~**that's**~~ ~~**an**~~ ~~**interesting**~~ ~~**topic,**~~ ~~**but**~~ ~~**yeah,**~~ I can't really go into too much detail about the ~~**kind**~~ ~~**of**~~ changes that are being made in stability after a mod lift, for instance.

HOST: Okay, cool. ~~**How**~~ ~~**about.**~~ It sounds like there's enough there for me to ask that question when the time comes and you can give a version of it. If ~~**again,**~~ at the end, ~~**want**~~ ~~**to**~~ run it by stability people or just want to edit it yourself, you'll certainly have that option.

I'm not going to press you on anything that you're not~~**.**~~ comfortable talking about or certainly not that you're not supposed to talk about. But ~~**I**~~ ~~**think**~~ if there is anything that you want to say, then ~~**I**~~ ~~**do**~~ ~~**think**~~ it would inherently be of interest, not just because of the recent changes there, but ~~**just**~~ more generally. It's obviously a company that has~~**,**~~ ~~**I**~~ ~~**think**~~, confused analysts, ~~**let's**~~ ~~**say**~~. I personally feel that people have often missed the point or~~**,**~~ but I'm not sure I fully understand the point either or the strategy, but ~~**I'm**~~ ~~**like**~~, I always take a research first lens. I take this ~~**kind**~~ ~~**of**~~ to Google deep mind as well. People are like, ~~**Oh**~~ ~~**my**~~ ~~**God**~~, they've lost their way. They've fallen behind or whatever.

And I'm always the black George Washington's aside, which are a weird outcome that I'm sure they didn't intend. And by the way, ~~**like**~~ it seems ~~**like**~~ almost every foundation model company has had their moment of this kind of thing. When you look at ~~**just**~~. the research breadth, the quality of research coming out of DeepMind, ~~**like**~~ it's still probably the world's leading AI research lab. And ~~**I**~~ ~~**feel**~~ stability is similar in the sense that not ~~**like**~~ as well resourced, obviously, or as big as DeepMind. It's ~~**like**~~ there's continued to be a meaningful stream of quality research that ~~**I**~~ ~~**find**~~ ~~**super**~~ ~~**interesting**~~ ~~**and**~~ ~~**I**~~ ~~**think**~~ continues to matter. So that sort of disconnect between the work that's getting done on the one hand and is this a business or not is, ~~**I**~~ ~~**think**~~, confusing. A lot of people are making people miss some of the stuff that is important.

Though at the same time, I guess that's stability. You don't have ~~**a**~~, at Google, they're making like $200 billion a year in revenue, so they can afford a couple. HOST: You don't have ~~**a**~~, at Google, they're making like $200 billion a year in revenue, so they can afford a couple. GUEST_02: Yeah, I think that's the problem. HOST: Misteps. GUEST_02: They need to make money and then there's ~~**like**~~ disconnect with investors and everything. HOST: Yeah. Playing a deep mind strategy with that deep mind resources is not necessarily going to work, even if it does create some really remarkable research while the resources last.

So yeah. Yeah. I've certainly been very intrigued by the company and ~~**first**~~ ~~**and**~~ ~~**foremost,**~~ just impressed by ~~**the,**~~ a lot of the output over time. ~~**Anyway,**~~ let's get into that when we get to it. ~~**And**~~ ~~**again,**~~ you can always self-censor after the fact. So we're not playing a gotcha game here at all. Sounds good. I appreciate that.

Yeah, you bet. I pride myself on a good guest experience and hope to have people want to come back in the future. So that's always my outlook. ~~**Can**~~ ~~**you**~~ ~~**pronounce**~~ ~~**your**~~, ~~**any**~~ ~~**other**~~ ~~**questions**~~ ~~**or**~~ ~~**concerns**~~, ~~**anything**~~ ~~**you**~~ ~~**want**~~ ~~**to**~~ ~~**discuss**~~ ~~**before**~~ ~~**we**~~ ~~**officially**~~ ~~**start**~~ ~~**recording?**~~ ~~**No.**~~ ~~**Okay,**~~ ~~**cool.**~~ Can you pronounce your full name for me? And then I'll say it back to you ~~**and**~~ ~~**welcoming**~~ and we'll be rolling.

GUEST_02: Yeah. My name is Paul Scottie. HOST: Paul Scottie. Okay, cool. Paul Scottie. ~~**HOST:**~~ ~~**Paul**~~ ~~**Scottie.**~~ Welcome to the cognitive revolution. Thanks for having me.

I'm excited for this conversation. So you are the lead author on a recent paper called Mind Eye 2. Shared subject models enable fMRI to image with one hour of data. Long-time listeners will know that we had an earlier episode with your teammate and the author of the original Mind Eye paper ~~**just**~~ nine months ago. And ~~**I**~~ ~~**thought**~~ this one was worth following up on ~~**because**~~ multiple reasons. ~~**In**~~ ~~**general,**~~ ~~**the**~~ ~~**fact**~~ ~~**that**~~ we have brain reading technology in the world today and ~~**that**~~ you can literally use a machine to see what somebody is seeing is ~~**like**~~ a pretty striking reality that ~~**I**~~ ~~**think**~~ a lot of people are missing amidst all the other noise and developments that are happening. ~~**But**~~ ~~**also**~~ ~~**that**~~ the techniques under the hood are really interesting. So I'm excited to get into a lot of the technical details and learn how you've made this modern marvel happen.

You want to start by just folks can go to the original episode to hear the full deep dive. You want to maybe just set up ~~**like**~~ briefly where the first one left off and what ~~**kind**~~ ~~**of**~~ motivated the part two of this project.

GUEST_02: Yeah, to give a bit of history to this, ~~**I**~~ ~~**guess**~~ back in 2022. I joined Lion Discord server, and I joined basically to get better acquainted with machine learning methods, learn how they are publishing really high quality work as an open community. And I appreciated their open science approach to things. And as I was scrolling through that Discord, I found that there was a channel dedicated to reconstructing images from brain activity. And the person who was leading that was Tanish. ~~**And**~~ so I basically introduced myself and said, hey, I'm already doing this.

So I was a postdoc at the time in Ken Norman's lab. And this is the direction that we wanted to go, more so regarding memory. But~~**,**~~ ~~**you**~~ ~~**know,**~~ to reconstruct memory, you first need to reconstruct vision, right? So that's the direction we were going. And at the time there was not as much work being done in this space. At least the quality of the results were not the quality that now these papers are able to show. And it was actually ~~**like**~~ a perfect timing because there was a new dataset that came out~~**.**~~ the natural scenes dataset.

This was really high-quality data from eight participants being scanned for 30 to 40 hours each, a very intense ~~**kind**~~ ~~**of**~~ data collection effort from the nasal RSNK labs. ~~**So,**~~ This was an amazing opportunity to use better data quality. And there were new open source models being released. ~~**So**~~ ~~**for**~~ ~~**instance,**~~ CLIP and Stable Diffusion were very great open source models that you could tap into. ~~**And**~~ ~~**so**~~ everything was there. The pieces were there to get a new pipeline working for state-of-the-art results. ~~**And**~~ ~~**so**~~ we teamed up and made this an open science project with MindEye 1. And when we released ~~**Might**~~ ~~**I**~~ ~~**One,**~~ ~~**I**~~ ~~**think**~~ it was a great reception.

And there were also many other papers that were doing the same sort of approaches. For instance, Furkan Erselik had a brain diffuser paper that was also showing really good results using similar approaches. And the constraint with MindEye 1 and the similar papers was that all the models are being trained independently. So you have 40 hours of data from one person and you train the entire pipeline on just that one person. It's not being trained across multiple people. So if you have a new person that you want to apply these models to, you would basically need to put them back into the MRI machine and scan them for 30 to 40 hours. And ~~**obviously,**~~ ~~**I**~~ ~~**don't**~~ ~~**know**~~ ~~**exactly**~~ ~~**how**~~ ~~**much**~~ ~~**it**~~ ~~**costs.**~~ It's like a thousand dollars an hour to scan somebody.

So it's really not realistic. for follow-up work, ~~**right?**~~ So if you want to apply this kind of stuff to new directions, like memory, mental imagery, dream reconstructions, biomarker diagnosis, have it usable by people in hospitals, like clinicians, you're just not going to get the ability to apply these models. So that's the ~~**kind**~~ ~~**of**~~ focus we are working towards, and MindEye 2 is that step in that direction that we now have output. So it basically allows you to, with a totally new subject, collect one hour of data and get good, not as good as if you had 40 hours, but a lot better than you would have had with only one hour. So it's like near the same level of quality with 2.5% the amount of data as before. HOST: ~~**Yeah,**~~ There's a couple of really interesting things here about the relative magnitudes of data. And ~~**I**~~ ~~**think**~~ there's a couple of interesting connections also to some recent, ~~**like**~~ a recent episode we did on robotics.

So let me just try to highlight a couple of things and you can react to these. One of the challenges, obviously, as you said, for this sort of problem is scarcity of data, ~~**right?**~~ ~~**And**~~ that's, ~~**again,**~~ true in robotics as well. There's just a limited amount of how does a robot go about solving this problem? In this case, there's ~~**like**~~ a very limited amount of available fMRI ~~**excuse**~~ ~~**me,**~~ ~~**a**~~ ~~**very**~~ ~~**limited**~~ ~~**amount**~~ ~~**of**~~ ~~**fMRI**~~ data available to train models on. So even in the first MindEye paper, one of the big ideas was we can combine this ~~**like**~~ relatively modest data set that we have with these foundation models in a clever way to tap into this ~~**kind**~~ ~~**of**~~ general purpose understanding of images that they have already learned from their ~~**like**~~ web scale training. And we're going to find a way to tap into this with a data set that's ~~**like**~~ comparatively much smaller, but is still

So I think that's really interesting because these sort of foundation models that embody ~~**a**~~ ~~**sort**~~ ~~**of**~~ a world model. ~~**And**~~ ~~**obviously**~~ that topic is ~~**like**~~ hotly debated in terms of exactly how we should understand it. But it is increasingly clear with all these different applications that it has a lot in there that if you figure out how to augment it in just the right way, you don't necessarily need to bring a ton of new data to the table to make really cool things happen. I think that's just a really profound takeaway from this work. Can we talk about the data set itself for a second? First of all, do I understand ~~**quickly**~~ that the MindEye 1 and MindEye 2 papers are using the same data set? Is that right? HOST: Yeah.

Okay, cool. Let me just run down a couple of things to make sure I understand it. You can correct me if I have any misconceptions. I believe there are eight patients that participated in this collection set. ~~**they**~~ ~~**participated**~~ between 30 and 40 hours in an fMRI each. And they were shown, there's one table in the paper that I wasn't quite clear on, which is ~~**the,**~~ in figure two, where there's this list of subjects and how many hours they were in there. And then the number, ~~**is**~~ ~~**that**~~ ~~**the**~~ ~~**number**~~ ~~**of**~~ ~~**input**~~ ~~**output**~~ ~~**pairs**~~ ~~**for**~~ ~~**each**~~ Oh, ~~**no,**~~ I think that's the number of voxels that get fed into the model to put them in a shared space. Gotcha.

Okay. Let's come back to voxels in just one second. So they're in there for 30 to 40 hours each. Your experience during that time is what? They're getting shown an image every ~~**how**~~ ~~**many**~~ ~~**seconds?**~~ ~~**Do**~~ ~~**you**~~ ~~**know**~~ ~~**exactly**~~ ~~**what**~~ ~~**that**~~ ~~**looks**~~ ~~**like?**~~ GUEST_02: Every four seconds. ~~**So**~~ ~~**it's**~~ ~~**not**~~ ~~**everybody.**~~

Everyone was requested to be scanned for 40 hours. Not everybody got through the full amount, but for instance, for subject one, they were there for 40 separate sessions, 40 visits to the MRI. And they saw 750 images per visit, so per hour, basically. And so it's three seconds of looking at an image, then one second of blank period, and then another three seconds of a different image. And the task is to press a button inside of the machine if they ever see an image again. So every image was shown three times throughout the entirety of the 40 sessions. ~~**HOST:**~~ ~~**Interesting.**~~ ~~**HOST:**~~ ~~**Interesting.**~~

So they, and they are. GUEST_02: There'll be a total of 40,000 images that a person sees 30,000 to 40,000 images where every image gets~~**,**~~ ~~**gets**~~ shown three times. HOST: Gotcha. Okay. Interesting. ~~**And**~~ ~~**their**~~ ~~**job.**~~ ~~**And**~~ is that~~**,**~~ is there something that~~**,**~~ that sort of clicker data is used for, or is that primarily just to keep them engaged and make sure that they're actually still looking at thinking about the images that they're seeing? GUEST_02: Yeah, we didn't use it, but I think it could be used.

There's been some other papers that try to do the opposite of what we're doing. Like you try to predict the brain data instead of predict the image. And the state of the art for that is you use all the behavioral information that you have possible to better predict the brain data. So that shows that it can be useful. We tried to get that to work. We weren't having much success, so we ultimately didn't use it. HOST: Gotcha. Okay, cool.

Then let's talk also about just what is measured. So every four seconds they get a three second view of an image, a one second blank. During that time when eyes are on a particular image, we're taking, ~~**is**~~ ~~**it**~~ ~~**just**~~ one data point or multiple data points of the brain activity during that three second window?

GUEST_02: Yeah. ~~**So**~~ ~~**I**~~ ~~**think**~~ it's taking a snapshot every second. ~~**I**~~ ~~**might**~~ ~~**be**~~ ~~**wrong.**~~ This is the so-called repetition time of the MRI machine. So you're taking a full picture of the entire 3D brain every X seconds, where X is the repetition time of the machine.

So it's not a continuous measurement.  It's every second you get a full picture of the brain. And then they use a general linear model to basically convert it from this time series ~~**like**~~ every second measurement to a single measurement per image.

HOST: ~~**Gotcha**~~. Okay, cool. And then in terms of the actual biological function that is measured and the space that is measured, the fMRI is measuring blood flow, ~~**if**~~ ~~**I**~~ ~~**understand**~~ ~~**correctly,**~~ and ~~**I**~~ ~~**had**~~ ~~**to**~~ ~~**look**~~ ~~**back**~~ ~~**at**~~ ~~**my**~~ ~~**notes**~~ ~~**with**~~ ~~**Tanishka**~~ ~~**on**~~ ~~**this**~~ ~~**one,**~~ ~~**but**~~ he described it as a two millimeter cube resolution. ~~**So**~~ ~~**yeah,**~~ ~~**yeah.**~~ The way I thought of that was if I imagine my visual cortex being at the back of my brain and the number of voxels from this chart is ~~**like**~~ in the ~~**sort**~~ ~~of

It would seem to correspond to If a single voxel is two millimeters cubed, then you have five by five with a centimeter cubed, which should give you 125 voxels per cubic centimeter. And then to get to those numbers, you'd need to have a 10 by 10, ~~**like**~~ layer of, ~~**so**~~ it's 10 centimeters by 10 centimeters. with these sort of rice-grained two millimeter cube bits, each one measuring with a single number for each one that measures the blood flow to that small little region within the broader, ~~**I**~~ ~~**assume**~~ ~~**this**~~ ~~**is**~~ ~~**all**~~ visual cortex. GUEST_02: Yeah. HOST: Does that all sound right? GUEST_02: Yeah. ~~**Yeah.**~~ Basically ~~**just**~~ restate ~~**like**~~ with fMRI, you are processing these images and the parts of your brain that are more involved in processing the image are going to consume more resources.

And basically, the brain needs to resupply those parts of the brain with more blood when it's been used. And the machine itself is measuring the oxygenation changes that are associated with increased blood flow to those parts of the brain. And so we are measuring that oxygenation, blood level oxygenation change in the corresponding 1.8 millimeter cubes of the brain, where every voxel is ~~**like**~~ hundreds of thousands of individual neurons. So you have a whole brain that's completely divided into all ~~**of**~~ these voxels. We only take the voxels from the visual cortex specifically and feed ~~**in**~~ those into the model. HOST: Okay. HOST: So each person, first of all, that's remarkable that it's a pretty crude measure in some sense, ~~**right?**~~ It's ~~**like**~~ ~~**a,**~~ ~~**it's**~~ both ~~**I**~~ ~~**guess**~~ at least one step conceptually removed from what you would presumably think ~~**of**~~ would be the ground truth of what neurons are firing.

It's aggregating also, as you said,  the hundreds of thousands of neurons being compressed into one data point. And ~~**I'm**~~ ~~**also**~~ ~~**a**~~ ~~**little**~~ ~~**curious**~~ ~~**about**~~ how the different individuals end up with different numbers of voxels~~**.**~~ ~~**Is**~~ ~~**that**~~ ~~**just**~~ ~~**a**~~ ~~**reflection**~~ ~~**of**~~ ~~**anatomy?**~~ ~~**Like**~~ people have different brain sizes and you're segmenting? GUEST_02: Yeah, some people have bigger brains than others. Some people have differently shaped brains. And also people have different topography of ~~**just**~~ how they represent things, ~~**like**~~ the functional topography. So you're looking for voxels that are receptive to visual processing, right?

And so where you cut off where the visual cortex is going to be different for every person, and it's going to be differently shaped. And so you'll have different numbers of voxels that are corresponding to the visual cortex. But in addition to that, just in general, people will have different numbers of voxels. And not every voxel has a correspondence to the same voxel in somebody else's brain. So that's why it's difficult to have shared subject modeling, because there's all these variations in how people represent things and how the brain, ~~**like**~~ the input data itself~~**,**~~ has a different dimensionality. ~~**Right.**~~ HOST: Yeah. Interesting.

So is that something that a clinician or a technician is doing based on their ~~**like**~~ familiarity with fMRI scans? Is somebody actually ~~**like**~~ sitting there and drawing a line around it? Or is there some automated process for determining what the region is that will be considered for an individual patient? GUEST_02: For large scale~~**,**~~ determining of where the different regions are ~~**like,**~~ You can use some automated software for that. There's parcellations that you can use that are based on large-scale data sets where, generally speaking, you can look at the different psilocybin gyri of the brain and figure out where it should be cut off. For more specific brain regions, there's functional localizers. You can do retinotopic mapping. Basically, the occipital cortex has its own special way of representing where it should correspond to where in your visual field your inputs from the retina are coming from.

So there's some more nuanced stuff that you can do for it, but for ~~**just**~~ ~~**like**~~ visual cortex, you could rely on automated software. ~~**Yeah,**~~ it's not like you're subjectively looking at someone's brain and drawing where the different regions are.

HOST: So did I understand correctly that the one technique that you're describing there would basically amount to calibrating a system by showing a light in one corner of the visual field and then looking for where that lights up and creating a map of, okay, this is how this person seems to represent the visual field based on the fact that we know what we showed them and we know that there's going to be a surge of activity corresponding to that. Am I getting that right?

GUEST_02: Yeah, pretty much. ~~**Yeah,**~~ It's ~~**like**~~ you basically are just looking at where in the brain ~~**are**~~ you ~~**getting**~~ ~~**visual**~~ ~~**inputs**~~ ~~**.**~~ ~~**And**~~ essentially, you have these repeated images, ~~**right**~~? You could directly quantify what parts of the brain are receptive to these visual images because you expect that if you have

They should look similar if you look at the same thing, ~~**right**~~? But if there's no ~~**sort**~~ ~~**of**~~ correlation between image repeats, then probably that information is not what you want for perception ~~**at**~~ ~~**least**~~. So that's one way that you could quantitatively figure out the brain regions that you want, but ~~**also**~~ ~~**just**~~ we know that visual cortex is probably the most important part of the brain to use for this. We did try using the whole brain, but it didn't work out as well. HOST: Interesting. So this data set actually contains a lot more voxels than are being specifically used for this project. GUEST_02: Yeah. ~~**Yeah**~~.

You get the whole whole brain for every snapshot, but we only~~**,**~~ ~~**we**~~ subset out the visual cortex. GUEST_01: Interesting. Okay, cool. HOST: All right. Fascinating. So we've got something on the order of~~**,**~~ 10,000-ish images, you said, each ~~**kind**~~ ~~**of**~~ shown three times. Each measurement of activity corresponds to essentially a vector of 12 to 17,000 numbers, which indicate activity in these tiny little regions, 12 to 17,000, because everybody's anatomy is different. And so you're on the order of 10,000 times 10,000, like 100 million numbers for an individual patient.

And that's all drawn from this 30 to 40 hours in the fMRI machine. So that is ~~**still**~~ that's not nothing. But unto itself, ~~**let's**~~ ~~**like**~~ ~~**still**~~ not enough to train this sort of image reconstruction. HOST: ~~**But**~~ ~~**unto**~~ ~~**itself,**~~ ~~**let's**~~ ~~**like**~~ ~~**still**~~ ~~**not**~~ ~~**enough**~~ ~~**to**~~ ~~**train**~~ ~~**this**~~ ~~**sort**~~ ~~**of**~~ ~~**image**~~ ~~**reconstruction.**~~ So maybe give us a little bit of the ~~**kind**~~ ~~**of**~~ motivation for, or ~~**the**~~ maybe the intellectual history of ~~**like**~~ how you guys realized that there was this opportunity to tap into even bigger scale, web scale foundation models that had already been pre-trained and were available to you. GUEST_02: Yeah, so the key thing is that you don't want to map the voxels to pixels, right? It would just be an insane task. The number of parameters would explode and you're

That's the kind of space that you want to map the brains into. You want to map from brain to some more compressed rich space. And that's why we mapped to clip space in particular. ~~**And**~~ ~~**because**~~ clip has this kind of rich semantic information. And if you use even better clip models, it even has ~~**basically**~~ low level information. You can exactly reconstruct the image from the latence. So that was ~~**like**~~ the key insight. Map it to clip.

And that'll allow you to really ~~**allow**~~ ~~**yourself**~~ to get high quality reconstruction results. ~~**I**~~ ~~**think**~~ the past work would do stuff like Gabor filters and ~~**like**~~ CNN based approaches where the key insight ~~**I**~~ ~~**guess**~~ here is just do whatever you can to map into this frozen pre-trained space. HOST: Yeah, ~~**I**~~ ~~**think**~~ ~~**of**~~ this ~~**as**~~ we recently covered units in ~~**a**~~ ~~**bit**~~ ~~**of**~~ depth on a recent episode, specifically ~~**like**~~ a Mamba variation on a unit. And so I think of this as analogous to mapping onto the bottom of the unit. You're mapping to the conceptual space where the image data has been convolved. If it's ~~**a,**~~ ~~**if**~~ ~~**it's**~~ a convolutional network and abstracted away from the lowest level raw pixel inputs and to a state where it is ~~**like**~~ fewer channels, but deeper dimensions, more meaning per channel. And then as the unit sweeps up the other side, it is gradually unpacking

They're starting with this like high meaning, but low precision, and then unpacking that into something that you can actually render again as pixels and see what it looks like. ~~**I**~~ ~~**think**~~ ~~**that**~~, ~~**I**~~ ~~**don't**~~ ~~**know**~~ ~~**if**~~ ~~**you**~~ ~~**have**~~ ~~**any**~~ ~~**other**~~ ~~**kind**~~ ~~**of**~~ ~~**intuition**~~ ~~**drivers**~~ ~~**for**~~ ~~**this**~~ ~~**sort**~~ ~~**of**~~ ~~**thing**~~ ~~**that**~~ ~~**you**~~ ~~**could**~~ ~~**share.**~~ ~~**I**~~ ~~**think**~~ this is a really interesting point. And one of the big things I try to do with this show is help people develop their sort of conceptual intuition because there's so much technical stuff. There's so much notation. Is there anything else that you could offer about the nature of the space into which you are mapping the brain data? GUEST_02: Yeah, ~~**I**~~ ~~**think**~~ these are foundation models that have uses beyond just image

And you could see that as ~~**like**~~ another modality that you're appending to a frozen model. Basically, my take on it is that as AI foundation models get better and better, the ability for medical applications will also get better and better. So, for instance, we used Stable Diffusion XL for this new paper, whereas we used Versatile Diffusion for MindEye 1. And the better the image generators get, the better we'll be able to tap into that. And the better the representations get, for which we map the brain data into, the better the results will be. GUEST_01: Yeah, there's ~~**a**~~, let's come back to that in a second. HOST: So it seems like there's two big drivers of why this project ultimately at the end of the day, each project has a figure and the figure from MindEye one is here are the images that the people saw and here's what we are able to recreate. And some of them are ~~**like**~~ incredibly good where you're like, wow, that is ~~**like**~~ crystal clear brain reading.

And then I think as you get a little deeper into the dataset, ~~**like**~~ some of them are. not so good or uncanny valley. And mind I too, at the simplest, highest level analysis, ~~**like**~~ the images look even more like what the person saw and the reconstruction ~~**is**~~ at just a glance is ~~**like**~~ obviously improved. It seems like there's two main drivers of this improvement. One is that you've figured out a way to not have to train a single model for each individual person, but instead mix their data together. This is. Conceptually super important, ~~**uh,**~~ as ~~**that,**~~ ~~**as**~~ you've mentioned at the top, it allows you to collect a lot less data for a person, which means ~~**like**~~ to the degree that there are going to be clinical applications for this, you can actually get there in a reasonable amount of time and, ~~**um,**~~ effort and expense. And then the other thing ~~**is**~~ you're getting at just now ~~**is**~~ ~~**the**~~.

image generation models themselves are getting ~~**a**~~ ~~**lot**~~ better. ~~**And**~~ ~~**so**~~ There's some of this stuff ~~**is**~~ ~~**like**~~ coming for free because rising tide of ~~**just**~~ general performance is lifting all ~~**of**~~ the different project boats. Tortured metaphor. How would you break those down? What do you think is important and ~~**how**~~, ~~**where**~~, what is the ~~**kind**~~ ~~**of**~~ relative contribution of those two conceptual advances to the ~~**just**~~ overall headline view that, ~~**oh**~~ ~~**my**~~ ~~**God**~~, the reconstructions are looking really good. GUEST_02: ~~**Yeah**~~, ~~**so**~~ ~~**I**~~ ~~**guess**~~, ~~**yeah**~~, there's two points to this. One is specifically if you isolate the shared subject part and discount the fact that we're using different models, ~~**like**~~ we're mapping to a different clip space, we're using a different image generation model. So if you only look at the shared subject part, we do an ablation in the paper and show that is

So just by itself, using shared subject modeling, even if you're using the full data set. ~~**you**~~ ~~**know,**~~ ~~**that**~~ you'll get ~~**where**~~ ~~**we**~~ ~~**get**~~ state-of-the-art results. So we focus on the one hour use case here. But if you use the full amount of data, then you get state-of-the-art reconstruction retrieval results. The second part, the clip model. So you can take out the shared subject part and use the new models and everything. And that also gives you notable improvements in performance. So they're both, ~~**I**~~ ~~**can't**~~ ~~**say**~~ ~~**which**~~ ~~**is**~~ ~~**like**~~ ~~**more**~~ ~~**important**~~ ~~**than**~~ ~~**the**~~ ~~**other.**~~

They're both giving obvious improvements for both of these. ~~**And**~~ ~~**also,**~~ ~~**another**~~ ~~**part**~~ ~~**of**~~ ~~**the**~~ ~~**paper**~~ ~~**is**~~ ~~**that**~~ ~~**we**~~ ~~**don't**~~ ~~**really**~~ ~~**focus**~~ ~~**that**~~ ~~**much**~~ ~~**on.**~~ ~~**It's**~~ ~~**not**~~ ~~**the**~~ ~~**focus**~~ ~~**of**~~ ~~**the**~~ ~~**paper.**~~ But we trained our own unclip model for this. So it's actually state-of-the-art unclip, clip image embedding to image, state-of-the-art performance in that domain by itself. So we fine-tuned Stable Diffusion XL to be able to reconstruct the image from ground truth clip image embeddings in a way that preserves both the high level meaning and the low level ~~**kind**~~ ~~**of**~~ image details. And that raises the possible ceiling of the ~~**kind**~~ ~~**of**~~ quality of results you can get because the previous models that

You'll get like something that maybe resembles a kitchen and a chef, but you're not going to get them in the right spot. They're not going to be wearing the right things. Lighting's going to be different ~~**and**~~ ~~**all**~~ ~~**that**~~. So we specifically had to train our own thing to raise the performance of that. And so we tap into the new space allowable by the ability to unclip better than before. HOST: Okay. ~~**So**~~ ~~**there's**~~, ~~**yeah**~~, ~~**I**~~ ~~**want**~~ ~~**to**~~ ~~**break**~~ ~~**this**~~ ~~**down**~~ ~~**and**~~ ~~**get**~~ ~~**a**~~ ~~**little**~~ ~~**bit**~~ ~~**more**~~ ~~**technical**~~ ~~**on**~~ ~~**each**~~ ~~**side.**~~ ~~**I'm**~~ ~~**looking**~~ ~~**at**~~ ~~**figure**~~ ~~**two.**~~

We can maybe overlay this on the, ~~**the**~~ YouTube version and maybe put this ~~**on**~~ ~~**the**~~, ~~**um,**~~ in the link to this image in the show notes as well. There's two~~**,**~~ ~~**two**~~ main parts of the overall pipeline, right? There is the part that works on the image data or sorry, it works on the brain data and ~~**there,**~~ The big advance is creating a single model that combines all of the individuals into a single shared latent brain space. And then the other side is, okay, now we're figuring out how to tap into the power of these foundation models by figuring out the best possible way to convert the latent into an actual good-looking image. This wasn't maybe entirely clear to me, but you're saying that the image portion In this figure, figure two in the paper, the brain side is shown ~~**as**~~ with fire emojis, which means that those parts are actively trained. And then the image parts are shown with the snowflake indicating that they're frozen. ~~**Yeah.**~~ You're saying that they were first fine-tuned and then frozen for the purpose of doing each individual person

Is that right? But there is some kind of customization. GUEST_02: The difference between the flames and the snowflakes is basically ~~**like**~~, ideally all of these models that are in the snowflakes would already be out there. And we would just ~~**like**~~ tap into them during inference to get the final results. And ~~**like**~~ the actual MindEye 2 model itself is only training the parts that are indicated with the flames, ~~**like**~~ the MLP and the diffusion prior stuff. The nuance that I was talking about is that when you've converted the brain into clip image space, and then you want to undo it, ~~**a.k.a.**~~ use an unclip model, there didn't exist a good unclip model. ~~**So**~~ ~~**ideally**~~, someone else would have done it already, and we would just use theirs.

But since a good one didn't exist, we fine-tuned SDXL to make a better unclip model, and then we used that. ~~**HOST:**~~ ~~**Gotcha.**~~ ~~**HOST:**~~ ~~**Gotcha.**~~ ~~**Okay.**~~ ~~**Cool,**~~ ~~**that's**~~ ~~**helpful.**~~ Let's just go into a little bit more detail then on each half of this. Maybe start with the image one, because it seems like that's the first part of the work. And once that's solid and working, then you can go over and do the brain to image mapping portion.

I think you said earlier that if you take a clip latent and try to unclip it, you get something that is semantically aligned with whatever the input was, but may look quite different. This ~~**I**~~ ~~**would**~~ ~~**take**~~ ~~**to**~~ ~~**be**~~ a reflection of the original clip conceptually was aligning text and image space ~~**and**~~ driven by going back to your Lion project, the Lion 5B, right? It's like the 5 billion images pulled off the internet that are captioned~~**.**~~ ~~**with**~~ ~~**obviously**~~ a lot of noise in those captions, a lot of not super specific. Typical web caption is not ~~**like**~~ describing in specific detail~~**,**~~ ~~**like**~~ which elements are in which portion of the image. It's not ~~**like**~~ containing all these sorts of layout type details. It's just saying high level, what is this ~~**right**~~ at best~~**?**~~ ~~**And**~~ ~~**it**~~ ~~**might**~~ ~~**even**~~ ~~**be**~~ ~~**worse**~~ ~~**than**~~ ~~that~

So there's a lot of noise in the underlying data set for aligning text and image, the captions are not really meant to allow you to reconstruct, they're just meant to summarize and give a sense for what the image is. So it makes sense that when you have this aligned space, and you try to reverse out of the space into image space, that you would have a fairly wide range of possibilities, all of which would correspond to the meaning. Like there is a tiger ~~**is**~~, ~~**whatever**~~, it's a tiger. There could be a tiger in a lot of poses, doing a lot of different stuff and with a lot of different backgrounds, ~~**whatever**~~. So all of those are possible from ~~**this,**~~ the diffusion prior or the latent that represents a tiger. They can go in a lot of different directions of images. That's also, ~~**that's**~~ cool, ~~**right?**~~ ~~**Cause**~~ ~~**that's**~~ ~~**why**~~ you can get different images every time from the same prompt out of a diffusion model.

There's like just a lot of ways you can go. But you don't want that flexibility in this use case, ~~**right**~~? You want to actually get back very specifically to the thing that the person saw. So there's a complicated mix of techniques. You want to describe the mix of techniques that augment this ~~**just**~~ semantic content with ~~**like**~~ more low level visual detail. And there's also the captioning portion. ~~**I'm**~~ ~~**interested**~~ ~~**to**~~ ~~**hear**~~ ~~**how**~~ ~~**you**~~ ~~**conceive**~~ ~~**of**~~ ~~**all**~~ ~~**that**~~ ~~**working**~~ ~~**together.**~~ ~~**GUEST_02:**~~ ~~**Yeah,**~~ ~~**so**~~ ~~**basically,**~~ ~~**to**~~ ~~**get**~~ ~~**back**~~ ~~**to**~~ ~~**the**~~ ~~**idea**~~ ~~**of**~~ clip is semantic~~**,**~~ ~~**right?**~~

So with unclipped models, going from clip back to image, the typical way that these models work is that you take the final output of ~~**basically**~~ the classification token, the pooled output~~**,**~~ of the contrastively trained clip model trying to align the image captions with the original image. And then you feed that through some model that gets you back the pixel image. And that is going to be highly semantic. And people ~~**like**~~ ~~**that**~~ ~~**because**~~ ~~**it**~~ gave you this ~~**kind**~~ ~~**of**~~ creative flexibility. It was called image variations models. So you give the model an image, and then it gives you a creative re-representation of that image. We didn't want that. ~~**Like**~~ ~~**you**~~ ~~**said**~~, ~~**we**~~, ~~**we**~~ ~~**wanted**~~ exactly the original image, which at first glance might seem contradictory.

Why do you want to be able to input an image and then output the same image? ~~**Like**~~ ~~**why**~~, what's the point? But the point is that you're able to then cut off the beginning, just feed in the latent and then get back the original image.

GUEST_02: ~~**And**~~ ~~**so**~~ ~~**that**~~ ~~**we**~~, ~~**that**~~ ~~**that's**~~ the basis for training our own unclip model. We don't use the pooled output. We actually use ~~**the**~~ ~~**kind**~~ ~~**of**~~ ~~**like**~~ last hidden layer. So all ~~**of**~~ the outputs from each of the tokens that were represented in the vision transformer. So it's a much higher dimensionality than just the final output of the clip model.

And what's actually pretty interesting in its own right is that you can basically get back the exact original image from the clip latent, if you take it from that point. So even though it's only trained to contrastively match the captions with the image, the clip model somehow learns to represent ~~**basically**~~ the entirety of the image itself. So you can get back ~~**basically**~~ the original image from the clip latent.

GUEST_02: Now, this wasn't possible with MindEye 1, which meant that we needed to have a ~~**total**~~ low-level pipeline. where we are predicting a blurry representation of this image that doesn't care about semantics at all. And then we use that as the initial starting point for the denoising diffusion process. So instead of starting from noise, we start from this ~~**kind**~~ ~~**of**~~ separately obtained blurry reconstruction, and then go to the finished product from there. And the blurry representation was not using a clip model.

It was using other models, like the VAE of stable diffusion. And it had no semantics to it, but it was able to more or less have a darker region where the person should be, and maybe it has the sky as blue. And that's the extent of it. But at least it had this kind of low-level representation to it. With MindEye 2, we consolidated that low-level pipeline into the full pipeline. So ~~**it's**~~ ~~**not**~~ ~~**that**~~ we don't have to train two separate pipelines anymore. ~~**And**~~ ~~**also,**~~ ~~**we**~~ ~~**realized**~~ ~~**that**~~ this new clip latent with the unclip basically does have the low-level properties now. It's not as important to have the low-level pipeline at all.

And the ablations that we use for the paper, we don't even need to start from the low-level reconstruction and use that as a starting point. It actually does worse when you do it like that. There are a couple of points to the figure two, which we do, which ~~**kind**~~ ~~**of**~~ makes it look complicated, but you could actually remove and it doesn't really affect the performance that much. So the low level stuff, getting this blurry reconstruction from ~~**my**~~ ~~**day**~~ ~~**two**~~, you really don't need it. It's more ~~**of**~~ like it improved the low level statistics in the ~~**like**~~ table to get the state ~~**of**~~ ~~**the**~~ ~~**art**~~ results. But subjectively speaking, it didn't really~~**.**~~ do that much ~~**to**~~ ~~**it**~~. We also have this ~~**kind**~~ ~~**of**~~ captioning module that is trying to predict from the brain the caption associated with the image to reconstruct.

And we use that to further guide the final output. So maybe the caption is a cat on a table. And so we feed that caption into the refinement process to get the final image. That also didn't really matter ~~**that**~~ ~~**much**~~. ~~**Like**~~ it's cool to show that you can predict the caption from the brain, but honestly, the vast majority of the information here is contained in that clip laden. You could throw out the captioning and the low level and it will do nearly as well. GUEST_02: You could throw out the captioning and the low level and it will do nearly as well. HOST: Okay, that's fascinating.

Let me try to restate some of that back to you and make sure that I understand it and then hopefully everyone will understand it. In the earlier version, going back to MindEye 1, the foundation model that you were able to use was semantic but not structural in terms of what it represented. This is why you would have this ~~**kind**~~ ~~**of**~~, ~~**like**~~ you see when you go use stable diffusion, you could say a tiger and it'll give you many different tigers because there's a lot of different ways to represent that. So to get something that actually looked like the image that the person was using at that time, you had to start the diffusion process from essentially structural hint. So you're combining two things saying, here is what is represented. This is where we want to go. And here's the loose hint starting point of the regions of the image so that you can fill in appropriately. And at that time ~~**that**~~ ~~**was**~~ ~~**sounds**~~ ~~**like**~~ ~~**quite**~~ ~~**necessary**~~ to get things to work reasonably well.

Now, if I understand correctly, it's largely because ~~**the**~~, this is a really~~**,**~~ ~~**this**~~ ~~**is**~~ ~~**a**~~ ~~**really**~~ interesting and potentially quite subtle point. Maybe you have clarity on it, but I don't quite yet. It seems ~~**like**~~ it's because the foundation model is so much better that essentially now you can just take this diffusion prior, which is what the second half, which we'll get to in a second, the mapping from the brain data to this space, what that outputs is now so rich in meaning that it basically works straight away and you get images that look like the original image. ~~**And**~~ ~~**then**~~ You also experimented with a couple of these other things to do that hint, that visual hint suggestion again. And ~~**then**~~ now you also have the caption hint suggestion and those sounds like boost. ~~**Let's**~~ ~~**say**~~. GUEST_02: Yeah, it gives a slight boost to the metrics, but overall it doesn't actually make that big of a difference. And ~~**yeah,**~~ we're ~~**allowed,**~~ ~~**we're**~~ able to get really good caption

but more or less the bulk of the model is explained by mapping brain into a frozen pre-trained clip image embedding space. HOST: So even ~~**the**~~, ~~**so**~~ ~~**I**~~ ~~**guess**~~ the very practical interpretation of that is if you remove these additional hints and just run this down the fairway, simplest version of this, then the images you get out still look like what you expect them to look like. And as a human looking at them, you're like, this is working. And then you add these other things and it improves these scores, which are ~~**like**~~ obviously more objective, but perhaps not ~~**like**~~ necessarily super meaningful. These are ~~**like**~~ pixel level comparison type. GUEST_02: Yeah, it's ~~**like**~~ you compute the correlation of the original image and the reconstructed image, or you ~~**like**~~ feed them both through a frozen network and you compare how similar the intermediate parts of the networks are. And honestly, we had some complaints about the metrics themselves, which we hinted throughout the paper, saying that these metrics are not very good. You can have reconstructions that to me are obviously better and they'll score worse on these metrics

And we actually did some human testing. We showed people online, human raters, the different images and ~~**have**~~ ~~**them**~~ pick the ones that they ~~**prefer**~~ ~~**better**~~. And ~~**it's**~~ ~~**clearly**~~ there's a disconnect between these quantitative metrics that are being used in all these papers and subjective preference for what people think is the better reconstruction from perception. HOST: Yeah. ~~**HOST:**~~ ~~**Yeah.**~~ ~~**Okay.**~~ ~~**That**~~ we're hitting a lot of frontiers, ~~**I**~~ ~~**think**~~ at the same time, generally across the whole vast field of AI, we're ~~**like,**~~ ~~**that**~~ ~~**is**~~ becoming a very live issue. These sort of robotic scoring methods that we've used in the past are hitting their limits.

And all of a sudden it's like, ~~**Is**~~ ~~**there**~~ ~~**a**~~, ~~**I**~~ ~~**didn't**~~ ~~**see**~~ ~~**this**~~ ~~**in**~~ ~~**the**~~ ~~**paper.**~~ ~~**I**~~ ~~**don't**~~ ~~**know**~~ ~~**if**~~ ~~**this**~~ ~~**would**~~, there's a lot of noise obviously in human evaluations as well. Is there ~~**a**~~, ~~**just**~~ ~~**like**~~ ~~**human**~~, ~~**like**~~ how good is this score? Did you guys ~~**just**~~ try doing everybody rate this one to 10 or something? GUEST_02: ~~**Oh**~~, we~~**,**~~ ~~**we**~~ had ~~**a**~~ alternative forced choice. You show two things and then they pick the one that they think better resembles the original image. which could have its own problems. Like it might be more biased towards looking like a naturalistic image rather than retaining low level properties.

That was not perfect solution, but ~~**yeah,**~~ I'm not quite sure what the best way is to measure the quality of these kinds of results. Because, for instance, the low-level modules, the blurry stuff, we get really good results with the low levels just by doing a simple averaging process with the blurry reconstruction. It's very crude, but it works really well for the final evaluations. But subjectively, ~~**I**~~ ~~**think**~~ it looks worse. So I don't really know what to make of that. But for these kind of papers, you have to show that you're able to get the state-of-the-art tables with these metrics. HOST: ~~**Yeah,**~~ ~~**it's**~~ ~~**tough.**~~ One of the things that I've done that has informed my thinking about ~~**a**~~ ~~**lot**~~ ~~**of**~~ these metrics, perhaps more than anything else ~~**really**~~, is trying to get a system together that would do an effective aesthetic evaluation of images.

This is something that in my company, which people have heard me talk about many times, Waymark, we make videos for small businesses, ~~**blah**~~, ~~**blah**~~, ~~**blah**~~. We need to choose from their image library, images that both semantically correspond to what they're talking about in the videos that we're generating, and also ideally the ones that look good. And that looking good part is ~~**like**~~ a real challenge. And part of the reason it's such a challenge is that people just have a lot of disagreement as to what looks good. The Ava dataset that came out six, seven years ago, ~~**maybe**~~ ~~**at**~~ ~~**this**~~ ~~**point**~~ has ~~**a**~~ ~~**I**~~ ~~**think**~~ about 100 data points per image, ~~**like**~~ they went on Mechanical Turk ~~**or**~~ ~~**whatever**~~ and just had people rate these images. But you would think, geez, why do you need 100 ratings per image in this data set? And the answer is that there's so much variation. that they ultimately report a mean and a standard deviation for

Because at the end of the day, ~~**it's**~~ ~~**just,**~~ ~~**man,**~~ there's just too much noise in how people think about these images. And we can't really reduce that for any practical purposes. The evaluation ~~**is**~~ like, across the board ~~**is**~~ becoming a very interesting topic where ~~**it's**~~ ~~**like**~~ we're hitting the end of, or certainly the limits, if not the end of what robotic scoring can do for us. But then when we look inward and say, what do we think about this? We're confronted by the fact that we often disagree. I would also be really interested to see research as to how individuals disagree with themselves over time. There probably is some of that in other settings, but definitely across people, we disagree quite a bit. And I would suspect even an individual will be subject to quite a bit of drift over time.

Okay. HOST: Here's one thing I don't really understand. And maybe you answer this in part with the description of ~~**more**~~ the fMRI side of this equation. Maybe not. ~~**I**~~ ~~**guess**~~ ~~**I'm**~~ ~~**not**~~ ~~**sure.**~~ It depends on the answer. where is the semantic information coming from? ~~**I'm**~~ ~~**understanding**~~ ~~**that**~~ we have these voxels in the visual cortex.

This corresponds to blood flow. My ~~**sort**~~ ~~**of**~~ very naive version of what's going on ~~**in**~~ ~~**their**~~ ~~**understanding**~~ ~~**of**~~ ~~**what's**~~ ~~**going**~~ ~~**on**~~ in the brain is that the visual cortex is like a convolutional network and it's working up from ~~**like**~~ low level edge detection type stuff into higher order concepts, but I'm not really sure how far that goes. ~~**And**~~ ~~**I'm**~~ ~~**not**~~ ~~**sure**~~ ~~**if**~~ in my visual cortex, ~~**I**~~ ~~**get**~~ ~~**to**~~ ~~**the**~~ ~~**point**~~ ~~**where**~~ ~~**back**~~ ~~**there,**~~ it's understanding that it's a tiger, or if these high level concepts need to be sent forward to even higher order processing that would be like, okay, that's a tiger. But you're mapping this stuff from the back of the brain to the latent space where you're saying that ~~**this**~~ both the, what I would expect is that the, ~~**like**~~

I can also get a caption out of that. I'm like, huh. Is that because that semantic data is in the data pulled from the FMRI? Like it's represented by the blood flow or is this something that the clip model is doing for us, because it has done the work of doing ~~**like**~~ structural to semantic connection. GUEST_02: The brain has both of these. So I'm including higher level processing areas of the brain in the visual cortex, right? The very back of the brain, primary visual cortex, you'll get V1, V2. These are just representing orientation, lines, maybe colors as you continue the visual hierarchy.

But you get to the inferior temporal cortex, and this area of the brain has specialized semantic processing. ~~**Like**~~ one example would be the fusiform face area. It's very specific to processing human faces. So there's a lot of rich semantic information in the brain that is being fed through the model. Now you could feed only the primate visual cortex into this model, in which case it's not going to have the semantic stuff and it's not going to do as well in mapping to clip space. HOST: Gotcha. That sounds like something that you could do by segmenting, basically you would have just fewer voxels to work with and you could say, ~~**and**~~ we have a good enough understanding of the regions of the brain to~~**,**~~ ~~**I**~~ ~~**apologize**~~ ~~**for**~~ ~~**being**~~ ~~**relatively**~~ ~~**ignorant**~~ ~~**of**~~ ~~**the**~~ ~~**biology**~~ ~~**and**~~ ~~**the**~~ ~~**neuroscience**~~ ~~**under**~~ ~~**this.**~~ Would you say there's a general consensus as to what regions you would not include if you were going to try

GUEST_02: Yeah, if you only cared about low level, then you can just isolate to the V1234 primate visual cortex. But part of the reason this is an exciting field of work is because you're tapping into someone's representation, ~~**like**~~ their cognitive representation of what they are looking at. If you only cared about getting an exact reconstruction of the image, then just ~~**like**~~ tap into your retina or put a camera in front of your eye. ~~**Where**~~ ~~**is**~~ ~~**the**~~ ~~**importance**~~ ~~**to**~~ ~~**these**~~ ~~**kind**~~ ~~**of**~~ ~~**works**~~ ~~**if**~~ ~~**you're**~~ ~~**just**~~ ~~**getting**~~ ~~**a**~~ ~~**one-to-one**~~ ~~**representation**~~ ~~**back**~~ ~~**of**~~ ~~**what**~~ ~~**you're**~~ ~~**looking**~~ ~~**at.**~~ What you actually care about is the nuance behind what I am representing versus what somebody else is representing. It's ~~**like**~~ looking into the brain and seeing specifically what is leading to your specific representation.

HOST: So this is another kind of key theme that ~~**I**~~ ~~**feel**~~ comes up all the time, where the space that we're ultimately mapping into was originally created by aligning text and image. And with that initial work, you could get into that space either by giving it text or giving it an image, ~~**right**~~? You could get into this ~~**sort**~~ ~~**of**~~ shared aligned text image space either way. But you couldn't necessarily get to the spaces that we're actually finding here with the brain data because This is essentially bringing both, ~~**right**~~? It's bringing both the what did I see in the raw low level ~~**kind**~~ ~~**of**~~ data and also how did I conceptualize that and finding a space that you could presumably not access exclusively with text or exclusively with an image and representing both kinds of information there at the same time. That's definitely really interesting.

GUEST_02: One thing to highlight on this is that what people, ~~**I**~~ ~~**think**~~, get away from this paper is we can get really high quality reconstructions of what you're looking at from the brain. What ~~**I've**~~ ~~**found**~~ ~~

This is mapping brain space to basically a third modality of clip. This is a retrieval module in the paper, trained with contrastive loss~~**,**~~ ~~**right?**~~ And that allows you to do stuff like find the nearest neighbor in clip space. It allows you to basically have a brain latent that you can compare to any other clip latent you want and see where you are along that axis, stuff like that. And this is where we show that it has such a fine-grained representation. One of the criticisms that ~~**I've**~~ ~~**been**~~ ~~**seeing**~~ is okay. It knows it's an elephant and whatever, but ~~**you're**~~ ~~**like,**~~ we show that you can~~**,**~~ ~~**you**~~ have a really fine grained nuance here. You can determine that it is this exact zebra image out of like 20 zebra emissions.

Like you can get perfect matches with pretty high accuracy if you are relying on ~~**like**~~ this retrieval sub module. And there's a lot of applications that can be done from that. knowing that you're having such fine-grained information. It's not just ~~**like**~~ the overall category of the image. It's not just a picture of an animal. HOST: Yeah, okay. HOST: ~~**Cool.**~~ ~~**I**~~ ~~**do**~~ ~~**find**~~ ~~**this**~~ ~~**stuff**~~ ~~**to**~~ ~~**be**~~ ~~**incredibly**~~ ~~**thought**~~ ~~**provoking**~~ ~~**and**~~ ~~**quite**~~ ~~**profound.**~~

Let's talk a little bit about the details of just the fireside of the figure two where the voxels are being merged into the same space and then ultimately mapped. In some ways that might be more intuitive for people. ~~**I**~~ ~~**guess**~~ The basic strategy is ~~**just**~~ we need a single latent space, but we have different dimensional data because people's brain sizes and anatomies are different ~~**as**~~ ~~**we**~~ ~~**talked**~~ ~~**about**~~. So there is a per patient adapter that is trained to map each individual's data into this shared space, and then from that shared space, everything is mapped into the diffusion prior space. And then you systematically hold one out, train a model on the other seven patients from the original dataset, get something that's shared across those seven and then see, okay, what if I just use a little bit of data from this eighth person? Do I now have enough of a shared, rich enough representation, robust enough representation in the shared space that I can quickly map this new person onto that successfully. ~~**Anything**~~ ~~**I'm**~~ ~~**missing**~~ ~~**there?**~~ ~~**What,**~~ ~~how

No, yeah, that's all right. Okay,cool. How big are those? How long do those take to train? ~~**Would**~~ ~~**it**~~ ~~**be**~~, ~~**would**~~ ~~**we**~~ ~~**expect**~~ ~~**that?**~~ Obviously this is ~~**like**~~ a ~~**kind**~~ ~~**of**~~ unique dataset where there are these unusually long time spent in the fMRI for an individual person. So if I instead said, Hey, ~~**um**~~, I don't want to do that again, but I went and got 240 or a thousand people to sign a release for the one hour that they spent. Would you expect that to work?

Like how, how, how do you think this would work with a little bit of a different dataset and how much compute, how many kind of parameters give ~~**them**~~, give us the specs on that kind of stuff? GUEST_02: Yeah. So you're referring to instead of having eight subjects scan for 40 hours, like a lot of, ~~**like**~~ hundreds of people scan for maybe an hour or something. HOST: Yeah. I'm just thinking about what data is just already sitting out there that wouldn't take such a heroic effort. Now it might take a legal heroic effort to unlock some of that stuff, but there's a ton of people that have spent one hour, not necessarily for image reconstruction. GUEST_02: ~~**I**~~ ~~**guess**~~ two points on that. One is, this is relating to the idea of precision neuroscience.

which is the kind of trade-off between shorter scanning sessions with a lot of people versus a lot of scanning with a single or just a few people. And there are advantages to both. Obviously, the NetShow Scenes dataset did the latter. And the advantage you get is higher quality data, the ability to really get subject-specific results. And then the former is more so ~~**like**~~ you can really figure out how one person's brain is different from all the other people's brains, but the overall quality of the data is worsened. So maybe you'll never really get the kind of high quality results that you would get with something like the natural scenes data set. The point about~~**,**~~ ~~**like,**~~ you have a lot of neuroimaging datasets currently out there that could potentially be tapped into to create~~**,**~~ ~~**like,**~~ an overarching model space, that is something we are actively working on. So we're currently training a foundation model on fMRI, taking basically all possible data from the entirety of the publicly available datasets that we can get our hands on.

And the potential there I think is huge because you're essentially creating a new avenue towards getting the signal that you care about from a brain scan. You're able to get a generalizable latent, convert the raw brain scans into a generalizable latent trained across hundreds of thousands of people's brains. That's been imbued by the heuristics of how brain patterns typically work. So I'm really excited about that. That is a definite potential that hopefully can work out. ~~**HOST:**~~ ~~**Yeah.**~~ ~~**HOST:**~~ It seems like foundation models are coming to every modality and all modalities all pretty quickly. ~~**So**~~ ~~**these,**~~ I asked cloud three to help me figure out how big the models were on the fMRI to diffusion prior side of this, and it estimated 940 million parameters.

I don't know if that was,~~**I**~~ ~~**didn't**~~ ~~**find**~~ ~~**that**~~ ~~**myself**~~ ~~**in**~~ ~~**the**~~ ~~**paper.**~~ ~~**I**~~ ~~**don't**~~ ~~**know**~~ ~~**if**~~ ~~**it**~~ ~~**found**~~ ~~**it**~~ ~~**or**~~ ~~**if**~~ ~~**it**~~ ~~**calculated.**~~ That was the parameter count from MindEye 1. Okay. So tell me ~~**how,**~~ give me a sense of the order of magnitude of parameters and compute for this work? And then ~~**I'm**~~ ~~**really**~~ ~~**interested**~~ ~~**to**~~ ~~**hear**~~ ~~**also**~~ for your upcoming work, how much data is out there? Like how many hours of fMRI data are you able to tap into? And what are the ~~**sort**~~ ~~**of,**~~ ~~**you**~~ ~~**know,**~~ compute budgets you're going to need to do that ~~**sort**~~ ~~**of**~~ work?

GUEST_02: Yeah, for my day two,  the models that we trained were probably larger than they needed to be because if you're going to do ~~**every**~~ everything for a deadline for a conference, just ~~**like**~~ maximize the compute that you currently have ~~**to**~~ ~~**just**~~ train it and get the results. So that was ~~**like**~~ 2 billion parameter on my day two models. But since then, I've looked into reducing the parameter count and you can ~~**like**~~ more than half ~~**of**~~ that parameter count and still get basically the same quality of results. But also 2 billion sounds huge, but actually it's not that big because it's basically coming from a single nn.linear part of the model. So ~~**it's**~~ ~~**basically**~~ over a billion of that is ~~**only**~~ coming ~~**through**~~ ~~**the**~~ ~~**residual**~~ ~~**MLP**~~ ~~**backbone.**~~ ~~**And**~~ ~~**MLPs**~~ ~~**are**~~ ~~**a**~~ ~~**lot**~~ ~~**more**~~ ~~**computationally**~~ ~~**efficient.**~~ ~~than~

It's literally just this one linear layer that leads to this ~~**kind**~~ ~~**of**~~ over a billion parameter count situation.

HOST: And that is primarily the layer that is mapping the shared space to the diffusion prior?

GUEST_02: ~~**It's**~~, ~~**yeah**~~, ~~**it's**~~ ~~**like**~~ in figure two, ~~**it**~~ ~~**goes**~~, ~~**it's**~~ ~~**like**~~ this shared subject latent space where every person's brain gets compressed to 4,096. ~~**And**~~ ~~**then**~~ ~~**goes**~~, going from 4,096 to the clip latent dimensionality, which is 256 times 1,664. And that's flattened. So it's half a million, ~~**I**~~ ~~**think**~~. So it's that NN dot linear 4,000 to half a million. that leads to the parameter count.

HOST: Gotcha. HOST: Gotcha. GUEST_01: Okay,cool. HOST: Yeah, that's really helpful. So what happens next? How many orders of magnitude more fMRI data would the foundation model be trained on, and what does the compute budget look like to make something like that happen? GUEST_02: Yeah, so right now we are working with~~**,**~~ first I'll say the neuroscience community is amazing in the sense that there is a bit of a standard towards releasing your data that you collect as a lab, open source, according to standardized format, specifically it's called BIDS, Brain Image Dataset Specification~~**,**~~ ~~**I**~~ ~~**think**~~. That gives, that's amazing that there's already this push towards standardization and easily accessible datasets.

That is available publicly. There's also the Human Connectome Project, which was a massive large-scale neuroimaging dataset. There's the UK Biobank, which is another one, although you have to pay to get access to that one. But basically, we're talking about millions of hours of people in machines and trying to use a foundation model to compress all ~~**of**~~ this data into some sort of generalizable purpose latent that can then be fed for downstream applications. One example would be better reconstructions, but it doesn't have to be that it could be anything~~**,**~~ ~~**right?**~~ HOST: Yeah. ~~**Okay.**~~ ~~**So**~~ ~~**that**~~ ~~**is,**~~ ~~**um,**~~ We're going from hundreds of hours to millions of hours, which is~~**,**~~ ~~**uh,**~~ obviously.

Quite a jump. GUEST_02: Uh, yeah,we haven't fully gotten all the data sets out, so I don't know an exact number. Maybe it's less than a million, but it's, ~~**you**~~ ~~**know,**~~ a lot more data. HOST: So where do you think this is all going? You may have some interesting views of the future. We're going to do another ~~**kind**~~ ~~**of**~~ somewhat companion episode to this, where we look at ~~**like**~~ all ~~**of**~~ the brain computer interface technologies that are out there for reading. Increasingly, there's some that are~~**,**~~ ~~**you**~~ ~~**know,**~~ attempting writing or at least steering the brain activity from the outside. ~~**And**~~ ~~**I**~~ ~~**guess**~~ there's ~~**like**~~ interesting progress across a lot of different hardware modalities and different ~~**kind**~~ ~~**of**~~ signals to try to pick up on ~~**and**~~ ~~**so**~~ ~~**on**~~ ~~**and**~~ ~~**so**~~ ~~**forth**~~.

But where is this going in the big picture? Do you have a view of what is going to become possible maybe in a medical setting is one question, but also in just a general daily consumer device, ~~**like**~~ daily life view. It seems like we're not too, ~~**you**~~ ~~**know,**~~ you guys are going to have your way with this foundation model. I fully expect that you're going to come up with something that's going to be quite interesting, but then what are we going to do with it?

GUEST_02: Yeah, ideally everything works out. The foundation model allows us to have such rich latents and then follow-up work could do multimodal stuff. So ~~**like**~~ you could take inferior non-invasive imaging tech and map it to a better quality latent space and ~~**Like,**~~ the dream would be that you can take smaller datasets that are higher quality, maybe even invasive datasets, and you can just merge all these different modalities together to be able to make use of that kind of higher quality data. fNRI right now is the best non-invasive tech, but the downside obviously is that it's very expensive, and it's also very difficult

Like, just from a patient's perspective,  you can't move at all. If you're someone who gets claustrophobic, then you can't get scanned, ~~**basically,**~~ especially for these ~~**kind**~~ ~~**of**~~ hour-long experiments. So that's not ideal, but it is the best that we have. In terms of invasive stuff, you can get electrodes planted in your brain ~~**already**~~. If you're a seizure patient, then people will put electrodes across your brain and try to localize where the seizure is coming from. You can get really high quality data if you have invasive electrodes on your brain, much better than fMRI even. But the downside is that it's not spread out across the entirety of the brain. Neuralink is specifically in the motor cortex, ~~**right?**~~

So you're not going to be able to do, you're not going to be able to make use of the entirety of the brain unless you've put these, unless you've completely taken out your scalp and put these electrodes over the entirety of your cortex. So you're going to be constrained in terms of localization, but you benefit from the higher quality data. In terms of non-invasive stuff like EEG, you're not really able to localize at all. You don't get a 3D brain like you do with fMRI. You get basically, you put some electrodes in certain positions and then they bounce along the skull and eventually you get some changes in electrical activation and you deduce things based on that. But you don't really know where the initial neurons that are leading to that change in electrical activation ~~**is**~~ ~~**coming**~~ ~~**from**~~. This is a localization problem. Now, ~~**I**~~ ~~**think**~~ there's a lot of new tech that's coming out that's really promising.

So there's ultrasound, there's near-infrared proxies to what you would get with fMRI that seem very promising. Currently, the tech is not there yet, but it's clear that it's in the right direction, that something's going to come out~~**.**~~ in the foreseeable future that allows us to get hopefully close to fMRI quality results with non-invasive technology, or we figure out a way to~~**,**~~ ~~**yeah,**~~ get ~~**like**~~ a multimodal thing working that can leverage higher quality data with worse inputs. HOST: So then what's my life going to be like? Do you have a vision for~~**,**~~ is there a Jetsons level future that you envision for the general public with brain reading technology on a daily basis? Or what is your core reason why? Is it medical? Is it daily life?

What do you think are the most exciting midterm applications, let's say? GUEST_02: Oh, yeah. I don't know about Jetsons. ~~**This**~~ ~~**is**~~ The first step that we're going to is get these complicated machine learning things that are currently only able to be done in an academic setting with a huge data set to work with very limited data. Hopefully do things in real time, be able to do things with very constrained data sets and still get high quality results. And furthermore, be able to generalize this to different kinds of outputs that are more relevant. ~~**than**~~ ~~**just**~~ ~~**perception,**~~ ~~**right?**~~ So, for instance, we are collaborating with the University of Minnesota to try to do mental imagery reconstruction, like visualizing an image and recreate that.

You can imagine that there's a lot of different pipelines that would be relevant here, like memory decoding, stuff like that. Furthermore, something that we didn't really talk about was ~~**like,**~~ The shared subject space stuff shows the potential for this kind of approach to work, where maybe you have some subjects that have a lot of data, but you can tap into this shared space with very minimal data. And that approach by itself is not specific to reconstructing images. So basically, there is a lot of potential here, ~~**I**~~ ~~**think,**~~ in being able to get the cutting-edge machine learning pipelines that are currently only reserved for ~~**kind**~~ ~~**of**~~ academic basic science research to more real settings that are actually feasible for clinicians to use, ~~**for**~~ ~~**instance**~~. Beyond that, when it comes to ECI tech, I would love to be involved in that space. Obviously, I don't know what ~~**kind**~~ ~~**of**~~ the potential is for consumer applications or things like that.

HOST: Yeah, ~~**I**~~ ~~**feel**~~ ~~**like**~~ in general, we're all groping. I ask these questions and in

And that's a much broader statement than brain data reading and interpretation. But just what is the sort of positive vision for where all this AI tech is taking us is one that ~~**I**~~ ~~**think**~~ everybody would do well to spend a little bit more time thinking about. HOST: ~~**But**~~ ~~**just**~~ ~~**what**~~ ~~**is**~~ ~~**the**~~ ~~**sort**~~ ~~**of**~~ ~~**positive**~~ ~~**vision**~~ ~~**for**~~ ~~**where**~~ ~~**all**~~ ~~**this**~~ ~~**AI**~~ ~~**tech**~~ ~~**is**~~ ~~**taking**~~ ~~**us**~~ ~~**is**~~ ~~**one**~~ ~~**that**~~ ~~**I**~~ ~~**think**~~ ~~**everybody**~~ ~~**would**~~ ~~**do**~~ ~~**well**~~ ~~**to**~~ ~~**spend**~~ ~~**a**~~ ~~**little**~~ ~~**bit**~~ ~~**more**~~ ~~**time**~~ ~~**thinking**~~ ~~**about.**~~ So the memory thing is really interesting. Are there any sort of surprising lessons from this work that people should

These sorts of questions have been ~~**like**~~ age-old philosophical questions. ~~**Like**~~ ~~**when**~~ ~~**you**~~ ~~**see**~~, ~~**you**~~ ~~**know,**~~ an apple and it's red, what does that look like to you versus me? ~~**Yeah,**~~ ~~**I'm**~~ ~~**curious**~~ ~~**as**~~ ~~**to**~~ ~~**if**~~ ~~**you**~~ ~~**feel**~~ ~~**like**~~ ~~**this**~~ ~~**work**~~ ~~**is**~~ ~~**informing**~~ ~~**the**~~ ~~**way**~~ ~~**you**~~ ~~**think**~~ ~~**about**~~ ~~**what**~~ ~~**once**~~ ~~**were**~~ ~~**limited**~~ ~~**to**~~ ~~**philosophy**~~ ~~**questions,**~~ ~~**but**~~ ~~**now**~~ ~~**are**~~ ~~**maybe**~~ ~~**starting**~~ ~~**to**~~ ~~**be**~~ ~~**penetrated**~~ ~~**a**~~ ~~**little**~~ ~~**bit**~~ ~~**by**~~ ~~**science.**~~

And then you can look at that individual difference and help extract that out. And then you have this sort of shared subject component and the individual component. ~~**Right**~~. We didn't have that here. There was a small subset of images that everyone saw, but that was the test sets. We didn't have access to that at all for the training of the models. ~~**And**~~ ~~**so**~~ ~~**I**~~ ~~**think**~~ ~~**it's**~~ ~~**pretty**~~ ~~**revealing**~~ ~~**that**~~ The approach that we used is literally, it's a lot simpler than it seems. It's just ridge regression.

It's linear regression, right? So you just take, no matter what the input was. You take my brain, you take your brain, we looked at different images, you put them in the same batch and then you get your own linear mapping. I get my own linear mapping and that just can get the dimensionality to be the same and account for differences in our voxels, how we represent things. Then everything else after the model is the same for both of us, ~~**right?**~~ And the simplicity of this and the fact that it worked so well is very interesting. This was the first thing that we tried. ~~**It's**~~ ~~**okay,**~~ ~~**how**~~ ~~**do**~~ ~~**we**~~ ~~**do**~~ ~~**shared**~~ ~~**subject**~~ ~~**modeling?**~~

The most obvious way, just do a linear mapping to a shared space and then everything else is the same, ~~**right**~~? ~~**And**~~ ~~**then**~~ the first try and it worked great. ~~**And**~~ ~~**it's**~~ ~~**like**~~, why is nobody else doing this? Why is everyone doing fancier things? ~~**of**~~ manifolds and complicated neural networks that are shared between people. The very easy solution of just linear mapping from my brain and your brain to the same space works so well. There has to be some shared representations that can support that ~~**by**~~ ~~**one**~~, and you're able to extract that kind of variability in a pretty simple manner. ~~**And**~~ ~~**so**~~ just the fact that it works so well tells us something about the similarities in how people's brains represent things.

Even though the inputs are different,you can get the same kind of underlying relevant signal out. And the individual specifics of things can be further elaborated by fine-tuning the complicated part of the model. ~~**GUEST_02:**~~ ~~**And**~~ ~~**the**~~ ~~**individual**~~ ~~**specifics**~~ ~~**of**~~ ~~**things**~~ ~~**can**~~ ~~**be**~~ ~~**further**~~ ~~**elaborated**~~ ~~**by**~~ ~~**fine-tuning**~~ ~~**the**~~ ~~**complicated**~~ ~~**part**~~ ~~**of**~~ ~~**the**~~ ~~**model.**~~ In terms of failure cases, There are some obvious failures when it comes to the reconstructions, and ~~**I**~~ ~~**think**~~ ~~**that**~~ usually happens when the images are more complicated. If you are just looking at a simple image that has maybe one object in it, then the reconstructions will typically be quite good. If it's a single giraffe you can recreate the draft, it'll look the right way in the image, it'll basically be in the right setting, and it'll be great. But if you have

And I think part of the reason for this is simply the datasets were collected in a way where the images come from Microsoft Cocoa. And the majority of the images that people saw are simple scenes. And so the farther away you get from that distribution of images, the less the model is going to be able to account for that. HOST: ~~**Yeah,**~~ ~~**that**~~ ~~**sort**~~ ~~**of**~~ ~~**suggests**~~ ~~**that**~~ ~~**somebody**~~ ~~**like**~~ ~~**an**~~ OpenAI is in a really good position to do a version of this because reading their DALI 3 technical report ~~**or**~~ ~~**whatever**~~ ~~**they**~~ ~~**officially**~~ ~~**called**~~ ~~**that**~~. There was ~~**like**~~ a huge ~~**sort**~~ ~~**of**~~ data enrichment process where they were able to get much~~**,**~~ much better results on the image generation, ~~**like**~~ much more controllable by first doing super detailed captions of the images that they used for training and getting past It was ~~**sort**~~ ~~**of**~~ a bootstrapping process where these ~~

HOST: One way that I'm thinking about that is how much of the compute in this project was incremental to the original foundation model effort ~~**versus**~~, ~~**and**~~ if that ratio is really small, then that would suggest that these sort of mega projects potentially. Become even more valuable than we ~~**may**~~ ~~**be**~~ naively thinking, ~~**right**~~? Because then if it's just a little extra compute ~~**to**~~ ~~**do**~~ ~~**it**~~, to unlock all these other use cases on top of these foundation models, then the best foundation model wins maybe. ~~**So**~~ ~~**do**~~ ~~**you**~~ ~~**have**~~ ~~**a**~~ ~~**number?**~~ ~~**I**~~ ~~**again,**~~ ~~**asked**~~ ~~**cloud**~~ ~~**three**~~ ~~**to**~~ ~~**analyze**~~ ~~**this**~~ ~~**and**~~ ~~**it**~~ ~~**guessed**~~ ~~**that**~~ ~~**the.**~~ Incremental compute was somewhere between. 0.1% and even less than that, like at the high end of its range, it guessed that it was essentially

GUEST_02: For instance, we train the full thing of the pre-training and then the fine-tuning. It's less than a day on each 100 node for the pre-training, and then also less than one day for each fine-tuning. And then for the Stable Diffusion XL unclip, we trained it for less than one epic, fine-tuned it from the base SDXL. So ~~**yeah,**~~ it's along that kind of scale in terms of GPU hours. ~~**It**~~ ~~**really**~~ ~~**is.**~~ ~~**GUEST_02:**~~ ~~**It**~~ ~~**really**~~ ~~**is.**~~ The foundation model is quite important. It makes a big difference in terms of the starting point for which you can add all these other things.

And also the applications that you get out of it are potentially bigger than you would initially think. GUEST_02: ~~**And**~~ ~~**also**~~ The applications that you get out of it are potentially bigger than you would initially think. ~~**For**~~ ~~**instance,**~~ Back when we were trying to get the unclip to work, Tanish reached out to Robin, the head of Stable Diffusion team. And Robin was like, yeah, if you want to get exactly the original image back from the clip image embedding, just use the OpenClip unpooled embeddings, and ~~**yeah,**~~ you'll get it perfectly. Here's the kind of results that you can get. And we were like, what? Why can't you release this? This is amazing.

There's a lot of applications for that, but I don't think that Robin understood the potential of having that. So he was basically like, I don't see the potential in it. So then we ~~**ha**~~ ~~**we**~~ went and basically followed up and actually fully fine tuned and got the thing working and everything. But ~~**yeah,**~~ there's probably more of a scope than people initially think that is possible from foundation models. I don't think that when people train clip that they would think that people would add a brain modality to clip. ~~**HOST:**~~ ~~**Yeah.**~~ ~~**That**~~ ~~**does**~~ ~~**sort**~~ ~~**of,**~~ ~~**this**~~ ~~**is**~~ ~~**again**~~ ~~**another**~~ ~~**kind**~~ ~~**of**~~ ~~**theme**~~ ~~**across**~~ ~~**a**~~ ~~**bunch**~~ ~~**of**~~ ~~**different**~~ ~~**topic**~~ ~~**areas**~~ ~~**that**~~ ~~**we've**~~ ~~**explored,**~~ ~~**but**~~ ~~**the**~~ ~~**idea**~~ ~~**that**~~ ~~a~

And then here with the foundation models being open source, ~~**like**~~ you're able to hack in a million, ~~**I**~~ ~~**don't**~~ ~~**know,**~~ ~~**I**~~ ~~**mean,**~~ hacking is the wrong word, but come up with all these ~~**like**~~ interesting, clever ways to map spaces to spaces and connect spaces to spaces that were never really intended and really don't ~~**actually**~~ end up requiring all that much compute. I wonder if you have other ~~**like**~~ areas in mind where you think that might work. I was trying to map this process onto something for ~~**like**~~ protein discovery or novel protein generation. HOST: ~~**And**~~ it seemed ~~**like**~~ a lot of the parts ~~**kind**~~ ~~**of**~~ line up with obviously with different models and different data sets. But ~~**the,**~~ this approach seems like something that I would expect to see a lot more of. Do you have other things in mind that you think~~**?**~~ People should start to bring this ~~**like**~~ predict the diffusion prior approach to. GUEST_02: Yeah, I don't know about specifically predict the

I think there might be better ways to do these kind of things. And the ability that we had to tap into the clip image space and image generation models was possible because our goal was to output an image. When you're outputting proteins or other kinds of things, it's probably ~~**a**~~ ~~**bit**~~ more difficult to get the alignment of using foundation models that were trained for other purposes, although it's probably still possible. GUEST_02: ~~**And**~~ ~~**I**~~ ~~**think**~~ at that point, it's like the potential of foundation models is so apparent for other use cases that training medical foundation models is going to be very important. ~~**So**~~ that's why we're trying to do the fMRI foundation model, ~~**for**~~ ~~**instance,**~~ because it would unlock so many new applications, even outside of fMRI, ~~**like**~~ with multimodal possibilities. HOST: So how big of a project is this next one? ~~**I**~~ ~~**feel**~~ ~~**like**~~ we covered the amount of data is orders of magnitude more. Presumably the compute would be orders of magnitude more than one day on one H100 ~~still~

HOST: Is this foundation model project something that is moderate scale compute that you guys are getting I'm interested to hear how, ~~**cause**~~ Tanishq is the founder of MedArk and you guys are both on the paper affiliated with MedArk and with stability. So I'm interested to hear ~~**like**~~ a little bit about how that relationship works. ~~**I**~~ ~~**think**~~ people are very curious for many reasons right now about how stability works. But I'm curious, first of all, the nature of that relationship, but also is this next thing going to be the scale of compute that can ~~**like**~~ easily come out of a stability budget, or is it going to be big to the point where it becomes ~~**like**~~ either something that creates scarcity ~~**and**~~ ~~**stability**~~ or requires other resources to be brought to bear? Just how big is it?

GUEST_02: Yeah, yeah. First off, yeah, Taneesh, head of MedArc, he's been instrumental in helping to organize various projects, even outside of these neuroimaging projects where I'm the lead of the team for NeuroAI. And Ahmad ~~

And he was very generous to basically allow us to use the compute that Stability had for external teams and for open source projects in medical domain that I believe these projects do have profitability, but it's more of a longer term. GUEST_02: ~~**And**~~ ~~**he**~~ ~~**was**~~ ~~**very**~~ ~~**generous**~~ ~~**to**~~ ~~**basically**~~ ~~**allow**~~ ~~**us**~~ ~~**to**~~ ~~**use**~~ ~~**the**~~ ~~**compute**~~ ~~**that**~~ ~~**Stability**~~ ~~**had**~~ ~~**for**~~ ~~**external**~~ ~~**teams**~~ ~~**and**~~ ~~**for**~~ ~~**open**~~ ~~**source**~~ ~~**projects**~~ ~~**in**~~ ~~**medical**~~ ~~**domain**~~ ~~**that**~~ ~~**I**~~ ~~**believe**~~ ~~**these**~~ ~~**projects**~~ ~~**do**~~ ~~**have**~~ profitability, but it's more of a longer term~~**.**~~ profitability to be aware of. So it is a very generous use case of stability employing me and

And so that's been great. GUEST_02: ~~**And**~~ ~~**so**~~ ~~**that's**~~ ~~**been**~~ ~~**great.**~~ ~~**And**~~ ~~**also,**~~ ~~**interestingly,**~~ you would think that would be prohibitively expensive of onboarding multiple people that are not employees into a cluster and using all of this. But honestly, so far throughout everything, we've been able to get away with only using a few nodes across the whole team, which involves more projects than even the ones that we've talked about right now. ~~**Basically,**~~ ~~**we're**~~ ~~**able**~~ ~~**to**~~ ~~**make**~~ ~~**use**~~ ~~**of**~~ ~~**large**~~ ~~**teams**~~ ~~**of**~~ ~~**people**~~ ~~**across**~~ ~~**multiple**~~ ~~**projects**~~ ~~**without**~~ ~~**that**~~ ~~**much**~~ ~~**compute,**~~ ~~**at**~~ ~~**least**~~ ~~**in**~~ ~~**relation**~~ ~~**to**~~ ~~**the**~~ ~~**amount**~~ ~~**of**~~ ~~**compute**~~

What kind of scale of compute we'd actually need and whether or not we would have access to that kind of compute as an external team ~~**like**~~ this is the sort of reality of things. Hopefully we're able to show more promising results and petition for more resources ~~**and**~~ ~~**that**~~ ~~**kind**~~ ~~**of**~~ ~~**thing**~~. But ~~**yeah,**~~ so far things have been good with little compute, honestly speaking, which is interesting. And hopefully we can petition to continue to grow the project out. HOST: ~~**Yeah,**~~ the embodied compute, ~~**I**~~ ~~**sometimes**~~ ~~**think**~~ ~~**of**~~ ~~**it**~~ ~~**as**~~ these models make portable to other projects ~~**is**~~ ~~**again,**~~ just another fascinating thing to ponder in the context of your work. ~~**I**~~ ~~**think**~~ this is a super interesting line of research and it's going to be very interesting to see just how far the foundation models can go. ~~**HOST:**~~ ~~**I**~~ ~~**think**~~ ~~**this**~~ ~~**is**~~ ~~**a**~~ 

But I'm pretty sure they're coming quite quickly when it comes to all ~~**of**~~ these other things that are non-human modalities. We have just no evolutionary basis for interpreting the voxel level activity of the brain. We have no evolutionary basis for reading DNA sequences that are super long and making connections across great distances in the genome. beginning to predict how those things are going to interact as they work up the layers of activity in a cell. And so the foundation models in these areas are headed for superhuman status, ~~**like**~~ basically on the first go. And then the question is going to be ~~**just**~~ ~~**like**~~, just how powerful are they? But it seems like they're going to be quite powerful. We're already decoding the brain on small data to think what might be possible with several orders of magnitude scale up on top of that is~~**,**~~ ~~**is**~~ really quite remarkable.

HOST: Do you have any thoughts on~~**,**~~ this is obviously a very complicated question and there's ~~**like**~~ ~~**a**~~ both pros and cons to it. ~~**And**~~ ~~**I**~~ ~~**don't**~~ ~~**have**~~ ~~**a**~~ ~~**highly**~~ ~~**ideological**~~ ~~**perspective**~~ ~~**on**~~ ~~**it**~~ ~~**myself,**~~ ~~**but**~~ How do you think about open sourcing things that are so different than anything that's been out there in the past and that clearly have this ~~**like**~~ ability for people to come along and add ~~**on**~~ a little bit to it and unlock ~~**like**~~ something that you didn't even expect~~**.**~~ ~~**Obviously**~~ There's a lot of people who are super pro open sourcing and just say it's the only way. ~~**And**~~ ~~**then**~~ there's a lot of people that are like, I'm afraid of what somebody might do if you train something at 10 to the 20, whatever flops and then put it out there and people can do something with one one thousandth of that incremental

To allow this kind of progress to be made to make an actual difference, allow for novel clinical diagnosis approaches, allow for novel basic science advances that would be possible from the massive training of models. Usually people are not going to have access to the kind of compute that might be necessary to train a foundation model, but after it's trained, If you just use it for inference, ~~**yeah,**~~ you can get that to work probably. And you can just map into that latent space and make use of that however you want. You can fine tune it or do all sorts of things with it. So I'm very pro in terms of making sure that it's broadly accessible. ~~**and**~~ ~~**the**~~ ~~**open**~~ ~~**science**~~ ~~**component**~~ ~~**of**~~ ~~**getting**~~ ~~**people**~~ ~~**to**~~ ~~**collaborate**~~ ~~**with**~~ ~~**you**~~ ~~**to**~~ ~~**expand**~~ ~~**the**~~ ~~**frontier**~~ ~~**of**~~ ~~**what's**~~ ~~**possible**~~ ~~**with**~~ ~~**these**~~ ~~**data**~~ ~~**sets.**~~ There's just so many amaz

Yeah. And like open sourcing everything is great if there are ~~**like**~~ practical issues ~~**of**~~ all the data that we're using is already public. So it's not ~~**like**~~ there's a privacy thing. People consent to share their data. So I don't think that's an issue. ~~**Like**~~ sharing how things were trained~~**,**~~ ~~**I**~~ ~~**think**~~ is really great. If there are ~~**like**~~ profitable avenues for applying downstream applications from foundation models, ~~**I**~~ ~~**think**~~ that is very ripe. And there would be a lot of interest after it's shown to be effective as basically the new way to process first fMRI, but then hopefully additional modalities after that.

HOST: Yeah, it seems clear that there is a significant, from what we've seen so far, all this activity has broadly been ~~**like**~~ very positive. And I'd be pretty confident saying there's ~~**like**~~ a 10 to 1, maybe even 100 to 1 ratio of ~~**just**~~ ~~**like**~~, tapping into this talent that otherwise wouldn't have access and creating all these new things and the creativity and the fact that people will build things that you didn't think to build on top of the foundation, as you said, is the point. And then I'm also ~~**like**~~, man, some of this stuff is so wild in terms of what it might enable that I do wonder if we might one day wish that we had a more structured approach ~~**in**~~, ~~**in**~~ the AI safety, you may be aware of this kind of terminology, but in some corners of the AI safety world, there is the notion of structured access, which is ~~**like**~~ trying to find this middle space between open sourcing everything in a way where you can't take it back if you ever wanted to, but still trying to create the kind of access that people need to be able to do the

But for fMRI, there are so many limitations that ~~**I'm**~~ ~~**pretty**~~ ~~**confident**~~ there's not going to be a huge problem with sharing these kind of foundation models. As things get more invasive and the quality of data gets better and better, then there are going to be privacy concerns, and then you need to be very careful~~**.**~~ about what is being released and who has access to it and what companies are maybe going to benefit from it. You need to be very careful and make sure that people's brain data is not being taken advantage of. GUEST_02: You need to be very careful and make sure that people's brain data is not being taken advantage of. HOST: It doesn't seem like there's a major barrier ~~**to**~~... There's a lot of little barriers. Not to say it's not going to be a lot of work, but it seems like ~~**lie**~~ ~~**detection**~~ ~~**type**~~ ~~**of**~~ technology would pretty predictably get to the point over the next couple ~~**to**~~ ~~**few**~~ years where you could imagine ~~**like**~~ the state ~~**like**~~ just putting a headset on people and being like,

And I would be a little concerned about that future. ~~**It's**~~ on the one hand, you can imagine a very positive version of that too, where people might voluntarily wear these things and demonstrate to others that they are being honest because look, my brain readout shows that I'm being honest and maybe that creates a high trust future that is really beautiful in a lot of ways. But ~~**it**~~ ~~**also**~~ all these, it's just always so striking how dual purpose~~**,**~~ a lot of these things are, ~~**and**~~ especially when they are still noisy. And ~~**it's,**~~ we've seen these issues even with just face recognition sometimes where police ~~**like**~~ show up at somebody's door based on nothing but a face match. ~~**It**~~ ~~**turns**~~ ~~**out**~~ it was a false match. And now they're kicking somebody's door down who literally had nothing to do with whatever was the issue. So ~~**it's**~~ ~~**very,**~~ ~~**I**~~ ~~**find**~~ fraught. ~~**It's**~~ ~~**I**~~ ~~**personally,**~~ ~~**I'm**~~ ~~

And at the same time, yikes,~~**what**~~, what might happen here as this stuff gets really good. ~~**So**~~ ~~**at**~~ ~~**a**~~ ~~**minimum,**~~ ~~**it's**~~ ~~**good**~~ ~~**to**~~ ~~**hear**~~ ~~**that**~~ ~~**you**~~ ~~**are**~~ ~~**thinking**~~ ~~**very**~~ ~~**actively.**~~ HOST: ~~**So**~~ ~~**at**~~ ~~**a**~~ ~~**minimum,**~~ it's good to hear that you are thinking very actively. GUEST_02: Yeah, to be clear, there's no way that, ~~**for**~~ ~~**instance,**~~ lie detection or applications by law enforcement would be relevant to the fMRI domain. The quality of the data is not good enough to do these kind of things. You cannot access things that the person is not directly thinking about. If you move around more than a millimeter, the data gets distorted. Like, you're not going to be able to get the kind of signal to noise that would be at all relevant for ~~**like**~~ practical applications that you're talking about.

But it can get good enough for very important medical purposes of comparing clinical groups, identifying biomarkers, trying to assess ~~**like**~~ mental imagery stuff. You have to be concentrating on the task and be very still in a scanner to get any of these fMRI approaches to work. And I don't want to err on the side of being too careful in this current stage of knowing what the quality of the data looks like, because I know that you want to make advancements first that are going to practically make a difference in terms of helping people with their disorders, understanding basic science of how the brain works. It would be much later on that there's the potential, ~~**I**~~ ~~**think**~~, for the kind of technology you're talking about. There would have to be some big innovation that happens where the quality of the data gets so good, you're able to access it, even with the person not really wanting it to be accessed. This is much farther down the line. At which point, if that technology does become accessible, that would open new questions. ~~**Yeah,**~~ ~~**but**~~ at this stage, ~~**I**~~ ~~**don't**~~ ~~**think**~~ it's ~~as~

And I'm not even sure my concrete stance is on everything. I'd have to think more about it. HOST: ~~**Yeah,**~~ ~~**I**~~ ~~**think**~~ ~~**that's**~~ ~~**an**~~ ~~**important**~~ ~~**commentary.**~~ It seems ~~**like**~~ we're maybe at the GPT-1 or GPT-2 phase of this line of research where it's ~~**like**~~ just entering into the zone where it can start to become useful, but it still takes a lot of effort to actually get real value from it. And there are probably ~~**like.**~~ at least a couple generations to go before it gets to the point where it's just really easy to use and ~~**like**~~ broadly applicable. I often say about GPT-4 that we're in a sweet spot right now where it's on the one hand super useful and on the other hand, ~~**like**~~ not so powerful as to be ~~**like**~~ serious risk of becoming a destabilizing force in the world. And ~~**I**~~ ~~**think**~~ maybe we have another generation to go before that happens.

At some point, like it does seem clear, ~~**like**~~ we're headed for something that's ~~**like**~~ quite powerful and potentially destabilizingly. But it is good to calibrate on all these different lines of research. Are we just entering that sweet spot? Are we getting to the late stage of that sweet spot? ~~**So**~~ ~~**to**~~ ~~**say**~~ ~~**that**~~ we have a couple of years at least to really allow the research to mature and continue to figure out what the plan is as it gets into that sweet spot of easy, broad use across a lot of use cases is a really good frame for us to all keep in mind. HOST: This has been an awesome conversation. I appreciate the technical depth and the philosophical engagement as well. Is there anything that we haven't covered that you want to make sure we touch on ~~**a**~~ ~~**little**~~ ~~**bit**~~?

GUEST_02: I guess I want to give a shout out to the other people responsible for the work. Clearly, Tanisha was the senior author on the paper, both for Mai Dai 1 and Mai Dai 2. Mihir Tripathi, Cesar Tirico, Rhys Neeland, Ken Norman, all very instrumental. There's even more co-authors, but there's a dozen people involved. ~~**and**~~ ~~**you**~~ ~~**know**~~ ~~**who**~~ ~~**you**~~ ~~**are.**~~ But ~~**yeah,**~~ it's a very widespread effort. We're still working, ~~**for**~~ ~~**instance,**~~ with Princeton to collect new data. We're trying to have more collaborations with academic institutions and the broader open science community.

And in that sense, we are open for people to help out. If you think you can help us with these projects, we have several projects going on right now on the MedArc Discord. So if you just go to MedArc.ai, you can find the Discord link. ~~**and**~~ join our server. We have weekly meetings for different projects. We just meet on Zoom once a week and work with public GitHub's for most of these projects. So it's an opportunity to work on some cutting edge research together and ~~**yeah,**~~ I encourage anyone who's interested to look into it. ~~**HOST: Cool.**~~

So that's medarc.ai is the website.~~**Definitely**~~ ~~**at**~~ ~~**a**~~ ~~**minimum,**~~ ~~**everybody's**~~ ~~**going**~~ ~~**to**~~ ~~**want**~~ ~~**to**~~ check out some of these figures and see just how high fidelity the reconstructions are of what the mind's eye is seeing. ~~**And**~~ ~~**I'll**~~ ~~**definitely**~~ join the discord and lurk a little bit ~~**at**~~ ~~**least**~~. ~~**I'm**~~ ~~**sure**~~ you'll get a couple other new entrants to the conversation ~~**as**~~ ~~**well**~~. ~~**For**~~ ~~**now,**~~ ~~**again.**~~ Fantastic conversation. Thank you for sharing all this research and perspective with us. Paul Scotty, thank you for being part of the Cognitive Revolution.

Thank you so much. Great job.  I think this will be a fantastic episode.

GUEST_02: Yeah, great questions. I appreciate the deep discussions.

HOST: ~~**I'm**~~ ~~**just**~~ ~~**endlessly**~~ ~~**fascinated**~~ ~~**by**~~ ~~**the**~~ ~~**subjects.**~~ ~~**It's**~~ ~~**my**~~ ~~**pleasure**~~ ~~**and**~~ I really appreciate you taking all the time. I will Get this into production pipeline, obviously.

We'll get our initial edit done.  If you would like to take a review, we can send it to you and you can watch our edit and tell us if you have any concerns. I didn't think we got to anything that seemed too sensitive. I don't think there was anything too juicy that I need to edit ~~**things**~~. ~~**Yeah,**~~ ~~**but**~~ ~~**we're**~~ ~~**happy,**~~ ~~**still**~~ ~~**happy**~~ ~~**to**~~ ~~**give**~~ ~~**you**~~ ~~**a**~~ ~~**chance**~~ ~~**if**~~ ~~**you**~~ ~~**want**~~ ~~**to**~~ ~~**take**~~ ~~**it.**~~ But if you feel confident that you don't want to spend time that way, I endorse you getting back to your core line of work rather than reviewing podcasts. So it's totally up to you. GUEST_02: Yeah, I appreciate it.

HOST: I don't think that it's necessary for me to take a look over it. ~~**So.**~~ ~~**Okay,**~~ ~~**cool.**~~ ~~**Then**~~ we're probably two-ish weeks out from publishing. We've got a few things ~~**in**~~ a little farther along in the production process, but it won't be too long. I always try to get these things out before~~**,**~~ ~~**before**~~ you guys publish your next result. And sometimes that can be quite a race. So thank you again.

