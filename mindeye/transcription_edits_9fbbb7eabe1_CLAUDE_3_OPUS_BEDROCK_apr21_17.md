00:00:00 
 HOST: Dean W. Ball, Research Fellow at the Mercatus Center and author of the Hyperdimensional Substack. Welcome to the Cognitive Revolution. GUEST_01: Thank you, Nathan. Great to be here. HOST: I'm really excited for this conversation. You and I are kindred spirits in that we both have had our day jobs and then we've had our obsessions with different areas of technology. We connected online after a couple of pieces of writing that you put out that ~~**I**~~ ~~**thought**~~ were excellent and ~~**learned**~~ as ~~**I**~~ ~~**learned**~~ more about you, ~~**I**~~ was fascinated to see just how much time ~~**and**~~ energy you have put into learning all about the brain computer interface space. And so today we are going to do a survey on all things neurotech.

00:00:41 
 And I'm really excited to learn from you in this domain that ~~**I**~~ ~~**really**~~ ~~**don't**~~ ~~**know**~~ ~~**a**~~ ~~**ton**~~ ~~**about**~~ and to share that with our audience. Awesome. ~~**Yeah**~~, ~~**me**~~ ~~**too**~~. So you want to start by just maybe introducing yourself. You've got a varied set of experiences and background interests, and then we can dive into your ~~**kind**~~ ~~**of**~~ high level thesis and dig into the technologies that will take us there one by one. GUEST_01: ~~**Yeah**~~, as the title of my sub stack suggests, I have a varied background with a lot of different threads going into it. I spent most of my career in the think tank world. So about a decade working in state and local economic policy for the most part, housing, infrastructure, things like that.

00:01:19 
 But I've always maintained a very strong interest in technology. I started open source forum software, if there's anyone who remembers PHP BB. from way back in the day. I was doing commits to that and writing technical documentation for that at a young age. And the coding bug hit me at that point. And ~~**I**~~ never went into it professionally, but it's always stuck with me. And just a general interest in technology. And that morphed as the deep learning revolution unfolded in the last decade into following the ML literature pretty closely.

00:01:53 
 And Once ChaptionDT came on the scene, I realized that this was a perfect moment for me because I have a background in policy and there's a lot of interest in the societal and policy implications of this technology. And at the same time, I've got a pretty good ground truth technical sense of what's actually going on, which is often lacking in the policy space. So I try to bring that to everything that I do. HOST: I love that. ~~**I**~~ ~~**think**~~ ~~**I**~~ often say that we really need to figure out what is before what ought to be done about it. And there's all too much jumping to what ought to be done about it. That is not based in a good ground truth understanding of what technology actually ~~**is**~~ exists today, what can be done with it, where that's sitting in the near term and so on. So I appreciate that perspective and I'm excited to get into the details of what you think is going to happen in the big picture of the possibility of a merge between man and machine.

00:02:53 
 GUEST_01: Yeah, so I guess I'll explain a little more about how I came to this particular issue. I am a longtime fan of J.C.R. Licklider, the DARPA senior researcher who wrote an article back in 1960 called Man-Machine Symbiosis. It's not quite the physical merge that we're going to be talking about. It anticipates the development of something like AGI, as a lot of people did in the 60s, being way easier. The thesis of that paper is AGI is going to be here in the next five to 10 years. And who knows what happens after that? ~~**Probably**~~ it'll become super intelligent and replace us.

00:03:29 
 But in the meantime, we'll have a lot of fun. That's basically the thesis of this senior government official at that time. It shows you how much things really changed in terms of ~~**just**~~ approaches to technology. HOST: Although I feel like I've heard similar quotes from some notable technology leaders more recently as well. GUEST_01: from the technologists ~~**for**~~ ~~**sure**~~, but coming directly from the DoD is ~~**just**~~ interesting. And it shows you ~~**just**~~ how ~~**like**~~ greenfield everything felt with computing at that time. And what's exciting about right now is ~~**I**~~ ~~**do**~~ ~~**feel**~~ ~~**like**~~ we're at a similar moment where a lot of things are possible. But I've always thought about this issue of symbiosis and ~~**kind**~~ ~~**of**~~ thought about it in a fairly broad way.

00:04:08 
 But in the last few years ~~**is**~~ ~~**the**~~ ~~**the**~~ concept of artificial superintelligence has become maybe a little closer than any of us thought ~~**possibly**~~ ~~**eminent**~~ if you believe some people on twitter ~~**the**~~ concept of ~~**the**~~ merge just keeps coming up ~~**prominent**~~ technologist ~~**as**~~ ~~**you**~~ ~~**say**~~ sam altman has a famous blog post called The Merge, where he essentially describes it as an inevitability. Elon Musk ~~**is**~~ ~~**not**~~ ~~**just**~~ called it an inevitability, but ~~**is**~~ in fact working on technical solutions. So he ~~**is**~~ probably taking this more seriously than anyone else. And just in general, in my conversations with people building in the AI space at advanced levels, there ~~**is**~~ a ~~**a**~~ very common sense that this ~~**is**~~ something that will happen. And if artificial superintelligence ~~**is**~~ even remotely ~~**eminent**~~, then ~~**the**~~ merge must be not too far off either if that's going

00:05:23 
 And it's like, where is the literature? ~~**also**~~ thought about it in a broader sense, because in a lot of ways, computers already are neural interfaces. You don't control them directly with your brain, but they obviously interface with your brain, and you are using your brain via touch control to manipulate them. And ~~**I**~~ ~~**think**~~ ~~**that**~~ technology, throughout the history of technology, one of the things we see is that information technologies don't just change the way information propagates, they change the way we think. And so that was my starting point of almost seeing the merge as something that started a long time ago, a path that we are still on, and that will get more vivid and more visceral over time, but not a fundamentally ~~**Yeah**~~, ~~**that's**~~ ~~**interesting**~~. HOST: Certainly, ~~**I**~~ ~~**feel**~~ ~~**like**~~ ~~**I've**~~ ~~**been**~~ ~~**through**~~ a couple of waves already of technology changing how ~~**I**~~ think. People, it feels so quaint now, but there was the idea of, we don't have to remember anything anymore because we can just go

00:06:30 
 Just, we don't have to make spatial maps of our general environments because we, so many of us just default to using the GPS. ~~**I**~~ ~~**think**~~ for me right now, ~~**I**~~ ~~**wonder**~~ ~~**if**~~ ~~**you**~~ ~~**have**~~ ~~**experienced**~~ ~~**something**~~ ~~**similar**~~ with the current wave of technology. ~~**I**~~ ~~**do**~~ ~~**find**~~ ~~**myself**~~ ~~**getting**~~ ~~**into**~~ ~~**some**~~ ~~**pretty**~~ ~~**deep**~~ ~~**habits**~~, at least with how ~~**I**~~ relate to language models. ~~**I**~~ ~~**think**~~ coding tasks are a pretty good example of this when ~~**I**~~ reflect on how ~~**I**~~ used to code versus how ~~**I**~~ approach coding tasks now. Probably making a mistake in the past because ~~**I've**~~ never been an outstanding coder, but ~~**I**~~ ~~**feel**~~ ~~**like**~~ ~~**I**~~ often used to go into the editor first and maybe start writing some comments and trying to break things

00:07:35 
 And in general, I think that's starting to happen to me. We recently did an episode with Shortwave, the email client ~~**as**~~ ~~**well**~~, that has a really good email AI assistant. I see an evolution there too, where I used to remember keywords to navigate my vastly overflowing and unmaintained inbox. And now I don't have to be so keyword oriented anymore. I can ask ~~**like**~~ questions and that's definitely very helpful for me. But I also do, I'm starting to feel already a little bit ~~**like**~~ my keyword search ability is beginning to atrophy. So certainly I recognize and experientially feel the idea that the waves of technology have changed how I ~~**thought**~~. I don't know if you have any similar stories and this probably pales compared to what might be ahead of us still.

00:08:20 
 GUEST_01: Yeah, no, absolutely. And I think it is important to think about that kind of thing. For me, in a similar vein, ~~**I**~~ ~~**find**~~ LLMs just fundamentally changed the way I ask questions and made me much better at asking questions. Oftentimes, I remember, it's about a year ago since my first experiences with GPT-4, and 3.5 was not quite as vivid in this way. But when GPT-4 came out, I would ask it a question and I would find, this isn't quite what I wanted. This isn't quite what I was looking for, but I know this model must be capable of giving me what I ~~**look**~~ for. So the problem must be with me. And so ~~**I**~~ refining my question and getting down to what is it that you're really asking?

00:09:05 
 And so often in fields of intellectual inquiry, that is actually the more important thing, that finding the right question is 90% of the intellectual journey. And so something that accelerates that, that makes us better at that, accelerates all kinds of intellectual endeavor. And ~~**I**~~ ~~**am**~~ excited to see things like CLAWD3 Opus, which it seems that the qualitative difference in something like in that model versus GPT-4 is really at the frontier of all kinds of different fields, that people that are doing middle of the bell curve activities in four, and not even coding necessarily, but just like various kinds of questions on science, history, whatever it may be, that Those questions, the middle of the bell curve stuff, the difference between GPT-4 and Cloud3Op is not necessarily all that much, but that when you're asking things that are more at the frontier, Cloud3 really shines vis-a-vis GPT-4. ~~**I**~~ ~~**can**~~ ~~**definitely**~~ ~~**tell**~~ ~~**you**~~, having used LLMs a lot to help ~~**me**~~ accelerate this process of researching these articles, on neuroscience

00:10:38 
 And then on another level, obviously anyone paying attention to this space is asking themselves philosophical questions, ~~**at**~~ ~~**least**~~ ~~**I**~~ ~~**have**~~ ~~**to**~~ ~~**think**~~ ~~**so**~~, about what is the nature of cognition? What is the nature of intelligence? What distinguishes my intelligence from the kind of intelligence that I'm seeing here exhibited on this screen? And ~~**I**~~ ~~**think**~~ ~~**that**~~ ~~**is**~~ ~~**a**~~ ~~**kind**~~ ~~**of**~~, it's actually a journey that we should all go through anyway, regardless of whether LLMs exist. Because asking what special value do you bring to the table and what unique perspective do you have is a question that everybody should be seriously asking themselves all the time and not enough people do. And ~~**I**~~ ~~**think**~~ LLMs are a forcing function for a lot of people, for labor market concerns more than anything, ~~**just**~~ ~~**as**~~ ~~**much**~~ ~~**as**~~ anything else, to ~~**just**~~ really probe at that. So in a way, ~~**I**~~ ~~think

00:11:47 
 And obviously the other thing is that once So much about the history of technology is not how it impacts one person or one building or one place. It is instead about the nonlinear compounding effects when everybody has access to that same capability. And ~~**I**~~ ~~**think**~~ we're in the very early days of that. And that's part of why ~~**I**~~ ~~**just**~~ find this to be such an enormously exciting time to be alive. HOST: Yeah, it's crazy. You hit on something there that ~~**I**~~ often think about as well, which is just that the future dynamics of the world at large as AI ~~**kind**~~ ~~**of**~~ starts to infuse into everything are dramatically under-theorized. When ~~**I**~~ go around talking to everybody in AI today, largely their implicit world model seems to be The world is the world. I'm applying AI here.

00:12:40 
 Therefore, we're going to have the world plus my AI intervention. And that'll be ~~**like**~~ sweet because we'll get this productivity gain or whatever. And that is not wrong, but it misses the fact that everybody else is doing this at the same time. And therefore, there's going to be just a lot of ~~**sort**~~ ~~**of**~~, in my view, unpredictability to how all these new arrangements interact with each other at a higher level. ~~**I**~~ ~~**think**~~ ~~**it's**~~ ~~**also**~~, ~~**yeah**~~, ~~**go**~~ ~~**ahead**~~. GUEST_01: ~~**sort**~~ ~~**of**~~ order of magnitude complexification of the world beyond what we've already seen. If you think about, the world has just become denser in the last 30 years. You think about what was, what was New York City like 400 years ago?

00:13:23 
 The island of Manhattan versus what it is today. It's just~~**,**~~ ~~**it's**~~ the same geographic landmass today, landfill parts of it, but basically the same thing. And the difference is that ~~**it's**~~ just dramatically denser ~~**.**~~ and more complex. And ~~**I**~~ ~~**think**~~ ~~**that**~~ we are in for perhaps not geographical densification, but another substantial round of conceptual and cognitive densification. And one of the things ~~**I**~~ worry about, and another thing that attracted me to this issue area, is the question of how we're all going to keep up with that. ~~**I**~~ worry now that people like you and me are already in a bubble. We're extrapolating multiple points out on this LLM, on the dynamics of AI agents being all over the world.

00:14:12 
 Other people haven't even contemplated the idea of AI agents and have probably asked ~~**chat**~~ GPT 3.5 one basic question 18 months ago and haven't thought about it really ~~**that**~~ much since. And so I worry about people being ~~**like**~~ shocked and left behind. And ~~**I**~~ ~~**think**~~ ~~**that**~~ Maybe neural technology is our way, at least ~~**I**~~ ~~**hope**~~. for us all to just keep better pace with the complexity that ~~**I**~~ ~~**suspect**~~ is coming. HOST: Yeah, there was a recent study just this week that came out that said that 30% of young people, ~~**I**~~ ~~**don't**~~ ~~**know**~~ ~~**exactly**~~ what the age cutoff was, but 30% of young people are ~~**like**~~ using ~~**chat**~~ GPT for work or whatever. And it is a good reminder that that's a lot because it's only been a year and change since the first version and a year and just a few days since GPT-4. But it's also not ~~**that**~~ many yet. And the tools are still certainly

00:15:04 
 In terms of going along for the ride, which is an Elon Musk framing on this, it seems that ~~**I**~~ ~~**want**~~ ~~**to**~~ ~~**just**~~ ~~**now**~~ get into the meat of the technology and ~~**get**~~ really dig into these details. And this may be a little shocking for people that aren't aware of how much progress has already been made. ~~**I**~~ ~~**guess**~~ one way ~~**I**~~ ~~**think**~~ ~~**about**~~ ~~**it**~~ ~~**is**~~ ~~**like**~~, The way we use computers has, first of all, it's currently ~~**a,**~~ ~~**it's**~~ a bi-directional relationship, ~~**right**~~? We put information into them, they give us information back. The way in which we have entered information into the computers historically has been this sort of finite action space of you can type on the keyboard and you can click the mouse and you can do a lot with that. But it is, at any given time, you have a finite and often ~~**quite**~~ prescribed set of actions that you can take. And outside of that, ~~

00:15:56 
 That's one of the reasons that they cite for why the web agents historically didn't take off. They tried to do reinforcement learning on the web, and the reward signal was just too sparse. Agents couldn't get anywhere, and they couldn't chain enough success together to get any reward. ~~**And**~~ so the whole project ~~**a**~~ few years ago went nowhere for that reason. ~~**Of**~~ ~~**course**~~, the signals that we get back have been getting richer and richer over time, like higher resolution, more realistic graphics, ~~**etc**~~. Now, with this language moment, we have the ability to speak in natural language to the computers, and this really opens up a sort of richness of communication for what they can understand ~~**in**~~ ~~**terms**~~ ~~**of**~~ our mental states and desires. ~~**I**~~ ~~**think**~~ ~~**Ilya**~~ ~~**from**~~ ~~**OpenAI**~~ ~~**once**~~, ~~**and**~~ ~~**possibly**~~ ~~**still**~~ ~~**from**~~ ~~**OpenAI**~~, put ~~**that**~~ when he said, the thing that's most incredible about this is I speak

00:16:51 
 And I think that's a really good~~**,**~~ ~~**just**~~ a baseline reality check of what makes this different than before. But now recalling your Substack title, Hyperdimensional, we're moving We're on the verge of potentially moving from this language mode of communication, which has ~~**just**~~ opened up for us to talk to computers in our own natural language. That is obviously a richer space, much more wide open space than you can click on these particular buttons within an interface. But it is still~~**,**~~ ~~**of**~~ ~~**course**~~, the compressed form, token by token, that we're reducing our internal thoughts to this output stream that we can encode in language. And one big way to think about the set of technologies that we're going to talk about today is that it goes up yet another level in terms of the richness of the sort of dimensionality of the space in which we can communicate with computers. GUEST_01: Yeah, absolutely. So let's get into it. I think at a fundamental level, all of this, the neural technologies, it's really a story of very advanced sensor fusion and signal processing, which is the story of so much technological innovation over the last hundred

00:18:01 
 We don't think about it in that way, but from Bell Labs on, it's been a story of signal processing. In this case, signals are the biomarkers of thought. Thought is fundamentally mediated by electricity and magnetism. And so you can measure electrical and magnetic fields generated by brain activity. That's the primary way that this is done, ~~**or**~~ ~~**at**~~ ~~**least**~~ ~~**it's**~~ the most straightforward way~~**,**~~ ~~**let's**~~ ~~**put**~~ ~~**it**~~ ~~**that**~~ ~~**way**~~. There are other interesting ways that we'll get into. ~~**I**~~ ~~**think**~~ the most common that you see for neural monitoring monitoring brains, not so much modifying them, but understanding what a person is thinking is called EEG electroencephalography. And it has a closely related cousin called MEG magnetoencephalography, which is measuring the magnetic fields.

00:18:51 
 So same idea. The difference is that EEG can be done with tiny electrodes placed on the head. Magnetoencephalography requires pretty substantial equipment. ~~**that**~~ ~~**I**~~ ~~**don't**~~ ~~**think**~~ is going to make it out of the lab anytime soon. The thing that ~~**just**~~ a lot of people ~~**don't**~~ ~~**know**~~, ~~**I**~~ ~~**think**~~ even in the tech space, is that it's not just Neuralink in this field shipping products. For more than a decade, there have been neural technologies using EEG, especially because it's relatively cheap, to do all kinds of things, practical things. And it's obviously also been used in a lot of lab settings. The difference between ~~**really**~~ a lab EEG and a consumer EEG is ~~**just**~~ the number of channels that you get.

00:19:35 
 So consumer EEG might be stereo,~~**just**~~ two electrodes up to maybe at the maximum 32. That would be a lot. 32 would be like a whole helmet that you're wearing. Something like 8~~**16**~~ is more common in the consumer setting. Lab can go all the way up to 256. And this is just taking an enormous amount of data. That's something that ~~**I**~~ ~~**think**~~ is also not appreciated is just that the brain generates a torrent of data. A lot of EEG, you can change the sampling rate, but the sample rate is usually around a thousand hertz.

00:20:07 
 So a thousand samples per second.  An eight channel EEG in a consumer headset is going to be generating 8,000 data points from your brain per second. So you quickly get into millions if you're using it for any substantial period of time. ~~**The**~~ ~~**problem,**~~ ~~**though,**~~ ~~**is**~~ ~~**that**~~ ~~**fundamentally,**~~ ~~**actually,**~~ ~~**the**~~ ~~**problem**~~ ~~**with**~~ ~~**all**~~ ~~**of**~~ ~~**these**~~ ~~**technologies**~~ ~~**really**~~ ~~**is**~~ ~~**the**~~ ~~**human**~~ ~~**skull.**~~ The skull was designed to protect your brain, and it does a good job of that. ~~**and**~~ ~~**it**~~ ~~**is**~~ ~~**not**~~ ~~**particularly**~~ ~~**conductive.**~~ So it tends to attenuate signal coming out of the brain and going in. ~~**There's**~~ ~~**other**~~ ~~**things**~~ ~~**too.**~~

00:20:43 
 There's cerebrospinal fluid. There's other mediating layers between the actual surface of the cortex and the brain and the scalp, but the skull is the most important one. ~~**And**~~ so modeling that, modeling the skull, which also varies between people, In thickness, density is very difficult. It also varies not just between people, but across your skull. So your skull is not homogenous in terms of its density or thickness. ~~**And**~~ so it has to be modeled in real time for an ideal reading. ~~**And**~~ that is where ~~**I**~~ ~~**think**~~ we're at the frontier of these technologies. But I'll talk a little ~~**just**~~ about what has been done so far, particularly with EEG.

00:21:24 
 EEG devices can do things like screen for signs of cognitive decline. They can read~~**,**~~ ~~**for**~~ ~~**example**~~, Parkinson's. There are biomarkers of mental illness that can be picked up in EEG devices. Generally, you don't see that on the consumer side of things. And the reason for that is that it relates to FDA regulation. We can get into that later. But for consumer devices, you'll often see devices to help you meditate better~~**,**~~ ~~**the**~~ brainwaves They fall into a few broad families, and it is actually intuitive. The electrical fields that your brain is generating oscillate at higher frequency when you are thinking more intensely.

00:22:07 
 That is an intuitive relationship. So when you're in deep sleep, the frequency is very low, and when you are focusing or in a panic, the frequency is very high. And so those ~~**kind**~~ ~~**of**~~ crude measures of, is this person sleeping? Is this person in a deep meditative state? Is this person attentive and focused? Or are they scared? These are the kinds of things that can be read right now in consumer devices ~~**still**~~ fairly crudely, but it can be done. HOST: So a couple of ~~**just**~~ very practical, rookie level questions about exactly what we're measuring and exactly what we're doing with it here.

00:22:48 
 The electrodes that you put on your head to measure these electrical signals, Are they~~**,**~~ ~~**you**~~ ~~**said**~~ the frequency with which they can take a measurement is about a thousandth of a second, a thousand hertz is a thousand cycles a second, so it's one thousandth of a second measurement. The rate at which a neuron can fire, ~~**I**~~ ~~**believe**~~, is also roughly a thousand times a second. Is that right? GUEST_01: It can be quicker than that. Neural activations can be quicker. So ~~**I**~~ ~~**should**~~ ~~**actually**~~, ~~**yeah**~~, ~~**I**~~ ~~**should**~~ ~~**just**~~ step back. First of all, we're talking here about non-invasive technology. There's a whole school of invasive stuff, which we can get into later.

00:23:26 
 But on the non-invasive side, yeah,~~**that's**~~ ~~**right.**~~ And EEG is also useful because of that temporal resolution, as it's called in the literature. ~~**is**~~ excellent for EEG. As a comparison, the kind of gold standard for spatially imaging the brain is fMRI. And that, at its best, can take one image every two seconds-ish. Roughly two seconds. So in the space that if you're fMRI, if you're using fMRI, you're in a lab because fMRI requires superconducting materials. You're not wearing that on your head.

00:24:03 
 anytime soon. If you're wearing, for example, let's just say a 64 channel EEG, then in the space that an MRI can take one image of your brain, the EEG has generated more than a hundred samples from the brain. So the temporal resolution is great. Not quite at the neural activation level from a temporal perspective and far from it. ~~**from**~~ a spatial perspective. So an electrode at its most precise can see millions of neurons still, down to a spatial resolution of centimeters or millimeters in some cases. HOST: I want to understand a little bit better what the electrodes are actually measuring, ~~**like**~~ what their output is and then how that gets translated into this sort of spatial model of what's going on in the brain. Is it accurate to say that an electrode creates one number every thousandth of a second that would represent ~~**like**~~ potential or sort of the strength of the electrical field?

00:25:00 
 GUEST_01: a point in time. Yeah,~~**exactly**~~. HOST: And that is, it seems like there's Fourier transform kind of math that must be happening here where what is actually this is similar to ~~**like**~~ how cell phone towers work from what limited understanding I have of this branch of physics. But basically we have all of these neurons firing off with these sort of spikes, ~~**right**~~? And there's a very short duration, hot, relatively high voltage signal that is getting sent very locally. Stop me if I'm wrong about this at any point, because I'm really not very expert in this at all. Because that's ~~**now**~~ happening in all these billions of locations concurrently throughout this whole region of the brain, that then sends out this sort of massive signal to the outside world, much like a cell phone tower is sending out a massive signal that is communicating with all ~~**of**~~ the phones in the area at once. And then in a similar way, if it's similar the analogy is going to break down so forget the analogy for a second.

00:26:02 
 All these electrodes are receiving this sort of messy signal where it's okay I'm here and this is what I feel right now and that is the aggregate of all ~~**of**~~ the signals that have been created from these individual neuron firings and then there's ~~**like**~~ a real computational challenge after that to say okay we've got 64 different signals because we ~~**got**~~ 64 electrodes on the head. What does that translate into? So can you tell us a little bit more about how that is happening? ~~**Like**~~ how does that sort of 64 numbers get translated into a spatial understanding of what's going on inside? GUEST_01: Yeah, this is a great~~**,**~~ ~~**it's**~~ a great question. First of all, ~~**yeah**~~, your intuition here and what you sketched out is basically correct. ~~**And**~~ ~~**yeah**~~, a lot of the basic analysis that's happening here is your standard Fourier transform. HOST: Just to define that, that basically takes a composite signal and breaks it down into the intensity of the signal at different frequencies over time.

00:27:01 
 So you have some wobbly signal that you're measuring and this says okay how would you ~~**how**~~ could you re-represent that signal as the sum of intensities at certain frequencies throughout the frequency spectrum? GUEST_01: Yeah, the way I think about it is almost like if your brain is playing a chord, the Fourier transform separates it into individual notes. And ~~**so**~~ you can see it in that way. And that's~~**,**~~ ~~**yeah**~~, that's essentially what's going on with EEG signal processing. ~~**I**~~ ~~**would**~~ ~~**just**~~ ~~**add**~~ ~~**that**~~ there's competing electrical signals~~**,**~~ ~~**right**~~? Your eyes, when you blink, are generating electrical fields. All of the muscles in your face are doing the same thing. The device itself that you're wearing is generating electrical fields.

00:27:50 
 So there's all of this competing electrical activity around the head, which is being sampled. The electrodes are going to sample some of that. That's got to be processed out. And then there is what I was alluding to earlier, that the electrical field itself from the brain is going to be attenuated in different ways dynamically by the skull and other sort of cranial media. Difficulty of processing that~~**,**~~ ~~**that's**~~ where there have been a lot of traditional techniques, traditional statistical analysis techniques that have been used and older MEL techniques. ~~**convolutional**~~ ~~**neural**~~ ~~**nets,**~~ ~~**recurrent**~~ ~~**neural**~~ ~~**nets,**~~ ~~**all**~~ ~~**that**~~ ~~**kind**~~ ~~**of**~~ ~~**stuff**~~ that have shown some promise. Basically everything that I'll talk about actually is really using those ~~**kind**~~ ~~**of**~~ older forms of statistical analysis to break this apart and figure out what's relevant and figure out what it means. There hasn't been a lot of transformer-based work and other types of things.

00:28:50 
 And that's where I think there's just a lot of low-hanging fruit. And it's actually still an open question to me why there hasn't been more. The transformer has been around for a while now. There are some papers that show EEG transformers and other types of neural signal processing with transformers, but why aren't there ~~**much**~~ more? Is it the conservatism of academia? ~~**I**~~ ~~**don't**~~ ~~**think**~~ ~~**My**~~ ~~**first**~~ ~~**intuition**~~ ~~**was**~~ ~~**maybe**~~ it's ~~**a**~~ data problem. These things generate a lot of data. So ~~**I**~~ ~~**really**~~ ~~**don't**~~ ~~**know**~~.

00:29:19 
 I'd love to know. If any of your listeners have insights, ~~**I'd**~~ ~~**love**~~ ~~**to**~~ ~~**hear**~~. HOST: I have a couple of intuitions, which may or may not be right, and people can correct me on this as well. We just did an episode on the first 90 days of Mamba literature. And one of the things that I think is really interesting about this new mechanism, the selective states-based mechanism, is that it does have different strengths and weaknesses compared to the attention mechanism, both in terms of how much memory it consumes, where the attention mechanism is quadratic in the length of the input, And that might be~~**,**~~ ~~**by**~~ ~~**the**~~ ~~**way**~~, one of the reasons ~~**like**~~, just as you talk about, ~~**like**~~ a torrent of data, and 1000 samples per second, if that were to be naively translated to 1000 tokens per second, then very quickly, you're getting to a level of tokens that we have only very recently reached with frontier grade transformers, ~~**right**~~? It was only with GPT for a year ago that the public

00:30:42 
 But also another interesting thing is that they~~**,**~~ ~~**when**~~ ~~**they**~~ break down these micro tasks and look at what the Transformer can do and can't do, one of the things it really struggles on is the hyper-noisy environment. There was an interesting result in this one Mamba versus Transformer comparison paper, which really, Mamba's such a great name, but it really should be~~**,**~~ ~~**it's**~~ more about the selective states-based mechanism and the attention mechanism. Those are really the two things that ~~**I**~~ ~~**think**~~ are more dueling it out than the higher-level architectures. And they're not even dueling it out because they actually work best together, spoiler. In the super noisy environment where what actually matters is quite rare in what you're signaling, then the transformer sometimes has a hard time converging. And the intuition ~~**I've**~~ developed for that is because it's changing all the weights at the same time across the entire range of the input, it may be that the gradient is often dominated by noise and has a hard time converging on the signal. ~~**whereas**~~ ~~**when**~~ 

00:32:25 
 The recurrent nature of the selective state space mechanism allows you to zero in and do the gradient on the signal when you have the signal. And then ~~**of**~~ ~~**course**~~, all the other noise, there's still a lot of noise, but that maybe can get separated from the signal because of this ~~**kind**~~ ~~**of**~~ bit by bit level processing and updating. I'm not 100% confident in that theory, but it ~~**does**~~, it is consistent with all the evidence that I know of so far. So we'll see how that evolves through time. GUEST_01: ~~**then**~~ we might be in for an exciting couple of years, if that's true. ~~**Yeah**~~, that's a great point. ~~**That's**~~ ~~**a**~~ ~~**great**~~ ~~**point**~~. It very well may be that.

00:33:02 
 I guess I should talk about some... ~~**I've**~~ ~~**talked**~~ ~~**a**~~ ~~**lot**~~ ~~**about**~~ ~~**the**~~ ~~**downsides**~~ ~~**and**~~ ~~**what's**~~ ~~**hard**~~. Once you get into this literature, what ends up happening is you just admire the very, very challenging problems that there are and just how complicated the brain is. But there is a lot that you can do. And what's shocking about EEG is that it works as well as it does non-invasively, especially when you consider that the only electrical signals really that can reach the brain or that can reach the electrodes on the scalp from the brain are very much on the surface, very much cortex, maybe one to three millimeters below at the most. ~~**everything**~~ ~~**else**~~ ~~**just**~~ ~~**gets**~~ ~~**totally**~~ ~~**attenuated**~~. So any deep brain structure, there's just nothing that EEG can really read. So ~~**everything**~~ ~~**we're**~~ ~~**talking**~~ ~~**about**~~ is stuff that's coming from just the surface of

00:34:05 
 That was demonstrated, in fact, I think about eight years ago in a lab setting. And there are devices that do this for people with epilepsy. So the telltale signs during which the patient feels absolutely nothing. There's no external sign for the patient that a seizure is coming, but there are brain patterns that can be interpreted by consumer EEG hardware that can predict with almost 100% accuracy whether a seizure is going to happen. Same thing with Parkinson's. Early onset Parkinson's has a pretty distinct neurosignature that can be read. ~~**That's**~~ ~~**something**~~ ~~**like,**~~ ~~**I**~~ ~~**forget**~~ ~~**exactly,**~~ ~~**but**~~ ~~**that's**~~ 90 plus percent accuracy. So already, the potential of a medical device, I'm wearing AirPods right now.

00:34:53 
 In fact, Apple has a patent that they've explored of putting electrodes on AirPods to do this exact thing. Just simply something, a pair of AirPods that can monitor for important medical conditions like that, and maybe help ~~**me**~~ relax. That's already something that could be quite useful and seems like it will happen within the next few years. Maybe not exactly in the AirPods form factor, but nonetheless, you can, and when ~~**I**~~ ~~**said**~~, ~~**by**~~ ~~**the**~~ ~~**way**~~, make you relaxed, that refers to something called neurofeedback. So rather than a direct modulation by hardware of your neural state, neurofeedback is basically giving you the data. and allowing you to change your patterns of thought in response to the data. So sometimes there are neurofeedback devices that have EEG sensors on a headband that you might wear, and it pairs with a phone app. And the phone app will show you your neural activity in real time, and maybe give you a game or some other kind of cognitive stimulator to play with, and it will tell you how focused you are.

00:36:01 
 and you can get focused more.  ~~**And**~~ the idea is that you can actually train circuits in the brain to become more focused. There are devices that do this for sleep without you having to do anything. A lot of devices, ~~**for**~~ ~~**example**~~, there's one called the BIA, ~~**I**~~ ~~**believe**~~, that resembles ~~**like**~~ a sleep mask. You can wear it around your head. ~~**And**~~ ~~**I've**~~ ~~**never**~~ ~~**tried**~~ ~~**this**~~, ~~**by**~~ ~~**the**~~ ~~**way**~~. ~~**I**~~ ~~**don't**~~ ~~**think**~~ ~~**the**~~ ~~**Bee**~~ ~~**is**~~ ~~**actually**~~ ~~**shipping**~~ ~~**yet**~~, ~~**and**~~ ~~**I'm**~~ ~~**certainly**~~ ~~**not**~~ ~~**endorsing**~~ ~~**it**~~, ~~**just**~~ ~~**to**~~ ~~**be**~~ ~~**clear**~~. ~~**But**~~ you can wear it ~~**like**~~ a sleep mask and it will play music.

00:36:30 
 And that music is dynamically tuned in response to the EEG signal and meant to bring you into a more relaxed state. ~~**And**~~ ~~**ultimately**~~, ~~**I**~~ ~~**don't**~~ ~~**think**~~ there's ~~**a**~~ ~~**lot**~~ ~~**of**~~ scientific literature backing this up, but there is general scientific literature to support the idea that neurofeedback is a thing. Your brain will develop circuits to get into a more relaxed state more easily on its own~~**,**~~ ~~**but**~~ enough practice at that. So that's the kind of thing that we're talking about in consumer devices. There's also motor control, which is a whole different interesting field that maybe you want to go into. But first, if you have any questions on the first set of things. HOST: Yeah, it seems like we're working ~~**.**~~ our way up through this sort of same paradigm seems to happen all over the place.

00:37:15 
 There's like echoes of this going with the increasingly rich signal, then we also have these sort of increasingly meaningful states that we're able to identify, ~~**right**~~? Within the transformer, this is pretty well studied now where In these sort of late middle layers, the inputs have been worked up to these rich concepts and you can even identify the direction in activation space that corresponds to justice or fairness or love or these kinds of things. You're like, wow, how did this AI learn to represent that when all it's doing is next token prediction? And here there's like a somewhat analogous thing where as the ability to read the signal gets better and better, we're able to see, OK, with your notes, you've got, OK, it takes two channels, two electrodes on the brain to detect if you're sleeping or awake. With four, you can get to ~~**are**~~ you stressed or ~~**are**~~ you relaxed? With eight, you can get to general emotional state like fear, happiness, disgust, anger. And then with 16 and 32, more advanced things ~~**are**~~ starting to happen. So ~~**I**~~ ~~**am**~~ actually really interested in the motor thing, because that seems

00:38:31 
 And then I also wonder if you have thoughts about the limits of that with this technology, ~~**of**~~ ~~**course**~~, but we're going to discuss about technologies, too. GUEST_01: ~~**Yeah**~~, so motor control, it really does turn out that when you think about moving something, it is tantamount to sending the electrical impulses to actually move that thing. And that can be translated for people that are unable to make that motion themselves. There's demonstrations, you'll see these on ~~**At**~~ ~~**least**~~ ~~**maybe**~~ ~~**it's**~~ ~~**just**~~ ~~**my**~~ ~~**algorithm**~~, but I see this on ~~**X**~~ all the time, a person wearing a helmet controlling a robotic arm, for example. And oftentimes the motions are jerky or slow, or you'll see sped up footage. And those are the signal processing issues that we have now. And those are also probably, if you see someone wearing a whole helmet, that's going to be a lot of EEG signal that they're reading. But you can interpret some basic motor control from the cortex, from cortical activity.

00:39:27 
 Now, a lot of motor control is coming from deeper brain structures. The limits on that, at least to me as someone who's not a neuroscientist, ~~**unclear**~~ exactly. But ~~**I**~~ ~~**actually**~~ ~~**think**~~ It's important to say at this point that some of the highest quality datasets for this kind of application are datasets that combine EEG readings with fMRI or PET scans. ~~**Now**~~, PET scans and fMRI, again, not the kind of thing that are going to make it into consumer use cases. Maybe ever ~~**certainly**~~ not anytime soon ~~**in**~~ ~~**lab**~~ ~~**settings**~~, you can record that and then you have an interesting relationship because the ~~**in**~~ ~~**particular**~~ is a 3d. Voxel as it's called in the literature of representation of brain activity uses blood flow. to infer ~~**a**~~ blood oxygenation, to be precise, to infer brain activity. And then you can connect that with the EEG signal.

00:40:29 
 So all of a sudden, that's an interesting AI application because you've got a much richer but more sparse signal coming from the fMRI and then a super noisy signal coming out of EEG. And there's an interesting connection. What actually, what are subtle differences that we might not notice, subtle patterns that we might not notice between different states of mind that we can ~~**easily**~~ see on the fMRI that we can't see on EEG, but perhaps they can be pulled out of the data. ~~**I**~~ ~~**think**~~ that's a very rich area of research. And we'll talk later about some companies that are using models exactly of that kind to do interesting things. But that could really advance, especially on the motor control. But non-invasive EEG based brain computer interfaces already exist. And ~~**I**~~ ~~**think**~~ that's ~~**to**~~ understand.

00:41:20 
 Are they super useful? No, not really. There are four enthusiasts of this kind of thing and they're not cheap, but you can today buy, ~~**I**~~ ~~**believe**~~ ~~**it's**~~ a 32 channel EEG headset and pair it with software that you can use to manipulate objects on the screen ~~**of**~~ ~~**your**~~ ~~**brain**~~. It requires some calibration. ~~**Can't**~~ ~~**just**~~ ~~**do**~~ ~~**that**~~ ~~**out**~~ ~~**of**~~ ~~**the**~~ ~~**box**~~, but it can do it. You can move, ~~**I**~~ ~~**think**~~ ~~**it's**~~ ~~**like**~~ they have a block that you can move left and right and things like that. Nothing like the cursor control that Neuralink has displayed, ~~**has**~~ shown, but that is possible in digital environments. And then certainly also it's possible to move robotic.

00:42:03 
 devices. And there are prosthetics that work in this way already. HOST: One question I've been pondering here is what is the ~~**kind**~~ ~~**of**~~ breakdown? We talked a little bit about traditional signal processing and Fourier transforms and whatever. And then I'm ~~**also**~~ reminded of Elon Musk saying on the Neuralink show Intel Day a year and change ago now, ~~**I**~~ ~~**think**~~ that turns out the best thing to decode a neural net is another neural net. And so I'm wondering what parts of the interpretation process you've got ~~**like**~~ a thousand numbers coming off each electrode per second and then ultimately that needs to be translated into something, ~~**right**~~? A classification of your state or ~~**like**~~ a direction in motion space that you're going to try to move your robot arm or whatever you're trying to do and Between there there's you could imagine ~~**like**~~ decoding with all traditional methods. You can imagine an entirely neural net thing that just takes in these raw numbers and translates that to an output with basically no principled approach other than ~~**just**~~ throw a model and a bunch of data at it and let it figure

00:43:08 
 Do you have a sense for what the right balance is there and whether ~~**like**~~ today's balance is likely to hold or are we headed for another bitter lesson where just all of this gets thrown into neural nets, ultimately? GUEST_01: It does feel that way, just based on ~~**we**~~ ~~**see**~~ problem after problem falling to the unprincipled application of neural nets. ~~**Obviously**~~, it's more complicated than just throwing data into architecture. But ~~**basically**~~, we've seen that over and over again. And every time it happened, the experts and the scientists who were specific to that field told us, that will never work. It's impossible. And then it works. And so I've learned to be humble in this regard, and I wouldn't want to hazard too much of a guess.

00:43:55 
 My intuition right now is that the field, frankly, is probably relying on a little bit more primitive methods of doing this signal processing than it could. and that moving in the direction of a fully neural net is at least worth experimenting with. ~~**I**~~ ~~**think**~~ part of the reason that the scientific community doesn't do that is because you want comparability with studies that have come before. There is this innate conservatism in ~~**this**~~ field of science that ~~**I**~~ ~~**think**~~ a fresh crop of startups focusing on this will be entirely uninterested in. And so ~~**I**~~ ~~**do**~~ ~~**think**~~ ~~**that**~~ part of it is just that we need, ~~**I**~~ ~~**think**~~ we're at the stage where we need to get this research out of scientific labs and more into at least corporate R&D labs. And obviously, something that's important to understand here is that every one of the big tech companies employs neuroscientists who work on things that are at least adjacent to this. Apple's got a lot of patents in the field. Apple's exploratory design group is probably looking at this.

00:45:12 
 That's their kind of skunk works operation. Meta has been more public about it. And in fact, Mark Zuckerberg, it's worth noting in his review of the Apple Vision Pro, which we should talk about ~~**by**~~ ~~**the**~~ ~~**way**~~, because that's ~~**like**~~ an interesting kind of half step towards all this. But Mark Zuckerberg in his review of the Apple Vision Pro said, Basically something along the lines of, I guess the eye tracking interface they've built is okay until we get the neural interface hooked up. Basically viewing it as an inevitability. ~~**Yeah**~~. ~~**I**~~ ~~**think**~~ ~~**that**~~ there are probably right now inside of both startups and large corporate R&D labs, fresher approaches to this being tried than at least what ~~**I**~~ ~~**have**~~ ~~**seen**~~ in the academic literature. Though in fairness, academic literature doesn't always go into a ton of detail about exactly how the signal processing is done at the ~~**kind**~~ ~~**of**~~ statistical analysis level.

00:46:01 
 But yeah, that's my general sense.

HOST: Availability of data also may be a real issue for some of these things. It seems like the way that this has progressed in ~~**neuroscience**~~ ~~**has**~~ backfilling a narrative here, but ~~**I**~~ ~~**feel**~~ ~~**like**~~ ~~**it**~~ ~~**is.**~~ The brain is the ultimate black box, right? Even more messy and black boxy and just hard to get into, obviously, for all sorts of medical and ethical reasons as compared to a digital neural network, which you can ~~**be**~~ ~~**into**~~ with clarity and chop parts off and see how it works with different permutations. So there's been just a huge initial challenge of figuring out, in rough terms, what is going on and how does it work. And so people are doing these ~~**kind**~~ ~~**of**~~ very small sample sizes of looking at people in fMRIs and trying to deduce ~~**like**~~ what brain region does what and ~~**like**~~ what frequency of waves seem to correspond to what and this ~~**like**~~ super low data regime figuring out in general terms what's

00:47:28 
 What were you trying to do with your robot arm or with the cursor on the screen? ~~**like**~~ ~~**that**~~ ~~**data**~~ ~~**just**~~ ~~**has**~~ ~~**never**~~ ~~**existed.**~~ So you can't really have ~~**the**~~ you can't take ~~**the**~~ ~~**better**~~ ~~**lesson**~~ approach until you have ~~**the**~~ ~~**scale**~~ ~~**of**~~ ~~**recorded**~~ ~~**data**~~ ~~**to**~~ ~~**make**~~ ~~**it**~~ ~~**go.**~~ And it seems like we've only recently realized that this is going to work for everything. And we need ~~**that**~~ ~~**scale**~~ ~~**of**~~ ~~**data.**~~ And so we're ~~**probably**~~ ~~**You**~~ ~~**know,**~~ ~~**nobody**~~ ~~**has**~~ ~~**really**~~ ~~**collected**~~ ~~**it**~~ ~~**globally.**~~ We just did this episode with Paul Scotty, who is the author of the Mind Eye 2 paper. They are beginning to work on a multimodal brain

00:48:10 
 And it sounds like that's really only getting underway. There's data ~~**super**~~ fragmented. It's all kinds of different places and forms. And so ~~**I**~~ ~~**guess**~~ that also seems like a big part of why this hasn't happened yet. Would you agree? GUEST_01: ~~**Yeah**~~, ~~**I**~~ ~~**do**~~. ~~**I**~~ ~~**do**~~. And ~~**I**~~ ~~**think**~~ we're at a very primitive state when it comes to the data.

00:48:31 
 I don't think we have a good sort of population level modeling of the variation in neurosignatures, even things like skull thickness. ~~**Right**~~ ~~**now**~~ ~~**it's**~~ ~~**like**~~ the literature is everyone's skull is different. And that's probably not true. It's probably not literally true that ~~**it's**~~ ~~**like**~~ a snowflake, ~~**right**~~? Like my skull ~~**is**~~, it might be unique at a very granular level of detail, but you can probably model population dynamics for this sort of thing. And that would make your imaging challenges substantially easier. Same thing goes for the neural activity. ~~**I**~~ ~~**think**~~ ~~**it's**~~ ~~**definitely**~~ ~~**quite**~~ ~~**likely**~~ ~~**that**~~ high dimensional thought ~~**in**~~ ~~**particular**~~ is probably pretty unique signature.

00:49:16 
 My way of, my associations that I have with the concept of flawed three opus or Mozart or something are very different from yours. And it's not entirely clear that we're going to be able to get to the point of, ~~**oh,**~~ Dean and Nathan are both thinking about Mozart. That seems hard to be able to reach, but there's a lot lower level thought that is still interesting, that it seems like you could model, that's higher level, somewhere in between Mozart and I'm scared of this tiger. I'm trying to run away from it. We can read that pretty easily. We know what that looks like. HOST: That has a pretty common signature that's easy to pick up on, but- The MyGuy2 paper actually did have an interesting, or may shed a little light on this because they, first of all, the dataset is only from eight people that they work on. And ~~**I**~~ ~~**think**~~ ~~**this**~~, ~~**I**~~ ~~**think**~~ that will probably be out before this one.

00:50:14 
 So people, it should be right before this on the feed. Basically what they're doing is looking at your brain state as measured in that case by fMRI and reconstructing what you were looking at. They had an earlier version of this where they created a custom model for each of the eight people that are represented in this one open source data set. Each person had to spend ~~**like**~~ 30 to 40 hours in an MRI machine over presumably a bunch of different sessions. ~~**they're**~~ ~~**being**~~ shown an image every few seconds. And then they ~~**just**~~ had to sit there and click a button if they ~~**had**~~ ~~**ever**~~, if they recognized that they had seen that image before, ~~**just**~~ to make sure that people ~~**are**~~ engaged in the task on an ongoing basis. What was interesting, the jump between MindEye 1 and MindEye 2 was that instead of creating a single model trained on all ~~**of**~~ the 30 to 40 hours of data per individual, they were able to train a single model based on seven of the eight individuals' data. And then they could create a little...

00:51:24 
 They report the number of voxels per individual, voxel being ~~**a,**~~ ~~**roughly**~~ ~~**speaking,**~~ a centimeter or a two millimeter cube. The lowest number of voxels from one person is 12. And this is how they choose the number of voxels is by the anatomy of the brain. So they're looking at ~~**basically**~~ the visual cortex, segmenting that off, and then just splitting it into voxels. And the resolution of the voxel is constant. So how many voxels you get depends on the total space of the portion of the brain that they identify as the relevant visual cortex for this purpose. So 12,000 ~~**in**~~ ~~**sum**~~ is the low end, and the high end is over 17,000. So you have ~~**a**~~ 12,000, a couple in the 13,000 range, a couple in the 14,000 range, a couple in the 15,000 range, and then one over 17, close to 18,000 is the highest.

00:52:07 
 So you see a roughly 40%  difference from the lowest number of voxels to the highest. And that requires then a little bit of a bespoke adapter portion of the model for each person because they ~~**just**~~ have literally different numbers of input. The vector that is measured is ~~**like**~~ a different length for each of these individuals. So the adapter to then get to the shared latent space has to be a bit different for each individual But once they create that shared latent space, then they are able to do an additional person with ~~**just**~~ one hour of data. So the key finding there is that they go from ~~**a**~~ ~~**like**~~ ~~**a**~~ previous technology, same data set, interestingly, same raw information, same~~**.**~~ These are ~~**like**~~ the same sessions that were recorded, but they're able to go from a version where it works at 30 to 40 hours, which is ~~**obviously**~~ prohibitive for most usage down to now they can get it to work with one hour of data because they're tapping into this ~~**like**~~ shared latent space that they've created from other individuals. So ~~**I**~~

00:53:35 
 And he said, unfortunately, in the data set that they were working with, there's very little overlap between the different images that people saw. So on the one hand, you would say ~~**in**~~ ~~**some**~~ ~~**sense**~~, that ~~**kind**~~ ~~**of**~~ ~~**maybe**~~ suggests that there is ~~**like**~~ a high level of generality because they're able to get this shared latent space to work even when people mostly didn't see the same images. On the other hand, it ~~**gives**~~ ~~**us**~~ ~~**a**~~ ~~**kind**~~ ~~**of**~~ creates a limitation in terms of ~~**can**~~ ~~**we**~~ ~~**say**~~ what the inner product of my CLAWD3 conception and your CLAWD3 conception is, unfortunately in this study, people ~~**just**~~ didn't see the same thing enough for them to be able to do that sort of analysis. But ~~**I**~~ ~~**did**~~ ~~**find**~~ ~~**it**~~ ~~**quite**~~ ~~**interesting**~~, truly profound, ~~**quite**~~ ~~**interesting**~~ ~~**is**~~ ~~another

00:54:43 
 And I think I think we're just scratching the surface there. Like ~~**I**~~ ~~**said**~~, ~~**I**~~ ~~**don't**~~ ~~**know**~~ ~~**that**~~ we're ~~**gonna**~~ ~~**get**~~ ~~**to**~~ ~~**my**~~ ~~**Mozart**~~ ~~**and**~~ ~~**your**~~ ~~**Mozart**~~ ~~**or**~~ ~~**whatever**~~. But ~~**I**~~ ~~**also**~~ ~~**think**~~ ~~**that**~~ there's a really wide space. There's a lot of surface area. ~~**that**~~ you can reach. So ~~**yeah**~~, ~~**I**~~ ~~**think**~~ ~~**that**~~, and anything that makes it easier to collect data is particularly appealing because having to have people go through these things for 30, 40 hours is just, is an unreasonable collection. So ~~**I**~~ ~~**suspect**~~ ~~**that**~~ we will see an acceleration of all this pretty soon. ~~**I**~~ ~~**think**~~ it's going to happen pretty soon.

00:55:19 
 And I think that, I don't know that EEG will be the technology specifically. There's one other approach called ~~**ESMER's**~~ ~~**Functional**~~ Near-Infrared ~~**Troscopy**~~ ~~**that**~~ ~~**can**~~ ~~**almost**~~ ~~**think**~~ ~~**of**~~ ~~**it**~~ ~~**as**~~ fMRI, but non-invasive, ~~**or**~~ ~~**not**~~ ~~**non-invasive**~~, but that you can wear around in a consumer device. It's not going to give you the 3D depth that fMRI ~~**is**~~, but it's doing the same thing where it's blood oxygenation to measure brain activity. It ~~**is**~~ ~~**actually**~~ ~~**pretty**~~ ~~**simple**~~. It's ~~**pretty**~~ comparable to EEG in terms of what it can do and in terms of its costs and things like that. So it could be that, but I think that there are devices that I can imagine existing that are not the full brain-computer interface that we're all dreaming of or like what Neuralink is doing, but it

00:56:36 
 So what you saw with the guy playing chess on a computer with an implanted device, it actually is ~~**very**~~ impressive and it's amazing to see and it's amazing to see some awareness of these issues coming up. ~~**Also**~~ not shocking. HOST: You want to describe that a little bit just in case people haven't actually seen it? GUEST_01: Yeah, there I go thinking that everyone's as engaged in this as I am. So yeah, this is the first patient of Neuralink ~~**is**~~ a young quadriplegic A very sad case, he had, ~~**I**~~ ~~**think**~~, a diving accident or something like that. Doesn't have any use of his arms. And he had this, he had the Neuralink implanted and was able to manipulate ~~**standard**~~ Windows computer. If you've ever used Windows, this is exactly the same interface.

00:57:22 
 It's not some special software. He's just using Windows to move the cursor around, to play chess. ~~**I**~~ ~~**believe**~~ ~~**he**~~, the first night that he got all this hooked up, he stayed up all night playing Civilization VI, which ~~**I**~~ ~~**can**~~ ~~**relate**~~ ~~**to**~~. And ~~**yeah**~~, so ~~**just**~~ this amazing ability to. more or less fully use a computer. And ~~**I**~~ ~~**should**~~ ~~**say**~~ typing with devices like this implanted, typing has been demonstrated not in a way that a consumer would ever want to use, but typing has been demonstrated with non-invasive technology. You can do that with non-invasive EEG, but being able to just fully manipulate a computer and live at least a digital life, surely with the brain. was what he was able to do.

00:58:05 
 And again, amazing, but not something that would come as a galloping shock to anyone who's been paying attention to this. We've seen stuff like that before. And a lot of what Neuralink actually has done in ~~**kind**~~ ~~**of**~~ a classic Elon Musk fashion, one of the most important innovations that they have done is the automated surgery device the robotic surgeon that can do this more or less without human intervention. There's Elon Musk thinking about how ~~**do**~~ ~~**I**~~ ~~**not**~~ ~~**just**~~ how ~~**do**~~ ~~**I**~~ bring innovative technology, but how ~~**do**~~ ~~**I**~~ change the cost structure? And he's thinking along those lines. So he's obviously thinking about wide scale deployment ~~**or**~~ something like this. HOST: Okay. So can we summarize ~~**or**~~ maybe there's a couple ~~**other**~~ ~~**like**~~ high level data points that could bring all this into focus.

00:58:50 
 It seems like. to zoom out and give this the sort of survey view of where we are in the taxonomy. We are. ~~**Yeah.**~~ So far, we've focused on two directions in which information can flow. We have focused on reading of the brain states, not yet the ability to change the brain states, except in as much as there is sort of a feedback thing of ~~**like**~~ you measure and then you show that to the user, then they can get into a certain rhythm and react to the measurement that they're seeing. But that neurofeedback is not direct manipulation of the brain state. And then within this reading half of the equation, you've broken down, there's a lot of different technologies.

00:59:29 
 We've got stuff that's outside the skull ~~**is**~~ subject to just really noisy signal, a lot of challenges with that. But with the EG, you do get high frequency ~~**of**~~ signal. Then you've got the fMRI, which is~~**.**~~ a million dollar machine or whatever, certainly not something you can wear around, gives you a much better spatial resolution. You can really see what's going on at a finer grain level, but less fast. There's some idea that these two modalities might be merged and there could be some really interesting generalization based on that. You can maybe unpack that a little bit more. And then in terms of what we can do with it, it's like with just a couple ~~**of**~~ electrodes on the head, you can do basic stuff.

01:00:12 
 like, are you asleep or are you awake? With a lot of electrodes on the head, you can do reasonably advanced stuff, although it's still clumsy and slow, ~~**like**~~ you can type with your brain. Neuralink then is going inside the skull and that gives them a much cleaner signal and gives them the ability to have higher bandwidth. That's the whole value prop that Oleskis talked about over time. Are there other sort of very ~~**like**~~ memorable ~~**kind**~~ ~~**of**~~ striking demos or products that you think people should understand that are coming out of all this work that they could go check out or watch a little demo videos of? GUEST_01: Yeah, I would say robotic control is definitely always worth looking at. And you can find if you just Google EEG robotic control, They're not really devices, they're not ~~**like**~~ marketed devices to do this, but you can find a lot of laboratory settings where it's been done. Beyond that, there are all kinds of interesting headbands, things ~~**like**~~ that, that you can wear sometimes, ~~**like**~~ headphones too.

01:01:10 
 Over-ear headphones is very common with a band over them and they integrate the EEG into the band that goes over your head. There's a lot of products like that out there and these are not~~**,**~~ ~~**they're**~~ not fantastically expensive. ~~**They're**~~ not super cheap. ~~**usually**~~ somewhere between $500 and $1,000, would be roughly my estimate. In terms of the brain-computer interface, though, the company that I know of that is furthest along in this regard is a company called Emotiv, E-M-O-T-I-V, and they are the ones that I was referencing earlier that sell headsets that you can~~**,**~~ ~~**it's**~~ a little DIY, but you can buy them and use them at home. ~~**pair**~~ ~~**it**~~ ~~**with**~~ ~~**software**~~ ~~**that**~~ ~~**allows**~~ ~~**you**~~ ~~**to**~~ ~~**do**~~ ~~**some**~~ ~~**very**~~ ~~**low**~~ ~~**level**~~ ~~**manipulation**~~ ~~**of**~~ ~~**objects**~~. You mentioned though, like

01:02:06 
 One thing that's interesting to me that we've seen ~~**some**~~, there's been some interesting work on this in~~**,**~~ ~~**in**~~ the lab is better decoding of language. So the best way to put it is mind reading. So we've seen that with image~~**.**~~ There've been a lot in MindEye as an example. Meta has had some research in this regard. There's a lot of people that are basically decoding images that you're imagining in your mind's eye into using AI into a predicted digital image. Basically doing the same thing with text word by word at first~~**.**~~ ~~**eventually**~~ getting higher bandwidth and higher latency~~**,**~~ that seems achievable.

01:02:43 
 And that seems very interesting, particularly when you think about it in the context of communicating with an LLM, prompting an LLM with thoughts. ~~**I**~~ ~~**think**~~, as we were saying at the beginning of this conversation, the challenge But also opportunity with a frontier LLM is how good is your question? And the better your question, the more precise and tailored your question, the better ~~**an**~~ answer you will get out of these systems. So some people ~~**just**~~ don't have the communication capability ~~**to**~~ ask exactly the question that they want to, but they can surely think of it. So can that start to be translated into questions that could prompt an LLM? ~~**I**~~ ~~**don't**~~ ~~**know**~~. Maybe.

01:03:28 
 Certainly it seems to me that basic motives could be. ~~**So**~~ ~~**I**~~ ~~**think**~~ about something like this device that came out at CES this year, the Rabbit R1, which uses that thing that they call a large action model, an LLM that sort of translates what you want into ~~**like**~~ intents, basic intents. ~~**I**~~ ~~**also**~~ ~~**think**~~ about something like the iPhone and Apple devices in general have these automation layer called shortcuts. where you can make little sort of modular pieces of software to do a simple thing. Could we start to translate thought into basic actions like that? ~~**So**~~ maybe not my thoughts on a painting by Picasso, very complex set of thoughts, but maybe I want to see what my schedule is for today. I want to see what the weather is for today. I want to turn the bedroom lights off, things like that.

01:04:22 
 Can we translate that and then use existing AI infrastructure, ~~**sort**~~ ~~**of**~~ automation infrastructure, ~~**are**~~ currently being built to actually just perform that. And essentially, that might feel shockingly like telepathy, if you can think, I want to turn the lights downstairs off, and they will turn off. That might feel shockingly like telepathy. So in a certain sense, something I think about with this technology and something that ~~**like**~~, I feel like what I've said during this conversation, the really stone cold reality of where we are, but everything feels like it's converging to this point where a real qualitative leap is possible. At least it seems to me like it could be in the relatively near future. But ~~**I**~~ ~~**think**~~, to your point, to get to the brain-computer interface that we all really want, you need write access, not just read access. That end, maybe do you want to move on to the neuromodulation, or do you have any other questions on moniker? HOST: Yeah, let's just, ~~**I**~~ ~~**think**~~ ~~**so**~~, but let's sketch out a little bit more the, how

01:05:25 
 It seems like the model is basically, ~~**this**~~ ~~**is**~~ ~~**a**~~ ~~**bit**~~ ~~**hackneyed**~~ ~~**now**~~ ~~**in**~~ ~~**the**~~ ~~**AI**~~ ~~**space**~~ ~~**to**~~ ~~**put**~~ ~~**everything**~~ ~~**in**~~ ~~**terms**~~ ~~**of**~~ ~~**what**~~ ~~**GPT**~~ ~~**level**~~ ~~**are**~~ ~~**we**~~ ~~**at**~~, but it seems like we're at GPT one on this, maybe between one and two where it's like, GPT-2 was just barely starting to be useful. When fine tuned, you could do some interesting things with it, but you couldn't do much with it. ~~**Right**~~. It was not ~~**like**~~ certainly wasn't doing much in the way of reasoning. It didn't have the few shot learning ability that ~~**that**~~ emerged with GPT-3. And so you could do classification type tasks or something like sentence completion, perhaps, but very ~~**kind**~~ ~~**of**~~ limited application. And it seems like we

01:06:27 
 But there's eight or so different consumer devices that have seen enough ~~**there**~~ where they're like, somebody's going to want to buy this. Let's get started building a company on it. And where I see this really changing is akin to Robotics 2, the amount of data that is going to be generated as the products hit the wild, even with the early adopter set, seems like it just goes vertical compared to what has existed before. Keeping in mind that Mind Eye 2 is eight patients, a total of 200 and some hours in an MRI across eight individuals. Now you've got things that are going consumer. So you've got orders of magnitude~~**,**~~ more data flowing in, and you have people that are actually attempting to use them. And so you're going to start to get feedback on what is working, what is not working. And so the regime~~**,**~~ ~~**the**~~ data regime is just totally changing.

01:07:19 
 So if there's ultimately a very simple story, it's ~~**like**~~, we've gone from the basic science. We've gotten through enough basic science. to get to the point where there's just this kernel of utility, which is going to tip us into a kind of much faster bootstrapping dynamic where we're going to soar through orders of magnitude of data available. That of course, we've already got architectures that can probably decode that signal if there is enough data to learn from. And so we can~~**,**~~ ~~**you**~~ ~~**can**~~ start to see unless there's something very fundamentally different about this as compared to other domains where machine learning, the same machine learning techniques are working everywhere. So unless there's some very odd reason why they won't work here, the data is about to come online. And that's going to be the big unlock that's going to allow for just tremendously more utility. And we might hit some limits around just how good of a signal ~~**can**~~ ~~**we**~~ ultimately get.

01:08:15 
 But it seems like at a minimum, we are headed for this sort of stored procedure triggering that you mentioned for image decoding is obviously what you're seeing is already quite decodable what you're thinking about and turning that into verbal form seems like very achievable and then even control seems like it likely gets refined to the point where it's ~~**like**~~ practically useful if not ~~**like**~~ super smooth just based on collecting a ton of data and applying known machine learning techniques to those signals. Is that basically your world model or how would you refine what I just said? GUEST_01: I think that's pretty much spot on. The GPT-2 comparison is an interesting one. I recall back in my housing policy days, YIMBY adjacent. Build more housing is what YIMBY means. Build more housing to make it cheaper, to put it simply. I tried to use GPT-2 to analyze municipal zoning codes, segments of municipal zoning codes, to give me just a thumbs up or down on how restrictive ~~**was**~~ it, how not restrictive ~~**was**~~ it in an automated way.

01:09:21 
 And the answers that you could get out of it compared to now are just so ~~**Simian**~~ ~~**was**~~ ~~**like,**~~ ~~**good,**~~ ~~**bad**~~ ~~**kind**~~ ~~**of**~~ ~~**thing.**~~ And you look at where we are now. The difference, ~~**of**~~ ~~**course**~~, is that language is a ~~**kind**~~ ~~**of**~~ ground truth that you can refine your understanding of over time. And the question really is, how consistent is the language of thought across people? ~~**I**~~ ~~**think**~~ ~~**that's**~~ the fundamental barrier, potential barrier, is how calibrated does this have to be? And ultimately, if I have to go get an fMRI scan to make any of this useful to me, then that ~~**kind**~~ ~~**of**~~ really changes it a lot. Maybe I'll do that. I feel like the craziest thing in the world.

01:10:06 
 Actually, especially if it's only an hour. ~~**Yeah**~~. How long does it take to go buy an Apple vision pro? About half an hour. If you're going to demo is 25 minutes. GUEST_02: ~~**Yeah**~~. GUEST_01: ~~**Yeah**~~. ~~**Yeah**~~.

01:10:16 
 Yeah. How hard would it be to put an ~~**FMRI**~~ ~~**in**~~ ~~**the**~~ ~~**back**~~ ~~**of**~~ ~~**an**~~ Apple store or something? ~~**I**~~ ~~**don't**~~ ~~**know**~~, but~~**,**~~ ~~**but**~~ still it changes it for sure. The level of calibration to me is really the big open question here, both on the interpretation side and the imaging or the skull side, but~~**.**~~ ~~**I**~~ ~~**have**~~ ~~**to**~~ ~~**think**~~ ~~**that**~~ substantial progress can be made and that it is possible to at least translate impulses, desires, basic desires, and that that can be fed into various kinds of AI architectures that can take actions on your behalf. That just seems super possible in the near term. ~~**I**~~ ~~**would**~~ ~~**be**~~ ~~**shocked**~~ ~~**if**~~ ~~**that**~~ ~~**didn't**~~ ~~**happen**~~ ~~**in**~~ ~~**the**~~ ~~**next**~~ ~~**five**~~ ~~**to**~~ ~~**10**~~

01:11:03 
 HOST: Yeah. I see a parallel also between the fact that all of these devices exist and the current state of AI agents broadly, where everybody sees where the technology is going, In the case of the agents, it's ~~**yeah**~~, GPT-4 wasn't ~~**quite**~~ trained. There's this weird juxtaposition where it's like closing in on expert level on things like medical diagnosis, but then it ~~**like**~~ sometimes can't click the right button on a very simple user interface. And it's ~~**like**~~. Why is that? Or it gets stuck, it gets into ~~**like**~~ loops that it sometimes can't break out of. ~~**Like**~~, why is that disconnect there? Presumably it's because there wasn't really the sort of task completion, mid-length episode data available to train the first version of ~~**GVD4**~~ ~~**on**~~.

01:11:49 
 I strongly believe now that the big tech leaders are investing heavily in creating that sort of mid-length episode data. And that in all likelihood, the next big shift is going to be ~~**At**~~ ~~**least**~~ ~~**from**~~, ~~**I**~~ ~~**would**~~ ~~**bet**~~ ~~**reasonably**~~ ~~**confidently**~~ ~~**that**~~ the next big shift is going to be that those start, those sorts of things are going to start to work. Even if ~~**like**~~ the MMLU score doesn't go up that much in the immediate term. And ~~**here**~~, so to close that thread, when that happens, then all these agent products start to work dramatically better all at once. And that gets us back to that dynamics question we talked about earlier. On the brain computer interface side, it does seem similar where people are ~~**like**~~ developing all these different form factors. You've got glasses, you've got helmets, you've got headbands, and they all don't work that well because there wasn't enough of the right kind of data to train them on. But you ~~**also**~~ had to get these things out there to get that data.

01:12:47 
 I don't know that OpenAI necessarily ~~**had**~~ ~~**to**~~ ~~**have**~~ all these agent products created. ~~**I**~~ ~~**think**~~ they probably could have created their own data. But in this field, ~~**like**~~ you actually need readings off of a lot of brains. And so it seems ~~**like**~~ we're in this ~~**kind**~~ ~~**of**~~ similar space where the hardware is starting to ship. It doesn't quite work, but again, it's going to collect a lot of data. And then you can imagine a lot of these things turning on relatively quickly and being just a lot more~~**.**~~ advanced, perhaps without even necessarily needing major upgrades to the hardware. You already have a 32-channel thing.

01:13:17 
 That may well be enough for a lot of use cases if you can decode the data effectively. So maybe it'll take more than 32, but it ~~**sure**~~ ~~**seems**~~. Do you know how many, what is the number of electrodes that the guy from the Neuralink patient had? Do you know how many channels his signal is? GUEST_01: That's a great question. ~~**I**~~ ~~**don't**~~ ~~**know**~~ ~~**exactly**~~, but it's a lot. They're very dense. ~~**Don't**~~ ~~**quote**~~ ~~**me**~~ ~~**on**~~ ~~**this**~~, but ~~**I**~~ ~~**guess**~~ it's possible ~~**in**~~ ~~**podcasts**~~, but it might be thousands.

01:13:44 
 It might be like a thousand plus. ~~**I'm**~~ ~~**not**~~ ~~**actually**~~ ~~**a**~~ ~~**hundred**~~ ~~**percent**~~ ~~**sure**~~ ~~**of**~~ ~~**that**~~, but it's quite a bit because of the fibrous way that they're doing the implant is ~~**like**~~ very dense and fine. HOST: Google both traditional search result and generative search result comes back with 1024 electrodes. GUEST_01: Yeah, my recollection, yeah. So it's a lot. It's a lot that they're doing. Now, ~~**the**~~ ~~**problem**~~, ~~**though**~~, ~~**with**~~ ~~**that**~~, ~~**obviously**~~, is that it's a lot in a very specific place. And there are~~**,**~~ ~~**like**~~, long-term issues with that.

01:14:17 
 Invasive It's very promising in terms of capabilities, but you got to have them all over the place. You can't just, the EEGs, they do have the same problem that the non-invasive ones have when they're planted in the skull. They can't read everything in the brain. They can read in ~~**very**~~ local areas. They can read down to the neuron level invasively. ~~**but**~~ they can only read locally. So same problem. And ~~**also**~~ ~~**obviously**~~ ~~**like**~~ the idea of connecting an EEG in my brain to the internet is ~~**like**~~ absolutely terrifying, ~~**right**~~?

01:14:45 
 Just from a cybersecurity perspective. So there's obstacles there too. ~~**Yeah.**~~ But ~~**no**~~, ~~**I**~~ ~~**think**~~ that tracks. And ~~**I**~~ ~~**think**~~ one other thing that ~~**I**~~ ~~**do**~~ ~~**think**~~ about though is in as much as these various platforms have, the problem with the agents, the foundation model agents, is that their test environment is the real world. They don't have a baby version of the internet or computing environment that they can interact in. They have to use our computing environment, which is weird. ~~**all**~~ ~~**kinds**~~ ~~**of**~~ ~~**history**~~ ~~**to**~~ ~~**it**~~ and affordances that are for us that they don't necessarily need, that probably ultimately serve to confuse them and complexify the environment for them.

01:15:24 
 It is why I mentioned the shortcuts idea from Apple's platforms, because what they have done is modularized the functionality of not just their whole operating system, but developers can plug into this too. So third-party apps are modularized too. And Android~~**,**~~ ~~**I**~~ ~~**believe**~~, has something similar. It's not just Apple. That's the kind of thing that again, it could take off pretty quickly and it could actually be local. So it could be fast. You could do this inference locally in theory, if you had a sufficiently powerful compute on board, but it wouldn't be ~~**like**~~, you don't need ~~**like**~~ an H400 to do this kind of thing. ~~**Yeah**~~.

01:16:01 
 And the other shocking thing is just how little compute has been applied to this problem. ~~**Yeah**~~. There's~~**,**~~ ~~**there's**~~, ~~**there's**~~ low hanging fruit. ~~**I**~~ ~~**am**~~ ~~**no**~~, ~~**I**~~ ~~**have**~~ ~~**no**~~ ~~**doubt**~~ ~~**about**~~ ~~**it**~~. ~~**So**~~ ~~**yeah**~~, ~~**I**~~ ~~**guess**~~ that takes us to the right section of this, which from a technological perspective is actually pretty simple for me to explain in a narrative way because it uses~~**,**~~ ~~**it's**~~ the same basic principle. We want to manipulate electrical and magnetic fields. The problem is that everything I described in terms of getting signal out of the brain in a clean way, those problems are magnified substantially for getting signal into the brain, because that's really what your skull is intended to do. It's not so much to keep things in, it's to keep things out.

01:16:48 
 So there's direct current and alternating current stimulation. Transcranially, this is non-invasive ~~**deck**~~. Transcranial magnetic stimulation would be the magnetic field equivalent of that. The problem is that there's been some promise. There's been some stuff that's been shown in the lab, but it's either very expensive hardware, it's not ~~**writing**~~ ~~**that**~~ ~~**good**~~ hardware curve ~~**like**~~ the electrodes are~~**,**~~ ~~**by**~~ ~~**the**~~ ~~**way**~~, which are getting denser, cheaper all the time. ~~**and**~~ the signal just diffuses. So it's just erratic, it's impossible to focus, and it's very varied between people. ~~**I**~~ ~~**don't**~~ ~~**really**~~ ~~**see**~~ the magnetic or electric field manipulation ~~**as**~~ ~~**being**~~ ~~**all**~~ ~~**that**~~ ~~**promising**~~ ~~**in**~~ ~~**the**~~ ~~**long**~~ ~~**term**~~ ~~**for**~~ ~~**neuromodulation**~~.

01:17:34 
 Some people disagree with what's called transcranial magnetic stimulation. ~~**Some**~~ ~~**people**~~ ~~**disagree**~~ and think that ~~**is**~~ ~~**going**~~ ~~**to**~~ ~~**be**~~ ~~**the**~~ ~~**path**~~ if we can just get the cost down. But right now the cost is very high. So it's just not what I'm focusing on because I'm interested in things that seem like they could happen in the relatively near future. There is a technology ~~**though**~~ This really was my exclusive focus when it comes to non-invasive neuromodulation. It's called transcranial focused ultrasound. It's a very old technology. It goes back a hundred years, but really its rebirth is in the past 10 or 20 years.

01:18:10 
 What this is, it's firing very high frequency sound waves at the brain. ~~**far**~~ outside the range of human hearing, ~~**far**~~ outside the range of the hearing of dogs and cats, though there are animals that can hear it. ~~**I**~~ ~~**think**~~ rats, bats, ~~**I**~~ ~~**think**~~ can hear it too, but that you can fire at a sufficient frequency and amplitude and pulse to target ~~**pretty**~~ precisely into the brain and deeper than anything else can. You can't get to deep brain regions, you can't write to the hippocampus, which is memory, with this, but you can go a few millimeters, as much as a centimeter, into the brain, and you can target it ~~**very**~~ precisely. So it's a few millimeters to a centimeter of depth, and then millimeter level resolution spatially. It's a ~~**very**~~ promising technology. ~~**Also**~~, it's cheap. And ~~**also**~~, much like the electrodes, ultrasonic transducers are on a good hardware curve.

01:19:09 
 They are getting cheaper and denser year by year. The weird thing about ~~**TFUS**~~ ~~**as**~~ ~~**it's**~~ ~~**called**~~, it's not very well understood how it works. And it's weird~~**,**~~ ~~**right**~~? It's just weird in principle that firing high frequency sound waves into the brain makes you~~**,**~~ ~~**like**~~, changes your thought in any way. That's just weird. But it does. The theory~~**,**~~ ~~**there's**~~ ~~**some**~~ ~~**who**~~ ~~**theorize**~~ ~~**that**~~ it changes the electrical receptivity of brain tissue ~~**.**~~ ~~**in**~~ ~~**certain**~~ ~~**ways**~~ ~~**that**~~ ~~**just**~~ make electrical signal flow more readily.

01:19:43 
 There's a theory that it just stimulates neurotransmitter release in the targeted regions of the brain, but no one's really found any safety problems with it at reasonable dosages. At high dosages, it has a thermal effect, as we see with other ultrasound. It can, at a very high dosage, it can cause brain tissue to heat in a way that ~~**you**~~ ~~**probably**~~ ~~**don't**~~ want. The other thing from a safety perspective~~**,**~~ ~~**of**~~ ~~**course**~~, is that all of these things are done one time in a lab. Nobody has modeled what the long-term effects are of using this~~**,**~~ ~~**for**~~ ~~**example**~~, every day. ~~**is**~~ ~~**very**~~ ~~**promising**~~. The FDA says it's safe below a certain level, and it's ~~**been**~~ ~~**shown**~~ ~~**to**~~ ~~**do**~~ ~~**some**~~ ~~**shocking**~~ ~~**things**~~ ~~**in**~~ ~~**my**~~ ~~**mind**~~. So we've talked a little bit about improving bandwidth.

01:20:34 
 Maybe the single most interesting finding to me of this whole chain of research that I've done is this finding, that with TFUS at the somatosensory cortex, fired at specific regions of ~~**basically**~~ your tactile sensation, the part of your brain that measures tactile sensation. You can perform tactile discrimination tests. So ~~**basically**~~, you cover the user's eyes and rub a pin on their hand and then rub two pins really close together and see if they can distinguish between the one pin and the two pins. And TFUS was shown to improve that when it was being fired ~~**.**~~ at the brain and also had some offline effect too. An offline versus an online effect. An online effect is do we observe an improvement in the desired cognitive activity when the treatment is actually being applied? Offline is do we observe it afterwards?

01:21:34 
 And 40 minutes afterwards, The discrimination attenuated a little bit, but was still there. So it had a long-term effect on what it would seem to me is the brain's channel bandwidth to the hand. So that is very interesting. There's a lot of other things that ~~**it's**~~ ~~**done**~~ too. Mood, focus. visual acuity and visual discrimination tasks in the same way that I described with the tactile discrimination, you can actually induce complete changes to the visual field or at least noticeable changes to the visual field by inducing these what's called phosphenes. If you imagine when you shut your eyes and you lightly press with your fingers on your eyelids, those ~~**kind**~~ ~~**of**~~ blobby shapes you see, those are phosphenes. You can induce that in a person's visual field using TFUS.

01:22:26 
 Auditory discrimination also goes up. People can~~**,**~~ ~~**or**~~ another finding that interests me too, is ~~**the**~~ people can ~~**feel**~~, people can discriminate between vibrations at different frequencies that are very close together. They can do that discrimination, not substantially, but marginally better~~**.**~~ with TFUS. And this is applied, all of these studies for the most part, are TFUS applied at fairly low dosage levels~~**.**~~ ~~**well**~~ below where ~~**that**~~ thermal effect is seen ~~**that**~~ ~~**I**~~ ~~**mentioned**~~. So there might be an interesting range of capabilities that you could obtain if you were to go beyond that dosage level. And ~~**I**~~ ~~**know**~~ ~~**I've**~~ ~~**talked**~~ ~~**to**~~ ~~**both**~~ ~~**of**~~ ~~**the**~~ ~~**leading**~~ ~~**scientists**~~ ~~**in**~~ ~~**the**~~ ~~**world**~~ ~~**who**~~ ~~**have**~~ ~~**had**~~ ~~**the**~~ ~~**top**~~ TFUS papers in the last 10 years,

01:23:21 
 this point. That's TFUS. And I think, can it write thoughts to the brain? Like, absolutely not. And I don't even want to suggest that I think that's remotely close. I think that's really hard. That's my intuition. But this idea of being able to improve not just mood, but things like channel bandwidth, in the sensory areas, in the senses rather.

01:23:48 
 That's really interesting to me. So ~~**yeah**~~, any questions? Nathan, I can't hear you. HOST: Sorry, my bad. There was a little noise here, so I muted myself for a second. Oh, gotcha. So you had said that the skull is a huge barrier in the most obvious way to getting these signals into the brain. It seems like that's also a problem on the way out, but we compensate for it by just ~~**like**~~ having a bunch of electrodes all around and collecting the mess and then trying to clean up the mess.

01:24:16 
 But on the way in where the precision really matters, that becomes a big barrier. Is there ~~**like**~~ an evolutionary story for why the brain would be good at this? Or do you think this is just ~~**like**~~ an accident of biological history that the brain blocks these electrical signals? GUEST_01: ~~**I**~~ ~~**think**~~ it's probably an accident of biological~~**.**~~ ~~**I**~~ ~~**think**~~ it probably has to do with the fact that it's probably useful for all kinds of other reasons for bone~~**,**~~ just in general, to be low in conductivity. ~~**and**~~ the skull is made of bone and you just benefited there. ~~**I**~~ ~~**have**~~ ~~**no**~~ ~~**idea**~~ ~~**though**~~. That's an interesting question.

01:24:50 
 I'd love to ask an evolutionary biologist that question. ~~**I**~~ ~~**will**~~ ~~**say**~~ ~~**though**~~, with TFUS, the exact same imaging problems are present. Weirdly enough, in certain cases, ~~**I**~~ ~~**don't**~~ ~~**have**~~ ~~**a**~~ ~~**good**~~ ~~**model**~~ ~~**for**~~ ~~**understanding**~~ ~~**when**~~ ~~**this**~~ ~~**is**~~ ~~**the**~~ ~~**case**~~ ~~**versus**~~ ~~**when**~~ ~~**it**~~ ~~**isn't**~~, but sometimes the skull can actually have a beneficial effect on getting ultrasound signal. into the brain, it can have a lensing effect. If you hit it just right and you're~~**,**~~ ~~**it's**~~ the kind of signal you're trying to create and the specific region of the skull is just right, it can actually have a lensing effect. But in general, ~~**it's**~~ ~~**still**~~, you still have to deal with signal processing. And can you~~**,**~~ modifying your signal based on a dynamic model of the

01:25:41 
 Sound is vibration. This is just high frequency vibration. And ~~**I**~~ ~~**don't**~~ ~~**know**~~ ~~**if**~~ ~~**you**~~ ~~**know**~~ the frequency ~~**off**~~ ~~**the**~~ ~~**top**~~ ~~**of**~~ ~~**your**~~ ~~**head**~~ that this operates at. Like middle A or middle C on the tuning note in the orchestra is like 440, ~~**right**~~? So this would be like you go up several octaves we can definitely still hear up into the range of a few thousand hertz it seems and then ~~**I**~~ ~~**guess**~~ this would be like north of ten thousand maybe? GUEST_01: In the range of ~~**I**~~ ~~**think**~~ it's five to ten ~~**usually**~~ in that range five to ten thousand hertz. HOST: Yeah, it's weird, ~~**right**~~? And this is, this basically at a neuron level, this basically amounts to shaking the neuron, ~~**right**~~?

01:26:25 
 It's this thing can fire off its electrical signal a thousand ~~**ish**~~ times a second, or sometimes even faster, as you said earlier. Now we bring in a ~~**lyrical**~~, a literal physical shaking that comes in at an even higher frequency than that. And that seems to wobble things loose and just, this is stimulus, ~~**right**~~? Is there a suppressive effect that can be achieved this way? Or is this purely a stimulus technology? GUEST_01: It has usually been in most of the literature I've seen ~~**is**~~ for excitatory signals, but ~~**no**~~, you can do inhibitory effects as well. So ~~**yeah**~~, you can block certain things too. HOST: And is there, do you have a sense of what is the difference if I'm imagining shrinking myself down to neuron scale and sitting in this region of the brain, ~~**like**~~ it seems that the vibrations coming through, ~~**like**~~ how would they, how would I know whether they're supposed to make me do stuff or not do stuff in my particular local area?

01:27:22 
 GUEST_01: Yeah, that's a very good question. ~~**I**~~ ~~**will**~~ ~~**say**~~ ~~**my**~~ ~~**general**~~ ~~**sense**~~, I'd love to ask one of the scientists who helped me with this research, I'd love to ask them this question. That's a great practical question for them. ~~**My**~~ ~~**general**~~ ~~**sense**~~ ~~**is**~~ ~~**that**~~ most of the inhibitory stuff There's not that much inhibitory stuff that actually gets done. Let me actually, I'm ~~**just**~~ going to quickly pull up my notes and I'll get you an answer to that question on what specifically has been inhibited at least. Because that's, ~~**I**~~ ~~**think**~~, that might answer. GUEST_02: question on this there. GUEST_01: Okay.

01:27:59 
 Yeah. So I'm not sure that inhibitory ~~**like**~~ use of this has... Oh no, reduced reaction time in a motor task. ~~**Yeah.**~~ ~~**Like**~~ you can actually do that. ~~**Yeah.**~~ So it's been... That's right. I'll restart. The inhibitory TFUS, most of it has been used for pain attenuation.

01:28:18 
 So it's looking at the parts of the brain that are sensing pain and targeting that And that seems to work for whatever reason. ~~**I**~~ ~~**don't**~~ ~~**think**~~ that is ~~**well**~~ understood at all. ~~**I**~~ ~~**don't**~~ ~~**know**~~ ~~**for**~~ ~~**sure**~~, but that also has the side effect of reducing motor time, reaction time for ~~**like**~~ various kinds of motor tasks and can reduce some of the things we were talking about. HOST: Yeah, it seems ~~**like**~~ there's also, even if it was purely stimulating activity in a particular region, it's an important note to keep in mind too, more than a footnote, but an important fact to keep in mind that the brain is self-regulating in all sorts of ways, ~~**right**~~? So there is this sort of, we see this ~~**in**~~ ~~**the**~~ at the cellular level too, ~~**right**~~? There is a gene that gets expressed to suppress the expression of another gene, and at the brain level there are regions that activate to suppress other regions. And so you can imagine, even if you could only turn things on

01:29:30 
 GUEST_01: It might well be what they're doing. That could very well be the case. It is definitely inhibitory in terms of activity though. ~~**I**~~ ~~**don't**~~ ~~**have**~~ a good model ~~**for**~~ why that works. The excitatory is at least intuitive. If anything, if you shake the TV set, it generally fixes the signal. There's a primitive part of us that can understand how that works. But ~~**yeah**~~, the inhibitory is a bit more of a puzzle to me as to why that works.

01:29:58 
 HOST: So this question of writing thoughts. I'm not sure we ever would want this exactly anyway. But putting that aside, ~~**I**~~ ~~**can**~~ ~~**start**~~ ~~**to**~~ ~~**imagine**~~ ~~**how**~~ ~~**you**~~ ~~**might**~~ close the loop here. Maybe there are some barriers that I'm not ~~**immediately**~~ seeing. I'm sure there are plenty of challenges that would have to be overcome and I'm not seeing any fundamental barriers. The resolution on this technology was down to a couple ~~**millimeters**~~. That's right. That's right.

01:30:27 
 Yeah. And so that's notably like the same scale as the voxels that come out of the fMRI. And so if I connect this to the MindEye paper again, and I'm ~~**like**~~, okay, there are 12 to 17,000 voxels per person back in the visual cortex. This is, and I'm far from, I got a ~~**little**~~, ~~**I**~~ ~~**got**~~ a mini lesson on this in that episode around ~~**like**~~ how much semantic information is encoded in the visual cortex. Is that just raw sensory workup until ~~**kind**~~ ~~**of**~~ lines and edges, and then the front of the brain does the sort of that's a tiger type stuff. And he said, this was Paul, the author of this paper, he said, no, there is still in the back of the brain as part of the visual cortex, there is a lot of semantic information that understands what it is and actually recognizes conceptually what it is you are looking at. So from these 12 to 17,000 voxels, two-ish millimeters cubed, they're able to extract both a general

01:31:45 
 and that's because there is enough semantic information there that not only do you have this ~~**kind**~~ ~~**of**~~ blurry, purely visual information, but you also have this conceptual information. So anyway, the point there is that you could read that at a two millimeter cube level and now you're saying that you can also focus this signal down to that same ~~**kind**~~ ~~**of**~~ scale It seems like the possibility, at least conceptually, exists to create a feedback loop where you might say, okay, I want to send some signal. How do I know that I sent that signal? I also need to then read the signal back. So I don't know how you can focus the signal down to a couple millimeters region, but maybe you can't do that 10,000 wide. Maybe you would have to do that 10,000 voxels at a time to create meaningful higher order concepts, to induce meaningful higher order concepts in the brain. But it does seem like you ~~**at**~~ ~~**least**~~ now start to have some ability to make a perturbation and then also read it on the other end. And if you have a sense for what it is you're targeting,

01:32:54 
 There seems to be at least some ability to start to be ~~**like**~~, okay, I'm going to send a signal and then decode what states arise from that. And was the person thinking about CLAWD3? Yes or no. Then I update my~~**,**~~ ~~**my**~~ network that is deciding what signal to send in based on the signal that is later read out. And this seems like a leap, but given what we have seen work, It doesn't seem too crazy to think that you could start to close this loop with, was I able to induce what I was trying to induce? Maybe the reward is too sparse, maybe you need to have ~~**like...**~~ broader regions subject to receiving a signal at the same time to really get anywhere. If you can only target one two millimeter cube, maybe that just gets lost in the overall broader state of the brain. But I'm starting to at least imagine how you can close a loop and begin to reliably induce certain things because you can read those things and that gives you some ability to correct or to gradually learn how to do the inducement in the first place.

01:33:59 
 GUEST_01: Hey, Nathan. HOST: You're back.  GUEST_01: Sorry about that. ~~**I**~~ ~~**have**~~ ~~**no**~~ ~~**idea**~~ what happened. The internet just died. HOST: No worries. It happens. ~~**Yeah**~~.

01:34:05 
 All good. Let's just make sure that your upload gets off of 0%. There we go. ~~**All**~~ ~~**right**~~. You're back into uploading. ~~**Yeah**~~. So I was just sketching out this sort of possible cycle of If the reading gets good, then you could start to get clarity on did you effectively induce the state that you meant to induce, and then you could create a network that would be trainable with the usual tools of gradient descent and back propagation to induce those states and then confirm that you did ~~**in**~~ ~~**fact**~~ induce them by the reading side. And so it seems like there is this at least conceptual possibility of closing the loop.

01:34:47 
 And then I'm imagining if you could only target one ~~**two**~~ millimeter cube at a time, if you could only target one out of 12 to 17,000 voxels of the visual cortex, ~~**like**~~ maybe that's just not enough to do anything. You might have to be able to target a bunch of them. Maybe, ~~**I**~~ ~~**don't**~~ ~~**know**~~, ~~**I'm**~~ ~~**not**~~ ~~**sure**~~ what ~~**other**~~ what are their ~~**like**~~ big barriers there would be to doing something like this. But it also seems ~~**like**~~ there's plenty of possibility that things could just be quite odd. And maybe you don't actually need that if you could really figure it out, if you really could figure out how the brain works, ~~**like**~~ all these sort of thoughts arise somewhere, and they presumably arise ~~**like**~~, locally first, there's some sort of trigger that ultimately propagates through broader regions of the brain, but starts with some sort of input or some sort of signal that, ~~**you**~~ ~~**know**~~, becomes dominant in the moment and ultimately rises to the level of

01:35:57 
 I'm gonna try to induce these things. I'll measure, did I, how close did I come? I'll update on that. And even if I don't have the ability to stimulate that much of the brain at a single time, potentially I can find ~~**like**~~ the key that unlocks the lock to induce the states that I want to induce. Possibly crazy, but what are the what, if anything, would say that can't happen? GUEST_01: ~~**I**~~ ~~**think**~~ the main thing is that we don't really understand at all, as far as I know. We don't have a good understanding of how information is coded, is encoded in the brain. And it can be read back when you're doing things like the mind eye, the fMRI, and especially when you're pairing it.

01:36:39 
 Like in the visual, when you're looking at the visual cortex, you can extract out of it, but actually the right process, ~~**like**~~ how do you do that? Where does the right happen? Probably more than one place, right? There's the thing that distinguishes brains from even the most sophisticated neural networks is just how much crosstalk there is and how much weird resonance and just all sorts of feedback that's constantly being conveyed. And ~~**I**~~ ~~**don't**~~ ~~**think**~~ we ~~**have**~~ a good understanding of that at all. Would ~~**I**~~ ~~**be**~~ ~~**shocked**~~ If you were able to induce a simple thought in someone's mind by firing ultrasound at V2, the visual cortex, no, ~~**I**~~ ~~**wouldn't**~~ ~~**be**~~. But no, ~~**I**~~ would be pretty surprised if that worked.

01:37:20 
 It wouldn't be a galloping shock to me because of the informational aspect of that. I'm not sure that high dimensional information can be~~**,**~~ ~~**first**~~ ~~**of**~~ ~~**all**~~, encoded in instructions to ultrasound, to ultrasonic transducers. ~~**and**~~ ~~**then**~~ ~~**successfully**~~ transmitted into the parts of the brain, probably several, where it would need to go in order to actually write information, particularly of any... I'm thinking of the moment in The Matrix where Trinity downloads the instructions for how to operate a helicopter, thinking of that as being your North Star. That seems hard to encode in all the various ways they would need to be encoded. But I wouldn't rule it out. At the very least ~~**though**~~, brain activity is possible to stimulate. And it is possible to change the mood of the wearer, the level of focus that they have, things of that nature.

01:38:19 
 All possible. We don't know how much and how high dimensional that can get. ~~**Obviously**~~, a good mood is a pretty general concept that could be refined quite a bit. There's a few TFUS devices that are on the market. There's a company called Prophetic. AI that is based in New York. They've had a lot of buzz on ~~**X**~~ and other social media with some of their various announcements. And there's a couple of things that are really interesting about what they're doing, ~~**I**~~ ~~**think**~~, because it is actually the convergence of a few of the things that we've talked about in this discussion.

01:38:58 
 First of all, they have developed what they call an ultrasonic transformer, which is a transformer based system trained on EEG and fMRI data which takes as its input EEG readings from the headband that they're going to sell and outputs instructions to the ultrasonic transducers also on that same headband. So it's inferring your brain state and then it's turning that into instructions for neuromodulation. And ~~**there**~~ Ambition is to do this with lucid dreaming. So the idea is that it's actually a pretty easy signature. From ~~**a,**~~ ~~**from**~~ an EEG perspective, this is not hard. Like you can absolutely recognize when somebody is lucid dreaming while they're sleeping because ~~**you**~~, the neuro signature of sleep is pretty consistent, pretty easy to recognize even for low channel EEG. And then there's a spike of gamma waves in the prefrontal cortex. It recognizes that spike and then it just keeps~~**,**~~ it directs the transducers to just keep firing to maintain that brain state and keep you in that lucid state.

01:40:13 
 I'm not, this, they actually, they might have slight corrections to that description, but the basic idea is that it can keep you inside of a lucid dreaming state. They also have ambition to do quite a lot of other stuff, conscious experiences of all kinds, much ~~**kind**~~ ~~**of**~~ higher dimensional conscious experiences. They want to start with focus and a positive mood, but ~~**I**~~ ~~**think**~~ ~~**that**~~ they have ambition to go beyond that. A lot of other people that are operating in the space have ambition to go beyond that. And again, you're writing a positive hard record. So just like you are with Eden. HOST: Yeah, ~~**the**~~ ~~**how**~~ high dimensional ~~**is**~~ ~~**thought**~~ ~~**is**~~ ~~**it's**~~ ~~**really,**~~ ~~**I'm**~~ ~~**thinking**~~ ~~**about**~~ the representation engineering work that recently came out of ~~**Case**~~ and other, there's been a bunch of ~~**authors**~~ on that paper, but they identify these high order concepts through a bunch of ~~**kind**~~ ~~**of**~~ clever, but honestly, it's

01:41:38 
 and then they can start to use that vector direction to detect those states in further downstream inferences ~~**blind**~~, ~~**right**~~, just looking at the activations and saying is this a happy state or is it an unhappy state. And then they can also start to layer those in and shape behavior based on just inserting that kind of just adding it to the whatever the thing is doing at a given time. If you just add on the happiness direction, then you can see that you actually steer the direction ~~**you**~~ steer the downstream model behavior in ~~**in**~~ a reasonably intuitive direction where it seems like, OK, now it is actually generating like happier outputs. So that is way, way easier to do in a neural network than it is to do in the brain. And the dimensionality of that is pretty high. So it might just be an engineering challenge that is like, at least in a non-invasive way, just too crazy ~~**.**~~ to get to, but it does seem like we have something pretty similar going on. And you're citing all these examples where it's, you're doing that at basically a low dimensional, very crude sort of way.

01:42:46 
 And as we get into these higher and higher level concepts, ~~**I**~~ ~~**also**~~ ~~**think**~~ about the anthropic sparse autoencoders with this, where they show that basically from these high dimensional states, you want to identify these human intuitive concepts. And the more space you give the sparse autoencoder, the more concepts it will find. They call this ~~**like**~~ feature splitting. So if you don't give it that many, if you give it ~~**like**~~ a limited number of concepts that it can differentiate its activation space into, then you'll get high order, high level, more general concepts out. And if you give it a lot more space, then the features will split and you'll start to see these ~~**like**~~ very granular level representations. And it seems ~~**like**~~ we're just operating right now at a very general level where ~~**like**~~ the features we have ~~**not**~~ ~~**yet**~~ ~~**got**~~ to the level of resolution where the features can split. And so we're making these very basic modifications to mood but it seems ~~**like**~~ it's really a question of resolution more than anything else over time. GUEST

01:43:54 
 Resolution and maybe it might also just be data. It might just be that we don't have a lot of the same data problems that we talked about earlier. And then obviously ~~**like**~~ with TFUS in particular, that's even more limited of a data set. It might be some combination of all those things. And ~~**Yeah**~~, but it also is... ~~**I**~~ ~~**don't**~~ ~~**know**~~ how far it can go. In principle, any conscious state is inducible, but how far can that go with non-invasive? My guess would be somewhere between exactly what I'm experiencing right now at the high end and being happy at the low end.

01:44:33 
 I would suspect you can do more than that, more than being happy, less than what I'm experiencing right now. But where exactly? ~~**I**~~ ~~**think**~~ we just have to find out. It is~~**,**~~ ~~**though**~~, possible. It takes some time to even get your head around what that would be like. ~~**I**~~ ~~**think**~~ the closest thing is probably drug use. ~~**I**~~ really don't want to say that this is like using a drug, because ~~**I**~~ ~~**think**~~ that this might be much more than that. And ~~**I**~~ don't even necessarily mean illicit drug use, ~~**I**~~ just mean something that's meant to enhance your mood.

01:45:06 
 But even then, I've never used,~~**I**~~ have no experience with things like that. But even then, my guess would be that because of the sort of slower mechanism of action that most pharmaceutical mood drugs have, it's going to feel like it's coming from inside you more. Whereas something like TFUS, you will feel good or focused suddenly, and then not. ~~**I**~~ ~~**don't**~~ ~~**know.**~~ ~~**I**~~ ~~**don't**~~ ~~**know**~~ exactly how long the feelings persist. ~~**I**~~ would be shocked if it were like a steep drop off, like a completely cliff, but it is also probably fairly quick that it drops off. ~~**Yeah,**~~ ~~**I**~~ ~~**think**~~ all that is, there's a lot to think about. And certainly, ~~**yeah,**~~ ~~**I**~~ would guess that the experience of using something like the prophetic headband to lucid dream.

01:45:57 
 Yeah, I think if you use that every day, it would be like taking LSD every single day. And you would eventually go into a Sid Barrett doom loop would be my guess. But ~~**yeah**~~, how far can you go? ~~**I**~~ ~~**have**~~ ~~**no**~~ ~~**idea**~~. It's very early days. This is the kind of thing that ~~**like**~~, this is very much at the frontier. At the same time, immediately ~~**into**~~ integratable, into consumer hardware, not expensive. And FDA approved below a certain limit.

01:46:24 
 And that might actually be a good segue. I'm happy to explore this more. That might be a good segue into some of the policy. ~~**I**~~ ~~**am**~~ ~~**a**~~ ~~**policy**~~ ~~**person**~~, ~~**so**~~ ~~**I**~~ ~~**have**~~ ~~**these**~~ ~~**kinds**~~ ~~**of**~~ ~~**questions**~~ ~~**too**~~. ~~**I**~~ ~~**would**~~ ~~**hesitate**~~ ~~**to**~~ ~~**offer**~~ ~~**policy**~~. What are the basic questions we should be asking, which are probably on everybody's mind right now, in a certain sense. HOST: Before we go into policy, let me just ask one more ~~**kind**~~ ~~**of**~~ technical intuition question. If I had to make a high-level guess based on everything we've talked about, it feels like I would put my money on, we're going to have pretty good brain reading in consumer devices over the next few years because the hardware is already there and the cost curve is good and the data is about to explode and the best thing to interpret one neural net is another neural net.

01:47:15 
 On the other end of the ~~**right**~~ ~~**or**~~ the modification to BrainState's side, it seems like resolution and just the overall ability to accurately send the signal that you want to send is limited and seems like it's probably going to continue to be limited relative to the bandwidth you would actually need to create ~~**like**~~ really rich input signal to the brain from outside the skull anyway. And so we're probably headed for something that is ~~**like**~~ fairly crude and valence level but ~~**not**~~ we're not going to be invoking or ~~**reducing**~~ thoughts of ~~**cloud**~~ ~~**three**~~ ~~**opus**~~ with precision with those ~~**sort**~~ ~~**of**~~ non-invasive techniques. And so if those things are to be done, it seems like they would require invasive techniques. And at that level, then you can ~~**like**~~ actually send the signal where you want ~~**the**~~ ~~**obvious**~~ ~~**at**~~ ~~**the**~~ ~~**obvious**~~ cost of having to implant thousands or potentially many thousands of electrodes into the brain. But if you imagine, is there a, what would the path be

01:48:48 
 Intuitively, I completely agree. The only thing I'll say ~~**just**~~ as a caveat is that the spatial resolution of TFUS and the spatial resolution of EEG are basically the same. ~~**As**~~ ~~**far**~~ ~~**as**~~ there might be practical differences in spatial resolution that I'm not thinking of. The obvious one being that ~~**You**~~ ~~**can't**~~, with EEG, you can't differentiate between all the neural signals between individual neurons, but you ~~**aren't**~~ picking up ~~**.**~~ the collective aggregate result of their electrical activity. Whereas with TFUS, maybe it is the case that you need neuron level or close to neuron level targeting to create really rich experiences. But I don't know. At the same time, neural networks tend to have a fair amount of redundancy.

01:49:39 
 Would you say that's a fair observation? That it's possible that you don't need to get down to the level of the individual neuron to ~~**to**~~ do quite a bit. So yeah, ~~**that**~~, but in general, I agree with your intuition. That's about where I am too. HOST: Cool. Then let's get to~~**,**~~ ~~**I**~~ ~~**think**~~, policy. And also, I wonder if you could sketch~~**,**~~ ~~**like**~~, maybe, I don't know if it, who knows what order things happen in, but I'm also interested in~~**,**~~ ~~**do**~~ ~~**we**~~ ~~**have**~~, ~~**do**~~ you have any intuition for as this technology maybe follows its natural course? How does life start to change?

01:50:09 
 Have you started to game out dynamics at all? And policy will be a part of shaping those dynamics. But we've had, ~~**I**~~ ~~**used**~~ ~~**to**~~ ~~**ask**~~ ~~**people**~~ ~~**all**~~ ~~**the**~~ ~~**time**~~, if the Neuralink had reached a million patients, and was generally considered to be safe, would you get one? And Interestingly, I got very different ~~**from**~~ ~~**people**~~ ~~**that**~~ ~~**are**~~ ~~**like**~~ very much on the frontier of AI. I got very different answers there. But some of the interesting answers that I got were rooted in the idea that I'd have to, how else would I keep up? So I feel like there's some game theoretic aspect to this and policy can shape the sort of game theoretic environment, perhaps, and maybe can do other things, too, including discouraging or encouraging ~~**or**~~ discouraging the development of different kinds of technologies. But I'm interested in both ~~**kind**~~ ~~**of**~~ how you think this ~~**in**~~ ~~**the**~~ ~~absence~

01:51:09 
 GUEST_01: Yeah, one of my favorite anecdotes, ~~**I'll**~~ ~~**go**~~ ~~**back**~~ ~~**to**~~ ~~**the**~~ ~~**1910s**~~ ~~**for**~~ ~~**one**~~ ~~**second**~~. The interior design of the average American's house before, and you can just imagine ~~**like**~~ an old house, ~~**right**~~? Victorian house, whatever. Beautiful houses in Detroit, dark red, dark green walls. You ~~**often**~~ ~~**think**~~ ~~**about**~~ those ~~**kind**~~ ~~**of**~~ ~~**like**~~ burgundy. Why? Why were all the walls dark ~~**back**~~ ~~**then**~~? That seems weird.

01:51:34 
 The reason is that interior illumination was provided by kerosene lamps, which stained walls. So you had dark colored walls to hide the stain. White walls are a luxury enabled by electricity. ~~**My**~~ ~~**point**~~ ~~**only**~~ ~~**is**~~ ~~**that**~~ it is very hard to predict the outcomes of what happens when these things are at scale. Obviously, ~~**like**~~ the things ~~**I**~~ ~~**can**~~ ~~**predict**~~ are going to pale in comparison to what's ~~**like**~~ what will actually happen. ~~**That**~~ ~~**being**~~ ~~**said**~~, a couple of things ~~**do**~~ seem apparent ~~**to**~~ ~~**me**~~. ~~**I**~~ ~~**always**~~ ~~**try**~~ ~~**to**~~ ~~**think**~~ ~~**about**~~ ~~**the**~~ ~~**legal**~~ ~~**system**~~ ~~**first**~~. And this is an area that interacts with the legal system~~**,**~~ ~~**I**~~ ~~**think**~~ in really interesting ways.

01:52:15 
 The concept of being under oath changes if we want it to. And that's the question. ~~**And**~~ ~~**I**~~ ~~**think**~~ ~~**a**~~ interesting way of thinking about the next 20 years of technology in general is we want to impose artificial constraints on various aspects of social life and technological development. ~~**And**~~ ~~**I'm**~~ ~~**not**~~ ~~**saying**~~ ~~**that**~~ ~~**I**~~ ~~**think**~~ ~~**we**~~ ~~**should**~~. ~~**I**~~ ~~**don't**~~ ~~**have**~~ ~~**a**~~ ~~**strong**~~ ~~**model**~~ ~~**of**~~ ~~**that**~~ ~~**yet**~~. But it ~~**does**~~ ~~**occur**~~ ~~**to**~~ ~~**me**~~ ~~**that**~~ if we're headed towards the Nicholas Bostrom solved world which ~~**I**~~ ~~**don't**~~ ~~**think**~~ ~~**we**~~ ~~**are**~~, ~~**by**~~ ~~**the**~~ ~~**way**~~. But if we are, if he's right, then one of the one of the

01:53:08 
 I really do believe that. HOST: Can you unpack ~~**the**~~ what exactly you mean by solved world? And then you're probably going there anyway, but more ~~**kind**~~ ~~**of**~~ granular specific questions. I take it that you're asking, would we ~~**You**~~ ~~**can**~~ imagine a lot of different regimes, but would we require you to wear one of these headsets to testify so we can also look at your internal brain states in addition to your ~~**like**~~ spoken testimony? That's the sort of thing that you are getting at, ~~**right**~~? GUEST_01: Yeah, exactly. The solved world is a concept from Nick Bostrom, essentially post-singularity, techno-utopia style thinking, where every conceivable good is available in enormous abundance and humans have godlike powers to assemble atoms in whatever way that they find desirable. Or something does.

01:53:58 
 Some form of conscious life does.~~**Maybe**~~ ~~**not**~~ ~~**us.**~~ But I'm not a big subscriber to any of that sort of techno-utopian thinking. But it does~~**,**~~ ~~**yeah.**~~ Ultimately, it's a very different kind of society. ~~**if**~~ ~~**we**~~ ~~**can**~~ ~~**actually**~~ ~~**know**~~ ~~**at**~~ ~~**a**~~ ~~**biological**~~ ~~**level**~~ ~~**whether**~~ ~~**or**~~ ~~**not**~~ ~~**you**~~ ~~**are**~~ ~~**lying**~~, ~~**for**~~ ~~**example.**~~ That's just a profoundly different society. Do we want that?

01:54:23 
 Is a society with no deception actually a desirable thing? Is an AI model with ~~**a**~~ deception, with no deception rather, actually a desirable thing? I'm not confident the answer to that question, ~~**either**~~ ~~**of**~~ ~~**those**~~ ~~**questions**~~, is yes. In fact, I'm pretty confident that the opposite is the case. I'm pretty confident ~~**some**~~ degree of deception is an important part of life. ~~**I**~~ ~~**think**~~ ~~**some**~~ degree of deception is probably an important part of judicial life. We assume that there will be ~~**some**~~ degree of bending of the truth And you could probably model that. You could probably model someone telling a white lie.

01:55:01 
 You could probably model all different nuances of deception. And do we really want to do that? ~~**I**~~ ~~**don't**~~ ~~**know**~~. And then~~**,**~~ ~~**of**~~ ~~**course**~~, also when it comes to the judicial case, ~~**I**~~ ~~**think**~~ ~~**that**~~ thinking about court is a fascinating way to think about how society will digest this. It's not the only way, but ~~**I**~~ ~~**just**~~ ~~**think**~~ it's a very concrete way to think about it. What are the limits on something like a warrant or a subpoena? ~~**in**~~ a world where varying degrees of dimensionality into human thought is recorded and in theory something that you can be examined for a court case. All that is~~**,**~~ ~~**I**~~ ~~**have**~~ ~~**a**~~, my instinct on that kind of a question is that it's probably beneficial to a certain point But there is an extreme where if you~~**,**~~ if everyone really has Neuralink and really~~**,**~~ truly every thought you have can be recorded in a high dimensional sort of compression, probably high resolution compression

01:55:56 
 You probably don't want that to be able to be available to~~**,**~~ ~~**certainly**~~ not for advertising purposes. That's another thing ~~**I**~~ ~~**think**~~ about. Within limits, it seems fine. At the extreme, it seems dangerous to have to figure that one out. And ~~**I**~~ ~~**think**~~ we will~~**,**~~ ~~**right**~~? My general sense is that from a policy perspective~~**,**~~ ~~**sure**~~. ~~**I**~~ ~~**mean**~~, if this exists, the Europeans will regulate it~~**,**~~ ~~**right**~~? ~~**Can**~~ ~~**we**~~ ~~**know**~~ ~~**that**~~?

01:56:22 
 And half of America at least will want to regulate it. Whether or not they'll be able to is a separate question. ~~**Actually,**~~ ~~**it's**~~ ~~**probably**~~ ~~**worth**~~ ~~**talking**~~ ~~**about**~~ ~~**just**~~ ~~**for**~~ ~~**a**~~ ~~**moment**~~, what the current regulatory state of all this stuff is. So these are not considered to be high risk medical devices by the FDA, by and large. They fall into this category two, this mid level of risk, but there's an exception to that, which allows you to evade most of the FDA's regulatory processes. ~~**if**~~ ~~**you**~~ ~~**are**~~ ~~**marketing**~~ ~~**a**~~ ~~**general**~~ ~~**wellness**~~ ~~**device**~~. So ~~**you**~~ ~~**will**~~, ~~**I**~~ ~~**remember**~~ ~~**when**~~ ~~**the**~~ ~~**Apple**~~ ~~**watch**~~ ~~**came**~~ ~~**out**~~ ~~**with**~~ ~~**its**~~, ~~**it**~~ ~~**had**~~ ~~**a**~~ ~~blood~

01:57:07 
 It was like in 2020 that watch came out. Apple was extremely careful and blood oxygen is an important measure for COVID. Apple was very careful to say, this has nothing to do with measuring for COVID. This is purely telling you your blood oxygen level. Because if you connect the device to diagnosing or treating any specific medical condition, then all ~~**of**~~ ~~**a**~~ ~~**sudden**~~ you're in a whole different world ~~**.**~~ ~~**from**~~ a regulatory perspective. Neural technologies have generally gone for this general wellness exemption from the FDA. The FDA has not actually been clear that exemption applies to them.

01:57:46 
 They've been asked to make that clear and they have refused to ~~**do**~~, which is a common thing that the FDA and other aspects of American bureaucracy tend to ~~**do**~~. Anyone who knows cryptocurrency will also be familiar with this, that ~~**A**~~ regulatory tactic is actually uncertainty. That is a tactic. That's a form of regulation that regulators use, is creating uncertainty and creating gray areas. So that's where most of these neural technologies exist right now, is sitting in a gray area. They're being marketed, and the reason that the FDA ~~**does**~~ that, ~~**I**~~ ~~**suspect**~~, ~~**I'm**~~ ~~**not**~~ ~~**in**~~ ~~**their**~~ ~~**heads**~~, but the reason that they ~~**do**~~ that is maybe they don't know. Maybe they don't know how they feel about it. That could be true.

01:58:23 
 But also for sure, they probably want to preserve the optionality to cut back on this stuff if they want to, and to remove all the general wellness devices from the market in a heartbeat. But at the moment, If you could make a non-invasive brain-computer interface that can induce all kinds of conscious experiences, and as long as you're not trying to treat or diagnose a specific medical condition, and as long as you're staying below certain thresholds of ~~**DFUS**~~ ~~**and**~~ other things, ~~**like**~~ there's certain safety guidelines, but as long as you're mechanically underneath those things, then you can just sell this on the general market. That's how it currently applies. And ~~**I**~~ ~~**guess**~~ the final part of your question was about long-term dynamics of adoption. And ~~**yeah**~~, ~~**I**~~ ~~**think**~~ if these things are cognitive enhancement devices, then there will be evolutionary incentives. There are evolutionary incentives to do all sorts of things right now that most people do not do, ~~**right**~~? We have enough data to know that ~~**I**~~ have an evolutionary incentive to go on a jog after

01:59:33 
 Probably not. That's the threat, or that ~~**I**~~ ~~**should**~~ only eat whatever Andrew Huberman recommends. All that stuff is true, and yet we don't do it, ~~**right**~~? We don't do it all the time. We shouldn't be drinking alcohol. ~~**Everyone**~~, ~~**not**~~ ~~**everyone**~~, but a lot of people drink alcohol. ~~**I**~~ ~~**don't**~~ ~~**know**~~. ~~**I**~~ ~~**think**~~ it is a mistake that a lot of people, and ~~**I**~~ see this in the AI safety community in general, a lot of people think that legible Darwinian evolutionary impulses are going to drive technological adoption ~~**of**~~ ~~**society**~~, ~~**sorry**~~, technology adoption in society.

02:00:03 
 And I think it's more complicated than that. ~~**I**~~ ~~**don't**~~ ~~**think**~~ it is ~~**not**~~ a straight Darwinian evolutionary algorithm that's being applied here. So that's my read on that. And ~~**I**~~ ~~**also**~~ ~~**think**~~ ~~**that**~~ there's a flip side. ~~**I**~~ ~~**think**~~ the impulse that people have a lot of the time is to think, oh no, if there's an incentive to use this thing, then everybody will have to use it, and what about the people that ~~**don't**~~ want to use it? ~~**I**~~ ~~**go**~~ ~~**there**~~ ~~**too**~~. ~~**I**~~ ~~**have**~~ sympathy for those people. ~~**I**~~ ~~**think**~~ about the Amish, and ~~**I**~~ ~~**do**~~ ~~**think**~~ ~~**that**~~ there's probably going to be forms of digital Amish in the future that we need to be thinking about.

02:00:41 
 At the same time, the people who want to enhance their cognition also should have the liberty to do that. And we should want there to be more cognition in the world, especially human cognition. There's a part of me that says, ~~**my**~~ ~~**God**~~, the idea that we would want there to be less human cognition in a world where GPT-5 is right around the corner, I am not sure from what world model that impulse derives. But it is ~~**a**~~ ~~**role**~~ ~~**model**~~ that I could poke holes at. Not that it's wrong, but I could certainly poke holes at it. So that's how my mind is not at all made up on any of this, but that is how I model this right now. HOST: I know you're potentially just a couple doors down the hall from Robin Hanson, who's also a recent guest on the podcast. I think he makes ~~**a**~~ extremely compelling case that we are in ~~**a**~~ what he calls strange dream time between what are almost sure to be much longer eras, both before and after us.

02:01:41 
 in which evolutionary dynamics and just the practical constraints of ~~**like**~~ available resources are in fact the dominant drivers of how things unfold. And we're in this weird moment now where we've created way more capital per capita and birth rates are down. And just, it seems ~~**like**~~ the sort of strangeness probably can't last in the course of evolutionary time scales. Then he also, it's funny you mentioned the Amish too, because he also has a part of his near-term world model, which ~~**I**~~ ~~**think**~~ is less compelling personally, but you can debate that with him ~~**.**~~ over lunch perhaps, where he thinks because of the higher birth rates, we may be headed for a period of, or because of the lower birth rates in general, but higher among the Amish, we may be headed for a period of technology stagnation and Amish domination. So that gets a little fine grained in terms of ~~**like**~~ how precise the crystal ball has to be to advance a theory ~~**like**~~ that for me. But there's definitely a couple of interesting lunch debate topics for you now at the Mercatus Center with him. GUEST_01:

02:02:41 
 And I think part of his point is not to put words in his mouth at all, but we have a lot of artificial hyperparameters on the way society works. And those things are called laws and policies. And that's what I study for a living. And ~~**yeah**~~, they've created all kinds of unintended weird effects. And that's not to say they're bad or good or really anything, just to say that they exist. And to say that things don't proceed according to mathematical models of the way that ~~**history**~~, ~~**nature**~~ has unfolded in the past for those reasons, because they didn't have occupational licensing a million years ago. And that does change the way that evolution works at a certain level, at least the evolution of society. And ~~**I**~~ ~~**guess**~~, ~~**yeah**~~, ~~**like**~~ just one other point about the ~~**kind**~~ ~~**of**~~ digital Amish thing and just in general, my read on, there's some polling that gets put out about AI and things like that.

02:03:33 
 I don't put a lot of stock in issue polling personally. And ~~**I**~~ ~~**think**~~ ~~**that**~~ those polls are not motivated. ~~**I**~~ ~~**don't**~~ ~~**think**~~ they're coming from organizations that are motivated to find the truth. ~~**I**~~ ~~**will**~~ ~~**just**~~ ~~**put**~~ ~~**it**~~ ~~**that**~~ ~~**way**~~ about what Americans think. ~~**I**~~ ~~**think**~~ the questions are along the lines of, Someone's going to make HAL 9000. How do you feel about that? It's coming to kill you. It's going to be here next year.

02:03:54 
 What do you think? 98% of Americans are opposed to. ~~**Okay.**~~ That's not that useful of a question, but issue polling is also not that useful of a field unless you do it really carefully. But ~~**I**~~ ~~**do**~~ ~~**think**~~ ~~**that**~~ this is going to, ~~**that**~~ these things, adoption of technology is going to start to have a political valence to it. And whether that is coded as AI or whether that ends up getting coded in terms of these neural technologies or the Apple Vision Pro, is the Apple Vision Pro a political statement ~~**of**~~ ~~**a**~~ ~~**sort**~~? ~~**Yeah,**~~ ~~**I**~~ ~~**see**~~ ~~**it**~~ ~~**as**~~ ~~**one**~~ ~~**already.**~~ ~~**I**~~ ~~**think**~~ a lot more people will ~~**in**~~ ~~**the**~~ ~~**future**~~.

02:04:29 
 So I don't know exactly how to model that. But ~~**I**~~ ~~**do**~~ ~~**think**~~ ~~**that**~~ we should expect this all to become more political, not just about who gets to control the existing platforms, but about sort of whether you are interested in these technologies at all. HOST: Yeah, unfortunately, I would love to see the discourse around AI and technologies like this remain separated from day to day political discourse as long as possible, just because it seems like everything gets worse once it gets cast through the lens of ~~**certainly**~~ partisan politics. ~~**I**~~ ~~**would**~~ ~~**agree**~~ ~~**with,**~~ ~~**I'm**~~ ~~**not**~~ ~~**sure**~~ ~~**I**~~ ~~**agree**~~ ~~**with**~~ ~~**your**~~ ~~**view**~~ ~~**on**~~ ~~**polling**~~ ~~**in**~~ ~~**as**~~ ~~**much**~~ ~~**as**~~ ~~**I**~~ ~~**would**~~ ~~**say**~~ ~~**like**~~ my read of a lot of those answers is that they~~**,**~~ ~~**I**~~ ~~**certainly**~~ ~~would

02:05:44 
 So I always advise people that we don't have too many chat GPT novices listening to this show. But ~~**again**~~, when I get outside the bubble and present ~~**like**~~ business leaders or whatever, I'm always just like, okay, first thing you got to do. Spend some actual time with the technology. Like you need to develop your own~~**.**~~ experiential understanding of what this stuff is. You can't just have it all be filtered through the media for you because the surface area is just so vast and the range of different use cases. And it's just so big compared to what you can get in ~~**a**~~, ~~**in**~~ an article or whatever. So you really do need to get hands on with it, feel it, mess around with it, give it your data, see how ~~**to**~~ ~~**read**~~, see how it reacts to stuff that is really personal to you.

02:06:27 
 And if that's useful to you,~~**and**~~ that also gives you a great sense of its strengths and weaknesses in doing that. Maybe in closing, I wonder if you could recommend some things that people might do to start to get themselves acclimated to this ~~**merge**~~ ~~**technology**~~ ~~**tree**~~. We are, again, a lot of people that follow this show will be very familiar with the language models. They'll have a sense for where AI is and some sense of where it's headed. I would guess that most people have never worn any of these devices. Even starting with an Apple Vision Pro, I would still guess that ~~**some**~~ 5% of the audience has even done the demo at the Apple store at this point. So what would you suggest? Is it ~~**like**~~ going and watching the demo videos on the company websites that we can put links to these companies in the show notes?

02:07:17 
 Is it going and trying an Apple vision pro? Is it what~~**,**~~ ~~**what**~~ do people do to start to. Orient themselves to this and start to develop their own, not just ~~**like**~~ through the media, not just by listening to you and me, but how do they get their own sense for how they should start to feel about this technology? GUEST_01: Yeah. First of all, ~~**I**~~ ~~**would**~~ ~~**just**~~ ~~**say**~~ in response to the polling thing, I totally agree with you. People are pessimistic about this. That is a fact. ~~**I**~~ ~~**think**~~ ~~**that**~~ the techno-optimists like myself, we have an uphill battle to fight.

02:07:45 
 So I don't want to suggest that the polling is ~~**like**~~ manipulating the reality. ~~**I**~~ ~~**think**~~ it's exaggerating a tendency that already exists. That's how I would put it. Anyway, it's a very good question. ~~**I've**~~ ~~**never**~~ ~~**thought**~~ ~~**exactly**~~ about what other people should do, ~~**I**~~ ~~**guess**~~. ~~**my**~~ ~~**somewhat**~~ ~~**myopic**~~ ~~**reaction**~~ ~~**is**~~ ~~**like**~~ the path I took was useful enough for me. There's a good book about all this that came out recently called The Battle for Your Brain by Nita Farahani. She's done some great reporting, more reporting almost, about this kind of stuff.

02:08:23 
 And I would recommend reading that.  ~~**I**~~ ~~**actually**~~ ~~**would**~~ ~~**recommend**~~ ~~**trying**~~ ~~**on**~~ an Apple Vision Pro. And the reason for that is that you would be shocked how close your gaze is to a kind of neural interface. ~~**and**~~ ~~**it**~~ ~~**is**~~ ~~**something**~~ ~~**we**~~ ~~**didn't**~~ ~~**talk**~~ ~~**about**~~ ~~**at**~~ ~~**all.**~~ But ~~**it**~~ ~~**is**~~ ~~**probably**~~ ~~**the**~~ ~~**case**~~ ~~**that**~~ the ideal form factor for any of this kind of neural technology we're talking about is probably something that wraps around the head, much as the MetaQuest and the Vision Pro do. And so when you start to combine the idea of we've already got eye tracking, and we've got hand tracking, and now we're adding in the neural technology, maybe the neural signal doesn't need to be that good to do something really interesting, ~~**right**~~? That's another~~**,**~~ ~~**it's**~~ a whole different angle 

02:09:13 
 So that's just one note. But ~~**yeah**~~, ~~**I**~~ ~~**would**~~ try a Vision Pro. HOST: It is, by the way, a breathtaking experience. ~~**I**~~ ~~**was**~~ really floored by just the demo. And ~~**I**~~ haven't bought one yet, only because ~~**I'm**~~ not sure how much content there is and how much time ~~**I**~~ ~~**would**~~ really spend in it. But it ~~**was**~~ definitely an eye opener for me in the sense that it ~~**was**~~ ~~**like**~~ akin to a GPT-4 type experience where ~~**I**~~ ~~**have**~~ the Oculus 2 and when ~~**I**~~ first got the GPT-4 access, ~~**I**~~ ~~**had**~~ ~~**been**~~ using a lot of GPT-3. But the step up in terms of the quality and the ~~**like**~~, Oh my God, ~~**like**~~ this is just an arrestingly different experience. It is a similar, ~~**like**~~ you have to experience it to feel the difference.

02:10:02 
 I can tell you how much better GPT-4 is versus GPT-3, but the best way is to get your hands on~~**,**~~ ~~**I**~~ ~~**would**~~ ~~**say**~~ the same thing. If even if you have done ~~**like**~~ relatively recent VR, but not done the Apple vision pro yet, ~~**I**~~ ~~**would**~~ ~~**say**~~ You do owe it to your own worldview, if not necessarily to buy it, but at least to get that sense of what this thing can do, how high resolution it can be, how immersive it is, just how compelling the overall sensory experience of it is. Because it is genuinely next level and it absolutely feels to me like a part of the future. We haven't quite figured out how to use it yet. The experience of it is~~**,**~~ ~~**yeah**~~, this is definitely going to be a thing. GUEST_01: ~~**Yeah**~~, ~~**no**~~, ~~**I**~~ ~~**totally**~~ ~~**agree**~~. And ~~**I**~~ ~~**think**~~ ~~**that**~~ it's not entirely a neural interface, but it's

02:10:54 
 of actual empirically demonstrated knowledge and not narrative. Because ~~**I**~~ ~~**think**~~ the narrative surrounding this technology in particular is likely to be quite toxic and quite misleading in a variety of different ways. So model it for yourself. That is ~~**like**~~ my best advice. Don't rely on someone else's model, including mine. HOST: in the future technology scouting business, it is important to maintain always a high degree of epistemic humility. And ~~**I**~~ ~~**think**~~ that's a great note potentially to close on. Is there anything that we didn't talk about that you ~~**think**~~ is important that you want to bring up now?

02:11:33 
 Or we could even maybe, as an aside, we could insert something earlier in the conversation. It doesn't have to be presented in the necessarily fully linear order that we had the discussion. Anything else on your mind? GUEST_01: No, ~~**I**~~ ~~**don't**~~ ~~**think**~~ ~~**so**~~. We talked, ~~**I**~~ ~~**was**~~ ~~**hoping**~~, ~~**I**~~ ~~**was**~~ ~~**thinking**~~ we would go a little more into the Vision Pro, but ~~**I**~~ ~~**don't**~~ ~~**think**~~, ~~**I**~~ ~~**think**~~ we actually went more substantively into the neural subs, and ~~**I**~~ ~~**think**~~ that's probably better for the purposes of this podcast. ~~**Yeah**~~. ~~**No**~~, the only thing is ~~**I**~~ ~~**definitely**~~ ~~**don't**~~ ~~**want**~~ ~~**to**~~ ~~**like**~~ ~~**be**~~ ~~**mean**~~ ~~**to**~~ the AI policy, whatever group it is that does those polls. They annoy me because issue polling, the problem with issue

02:12:10 
 So it's, okay, I think AI is bad. I'm mad about it, but I'm more mad about 5,000 other things. And that gives you a sense of how important it is. And you don't see it. When you look at issue polls that prioritize Americans, it's like, what issues do Americans care about? AI is not in the top 10. ~~**Right**~~ ~~**now**~~ it's 98% because people don't really even understand what it is. And another way of putting it, ~~**the**~~ ~~**equivalent**~~ ~~**questions**~~ ~~**that**~~ ~~**I**~~ ~~**would**~~ ~~**ask**~~ ~~**if**~~ ~~**I**~~ ~~**wanted**~~ ~~**to**~~ ~~**do**~~ ~~**what**~~ ~~**I**~~ ~~**think**~~, ~~**frankly**~~, ~~**that**~~ ~~**organization**~~ ~~**is**~~ ~~**doing**~~, ~~**I**~~ ~~**would**~~ ~~**call**~~ ~~**people**~~ ~~**and**~~ ~~**ask**~~ ~~**things**~~ ~~**like**~~, do you think the government should regulate your iPhone

02:12:49 
 And 98% of Americans would say no. ~~**And**~~ ~~**then**~~ ~~**I**~~ ~~**would**~~ ~~**say**~~ 98% of Americans are opposed to AI regulation, but ~~**I**~~ ~~**wouldn't**~~ ~~**do**~~ ~~**that**~~ because ~~**I'm**~~ ~~**not**~~ ~~**interested**~~ ~~**in**~~ ~~**that**~~ ~~**kind**~~ ~~**of**~~ ~~**thing**~~. ~~**But**~~, ~~**but**~~ ~~**I**~~ ~~**just**~~ ~~**don't**~~ ~~**know**~~ ~~**how**~~ ~~**much**~~ ~~**it**~~ ~~**advances**~~ ~~**the**~~ ~~**epistemic**~~ ~~**ball**~~. ~~**That's**~~ ~~**all**~~ ~~**I'm**~~ ~~**going**~~ ~~**to**~~ ~~**say**~~, but ~~**I**~~ ~~**don't**~~ ~~**want**~~ ~~**to**~~ ~~**be**~~ ~~**mean**~~. ~~**So**~~ ~~**maybe**~~ ~~**don't**~~ ~~**include**~~ ~~

02:13:14 
 GUEST_02: Let's do it. HOST: Dean W. Ball, Research Fellow at the Mercatus Center and author of the Hyperdimensional Substack, thank you for being part of the Cognitive Revolution. GUEST_01: ~~**Thank**~~ ~~**you**~~, Nathan.

