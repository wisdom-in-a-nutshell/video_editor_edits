00:00:00 
 SPEAKER_01: take or anything like that,  any interruptions that happen or coughs or whatever, that's all fine. We'll take that stuff out. We ~~**also**~~ will offer, this is typically more for ~~**like**~~ commercial folks from ~~**like**~~ DeepMind or whatever, but we will offer if you want, you can take a review of our edit before we publish, and then you could request any additional edits or if you find anything to be an issue there. Most people don't take us up on that because it does take time to review things, but you're welcome to if you want to, totally up to you. It's an offer, but not an obligation. ~~**Yeah**~~, ~~**it**~~ ~~**makes**~~ ~~**sense**~~. ~~**That's**~~ ~~**cool**~~. Any other questions for me before we dive in?

00:00:35 
 SPEAKER_00: Where do they get uploaded?

SPEAKER_01: We publish ~~**to**~~ primarily ~~**it's**~~ audio only, but we ~~**do**~~ publish to YouTube. So we can use visuals. We just need to be mindful to explain what we're looking at a little bit as we go. And then if it gets really intense~~**,**~~ ~~**whatever**~~, we can just tell people that they should open up the YouTube to see stuff. ~~**Yeah**~~, ~~**it**~~ ~~**makes**~~ ~~**sense**~~. We can also link to your GitHub project and Twitter thread in the show notes. So people have those.

00:00:59 
 readily available to. Yeah, it makes sense. Cool. Good question. Let me just pronounce it one more time and make sure I'm getting it right. ~~**Adam**~~ ~~**Majmoudar**~~. Yep, perfect. ~~**Alrighty**~~, ~~**cool**~~.

00:01:10 
 Adam Majmoudar, creator of the TinyGPU project. Welcome to the Cognitive Revolution. SPEAKER_00: Thank you for having me. SPEAKER_01: I'm excited about this. So you've created this project that really caught my eye called Tiny GPU, in which you describe yourself as having no prior experience creating GPUs, but nevertheless set out to ~~**speed**~~ ~~**run**~~, creating one from scratch in just a few weeks. And ~~**I**~~ ~~**imagine**~~ you have learned an unbelievable amount from doing that. ~~**I've**~~ ~~**learned**~~ ~~**quite**~~ ~~**a**~~ ~~**bit**~~ ~~**just**~~ ~~**from**~~ ~~**following**~~ ~~**your**~~ ~~**progress**~~. And hopefully the audience will be able to learn a lot from following the results of what you did as well.

00:01:47 
 You want to start off by just introducing yourself a little bit and telling us how you got this crazy idea. SPEAKER_00: ~~**Yeah**~~, ~~**sure**~~. I'm Adam. I've been working on a company called Third One for ~~**kind**~~ ~~**of**~~ ~~**the**~~ past three years. Started in college and worked on it full-time during college, and then worked on ~~**that**~~ for a couple of years ~~**in**~~ ~~**the**~~ ~~**after**~~ ~~**year**~~. And ~~**I**~~ ~~**just**~~ stopped working on that recently. And then now I've been doing a couple of deep dives, which I've been sharing on Twitter on a bunch of different industries that are the overlap between what ~~**I**~~ ~~**think**~~ are going to be defining technologies within the next one to two decades and areas that I've had demonstrated interest in for my whole life. And my goal has been how fast can you get to a point of technical competence and industry competence.

00:02:27 
 So it's not just like the technical side of things. It's actually like understanding the industry and how things are going right now, understanding tailwinds and also understanding the opportunity landscape specifically for young people. ~~**Plus.**~~ TinyGPU happens to be the result of the technical part of that, which is ~~**I**~~ ~~**just**~~ finished my deep dive on chips. ~~**I'm**~~ ~~**now**~~ ~~**planning**~~ ~~**to**~~ ~~**use**~~ ~~**other**~~ ~~**ones.**~~ But during that deep dive, my goal was basically ~~**learn**~~ ~~**the**~~ ~~**entire**~~ engineering stack of chips and computation and history of it. And part of that process was ~~**the**~~ basically two~~**,**~~ two weeks sprints. The first two weeks sprint was~~**,**~~ can ~~**I**~~ learn how to make a chip from all the way from EDA, which is like the electronic design side, and then understand how fabrication works and understand the architectural side all in two weeks, which is traditionally something that people ~~**can**~~ spend two ~~**to**~~ two plus years in college to do.

00:03:12 
 And my belief was like, it's basically you can cut it down about 100x time-wise. ~~**Of**~~ ~~**course**~~, you're not going to be learning the same thoroughness as what other people are doing. But the question you ask yourself is the thing that matters, the thoroughness and the quantity of stuff that you learn. ~~**Of**~~ ~~**course**~~, people who spent three years on this know way more than me about lots of different detailed stuff. Or is it that there's some, not really 80, 20, it's more like a 99 one where it's like 1% of the stuff is actually most of the important value. And if you can figure out how to extract that quickly, then you~~**,**~~ ~~**you**~~ get much more value out of the learning. So that was the context behind the project. And so I did the two weeks, spent that time learning how to make chips.

00:03:48 
 And then by the end of that, ~~**I**~~ formed this intuition, which is okay. Now that ~~**I**~~ know this, what's a really cool project ~~**I**~~ can build that will teach ~~**me**~~ a lot. It will also be fun. It also has potential to be interesting and valuable to people, and given ~~**kind**~~ ~~**of**~~ the whole wave of stuff around AI, and you have GPGPUs and tensor processing units and all kinds of AI accelerators, and also just the whole movement around ASICs, ~~**I**~~ thought it would be interesting to add that into the GPU as a foundational way to learn all of that. So that's the context on the product. SPEAKER_01: Love it. Let's get into it. ~~**I**~~ ~~**think**~~ one of the challenges that you noted right up front that ~~**I**~~ hadn't really considered but definitely makes a lot of sense is that there aren't a ton of great learning resources out there for things like this because a lot of the technology is proprietary that includes ~~**like**~~ designs.

00:04:37 
 I've done some of my own prep in realizing that ~~**like**~~ NVIDIA has not published every layer of their technology stack. And so there's questions even to this day, as ~~**I**~~ ~~**understand**~~ ~~**it**~~ about ~~**like**~~ how some of the most advanced GPUs work in some critical ways. And ~~**I**~~ ~~**imagine**~~ ~~**also**~~ ~~**like**~~ the design tools, the software packages, that those are not always super easy to come by either. So maybe for starters, ~~**like**~~ just characterize that landscape a little bit and tell us what sort of learning resources and software tools you were able to avail yourself of. SPEAKER_00: Lots of people will disagree with that statement, but take the core concept out of it. And because of that, there's so many learning resources. If you take an architecture course in college, you'll learn how to build a CPU. There's basically tons of courses around it.

00:05:34 
 And what it means to have it fully documented and taught is not just how it works at a high level. ~~**Oh**~~, here's the different parts. There's ~~**like**~~ the data memory and program memory. ~~**registers**~~, it's actually ~~**like**~~ getting into the control signals ~~**in**~~ ~~**the**~~ low level and understanding how machine language translates into stuff happening~~**,**~~ ~~**the**~~ CPU and stuff ~~**like**~~ that. So it's ~~**like**~~ the connection between the software and the hardware level. And that's really what it takes to be able to completely replicate it. If you want to build a CPU from scratch, at the end of the day, you need to understand how you can actually program it and how you can turn that into low level signals in your design. And so in contrast with GPUs, there's essentially nothing.

00:06:10 
 at that level. So what you'll find is if you go look for NVIDIA and AMD GPU designs, ~~**you'll**~~ ~~**see**~~ ~~**like**~~ high-level architecture and it'll show you all the different pieces of that. And ~~**of**~~ ~~**course**~~ it's a little bit ridiculous to even expect something at the control signal level for designs that large. But actually there's no resources that ~~**dumb**~~ ~~**these**~~ ~~**things**~~ ~~**down**~~ ~~**into**~~ the simple elements that you can at least try to implement something yourself. Now, one person pointed out that Intel, which ~~**I**~~ ~~**did**~~ ~~**not**~~ ~~**look**~~ ~~**into**~~, actually does have pretty low level diagrams. Again, you wouldn't really use it for learning because it's ~~**like**~~ an actual production GPU. So it's not really too easy to understand. And that could be one interesting approach for people.

00:06:45 
 Gotcha. SPEAKER_01: Okay. What about on the sort of tool side? ~~**I**~~ ~~**assume**~~ you were not able to use all the same software platforms that folks at NVIDIA or at AMD, maybe more at AMD because ~~**I**~~ ~~**believe**~~ there's stacks in a more open source, but what were you able to use to actually create these things? SPEAKER_00: ~~**Yeah**~~, ~~**so**~~ ~~**I**~~ ~~**expect**~~ my stack is~~**,**~~ ~~**certainly**~~ my stack is completely different from all the real players. So the thing is, in this space, the EDA tooling, like the electronic design automation, which is what you use to actually convert your architectural designs into an actual ~~**ship**~~ chip layout and also do a lot of stuff optimizing your design, which is basically the entire complexity of the process. Those things cost like hundreds of thousands ~~**to**~~ ~~**millions**~~ ~~**to**~~ millions of dollars per seat and per company. And so an individual can basically never actually use those things.

00:07:30 
 And in terms of shipping an actual production quality layout, you basically need those. They have a complete moat on this industry. The next thing though is there's this project funded by DARPA within the past~~**,**~~ ~~**I**~~ ~~**would**~~ ~~**say**~~ five to 10 years. ~~**I'm**~~ ~~**not**~~ ~~**sure**~~ ~~**on**~~ ~~**the**~~ ~~**exact**~~ ~~**number.**~~ ~~**I'm**~~ called Open Road, and it's an attempt at making an open source EDA software. It's obviously way worse than those real production ones in many ways. It's not feature complete, but for simple cases like what any individual would do or even small businesses would do, it's completely sufficient. And then the other piece, ~~**the**~~ last piece that made all this possible is that~~**,**~~ ~~**again,**~~ in the past decade, there's this company called Skywater, and they basically made this open source EDA ~~**stat**~~ for their own process node, which means that they have their own ~~**boundary**~~ foundry that produces chips at the 130 nanometer scale, which is ~~**just**~~ ~~**like**~~ the size of the gates and the transistors they make.

00:08:19 
 And they opened it with this thing called eFabless, where multiple people can basically pay ~~**like**~~ a much smaller amount to get ~~**like**~~ 100 chips on this process. And that costs ~~**like**~~ 10k. And ~~**this**~~ ~~**other**~~ group called Tiny Tapeout advertise the cost. So they'll be ~~**like**~~, okay, 100 people can now pay for a tiny slot, and we'll put ~~**like**~~ 100 projects on each of these chips. And so because of this incremental cheapening of everything, it now becomes possible for someone ~~**like**~~ me or others to use that basically the SkyWater process node to make a chip. And then you use something built on the OpenRoad stack called OpenLane. It's basically just OpenRoad specifically for that SkyWater stack. And now all that's open source.

00:09:01 
 Obviously, there are some synopsis and cadence, which are the real production tools, but completely functional for what I would want to do, or like any individual hobbyist. Cool. SPEAKER_01: So ~~**do**~~ ~~**I**~~, ~~**we'll**~~ maybe get back to the sort of hypothetical manufacturing side toward the end once we get through all the design considerations, but ~~**do**~~ ~~**I**~~ ~~**have**~~ ~~**it**~~ ~~**right**~~ ~~**that**~~ at the end of this, you have a micro, a tiny, I should say, GPU that could be included alongside like a hundred other projects on one chip and then that chip would be what you would actually get if you ordered through the process you described would be like a chip that has your design plus a hundred other designs all on it. And then everybody just shares in the production costs, but also gets access to one another's designs. Is that how that's working? SPEAKER_00: And I'm probably actually going to do that. So the thing that I mentioned called Tiny Takeout, created by this guy named Adhigan, he's basically like the GOAT of making ASICs accessible nowadays. You can find him on Twitter and you can find his course, which is Tiny Takeout, or sorry, Zero to ASIC.

00:10:03 
 He has basically made this process where they'll buy up one of those eFabless $10,000 slots, where you have to get 100 ~~**tips**~~, you have to pay $10,000. And as I said, they'll allow a hundred people or a couple~~**,**~~ ~~**a**~~ big group of people to submit their projects. And for example~~**,**~~ ~~**yeah**~~, exactly what you said is I could submit this to that, which I will do. And then I'll get the chip. And the cool thing is I'll get the chip and I'll be able to play with my project, but I'll also be able to play with the projects of all ~~**other**~~ 99 ~~**people**~~ who submitted, which is just like a cool byproduct of it. That's awesome. SPEAKER_01: Other than for learning, what is this good for? Are there projects that people are actually~~**,**~~ where they're building things where they can't get something that meets their requirements that they can only get this way?

00:10:45 
 Or is this really all for the love of the game and the fun of the DIY? SPEAKER_00: And you're just now with these stacks opening and Betula becoming better. ~~**Duh.**~~ It might be ~~**like**~~ an interesting analog market or a smaller scale ASIC market, where at least you could do the initial prototyping phase with the stack. ~~**I**~~ ~~**think**~~ in practice, there's not going to be huge businesses built on the server. Again, it will always make sense for them to go to the real production stacks. So ~~**I**~~ ~~**do**~~ ~~**think**~~ it's mostly hobbyists. And ~~**I**~~ ~~**think**~~ the most interesting thing on the learning side is getting more college students and younger people, just getting the intuition of this.

00:11:16 
 Since the industry was created and the engineering was much worse than Hockler ~~**like**~~ several decades ago. Getting a new generation into it is interesting given that there is a whole landscape of the Chips Act and there's some geopolitical incentives there. ~~**Yeah**~~, ~~**cool**~~. ~~**Makes**~~ ~~**sense**~~. SPEAKER_01: Okay, so let's get into the project itself. One thing I wanted to ~~**just**~~ try to do upfront is figure out the scope. When you say designing a GPU from scratch, there's ~~**like**~~ a lot of layers to the tech stack. Obviously you're not going all the way down to mining your own minerals.

00:11:46 
 So where, what was like the foundation that you said, okay, like this is deep enough down the tech stack, but I'll count it as from scratch, but I'll still be able to use this kind of foundation to build a pot. ~~**Yeah**~~, ~~**it**~~ ~~**does**~~ ~~**take**~~ ~~**pressure**~~. So there's a couple things. SPEAKER_00: First of all, ~~**obviously**~~, I'm not ~~**like**~~ building the whole EDA pipeline from scratch or anything like that. So the place where it really starts is Verilog, which is where most designs start, which is basically ~~**called**~~ your register transfer level logic. It's just designing the specific memory and the specific wires that connect up the chip to make all the logic. And so Designing something at that layer is the layer that I was targeting. And then also more specifically, because that's very generic, the architecture that I wanted to choose is ~~**obviously**~~ very nitpickable in the sense that, okay, you have a giant GPU.

00:12:31 
 There's like millions, thousands of features to this GPU. The question is, what are you going to include? And because the word GPU is an amorphous term nowadays, ~~**like**~~ it started out being referring to graphics processors. Now people are still using it for lots of stuff that aren't graphics processors, like transfer processors and other accelerators. And so there's still a blurry line in terms of what GPU counts as. And so because of that, I set my goal of what do I want to focus on with this, not as ~~**like**~~ graphics hardware or any of the specific things ~~**like**~~ that. If you want to understand this kind of giant blob of areas that spawned from GPUs, what are the core concepts you need to understand in order to be able to build on top of them and understand little details. And then my goal is, can I create a foundation where using this foundation, you'll be able to learn all ~~**of**~~ the important stuff.

00:13:19 
 And by choosing that, it allows me to strip away a lot of the complexities ~~**of**~~ ~~**some**~~ ~~**of**~~ the nuances ~~**of**~~ ~~**any**~~ little engineering decision. If you're choosing to make ~~**like**~~ graphics hardware or ~~**like**~~ any of the other specific things and just focus on the fundamentals that we got. And so some people would disagree, ~~**right**~~? Because it's a blurred line, some people would be ~~**like**~~, oh, that's not a GPU. It doesn't include graphics or whatever. There's nitpicks you can make everywhere. But at the end of the day, it's just a design decision for the purpose of the product, which is ~~**like**~~, how can you teach people how this stuff works most effectively? SPEAKER_01: makes sense to me.

00:13:49 
 And no doubt these are super complicated. It's often said these days that chip making is the most complicated industry in the world and arguably the most complicated thing that humans have ever devised to manufacture at scale. So ~~**yeah**~~, not surprising that you'd have to make some simplifying assumptions to pursue a project like this. One of the things that you mentioned in your Twitter thread that originally caught my attention was that the beginning of the exercise was a real challenge in just prioritization, figuring out what really matters most. You came up with three things that you felt mattered the most. We can get into each one in turn, but you want to just give a high level of what those three things are and how you identified them, maybe a little bit on what you decided to leave out, and then we can go deeper into each of the three. SPEAKER_00: ~~**Yeah**~~, ~~**so**~~ the three things I chose are architecture, parallelization, and memory. And granted, that whole Twitter thread, although it was written chronologically, I'm not sure if you'll link to it or not, but it's actually the result of hindsight.

00:14:52 
 It's like all the architecture design ~~**I**~~ ~~**chose**~~, for example, or the design decisions that ~~**I**~~ ~~**put**~~ ~~**up**~~ ~~**front**~~ are actually the result of me having gone through the entire process. ~~**I**~~ ~~**didn't**~~ ~~**have**~~ ~~**that**~~ ~~**foresight**~~ at the beginning. But the reason ~~**I**~~ ~~**chose**~~ those things is because. And ~~**I**~~ ~~**think**~~ ~~**even**~~ ~~**maybe**~~ ~~**I'll**~~ ~~**rephrase**~~ ~~**them**~~ ~~**into**~~ ~~**two**~~ ~~**really**~~. Like architecture is not really saying much. When ~~**I**~~ ~~**met**~~ by focusing on architecture, it's like, what are the key elements of each piece of the GPU that are important to all GPUs? Like different GPUs have different little specific elements tailored to their need. But one of the things, again, going back to the foundations, that you absolutely need to understand these concepts in order to understand how this whole thing works, broadly speaking.

00:15:30 
 And so I broke that down over time. Now I had the architecture diagram in my tweet, and ~~**I**~~ ~~**think**~~ that was a good breakdown of basically the most simple possible explanation of how GPUs work. And we can get into the specifics of that. Now, ~~**I**~~ ~~**think**~~ the other two terms were actually the more interesting parts. So I'll start with parallelization, because that's the one that ~~**I**~~ ~~**think**~~ is most obvious on the surface level. And that's, yeah, GPUs obviously are built to do parallel computation. That's their whole utility. Basically in graphics hardware, or just graphics use cases in general, and machine learning, they benefit a lot from doing matrix map.

00:16:03 
 And if matrix map happens to be element-wise, in other words, computations on individual elements don't depend on other elements, which means you can do them all at once. And that's the whole reason why GPUs are useful. ~~**And**~~ so the whole point of GPUs is instead of a CPU where generally things are thought of as being executed sequentially, in practice this isn't exactly true, but ~~**like**~~ generally you would execute ~~**L**~~ at one, then two, then three, then four. And now you've done this whole thing in four clock cycles or four cycles of computation. Instead ~~**of**~~ ~~**a**~~ GPU, you just distribute it out among a bunch of compute resources. And now you're accomplishing this all in a much faster time period. Now the interesting thing about parallelization, on the surface it seems like it's just, oh, you need to have a bunch of compute resources or compute cores, and then you can distribute these workloads across the compute cores. That's actually, it turns out ~~**a**~~ ~~**lot**~~ more trivial than some of the other difficulties.

00:16:52 
 So there's one, the problem of how do you distribute these workloads effectively with your resources and manage the resource utilization effectively to maximize it. So that's ~~**like**~~ one interesting problem that happens on ~~**like**~~ the hardware level. And then there's also, ~~**I**~~ ~~**think**~~ maybe a more interesting thing, which is what is the software pattern of parallelization as in how do you actually program these things? It's one thing to make the hardware ~~**.**~~ support this capability, but how do you actually make the software easy to use for developers who aren't usually used to it? And that's arguably one of the core problems that NVIDIA solved and what made them win, for the sake of the ~~**census**~~ takeout. But their CUDA software design is so good for parallel programming, probably by nature of how similar and integrable it was into the previous stack that these developers who needed to use it were used to, which is C. And so another interesting element to highlight there is Since the program side is so important, how does the programming pattern that people are so familiar with actually get implemented in the hardware? So that was an interesting element of parallelization.

00:17:49 
 So I guess just to break that down, the two interesting things in parallelization were resource management and resource utilization, and then software pattern to hardware. How does that get implemented? And specifically, the software pattern is called Same Instruction Multiple Data. And that just means that you're executing the same code on multiple pieces of data, ~~**like**~~ elements of the matrix. And in hardware, it's called Same Instruction Multiple Thread, which is just ~~**like**~~ the hardware manifestation of that. And then the last thing, which is ~~**I**~~ ~~**think**~~ the less obvious and more interesting ~~**or**~~ maybe ~~**most**~~ interesting realization for me, and maybe it's more obvious ~~**than**~~ ~~**you're**~~ ~~**actually**~~ ~~**doing**~~ ~~**the**~~ ~~**program**~~ ~~**to**~~ ~~**like**~~ GPU programmers, because it's actually ~~**like**~~ a first party thing ~~**in**~~, ~~**for**~~ ~~**example**~~, but memory management is really interesting to me. And the reason is because I've had a completely wrong notion of memory management in GPs. ~~**In**~~ ~~**fact**~~, ~~**I**~~ ~~**didn't**~~ ~~**even**~~ ~~**realize**~~ how important it was initially.

00:18:39 
 This is the thing I was saying in my tweet, or George Haas told me. ~~**I**~~ ~~**randomly**~~ ~~**ran**~~ ~~**into**~~ ~~**him**~~ and ~~**I**~~ ~~**was**~~ ~~**like**~~, ~~**hey**~~, can you give me feedback on my design? And he ~~**was**~~ ~~**like**~~, ~~**dude**~~, this doesn't accomplish the whole memory management problem, which is the most important thing. And what that is, ~~**it**~~ ~~**would**~~ ~~**see**~~ ~~**initially**~~ ~~**that**~~ when you're thinking about parallelization, the bottleneck on how much you can parallelize computation is how much compute resources you have. In other words, let's say I want to do ~~**a**~~ ~~**hundred**~~, ~~**like**~~ a hundred matrix additions at a time. That means I need a hundred different cores that support addition. And ~~**it**~~ ~~**seems**~~ ~~**like**~~ that's the bottleneck. In practice, ~~**it's**~~ ~~**actually**~~ ~~**not**~~ ~~**at**~~ ~~**all**~~.

00:19:16 
 In practice, it's actually memory. Because in order to do a hundred additions, yes, you need a hundred compute units, but you also need to be able to read from a hundred different memory locations at once from the global memory. And global memory is bottlenecked on how much bandwidth it has and that bottleneck tends to be the limit, because there's a pretty big latency in terms of this is the unit wants to access memory, then it needs to send the request, and then it needs to wait for it. And if all ~~**of**~~ these units are trying to access memory at the same time, you may actually have more requests for memory than you have bandwidth in global memory, which means that you need to actually manage all these requests in memory. And that causes ~~**like**~~ a huge latency. And to rip a quote directly from what Joy Todd sent to me~~**.**~~ ~~**said**~~ that GPUs is largely the task of trying to work around memory latencies and hide them away at different layers. And that's a lot of what the challenge of GPUs really is about, which is why~~**,**~~ ~~**like**~~ architecturally, they've had all these things to solve that.

00:20:13 
 Like memory control is the first standard, and then they also have multiple layers of caches to prevent having to access global memory and shared memory and all kinds of other stuff. So the memory pattern is another ~~**Very**~~ important part of GQueues. SPEAKER_01: Okay, cool. That's a great overview. Where do you think it makes the most sense to start in terms of spiking down each of these three? I originally had architecture first, but based on your description there, I might go parallelization first and then come back to architecture. But what do you think makes the most sense? SPEAKER_00: ~~**I**~~ ~~**think**~~ it actually might be easiest if we go in reverse order, because people are probably most familiar with software, ~~**right**~~?

00:20:49 
 So you can actually start with ~~**kind**~~ ~~**of**~~ the software side of things, the ISA software side, and then work backwards into ~~**like**~~ how this is actually implemented in the hardware. You think that makes sense? SPEAKER_01: Sure. I'll be happy to follow your lead. ~~**I**~~ ~~**mean**~~, define terms for us. ISA is instruction set architecture. Exactly. I was confident about the instruction set, ~~**I**~~ ~~**didn't**~~ ~~**know**~~ the A actually there.

00:21:15 
 SPEAKER_00: So this part might be a bit challenging to do without video. Like ~~**I**~~ ~~**think**~~ almost all of this art is going to be near impossible to do without video. So that's the caveat. But ~~**like**~~, ~~**I**~~ ~~**think**~~ it's basically, it's going to be really challenging to~~**,**~~ ~~**to**~~ talk about ~~**like**~~ different instructions and ~~**like**~~ the actual code and stuff ~~**like**~~ ~~**that**~~, which ~~**I**~~ ~~**think**~~ is important to understand it. SPEAKER_01: Let's do this. I'll share my screen. I've got these notes up and I can, I'm right on your~~**.**~~ github page.

00:21:44 
 So interestingly shaped. Yeah, there we go. Okay, cool. Let's just try to describe it, talk people through it. And then if you want to get a clearer view of this, then we'll have the video on the YouTube version. But most folks by default will be listening just to the audio. So let's try to talk them through it and send them to the YouTube if they need it. SPEAKER_00: ~~**Yeah**~~, ~~**so**~~ in that case, let's scroll down to the Kernels first then.

00:22:11 
 Yeah, perfect. We can do matrix addition. ~~**Yeah**~~, ~~**So**~~ if you just scroll up a bit, we really only need the top of the file plus the exclamation. ~~**Yeah**~~, that's perfect. ~~**Yeah**~~. ~~**Okay**~~. ~~**So**~~ ~~**I**~~ ~~**think**~~ the best way to start with understanding this whole system is by understanding the software pattern, just because that's generally most familiar ~~**with**~~ ~~**like**~~ most familiar to people, easiest to see the layout of what's happening.

00:22:32 
 And so generally the way you think about GPU software is ~~**right**~~. It's the ~~**Cindy**~~ SIMD pattern, which is ~~**same**~~ ~~**instruction**~~, ~~**multiple**~~ ~~**data**~~, as I mentioned before. And so all that means is you're just going to write one instruction. You're not going to do any loops or anything to be like, okay, execute this instruction on all this different data. You're just going to do an instruction that's basically invariant to the data that it receives. And what that means is your code is going to take in some data, it doesn't know what the data is, and based on that data from, let's say, two different matrices, it's going to perform a computation on the data to get the element of the resulting matrix, and it's going to put that back in memory. To take a concrete example of this, we can say matrix addition, which is probably one of the most simple GPU use cases~~**,**~~ ~~**right**~~? So let's say we have two 1 by 8 matrices, which means each matrix has 8 elements, and let's call them matrix A and B.

00:23:21 
 And all we want to do is quickly compute the addition of these matrices, which we'll call matrix C~~**,**~~ ~~**right**~~? So let's say that Matrix A and B are loaded into memory in elements 0 to 7 for matrix A, and then 8 to 15 for matrix B, and we ~~**just**~~ want to put matrix C, the result of this addition, into element 16 plus of memory. And one thing you could do in a CPU is write a program, and that program is going to take~~**,**~~ ~~**like**~~ in a for loop, it's going to be ~~**like**~~, okay, for i less than eight, ~~**as**~~ ~~**in**~~ ~~**like**~~ for each of the eight elements, add up the element for matrix A and B, put that into the element in matrix C. And we're going to do that. That's going to take eight cycles~~**,**~~ ~~**right**~~? ~~**Like**~~ eight iterations of the for loop. And so that's one way to do that sequentially. Now, alternatively, what you could ~~**instead**~~ do is write this thing called a kernel, which is a GPU program. And this kernel is ~~**just**~~ going to handle adding up one pair of elements in A and B and putting it into one slot in matrix C. And so we'll say this kernel is going to take the i-th elements of matrix A and B, add them up, and then put it into the i-th element of matrix C. And so the actual code for that's pretty simple~~**,**~~ ~~**I'm**~~ ~~**sure**~~ ~~**everyone**~~ ~~**can**~~ ~~**imagine**~~ ~~**what**~~ ~~**that's**~~ ~~**like**~~.

00:24:30 
 So you're going to basically read some global memory, the base address of A plus the ~~**ith**~~ element, which is just like getting the element from A. You're going to take the base address of B, get the ~~**ith**~~ element, read it out, add it ~~**off**~~, and then put it into C there. So actually the program itself is very straightforward, and there's not really any GPU element right there. So the main thing with the GPU is you just want to tell it how many times do you want to execute this instruction, and on how many different values of ~~**i**~~ do you want to execute ~~**on**~~ it. So in this case, we want this to be from the 0th to the 7th element of each matrix, there's 8 elements. So we want 8 different values of ~~**i**~~, and that's just because we want to add up the 8 different elements of the matrix. We're going to call each of those times where we need a separate execution of this code, that's going to be called a thread. And a thread is just executing the same code again on multiple different parts of the matrix.

00:25:21 
 And for this code, all we have to do is specify what each thread is going to do, which is just going to ~~**basically**~~ just read two addresses and then ~~**cut**~~ ~~**them**~~ ~~**off**~~ to write an address. And ~~**there**~~ ~~**was**~~ ~~**a**~~ run eight of them. And the question is, when you run eight of these threads, let's say we now have eight of these exact same pieces of code running in our GPU. We'll just say somehow we've accomplished that. The question is, how do you actually get each of these threads to do something slightly different, ~~**right**~~? They're all running the same instruction, but you actually need them to be able to perform it on separate data, like the different elements ~~**matrix**~~. And so what you need to do is, somehow, in the actual hardware of the GPU, you need to make each little area that's executing one of these kernel codes, you need each little area to know something about which thread it's executing. Is it executing the 0th thread?

00:26:08 
 Is it executing the 2nd thread?  ~~**Or**~~ ~~**whatever.**~~ And if you have a way to do that, then within that hardware, you can The hardware can know something about its own context. So let's say the hardware knows which thread it's executing. Now the hardware can say, hey, there's this instruction here that's supposed to load some data in from memory. The data that I want to load in is actually based on which thread is being executed here. So if I'm currently executing thread number 0, I want to load in the 0th element of A and the 0th element of B. If I'm ~~**trying**~~ ~~**to**~~ execute thread number 1, I'm bringing the first element of each, ~~**blah**~~, ~~**blah**~~, ~~**blah**~~.

00:26:40 
 And I want to store it in the first element of C. And so that is the most important thing here. So ~~**like**~~ the two important things are one, we're writing some code that just is meant to be executed by a single thread. And then we're going to specify how many threads ~~**you**~~ executed. And the second important thing here is these special values, which are going to be local in the hardware. So each time a thread is getting executed in the hardware, they have some broader context about what's going on here. And so if you look at those, for people who can't see, there's these values called block index, block dimension, and thread index. And this will probably be familiar to people who are familiar with CUDA. ~~**Obviously**~~, there's more complexity in the actual pattern.

00:27:15 
 But basically, the way this works is that when you have tons of threads that need to be executed for something, You're going to group these threads into batches that can ~~**basically**~~ be executed together on a single ~~**Qt**~~ unit. And those batches are called blocks. And so let's say our block size is 4, which is what it is on my current GPU design. That means that batches of 4 threads are going to be executed together. And so whenever these things are executed, ~~**basically**~~ what you want to know is which block number is currently being executed. And then within that block number, what is the thread index in that block number? So that means that the threads are each going to have an index from zero to three for each block, because there's four threads per block. And then let's say we have, ~~**I**~~ ~~**don't**~~ ~~**know**~~, 10 blocks being executed.

00:27:58 
 So now if we're on block three, how do we get the correct thread that's being executed? Very simply, we're just going to multiply the block ID three by the number of threads per block. So now that's ~~**like**~~ 12. So that means that there's 12 threads that have already been executed in the previous three blocks. And then we're just going to add the thread ID. So now ~~**it's**~~ ~~**okay**~~. Now we know we're on ~~**the**~~ thread number 12, 13, 14, 15. And now those threads can pull in the ReCypher data they need to ~~**commemorate**~~.

00:28:26 
 And so that is how the STEMD architecture ~~**like**~~ programming pattern is implemented in programming. And then we'll talk about soon how that's implemented in the hardware and it's very simple. But that's the core understanding you need to know. Some kind of what's the kernel programming pattern. And in terms of this as an asterisk ~~**like**~~ How did I decide what instructions to include in my ISA? It's ~~**as**~~ you can see, ~~**like**~~ I'm trying to make it as minimal as possible. And so really my decision function there was basically ~~**like**~~, what are the minimal set of things I can implement in order to make some cool, useful use cases. And so ~~**like**~~ matrix addition, matrix multiplication were the most obvious things there.

00:29:03 
 And so the instructions I chose are basically just~~**.**~~ what's actually needed to support those, which is just basically some basic arithmetic, loading and storing data from memory, because that's how you basically get your inputs and then store the result of your computation, and then ~~**call**~~ ~~**since**~~ just for some convenience. But yes. SPEAKER_01: Cool, ~~**I**~~ ~~**think**~~ that was all quite well said, so ~~**I'm**~~ ~~**not**~~ ~~**sure**~~ ~~**how**~~ ~~**much**~~ ~~**I**~~ ~~**can**~~ ~~**really**~~ ~~**add**~~ ~~**to**~~ ~~**it**~~, but just to try to summarize quickly back, when you look at this block of code, I'll go back to the block of code thing, then we'll come back to the instruction set. When you look at this block of code, the key thing to understand is this code only executes the addition of one element from within the arrays that are going to, or the matrices that are going to be added together. And so the key trick is you have to write the code in such a way that it can take in as a variable, which position are you in and then handle everything else just based on that one variable that the hardware will actually feed in at runtime. So everything is going to be the same except for what position you're in, and then that will determine the offset of the memory, which will determine what value you load in, which will determine what value you ultimately calculate, and therefore what value you're going to put back into which position in memory. All of that is determined by what position you are in the broader array of computation.

00:30:33 
 And you can think of that as in your case, you've got a one by eight, but obviously you could get, ~~**I**~~ ~~**believe**~~ the current GPUs go up to three dimensions of them. SPEAKER_00: Yeah. SPEAKER_01: You want to give us just a little bit of flavor as to how this kind of gets more complicated from here. SPEAKER_00: Yeah, so in actual GPUs, they have, first of all, they have more than just block index, block dimension, and thread index to identify a thread. There's a couple more dimensions of just like different groups of batching that ~~**basically**~~ just have to use every single dimension to identify what thread you're in. And then some other interesting things. So ~~**I**~~ ~~**basically**~~ just captured the most simple piece of GPU programming here. In reality, there's a couple ~~**other**~~ very interesting and important things.

00:31:11 
 So first of all, when you're loading and storing data, there's something that we'll maybe get to later, but by nature of how GPUs have different layers, different levels of memory, and they have shared memory, which is shared between an entire block of threads, which means that four different threads can actually store stuff and read stuff from the same location. It's called shared memory. And then each thread also has its own dedicated memory called a register file. That's important. And so one complexity that ~~**I**~~ ~~**didn't**~~ ~~**implement**~~ ~~**here**~~, which is there in real GPUs, and it's very important for actual use cases, is that you can actually choose which memory you're putting stuff in. So my pattern, there's only two types of memory you interact with. All of these MOL, ADD, CONST, all of these instructions. Everything that has ~~**like**~~ an R in its parameters, so it will be ~~**like**~~ multiply R0, R2, R3.

00:31:58 
 That's operating on registers for each thread, which means ~~**like**~~ that stuff is all just specific to a single thread, the thread that's currently executing. And it's storing stuff in these local register files that has local data. And it's basically just performing computations on the register values. And then there's the load and store instructions, which is either loading data from global memory into the register or storing data from the register into global memory. So there's really only two levels of memory happening here. In a real GPU, you have separate instructions that are going to let you interact with registers, global memory, and also shared memory, and there's also layers of caches. So that's one thing. ~~**Then**~~ one other interesting thing is in real GPU programming, you have the ability to synchronize different threads with each other with barriers, which means that because of the necessity of shared memory, ~~**that**~~ means ~~**let's**~~ ~~**say**~~ four threads, they want to store something in this shared area and then access it from each other.

00:32:51 
 But what happens if one of the threads is ahead of the other? In other words, one thread gets to some step and the other threads are not there. And now what if this thread is going to read some data from the shared memory? It's going to do that. It needs to make sure that the other threads are at the same point. Otherwise it can have that corrupt data or maybe the data is not ready yet to be read by that thread. And so there's these things called barriers where you can ~~**basically**~~ synchronize threads. So this thread is just going to wait until all the other threads ~~**will**~~ ~~**get**~~ ~~**are**~~ ~~**going**~~ ~~**to**~~ ~~**get**~~ to the same point.

00:33:17 
 And that is to be something maybe where we can share memory or something like that. So that's another important thing, which again, ~~**I**~~ ~~**didn't**~~ ~~**implement**~~ because it's explainable from here. And also the complexity of implementing it is not worth it. SPEAKER_01: Okay, cool. That's good. Do you want to next just talk through a little bit the instruction set and maybe give a little flavor ~~**to**~~ ~~**of**~~ what the~~**,**~~ ~~**uh**~~, what kind of the next, next ~~**in**~~ instructions would be if you were to expand from here? SPEAKER_00: Yeah. So instruction set, ~~**I**~~ ~~**would**~~ ~~**say**~~ a lot of it is very simple.

00:33:46 
 So you have addition, subtraction, multiplication,and division. Very straightforward. Everything is ~~**take**~~ ~~**free**~~ registers. And so the important thing I'll cover here is just the specifics of the register file. ~~**So.**~~ With addition, subtraction, multiplication, and division, for context, each thread has 16 registers, which means that it has 16 different places that it can store whatever values it wants to store for any arbitrary ~~**complications**~~. And importantly, the last three of these register values are restricted. And so we'll call those registers 13, 14, and 15.

00:34:17 
 Those are how in my GPU,~~**the**~~ ~~**actual**~~, those custom context values I was talking about are not a real GPU. It doesn't necessarily work like this, but basically the important thing is every thread has access to its context. I happen to implement that through these registers. And so these special registers are registers that you can't write to. The GPU itself handles writing to them. And that means that the GPU itself supplies this code execution environment with, Hey, you're this block number. ~~**there's**~~ ~~**this**~~ ~~**many**~~ ~~**threads**~~ ~~**in**~~ ~~**each**~~ ~~**block**~~, and you're also this thread number in this block. And again, that's how in the hardware, the code has access to those values.

00:34:54 
 Now the last 13 registers are read-write registers, which means the code is free to access these in ~~**however**~~ it wants. And so these add, subtract, multiply, and divide, what they do is basically take the values of two registers, which you then specify with four bits, which is what you need to specify an address of 16 different possibilities. And it will take the value from these two different registers, and it will perform a computation on them, like ~~**an**~~ ~~**issue**~~ subtraction multiplication division, and then store that result back into one of the registers, ~~**all**~~ the registers specified by 4 bits. Now, with the load and store instructions, similarly, the load instruction will take the address of one register, so it will take the value of one of the registers, that's going to specify a memory address. So for example, if that register has the number 4 in it, it's going to go load ~~**in**~~ element number four from global memory, and it's going to put that into another register. And for the store instruction, it's going to take the value of one of the registers and store it into a memory address in global memory. So ~~**again**~~, those are register computations. That constant is very straightforward.

00:36:01 
 You just load in a specific value that you want to ~~**into**~~ a constant ~~**to**~~ the register. And that might just be because you want to use it for addition or something else. Now, the interesting instructions that I'll spend a bit more time explaining are the BRNZP, which stands for branch. It's a branch instruction and the comparison instruction. And these two work together like the simplest implementation that allows you to do if statements and looping. And this is all ~~**like**~~ CPU stuff, ~~**of**~~ ~~**course**~~ you need it for GPUs too. But the main thing is that what you want to do is create some condition. And then if this condition is true, you can jump somewhere else in your code.

00:36:38 
 Which means let's say I want to do something like if one of my registers is greater than zero, then jump to the end of this, skip this entire segment of code, otherwise go through it. So that's basically like an if statement, ~~**right**~~? Or you could do a loop. So you could do like some code, you could put like ~~**a.**~~ ~~**a**~~ label at the top of the code. So let's call it like a loop label, which is actually in one of my kernels below. And then you have a whole part of code. And then below that code, you're going to say if some condition is true, ~~**dump**~~ ~~**all**~~ ~~**the**~~ ~~**way**~~ ~~**back**~~ ~~**to**~~ ~~**the**~~ ~~**top**~~ ~~**of**~~ ~~**the**~~ ~~**loop**~~, which is going to keep it looping until that condition is no longer true. The way that you implement this is with the comparison and branch instructions.

00:37:15 
 And it's always basically a comparison and a branch. So what you do is first you do the comparison instruction. And that's basically you're going to take two registers, it's going to compare them. And then it's going to check if the result of subtracting the second register from the first is positive, zero, or negative. And it turns out you can actually do a lot with that. So you're just going to compare two registers, and it's going to store that result, if they're positive, negative, or zero, the difference between them, in something called the NZP register, which is stored in the program counter unit of ~~**EEG**~~, ~~**like**~~ any compute unit, basically. In our case, it happens to be a thread execution unit in a GPU. And so the program ~~**calculator**~~ ~~**now**~~ ~~**has**~~ ~~**the**~~ ~~**current**~~ ~~**line**~~ ~~**of**~~ ~~**code**~~ ~~**being**~~ ~~**executed**~~, and ~~**now**~~ it also has this comparison thing, which is storing whether the result of the subtraction of the two registers is positive or negative.

00:38:06 
 Now with that, so you're going to have now some ~~**like**~~ NZP value, and it's going to either have ~~**the**~~ it's going to have a 1 specifying whether it's negative, zero, or positive. Now the next instruction, brnzp, which is branch, it's going to allow you to specify branch to some specific line, as in, jump to some specific line of code, if the NZP register ~~**hold**~~ ~~**some**~~ specific set of values. So you could say if the NTP register is saying that this comparison is negative, then jump back to ~~**her**~~ loop code, which is exactly what we did in the matrix multiplication. So the matrix multiplication needs to do a loop through some code. So what it does is it compares some counter, basically it's just incrementing a counter, and that counter is supposed to do some number of loops through the code. So ~~**let's**~~ ~~**say**~~ in our case the counter needs to go through basically two loops. And it's just storing the value, and that value is going to start at 0 and keep incrementing up until it's past the counter. And so basically the way you would use these instructions is use the comparison instruction.

00:39:03 
 So you're going to say, compare my counter to the value that the counter is supposed to always be less than. And then if the counter is ~~**still**~~ less than the max value, go back and loop. Otherwise don't loop. And so that's how you'd use those two instructions to create conditional logic, which is exactly what the code does. So those are pretty important. And then the last two instructions are very straightforward. So there's ~~**like**~~ the NOOP, which is basically ~~**now**~~ ~~**you're**~~ ~~**doing**~~ an instruction, it just moves onto the next line. And then there's a return instruction, which tells the GPU that it has reached the end ~~**and**~~ executing a specific ~~**cut**~~.

00:39:33 
 And that's how you have the entire ISA, and that also ~~**Honestly**~~, this part was actually a lot of the hard part. Once you understand this code layer, which is why we started here, then the hardware layer actually becomes a lot more clear what's going on. And that's partly a result of the fact that this code layer is the result of a lot of fine-tuning on the hardware layer ~~**also**~~, in terms of what ~~**I**~~ actually included here. SPEAKER_01: Okay, cool. Again, ~~**I**~~ ~~**think**~~ ~~**I'm**~~ with you, but maybe just to try to recap a little bit there. ~~**I**~~ ~~**hadn't**~~ ~~**understood**~~ coming in that you had these three dedicated registers, which is just a position in the local most memory, ~~**right**~~? That sort of declare to the kernel, which is the form of parallel programming that says, here's what to do, given what position you are in, ~~**right**~~? ~~**indication**~~ ~~**of**~~ ~~**where**~~ ~~**you**~~ ~~**are**~~ is provided in these dedicated registers.

00:40:30 
 And then separately, you have, I guess, how is that? That's not really related to the size of the instruction set. That's independent. Like how many registers? ~~**I**~~ ~~**think**~~ ~~**I**~~ ~~**was**~~ ~~**conflating**~~ ~~**those**~~ ~~**for**~~ ~~**a**~~ ~~**second**~~ ~~**in**~~ ~~**my**~~ ~~**head**~~, because you numbered the instruction set with a similar gap. Is there a connection there that ~~**I'm**~~ inferring correctly? Or am ~~**I**~~ hallucinating this? SPEAKER_00: So there's a couple of connections ~~**in**~~ the instruction set to the hardware.

00:40:50 
 There's two important connections, and they're very much demonstrating that this is a dummy project. So first of all, if you look at the instruction set, each instruction is 16 bits, which means that the first four bits are the opcode, meaning that ~~**basically**~~, since there can only be 16 different combinations of that, that means that with this design I've created, there's actually only space for 16 total instructions, which means there's only space for~~**,**~~ ~~**I**~~ ~~**think**~~, four or five more instructions if I wanted to add them. Now, the second constraint is that I've made it so that each register is specified by four bits, which means that there can only be up to 16 registers. If I wanted more registers, I couldn't do that. I would need a fifth bit to specify each register, which means the instruction would have to be longer. And then the third thing is that the constant and more importantly the branch ~~**destructions**~~, they both take immediate values, which means they're not taking some value from a register. They're literally taking values that are specified in the code. Those are numbers.

00:41:46 
 And if you look at those,each of the numbers is 8 bits, which means that the branch ~~**destruction**~~ ~~**can**~~ ~~**only**~~ ~~**specify**~~ the branch ~~**to**~~ 8 bits of program memory, because the branch instruction is telling it, if some condition is true, jump to this line of the program. Which means that, in order for the branch instruction to cover the entire program, my hardware, as in my program memory, can only use up to ~~**that**~~ ~~**by**~~ 8 bits, which means it can only have 256, or ~~**by**~~ ~~**default,**~~ ~~**I**~~ ~~**think**~~ 256 rows of 8-bit memory. So those are the constraints created by the instructions here. And again, in reality, for this reason, instructions are usually a lot longer than 16. Usually they're 32-bit-length instructions. SPEAKER_01: But ~~**yeah**~~. So if you wanted to have more memory, you would have to have longer addresses for that memory. And if you wanted to have more instructions, you would have to have ~~**obviously**~~ bigger labels for those instructions.

00:42:41 
 And I'm a little maybe this is where we're headed next, but I'm a little fuzzy on ~~**like**~~ basically each one of these~~**.**~~ instructions is implemented by hardware at this point. This is ~~**like**~~ the last layer of software. When this gets issued to the GPU, or to a compute core within a GPU, it now says, okay, I'm going to actually fire up a particular circuit that's going to do this stuff now, ~~**right**~~? There's no further software to adapt here. SPEAKER_00: And ~~**I**~~ ~~**think**~~ one caveat on, so ~~**I**~~ obviously oversimplify, or pretty much everything ~~**I**~~ said here in this whole thing is simplified in some way. But so ~~**like**~~ one example of an oversimplification here is ~~**like**~~ in a branch instruction, in reality, the number of bits in the immediate in a branch instruction, if it is an immediate, is not limiting to the program memory. The reason that ~~**I**~~ limited it is because.

00:43:35 
 Your alternative is to make this thing an offset, so instead of it being ~~**like**~~, literally branch this line, you could be ~~**like**~~, branch to some offset on the current line, as in branch plus 50, or branch minus 50, in which case you could do it a couple times, and then you're not limited, so you can get infinitely far in memory. There's tons of other ways to approach it. So that's just one example of a constraint. Because ~~**I**~~, one, did not support negative numbers here, and two, ~~**I'm**~~ not making an offset, ~~**I**~~ just wanted to jump to a specific line for simplicity's sake. Now that's a constraint that in reality there's tons of ways around this, but ~~**yeah**~~, that's an example. And then in terms of the second part, that's exactly right. The important thing to take away from this part, maybe there's still lots of fuzzy details, especially for people listening, The important thing is, one, you understand the importance of each instruction and why it's there and what it's supposed to do. You understand the general programming pattern.

00:44:23 
 Again, it's hard to draw even just because it's assembly and assembly itself is not necessarily the easiest thing to pick up. It makes sense when you look into it. But you just need to understand those parts and the important GPU programming patterns because a lot of the actual implementation will get revealed in the hardware. SPEAKER_01: Okay, ~~**I**~~ ~~**think**~~ it's just about time probably to turn to hardware, but let's just scroll through the multiplication one more time because this is where you have basically brought it all together in terms of the complexity that you can support, where we're still~~**,**~~ we've still got to be mindful of where we are in the thread. But this time, the calculation ~~**Do**~~ ~~**I**~~ ~~**have**~~ ~~**it**~~ ~~**right**~~ ~~**that**~~ this calculation determines one position in the output matrix? That's exactly right. So to do a 2x2, you're going to get a 2x2, so you're going to have four spots in the output matrix, so that's why this is four threads. Each one of those threads is going to do a different set of~~**,**~~ ~~**it's**~~ ~~**still**~~ ~~**going**~~ ~~**to**~~ ~~**touch**~~ ~~**all**~~ ~~**the**~~ ~~**numbers**~~ ~~**basically**~~, ~~**right**~~?

00:45:30 
 But it's going to do them slightly differently to get to the final spot in the output matrix. And to do the sort of rows and columns, it has to do this loop. And this is where you get to the point where you have to have this comparison and branching. SPEAKER_00: ~~**Yeah**~~, ~~**exactly**~~. The reason is just because of the nature of matrix multiplication being a little more complicated than addition. With addition, you just add up two elements. With matrix multiplication, ~~**unfortunately**~~, it's not just multiplying two elements. It's like a dot product between two vectors in the matrix, which means you ~~**just**~~ take two ~~**like**~~ vectors, which is just a row and a column of elements, and then you multiply their elements by each other, and then add up the results.

00:46:11 
 So it's actually like a bunch of different computations, not just like a single point-wise multiplication or something, which is why there's like a loop here, and why there's just a bunch more multiple ~~**case**~~ ~~**sheets**~~ and additions going on, which is what necessitates the comparison and reduction in this case. And ~~**I**~~ ~~**would**~~ ~~**say**~~ that's the most important thing. For people who want to understand the specifics of the kernel, you can look at it. It's relatively simple, but ~~**I**~~ ~~**think**~~ maybe a little bit beyond the complexity of what makes sense to explain on ~~**Yeah**~~. SPEAKER_01: Yeah. And ~~**I**~~ ~~**misspoke**~~ when ~~**I**~~ said that it's going to touch all the numbers. It's going to touch one row or column for each of the two input matrices. ~~**Exactly**~~.

00:46:45 
 Yeah. SPEAKER_00: So it's going to take, it's going to touch a lot of numbers. In this case, it happens to be three quarters of them for each computation, but. ~~**Okay**~~, ~~**cool**~~. SPEAKER_01: ~~**Um**~~, ~~**then**~~ ~~**I**~~ ~~**guess**~~ it's time to get on to the hardware. So how ~~**do**~~ ~~**you**~~ ~~**think**~~ best to approach that? SPEAKER_00: ~~**Yeah**~~, so this part is actually going to be a lot simpler, ~~**I**~~ ~~**think**~~, thankfully. And so the best way to start it is to just start with the architecture of the entire GPU, and then we can dive into the thread architecture.

00:47:10 
 And those are straightforward. Here? No,this is the... ~~**Yeah**~~, this is yours, ~~**right**~~? This one. ~~**Yeah**~~. Okay. So now we can dive into ~~**kind**~~ ~~**of**~~ the flow of, we talked about this program, ~~**right**~~? So now the nice thing is we can explore, okay, how is this program going to actually get executed in the GPU?

00:47:28 
 And that is going to take us through ~~**basically**~~ the entire architecture and also the limitation for why everything exists here. First of all, let's take the example of our matrix addition parallel. So we have this code. The question is, are we going to actually load this up into the GPU? And how are we actually going to run it? And so ~~**I**~~ ~~**guess**~~ what we'll do here is, first, we'll look through how this works ~~**high**~~ ~~**level**~~ in the architecture. Then we'll look into individual threads and ~~**how**~~ ~~**to**~~ ~~**see**~~ ~~**how**~~ this code is executed ~~**in**~~ ~~**individual**~~ ~~**thread**~~ ~~**level**~~. And then ~~**in**~~ ~~**the**~~ ~~**last**~~ ~~**case**~~ ~~**of**~~ ~~**this**~~ ~~**that**~~ ~~**might**~~ ~~**be**~~ ~~**interesting**~~ is we can actually look through the test case and just ~~**briefly**~~ explain ~~**in**~~ ~~**this**~~ ~~**repository**~~ how it's actually tested and simulated.

00:48:08 
 So it's not just a theoretical.~~**Those**~~ ~~**kernels**~~ ~~**that**~~ ~~**are**~~ ~~**written**~~ ~~**in**~~ ~~**there**~~, there's actually a setup for them to actually get run on this GPU design and see the entire output of everything that's going on. And so we can touch on that last and maybe ~~**like**~~ touch on some of the interesting things in the code. Maybe not. That's up to you. But so the place to start is we have this matrix edition kernel. And the question is, how do you actually get this to execute on the GPU? On one hand, you need to load up the code into there, but then ~~**it's**~~ ~~**okay**~~.

00:48:35 
 How do I actually make this thing run and get the result back? The way this is going to work is first we'll think about global memory, which is ~~**like**~~ the place where you're going to load up all ~~**of**~~ the prerequisite data ~~**on**~~ ~~**this**~~ code. First of all, you have the actual kernel code, and that's, as we wrote below or before, it's just a bunch of instructions. And those instructions happen to be written in assembly, which is ~~**like**~~ some nice verbal format for us to visualize them. In reality, we would have compiled that assembly~~**,**~~ ~~**just**~~ ~~**a**~~ ~~**pre-setting**~~ ~~**itself**~~, compiled that assembly into the object code, which is specified by the ISA. So it's just going to be a bunch of ones and zeros, all 16 bits long, and load that into program memory. So now, we would say that, let's assume we've loaded that into program memory. Let's say it's~~**,**~~ ~~**I**~~ ~~**don't**~~ ~~**know**~~, 15, 12, 15 lines long.

00:49:17 
 So now we have our kernel code loaded up into program memory, and that's just going to specify exactly where kernel set this thing fits. And then the second piece of it is, in data memory, we're going to load up the actual data that we want to perform computations on. In this case, this would be ~~**like**~~ the two 1 by 8 matrices. We'll call them A and B. So A, as ~~**I**~~ ~~**said**~~ before, A is going to be in addresses 0 to 7, and then B is going to be in addresses 8 to 15, and then we'll say that C is going to be in addresses 16 to 23. So that's going to be our result. ~~**Now**~~ currently our result matrix is going to start out with nothing in it. And so what is the goal of our entire program?

00:49:51 
 The goal is to somehow get the GPU to use that code that we just loaded up in program memory. And somehow at the end, it's going to need to fill out rows 16 to 23 in memory with the correct results of the additions of A and B. And then we're going to read that out at the end. So from the host machine's perspective, which is ~~**like**~~ the CPU talking to the GPU, What we need to do is basically get this compiled code, load it into program memory, load up our data into data memory, somehow get the GPU to start this whole computation. It's going to do its thing. And then at the end ~~**of**~~ ~~**it**~~, ~~**GPU**~~ is going to tell us that it's done. And then there's going to be in data memory, the answer and the host machine can just read out that answer. And so that's ~~**kind**~~ ~~**of**~~ the interface of the GPU~~**,**~~ ~~**basically**~~.

00:50:33 
 So in this picture here, we have global memory with program memory and data memory. And that's ~~**the**~~ one piece ~~**of**~~ ~~**that**~~. And then another important thing in GPUs is the actual bus where data ~~**kind**~~ ~~**of**~~ transfers between the CPU host and the GPU. Again, I left that out because of simplicity reasons. But the other important control thing here, besides the memory, is what's called the device control register. Now in real GPUs, this is a lot more complicated, but in this case, if you remember, what we need to specify is basically the number of threads to execute. We need to tell the GPU that, Hey, this thing in program memory, which is just a single kernel code. We need to tell it, you need to run this thing eight times.

00:51:12 
 And so the way that you do that is in my design, you would store the thread count inside the device control register. And so the device control register is ~~**mainly**~~ just used to store some high level data about ~~**like**~~ how you want the GPU to execute. In this case, the only piece of high level data we really need to specify is how many threads should you be executing? And so now we have program memory loaded up, data memory loaded up, device control ~~**registry**~~ register loaded up. Those are the three things you need to do to get this GPU to be prepared to run something. And then that's the external interface. Then we're going to send a start signal. And that's going to tell the GPU, Hey, everything's loaded and ready to go.

00:51:49 
 You need to start basically performing computations on data. And that's ~~**like**~~ ~~**the**~~, that's ~~**like**~~ a clean break. So that's ~~**like**~~ the interface to interact with the GPU. Now we're going to get into ~~**like**~~ the computational parts of it. Does that make sense? SPEAKER_01: Yeah, ~~**I**~~ ~~**think**~~ so. And probably just worth reiterating that there's ~~**like**~~ even in some ways more complexity on the CPU side when you say the host machine. We're assuming here that ~~**like**~~ you have a general purpose CPU that can do the compiling and can handle the ~~**like**~~ outer loop of Here's what I'm trying to do and get something back and show it to you on a screen that's done.

00:52:29 
 And this sort of sits within that. So it's funny because the programming paradigms in ways ~~**it's**~~ more complicated, but then in other ways, ~~**it's**~~ a lot less complicated because it can take advantage of all ~~**of**~~ the universal computing benefits of a CPU host machine. SPEAKER_00: Aside from that, ~~**I**~~ ~~**think**~~ ~~**I'm**~~, ~~**I**~~ ~~**think**~~ it all makes sense. That's a good point, because GPUs are not really meant to run alone. They're always in practice either hooked up to a host, like you'll see them next to a computer in ~~**like**~~ a rack, or you connect to one on the cloud or something like that. That's not really very effective, but depending on what you're doing. But ~~**yeah**~~, they usually have a host connected to them that can send them data and interface with them. But ~~**yeah**~~, that's a good point.

00:53:05 
 So now we can get into the computation pattern, ~~**right**~~? And ~~**high**~~ ~~**level**~~ is actually pretty simple. So the main things to focus on is you have this dispatcher and the dispatcher is just responsible for, as we said, these threads are executed in blocks and a block is just like a batch of threads that gets executed together and the same for key resources. And so basically the way to think about this is your GPU has some large number of compute resources that are organized inside cores. And so this GPU design happens to have four cores and each core has the capacity to execute a bunch of different threads in parallel. And from ~~**a**~~ ~~**high**~~ ~~**level**~~, before diving into what's in cores, let's just assume the core is some black box. And we just know that this black box, we can just get it ~~**from**~~ ~~**some**~~ group of threads, which is going to be a block size of threads. So in this case, four threads at a time.

00:53:50 
 We can give each compute core four threads at a time. We're going to do some adaptations and finish tossing the threads ~~**somehow**~~. And that's going to tell us when it's done. And so the high-level job of the GQ now is, okay, how do we manage ~~**how**~~ to divide up these threads into the available compute cores, wait for them to complete their job, and then give them more jobs when they're done computing their work. And so basically all it is is just managing all the compute cores, figuring out when they're available for new work, and then dispatching new blocks to these cores. Now, in practice, this is one place where there's a lot of complexity in GPU, because you can imagine when you have a ton of different cores performing stuff at once, and different cores can perform different blocks, ~~**like**~~ ~~**I**~~ ~~**see**~~ different blocks of threads at once, and they have a complex resource management. You can do a lot of different optimizations to make sure that you're getting maximum resource utilization. And so this is ~~**a**~~ software where there's ~~**like**~~ a ton of space for optimization in terms of using things more efficiently.

00:54:52 
 like you might call it, threading in different threads at once to make sure that everything's getting used. And then~~**,**~~ ~~**of**~~ ~~**course**~~, for simplicity's sake, in this example, we use a very simple version of that, which is just that the dispatcher just looks out for when a core is done executing its threads, and just has it ~~**mork**~~ work threads. And just like a round-robin style. So it just goes, looks at all ~~**of**~~ the cores sequentially, and then keeps doing the ~~**mork**~~ work threads if they're ready. That's like the simple way to think about it. And so, The dispatcher is one of the more complex elements of the GPU. Now, if we get into the individual compute cores. Dispatcher layer for a second.

00:55:25 
 SPEAKER_01: Fair. Yeah. One thing. What level of design did you have to go to in this project to create a dispatcher? Is that ~~**like**~~ something that you had to design? And if so, at what level? Or was it something that you could drag off a menu of available components? SPEAKER_00: Yeah, so pretty much nothing here was an available component, to be honest.

00:55:48 
 I have two. So the nice thing is ~~**I**~~ ~~**had**~~ two or three repositories to reference. So there's this thing called Meow GPU, which is like an open source Verilog GPU made a while ago by Sunskull Africa, which is cool ~~**it**~~ ~~**is**~~. And there's this thing called VeriGPU, so two open source GPUs. The thing is these things have like very little documentation and then they're not really written to be simple. They're written to be ~~**like**~~ attempt ~~**set**~~ production GPUs. One of them is like completely unfinished. Meow is actually finished.

00:56:12 
 And first of all, architecturally, they're somewhat different in some ways, because again, they're optimizing for production usability. And the second thing is because of how big they are, they're just not very easy to understand. And so my process for creating things was not really, ~~**you**~~ ~~**can't**~~ ~~**really**~~ go through them and try to understand it. ~~**I**~~ ~~**know**~~ the analogy is if aliens come to earth and try to~~**,**~~ they find the computer, ~~**like**~~ how are they going to understand it? Are they going to break it down and look at all the connections, the transistors and see what's happening? No, they're not going to understand it by doing that. They have to come to some understanding of their own and then test it against the computers that they have. It's a bit of a massively over-complex analogy for going through a GitHub repo.

00:56:52 
 That's actually the analogy for mapping out the brand, which is not a big deal there. But ~~**yeah**~~, ~~**so**~~ my approach to going through these repos was not ~~**like**~~ going through them and trying to understand. It was actually ~~**like**~~ talking to Claude and Chatfubity about, I see ~~**like**~~ names of things, ~~**like**~~ I'll see a folder called, I don't know, gistapp or schedule or whatever. And I'll try to figure out with Claude through first principles, ~~**like**~~ where does this have to fall into place in the GPU? And then that part's not too hard. So then, okay, I get to something, then I'll start to propose how I think it works. And the reason is because there's not really anything that teaches you how these things work, ~~**like**~~ anywhere online, basically, because it's all proprietary and they have their own algorithms and stuff. And so even to make a simple version, you'd have to be ~~**like**~~, so for the dispatcher, for example, I come up with a hypothesis ~~**like**~~, Oh, I think a simple version of this would be X, Y, Z. I think a simple version of this would be ~~**like**~~ round robin scheduling while looking at what intake courses are available, which is super simple.

00:57:45 
 And then I would have to get confirmation. So Claude would be like, ~~**actually,**~~ I don't think that's the best way to think about it. I think you should think about it like this or something like that, which is interesting that there's nothing publicly available, but Claude and ChatGPT, in some cases, ~~**it's**~~ things that are purely intuitive and ~~**it**~~ just has the benefit of engineering intuition on stuff that I haven't gone into. In some cases, ~~**it**~~ says stuff that's definitely implemented in proprietary GPUs and maybe ~~**it's**~~ still out there through intuition, but ~~**it's**~~ stuff that's a little more complicated than what I expected ~~**it**~~ to know, which is pretty cool. But ~~**yeah,**~~ as a long-winded answer to your question, there is no off the shelf stuff. I basically had to design everything myself from how I thought it would work and then get confirmation from ~~**it**~~. Maybe look into the repo to see, Oh, this repo actually happens to do something a little similar to what I suspected. And now I can actually understand it because I came up with ~~**a**~~ ~~**trues**~~ and I had to do that for all the elements.

00:58:36 
 Granted some of them are a lot simpler to understand than others, but ~~**yeah**~~. SPEAKER_01: Okay. So this thing implements. a round robin where it just says, I'm going to just keep looping through. As long as I'm live, I ~~**got**~~ ~~**to**~~ just keep looping through all ~~**of**~~ the compute cores, check their status. So that implies then that the compute core is ~~**like**~~ maintaining a status that can be checked. And if it's available, then I'll send it another thread to process. And if it's not, then I'll come back to it on my next loop.

00:59:12 
 And all of that is implemented in hardware. You're beyond the point of ~~**we're**~~ beyond the point of programming this at this point~~**,**~~ ~~**right**~~? SPEAKER_00: Yeah, this is implemented in hardware. SPEAKER_01: Yeah. So do you have a diagram of that? Can we look at that? SPEAKER_00: We can look at the code in the repo. There's no diagram.

00:59:32 
 The diagram is for thread execution,but there's no ~~**like**~~ this diagram. SPEAKER_01: Gotcha. Okay. SPEAKER_00: Let's look at the code just to ~~**just**~~ see what that looks like. So if you go into source and then dispatcher dot let's see. Yeah, this is it. You can see all the code is documented at the top. You see that at the important points and you can see there's some of the things that we're saying must be there ~~**are**~~ there.

00:59:53 
 First of all, the important thing with the dispatch is that it has a start signal. And what that means is ~~**that**~~ thing we were saying at the top level, we're going to tell the GPU to start ~~**the**~~ ~~**place**~~ ~~**that**~~ ~~**that**~~ actually goes into the dispatch. So the dispatch says, ~~**Oh.**~~ I've gotten ~~**full**~~ ~~**of**~~ ~~**the**~~ ~~**stuff.**~~ Now I'm going to start sending off these blocks to the actual cores. And that's what starts execution. So that's the start signal you see here. You see the thread count coming in from the device control register, which is again important for the dispatch because it needs to know how many threads does it need to execute in total, ~~**how**~~ ~~**many**~~ ~~**threads**~~ ~~**has**~~ ~~**it**~~ ~~**already**~~ ~~**executed**~~.

01:00:22 
 And that's what's going to tell it ~~**like**~~, oh, I have this many threads more to execute. Let me go send them off to the compute units. Then you can see the core stage. So that's the thing where you're saying, ah, it looks like the cores must have their own knowledge or ~~**like**~~ status of~~**,**~~ ~~**of**~~ if they're ready or not. And so that's exactly what you see the core done, which is, oh, this core is done processing. And then~~**,**~~ so you see core done, core reset and core start. Those are important. This is basically the way the dispatch gets to do stuff~~**,**~~ ~~**right**~~?

01:00:47 
 It's waiting for the core to be done. It's going to say, ~~**oh,**~~ here's a core that's done. Let me reset this core. So that's going to set all the state back to empty. So it's going to clear everything out. And then it's going to say, ~~**oh,**~~ this core just got reset by me. ~~**All**~~ ~~**right,**~~ I'm going to go start it up with the next block of threads, if there's still more blocks. And if you slow down a bit, you can see there's some intermediate variables which store ~~**like,**~~ how many more blocks do I have left?

01:01:10 
 So that's blocksDistached and blocksDone. And that's just saying, ~~**okay**~~, I need to process this many more blocks of threads before this is completely done executing. And then the last thing that the dispatcher does down in this loop, and you can see at the top, there's a done signal exported from the dispatcher. That done signal is sent out to the GPU itself. And so not that one, but in the actual module definition, a little bit farther up, there's just something called the straight up done. You can see it in a couple places, output registered done. And that's going to tell the whole GPU, ~~**Hey**~~, ~~**all**~~ everything is done. And then it's going to stop executing everything.

01:01:46 
 And now it's just going to wait for the host machine. to basically pull out the results from memory, and then it ~~**was**~~ ~~**machined**~~ and just reset the GPU. And that's like one full execution loop. And you can see that the dispatcher~~**,**~~ ~~**kind**~~ ~~**of**~~ high-level, is responsible for managing that whole high-level execution flow. So ~~**yeah**~~. ~~**Cool**~~. SPEAKER_01: This is code. How does this thing get translated into hardware?

01:02:08 
 This is what the package that you're working in does? SPEAKER_00: So the way that this whole thing works is ~~**just**~~ in EDA in general, you design your architecture based on what it's supposed to do. It's a functional logic in Verilog, or in this case system Verilog, which is ~~**just**~~ ~~**like**~~ a modern version of Verilog. And then there's these entire software stacks, which is what I was talking about before, which basically translate that Verilog into an actual design. And in practice, that's really hard because there's so many different steps that go into it. You need to synthesize the Verilog down into a logic, a chart of the logic that's specified in there. And then because of the way that people program stuff, that logic is inherently not going to be perfect. It's going to be ~~**like**~~ way more complicated than it needs to be.

01:02:52 
 You can do some processing to basically get that logic down into its simplest form. That's still equivalent. So that process of turning the dialogue into a logic flow is called synthesis. And then once you do that, you can~~**,**~~ ~~**you**~~ perform a bunch of analysis on whether this design is valid or not. Then you convert that~~**,**~~ ~~**that**~~ logic that you've created into gates. And then those gates are based on a specific process ~~**though**~~. So basically, you have these things called standard cells, which are specific gate designs designed for a specific foundry's process ~~**after**~~ ~~**using**~~ ~~**gates**~~. So you convert them into gates, and then you lay out all these gates on a huge chip design.

01:03:29 
 So the gates are just like laid out everywhere all over the chip based on a bunch of different optimizations. Then you hook up all the gates with wires, ~~**like**~~ the gates that need to be connected together. And then there's ~~**like**~~ several cycles of optimization on this whole process to make sure that everything is valid. All the timings are valid because ~~**like**~~ electrical signals need to flow through this chip. And if any of the signals flow through the chip slower than the actual clock time of the chip, that's going to cause ~~**error**~~. So there's ~~**like**~~ tons of different errors there, even just at a logic level. And then on a hardware level, because this is actually ~~**like**~~ electronics. It's not just ~~**like**~~ code.

01:04:02 
 It's nice to write it in code, ~~**like**~~ the complexity is way higher. There's these things called parasitics, which is ~~**like**~~, oh, you're actually dealing with wires and metals. And if there's wires near each other, they actually have capacitance between each other. And there's ~~**like**~~ tons of other electrical effects that can suck with the chip. And so you need to do computations to prevent that stuff too. And so there's so many complicated things that at the very end, your whole design is there. It gets outputted into ~~**a**~~ the layout, and that's in the form of a GDS2 file, which just happens to be the file format specifying all the metal layers to send to a foundry. And then you do a final comparison called a layout versus ~~**sematic**~~, which is make sure that this final layout you got after tons of optimization is actually logically equivalent to your original design.

01:04:42 
 And then you can submit that layout to ~~**like**~~ a foundry. And now in practice, you're actually going to do a ton of formal verification ~~**also**~~ on this layout to make sure ~~**like**~~ All of these bad ~~**stakes**~~ states are impossible to get into. And again, in hardware design in general, the verification process is so important because unlike software, you can't ship and iterate. You're just fucked if you mess it up. There's all that stuff. But that's what happens. Again, if anyone had to program all of that, that's impossible. It's a gigantic task.

01:05:06 
 But thankfully, all these EDA softwares do it for you for a couple million dollars. And thankfully for me, there's OpenLAN, which does it for you for free. And that's the core level, but ~~**yeah**~~. Even that has some complexity going through it. ~~**Cause**~~ ~~**I**~~, once you do that, you run through a bunch of errors and then you have to fix that in your code. SPEAKER_01: ~~**Yeah**~~. ~~**Yeah**~~. You can see where the decades have gone in terms of building out the~~**,**~~ ~~**the**~~ stack ~~**talk**~~ ~~**about**~~ on the shoulders of giants.

01:05:32 
 What does this feel like on the open version that you were working on when you go to~~**,**~~ ~~**I'm**~~ ~~**not**~~ ~~**even,**~~ ~~**I'm**~~ ~~**not**~~ ~~**even**~~ ~~**sure**~~ ~~**what**~~ ~~**the**~~ ~~**right**~~ ~~**word**~~ ~~**is,**~~ but ~~**I**~~ ~~**want**~~ ~~**to**~~ ~~**say**~~ compile, but it's almost~~**,**~~ ~~**it's**~~ more like translate from ~~**hart**~~ ~~**and**~~ ~~**a**~~ designs. SPEAKER_00: That's what it's called. SPEAKER_01: hardening. So how long does that take for something relatively simple like this? And you're running on your local? SPEAKER_00: ~~**Yeah**~~, so local laptop, ~~**I**~~ ~~**would**~~ ~~**say**~~ ~~**it's**~~ pretty powerful for this stuff ~~**in**~~ ~~**that**~~ ~~**it's**~~ ~~**like**~~ an M2. So that's probably on the better side of ~~**like**~~ a hobbyist doing this stuff. And It can take 20, 25 minutes to go through the whole flow.

01:06:06 
 It does 50 different steps and that takes ~~**like**~~ a bunch of time. It does ~~**like**~~ tons of optimizations, really cool things that you can see all the different steps going on and try to understand them. And then ~~**you**~~ ~~**do**~~ ~~**it**~~ ~~**as**~~ ~~**errors**~~ ~~**on**~~ ~~**any**~~ ~~**step**~~, ~~**often**~~ ~~**you**~~ ~~**do**~~. And the one thing is obviously the stuff isn't ~~**like**~~ perfectly documented or there's ~~**like**~~ millions of errors you can run into. ~~**Obviously**~~ ~~**I**~~ ~~**think**~~ ~~**saturation**~~, but so many different errors you can run into and ~~**It's**~~ ~~**not**~~ ~~**really**~~ ~~**well**~~ ~~**documented**~~. Almost none of them are well documented and they're also ~~**like**~~ really random error messages. And so one of the challenges ~~**of**~~ ~~**it**~~ is ~~**it's**~~ a lot less easy to use. ~~**Like**~~ you just ~~**got**~~ ~~**to**~~, you just ~~**got**~~ ~~**to**~~ figure stuff out and ~~**it's**~~ ~~**so**~~ ~~**it's**~~ ~~**not**~~ ~~**just**~~ ~~**stuff**~~ ~~**you**~~ ~~**have**~~ ~~**no**~~ ~~**idea**~~ ~~**what's**~~ ~~**going**~~ ~~**on**~~ and you just ~~**got**~~ ~~**to**~~ try your best to figure out what's going on and debug it.

01:06:46 
 So I think very low transparency on the debugging side, which is probably one of the biggest. ~~**Like**~~ that's the biggest thing. ~~**Cause**~~ that's the whole flow. ~~**Like**~~ that's basically the output of the flow. It's ~~**like**~~, what bugs do I need to fix before I can get a finalized GDS? But it's doable. ~~**And.**~~ ~~**You**~~ just push through it, then it works.

01:07:02 
 As soon as you get your cloud, it's ~~**like**~~ magical. Because then you can start visualizing it, and it's pretty sick. SPEAKER_01: Yeah, it's fascinating. How many cycles would you say you went through in this project of that 20, 25 minute hardening process before you got one that actually didn't error out on you? ~~**Oh**~~, a lot. SPEAKER_00: I have all of them saved in here. I can say it's ~~**like**~~ 30, 35, something ~~**like**~~ that. A lot of different cycles.

01:07:28 
 Sorry, they're not in the repo.  They're just in my local.

SPEAKER_01: ~~**Yes**~~. ~~**Okay**~~, ~~**cool**~~. So let's come back to the overall GPU architecture. So we've got ~~**kind**~~ ~~**of**~~ these modules, components that are defined in code that are hardened into an actual Ultimately, ~~**I**~~ ~~**guess**~~ the classical word would be like etching, but now it's with this, it's like ~~**a,**~~ ~~**it's**~~ a light process, ~~**right**~~? At least the most advanced ~~**nodes**~~. ~~**So**~~ ~~**yeah**~~, ~~**thank**~~ ~~**you**~~.

01:07:59 
 So the, that translation is happening with this ~~**like**~~ decades in the making shoulder ~~**of**~~ ~~**shoulders**~~ ~~**of**~~ ~~**giant**~~ software stack. And that gives you the ability to define each of these things and then ~~**it**~~ go and come back in~~**,**~~ ~~**in**~~ 20 minutes and see if it worked or not. ~~**Yep**~~, exactly. And you have to then also define the interface, ~~**right**~~? Like the compute cores have to maintain. So maybe let's go look into, you take me where you want to go next, but I'm interested to see a little bit of the interfaces ~~**is**~~ always, ~~**I**~~ ~~**think**~~ about programming a lot in terms of interfaces, ~~**like**~~. where is it that it's declaring whether it's done or not done so that the dispatcher can reassign or not reassign as appropriate. But take us through the rest of the architecture, however you think best, and we can check out a couple of those things.

01:08:47 
 SPEAKER_00: Let's look at the gpu.se file first. That's the high-level interface. That's going to be interesting. And then we can look into the compute cores in detail and put in the trial execution stuff. This is the high-level file. There's some parameters at the top. Those are mostly unimportant. The interesting ones are the number of cores and the number of threads ~~**per**~~ ~~**block**~~.

01:09:03 
 So here, yeah, I guess it's just like the two~~**.**~~ ~~**That**~~ thing you can just simply change around. So you can switch the number of cores to three or four or whatever. And you can switch the number of threads in each block, which is just going to change how many threads can get executed on each core at once, which means that there's going to be more compute resources and register files and everything on each core. And that will make more sense later. ~~**but**~~ you can see the high level interface here. You have the clock and reset. Those are mostly unimportant.

01:09:25 
 Those are just like the, you have that for every design and that's just going to allow you to have clock cycles on your GPU and reset stuff. SPEAKER_01: The interesting part of the question on clock also, it seems like. If everything is built perfectly, everything would take the same amount of time. I'm a little bit confused as to why, if I distribute four threads across this thing, ~~**like**~~, why don't they all finish at the same time? Good question. SPEAKER_00: So the reason, the sole reason for this, and this is the reason behind my memory intuition and why George Hopkins said the whole problem is memory, we'll see that when we dive into thread execution, is ~~**like**~~, if there was no reading and writing from memory, it would all finish at the exact same time, because everything is deterministic there. It just takes the same amount of cycles for every instruction. The thing is, when you have memory, ~~**like**~~ loading from global memory, that memory is DRAM.

01:10:17 
 So first of all, it's using capacitors to store memory, which has some latency to it. You don't just get it back in one clock cycle instantly, which has some non-determinism. The bigger thing is the bandwidth issue. So ~~**like**~~, if I have A thousand compute cores requesting some values from memory and memory can only support a hundred reads at once. That means that this is going to start to get chewed up. So there's going to be ~~**like**~~ a thousand compute cores request data. Then a hundred of them are going to get back data to the MIP center, which means that certain cores, even if they're on the same, even if they're being processed together, they might make requests to memory at the same time. They may not get the data back at the same time because memory has an unknown latency.

01:10:52 
 It's asynchronous. And that means that if one thread gets memory back earlier or later than the ~~**other**~~ ~~**ones**~~, then it may have to wait for the ~~**other**~~ ~~**ones**~~, or there's a bunch of stuff that happens around that. So my design, it waits for the ~~**other**~~ ~~**ones**~~, which means it can't just keep going. And then similarly across the cores, even if they all start executing threads at once, one core might finish way before the ~~**others**~~, because it just got the data from memory back earlier. And so that constraint has really no bottlenecks to see, ~~**I**~~ ~~**think**~~, which is why everybody said it's all about managing memory latencies. That really is the bottleneck. SPEAKER_01: And the clock, just to set a foundation on that ~~**as**~~ ~~**well**~~, is essentially an electrical impulse. It's a change in voltage that is getting applied through the whole chip.

01:11:37 
 Is that right? SPEAKER_00: Yep, so the clock is just basically a cycle, it's like a sine wave almost, of some signal going from high to low over and over at some periodic interval, usually a couple nanoseconds, like 20 nanoseconds or something like that. And basically every time the clock goes from 0 to 1, it's just like a sine wave, but every time the clock goes from 0 to 1, registers in the chip, which are like ~~**deep**~~ flip-flops, they will take in a new value. And so that creates the ~~**wafer**~~ state. That type of state transfers on the clock. So whenever the clock happens, which means that it sets the pulse of the ~~**sense**~~ happening. So it's like generally in each clock cycle, you're going to have the start of the clock ~~**cycles**~~ and new values are set. Then you have all these wires propagating signals and the next clock cycle, the values are going to get set again.

01:12:22 
 And so it's just this time interval where the state is set on the clock cycle, and then stuff still happens in between that time ~~**of**~~ where state is set. And that's going to help you influence what's the next state. And the other thing that's important about the clock is you can't just set the clock instantly fast because the chip actually has physical constraints, ~~**right**~~? Electricity actually needs~~**,**~~ ~~**if**~~ ~~**you**~~ ~~**need**~~ something on all the way on the left side of the chip to communicate~~**,**~~ something all the way on the right side of the chip in one clock cycle. There's actually constraints there because the electricity takes time to propagate across the chip, which means that your clock cycle needs to be longer than the maximum propagation time of signals that need to communicate in your chip. And that's actually something that EDA ~~**stacked**~~ ~~**up**~~. So that's why ~~**I'm**~~ ~~**doing**~~ some static timing analysis to make sure that all the timings in your chip are valid based on the clock cycle time. SPEAKER_01: Gotcha.

01:13:07 
 Okay, cool. So if something is.~~**Waiting**~~, ~~**again**~~, it sounds like you could implement this in probably a lot of different ways, but you could have your cores saying, I'm in waiting mode. So this cycle, I basically just do nothing because I'm still waiting for the memory to return. And then next cycle, if I got it back, then I can actually do it. And that's because there's different latencies. They can just diverge in terms of where they are in their particular execution flow. In an actual production environment with the ~~**however**~~ ~~**many**~~ gobs and gobs of gates ultimately that are being included in a single chip now, presumably there's also some difficulty there, some inconsistency or whatever that could create.

01:14:00 
 Divergence, I don't know if that's the kind of thing that would get disclosed, but ~~**I**~~ ~~**imagine**~~ there must be some sort of engineering just to try to not allow some local defect to ruin the whole thing, ~~**right**~~? SPEAKER_00: Yeah, ~~**I**~~ ~~**would**~~ ~~**say**~~ that's more on the fabrication layer. There's tons of, like local defects is actually one of the biggest problems in ~~**like**~~ the fabrication industry. There's, that's why contamination control is such a big thing because ~~**I**~~ ~~**do**~~ start scaling down the size of individual transistors. Now, smaller and smaller defects that didn't matter before can actually break everything because they can get in the way of gates or ~~**like**~~ getting, basically there's tons of different places in the chip where a contaminant can just ruin the whole thing. And that's why there's just ~~**like**~~ all the technology on actual fabrication has been advancing to smaller and smaller scales. The contamination control technology has had to get better and better. But that being said, that's more so a concern of the actual fabrication process.

01:14:51 
 The reason is that they actually have a code of testing and the testing is built around one, they have a lot of contamination control. Two, they have ~~**a**~~ testing to make sure that even if there is some contamination breaking the chip, that they're going to be able to test that and discard the bad ones. That process is imperfect. But generally speaking, that's one of the biggest things that determines chip yields in practice. And generally speaking, that's ~~**a**~~ concern of the foundries. Not really someone like NVIDIA who's doing the design. They'll just ship off their designs to TSMC, and ~~**the**~~ TSMC will often be concerned with that. But generally speaking, NVIDIA can probably assume that most of the chips are working once they get out.

01:15:25 
 SPEAKER_01: Cool. Alrighty, should we go back to the diagram? SPEAKER_00: Yeah, we can do that. So now we can go into what's going on inside each compute core. So we already saw the interface of the dispatcher. So each compute core is basically going to get told by the dispatcher, you're executing this block ID, and you're going to get this many threads in the block ID, would say, when dispatchers would say, ~~**hey**~~, execute four threads in block number two, and tell me when you're done, basically. So what's going to happen is the dispatcher is first going to reset the compute core, then it's going to tell it, here's your block number, here's how many threads to execute, and it's going to tell it start. It's going to send it a start signal similar to how the GPU got a start signal.

01:16:03 
 And then the question is, what's going on inside the compute core to process those threads up until the point where it's reporting back to the dispatcher, then it's done. That's ~~**like**~~ the high-level interface at compute core. And so there's a couple of key pieces ~~**of**~~ ~~**core**~~. And first of all, there's ~~**like**~~ the high level control logic that's shared between all the threads. And then there's ~~**like**~~ the actual thread execution where we split out into a number of different threads within the core. First, we can talk about the high level logic that's shared between all threads, and then we can dive into the thread. So high level, there's basically three units that are shared between everything. So there's the scheduler.

01:16:36 
 And again, this is where there's a ton of complexity. It's now managing the resources of however many different groups of resources there are in the Hadoop ~~**viewcore**~~. Now in practice, Hadoop ~~**viewcore**~~ isn't actually thought of as having a certain number of threads. It just has some finite number of resources. And then however many threads it needs to execute, they just ~~**stay**~~ executed on those resources. And typically, a scheduler is responsible for handling the difference between, if this docker says, hey, I have to do eight threads, And you may only have four different groups of resources on your compute core. So now the compute core needs to manage the execution of these threads in parallel or in sequence, or ~~**how**~~ ~~**her**~~ ~~**determinants**~~ is most efficient. So again, that's why there's a lot of complexity in the scheduler.

01:17:15 
 In my case, what I basically said is, Each compute core is just going to process one block at a time. So it's not going to be able to say, ~~**hey,**~~ you can give me eight threads and I'll do four at a time. It's just going to be like, just give me however many threads I support. So each compute core supports four threads at a time, which means that each resource ~~**a**~~ thread needs, it's going to have one~~**,**~~ ~~**one**~~ resource. So like each thread needs a register file. So compute cores have four register files. Each thread ~~**boots**~~ its own ALU, it's going to have four ALUs, so you get the point there. Basically, what happens is the scheduler handles the execution of the threads in terms of~~**,**~~ ~~**there's,**~~ let's say, 15 instructions that each of these threads needs to execute~~**,**~~ ~~**right**~~?

01:17:54 
 So now the scheduler is going to constantly execute the control flow of how these instructions get handled. And so the way that happens is, let's say we're on instruction number zero. So we've just been issued a group of four threads from the dispatcher, and we need to execute these threads. So we're on instruction number 0. So what we're going to do is, first of all, we ~~**only**~~ know to start with instruction number 0. We don't ~~**even**~~ know what that instruction is. So first we need to somehow get that instruction from where it's stored. It's stored in program memory, because that's where we put it before at the start.

01:18:26 
 And so the Fetcher is the first thing that's going to execute. So the scheduler is going to tell the Fetcher, ~~**hey**~~, ~~**don't**~~ retrieve this instruction from program memory. So that's the whole job of the Fetcher, basically, is to get instructions from program memory. And then in practice, it actually ~~**doesn't**~~ ~~**catch**~~ it, because instructions are very repetitive. All threads are going to be executing the same instructions. So one optimization we can do that's nice is that the Fetcher, once it gets an instruction, once it can just store it locally, and then all the threads can just use that again instead of going ~~**global**~~ memory. So that's the fetcher. And then what the decoder is going to do, which is going to basically get us into the ~~**throw**~~ ~~**down**~~ ~~**hole**~~ is we now have this instruction, which is a bunch of ones and zeros.

01:19:05 
 It has this op code, which is going to tell you what the instruction is supposed to do. And it has all the registers and specific values. Now we need to somehow translate that instruction into something that all these different compute resources can actually use. And so that's the job of the decoder. And as we get into the specific thread, it will become a lot more clear what the decoder is actually doing. So let's look at the actual compute resources that are in the thread. This is the last piece on this diagram, and then we'll go on to the next diagram. Within the thread in my design, there's basically four key pieces of memory and resources.

01:19:38 
 So each thread has its own register file that we've already talked about. That's each thread has its own ability to perform computations on some data. And then importantly, it has its own program counter~~**.**~~ ~~**which**~~ means that each thread can be on its own line of the program. And that's because based on the data, different threads might have to jump around different lines of the code. And in practice, that's challenging to implement. So that's called branch divergence, which means ~~**like**~~ different threads branch to different lines of the code. Now in our design, even though each thread has its own program counter, for simplicity, all the threads are assumed to be ~~**basically**~~ continuing on the same instruction.

01:20:14 
 And it just so happens that I wrote programs where that is the case, ~~**like**~~ all the threads stay on the same instruction. So this design was sufficient for implementing that. And then ~~**last**~~ two things~~**,**~~ ~~**which**~~ ~~**are**~~ ~~**important**~~. So each thread has its own ALU, arithmetic logic unit, which is the actual compute that's ~~**like**~~ performing multiplication ~~**for**~~ ~~**addition**~~. So that's doing all the arithmetic instructions ~~**in**~~ ~~**there**~~. And it's performing computations ~~**of**~~ ~~**the**~~ ~~**registers**~~. And the last thing is the load store unit, or the LSU. And that's the thing that's responsible for fetching data from memory.

01:20:42 
 And so that's the thing that does the load and the store instructions. So as you can see, each thread has its own unit that is separately able to load data from memory and store data into memory. And importantly, before we dive into the specifics of how all these pieces interact, because ~~**I**~~ ~~**know**~~ it's ~~**like**~~ completely unclear at this point, it's just ~~**so**~~ ~~**I**~~ ~~**guess**~~ it's ~~**like**~~ these high level things, ~~**I**~~ ~~**wonder**~~ how they actually work in practice. The important thing is that The fetcher and the LSUs, there's ~~**like**~~ a bunch of them across the GPU, ~~**right**~~? Because each core has a fetcher. And that means that if we have four cores, there's going to be four fetchers. And they might all be requesting instructions at once. And then similarly, there's going to be ~~**like**~~, there's four LSUs per core and four cores.

01:21:18 
 There's going to be 16 LSUs in the whole GPU. And those could all be requesting memory at once. The question is, how do we manage the constraint of memory has a fixed bandwidth? ~~**Like**~~ let's say I can only take two requests at once ~~**or**~~ ~~**something**~~, and all these resources are requesting memory at the same time. So you need something to actually ~~**like**~~ throttle all the memory requests and on the GPU side, hold them. So ~~**it's**~~ ~~**here**~~ all ~~**of**~~ ~~**our**~~ requests and then send them slowly to memory to respect the bandwidth that ~~**every**~~ ~~**app**~~ actually supports. And then slowly send those responses back into the individual compute units in the core. So that's what the memory controllers are for.

01:21:52 
 So there's one for program memory,one for data memory. And those controllers are basically responsible for respecting the bandwidth that the global memory actually accepts. And then taking all ~~**of**~~ the requests from compute resources and throttling them to the bandwidth of what the memory accepts. That's what the memory controllers do. And ~~**yeah**~~, with that, I'll pause there. And then after this, we can dive into the actual execution. SPEAKER_01: ~~**Yeah**~~, ~~**I**~~ ~~**think**~~ ~~**I**~~ get it. And again, all of these things are at this point, purely physical, ~~**right**~~?

01:22:21 
 There's we've left the realm of software ~~**some**~~ ~~**time**~~ ~~**ago**~~. So the software really stops at the point where the kernel is compiled. Is that the right word ~~**in**~~ ~~**that**~~ ~~**case**~~? And put into the program memory. And then everything after that is ~~**a**~~ purely ~~**a**~~ function of the hardware. Hardware is routing everything through just an insane tangle of ultimately logic gates. ~~**Exactly**~~. ~~**Yeah**~~.

01:22:54 
 Does anyone have an intuition for that at this point? It seems like we've got so high up this tech stack over the decades that I wonder, is there anyone that can really go to the ~~**sort**~~ ~~**of**~~ gate level and understand ~~**like**~~ how something as complicated as a data memory controller works? Does anybody have an intuition for that at this ~~**sort**~~ ~~**of**~~ logical level? SPEAKER_00: Yeah, definitely. Yeah. So I think most people ~~**in**~~ doing architecture and design stuff, they'll probably understand most of these elements at a gate level. Obviously, more complicated logic, it's futile and not really, there's no point really to try to understand exactly what's happening at a gate level. I think the main intuition you need is understand the core of how memory works at a gate level, usually like static RAM.

01:23:36 
 And that's a pretty interesting, you understand how ~~**like**~~ a latch works and then a flip ~~**block**~~ works. And that's ~~**like**~~ the core of understanding memory. In practice nowadays, every step, ~~**like**~~ most every dynamic and only ~~**like**~~ cache is static, but that's something people understand at a low level. And then ~~**like**~~ ALUs and again, we're going to go into the diagram and that's going to show you basically. as close to the low level as you get. It's going to show you multiplexers and stuff, which again, are pieces that people understand at the hardware level. But ~~**yeah**~~, ~~**I**~~ ~~**would**~~ ~~**say**~~ people do understand it at that level. Now in practice, you don't really need to, as long as you ~~**know**~~, as long as you trust the process, you're just ~~**like**~~, okay, ~~**I**~~ ~~**know**~~ that this is going to get translated into gates in this general way.

01:24:11 
 You can study the gates in computer architecture class and you don't really need to go past that. And then ~~**I**~~ ~~**would**~~ ~~**say**~~ really ~~**cracked**~~ architecture people. There's a ton of stuff. The thing that makes them really good is that they're managing a lot of stuff in their head, which most people are not. And usually that's not really gate level logic, although ~~**I'm**~~ ~~**sure**~~ they have that in their head too. It's more like understanding the implications of stuff besides just the high level. Like design's a really nice high level ~~**in**~~ ~~**practice**~~. There's actually a lot of complexity.

01:24:36 
 Like for example, writing code in Verilog, ~~**like**~~ knowing how many gates a piece of code actually translates into is important. ~~**Like**~~ for me, ~~**it's,**~~ ~~**oh,**~~ I might just write the divine sign as my divided instruction. Just to see. In reality, nobody would ever do that. ~~**Like**~~ the divide sign actually turns into a gigantic piece of hardware, which you would never actually want to use. ~~**Like**~~ you wouldn't actually want to implement the division thing as an instruction. So ~~**it's**~~ ~~**like**~~ things like that. And then also being able to manage all the parasitics and knowing the actual electronics of it and how the electronics work if you place things in different places.

01:25:05 
 There's like tons of things like that. Honestly, the way ~~**I**~~ ~~**would**~~ ~~**describe**~~ it high level is people have a surprising level of understanding of the EDA flow and the things that influence that flow in their head, and they can get it by intuition. Obviously not the whole thing, but pieces that most people want. And somewhat ~~**I've**~~ ~~**noticed**~~ from a very limited perspective, that's what a lot of the really good architecture people have. So ~~**I**~~ ~~**would**~~ ~~**say**~~ certainly there's people with intuitions and all these things. Fascinating. Okay, cool. SPEAKER_01: So should we go to the thread diagram?

01:25:34 
 SPEAKER_00: Yeah, we can do that.  And this is going to be nice, ~~**especially**~~ for people who are familiar with CPUs, because this is going to get into very familiar territory. So, you can see here, something that looks very similar to a CPU in many ways, it ~~**almost**~~ is ~~**exactly**~~ like a CPU, except for the LSU and the little details of the register file. But what you see here is, first of all, most importantly, everything that has red text and blue text, that is stuff that's coming from the decoder. So now you can actually see exactly what the decoder is doing, which is the decoder is responsible for converting the instruction into these red and blue signals. And these red and blue signals are going to be used as control signals to control the execution of an individual thread. So what you see here is a single thread. ~~**Obviously**~~ there's multiple of these per core.

01:26:19 
 And all of them are going to get the same control signals ~~**once**~~, since the decoders are shared between the core. And now you can see how the execution of a thread is happening with the control signals. And you can see the familiar elements from the diagram above. So you have the register file, you have the ALU, you have the LSU, and you have the PC. And this is standard CPU stuff, but the general flow of what you're seeing is you have the register file. You have the ability to access two registers from the register file, which are called the source and transfer registers. That's why they're called RS and RT. And those are also ~~**like**~~ there in our ISA.

01:26:53 
 And using those registers, there's a ton of different stuff you can do with it. And so the job of the decoder is to tell the thread, sorry, ~~**the,**~~ ~~**like**~~ the compute resources dedicated to the thread at any given point in time, what stuff am I supposed to do right now? And so it's going to basically do that by turning off and on different parts of the compute unit. For example, if you look in the ALU. Let's just focus on that one unit. So you see the arithmetic unit. And that can do 4 different things. So we know from our instructions that the arithmetic unit ~~**and**~~ ~~**the**~~ ~~**AL**~~ ~~**unit**~~ is able to do addition, multiplication, subtraction, division.

01:27:26 
 And those are going to be literally 4 separate hardware circuits that do each of those things. ~~**And**~~ so now the question is... the arithmetic unit is thinking in these two values, it needs to perform one of those computations on it, ~~**right**~~? So what it's going to do is it's going to take in this control signal, the ALU arithmetic multiplexer, which is a much shorter multiplexer, and that's going to tell it, ~~**hey**~~, so actually what it's going to do is it's going to do all four at the same time. It's going to both multiply, add, subtract, and divide all of them. And it's going to divide those four outputs into this thing called a multiplexer, which is just like a signal chooser. ~~**And**~~ so now you have all these possible values it could choose. Now this multiplexer value set by the decoder is going to say, ~~**hey**~~, which one should I actually take? So let's say we did ~~**like**~~ an addition instruction.

01:28:10 
 The decoder is going to set this mux value to zero, which is~~**,**~~ ~~**we'll**~~ ~~**say**~~, the thing associated with addition. And now that multiplexer is going to take these four inputs, ~~**just**~~ going to output the one specified by addition. And that's going to come out into the ALU output box. And then here's another multiplexer ~~**just**~~ to be exhaustive with the design. So now you see, okay, we have the output from the arithmetic unit. Then again, there's also going to be an output from the comparison unit on every single cycle. So it's not ~~**just**~~ we're only performing the computation done by the things specified by the instruction. Actually all the computations are happening because they're ~~**just**~~ in wires.

01:28:43 
 So they're all just connected up,~~**all**~~ the wires are always connected. The question is which computation are we actually taking this time. So the ALU output multiplexer is going to choose, hey on this instruction it should be set to 0 because we're going to take the arithmetic unit's output this time. And that's going to get outputted. And then that value you can see in the rest of the design, most importantly, that's going into the register input multiplexer. And so just to finish this part of the flow, the register input multiplexer is going to be 0 this time, and it's going to say, hey, we got some value from the ALU, and we actually want to use the ALU value this time and store it back in the register. So you see, OK, we're choosing the ALU output, and then that's going into the destination register, which is what RD is. And we're going to save that back into the register file in the ~~**registry**~~ register specified by the instruction.

01:29:28 
 And so now you see, OK,  that's actually a full loop, for example, with the add instruction. Like, it's basically computing the addition, choosing to output that addition from the ALU, and then choosing to store that output back in the register file, and that's really how you get the addition value. And so similarly with the LSU, pretty simple, so you have control signals to enable read ~~**a**~~ device, and when you set those signals, and everything else is going to be off those times, as specified by the decoder, it's going to make a request out to the memory controller, the memory controller is going to make a request out to memory, and it's going to ~~**type**~~ ~~**it**~~ back, and this LSU is going to It's going to be in a waiting state, and the scheduler, which we talked about before, is going to see that this thread's LSU is still waiting, and it's just going to keep waiting until the LSU gets a response from memory. And the last thing which will help here is the PC. So if you go over to the program ~~**panel**~~ ~~**unit**~~ in the top right, So this is the stuff I was talking about before with the whole branch ~~**of**~~ compare instructions. So generally the next program counter is just going to be the next line of code, which means we just want to add one to the current program ~~**center**~~. Very straightforward. ~~**I**~~ ~~**love**~~ ~~**it**~~.

01:30:31 
 If we're on a comparison instruction,~~**he**~~ ~~**stole**~~ ~~**out**~~ ~~**the**~~ ~~**tiny**~~ ~~**bit**~~ to the NCP register. So if we're on a comparison instruction, we are going to actually ~~**def**~~ the NCP register. And that's going to tell us, hey, the comparison we just did, it has these NZP values. It was either negative zero or positive. Then on the branch instruction, it's going to use that NZP ~~**tester**~~. And you can actually see the NZP signal coming in from the instruction that we set before. And what that's going to do is basically check, hey, did the comparison instruction match this~~**?**~~ ~~**like**~~ ncp, ~~**like**~~ the set of ncp values.

01:31:02 
 If it does, then we're going to jump to that immediate value specified by the instruction. Again, you don't really need to understand exactly what every single control signal is doing. If you want to, you can look at the diagram and it will become much ~~**more**~~ clear~~**,**~~ ~~**more**~~ ~~**clear**~~ than my explanation. But ~~**I**~~ ~~**think**~~ the important thing is that you understand generally that, oh, this is the layer where actually the instructions are getting literally translated into hardware signals. And ~~**it**~~ generally the flow of how the thread gets executed, how the program counter gets updated. ~~**and**~~ how register values and global memory data values get updated. And that's basically how the entire thread gets executed, ~~**like**~~ in the kernel. And if you remember in the register file over there, there's those three special values, which give us context on where we are in thread execution.

01:31:44 
 That's getting set by the core.  And that's basically, that's how threads get executed. And this is ~~**like**~~ the low level of the individual compute of the CPU. And again, it's very similar to a CPU because at this level, we have basically ~~**entirely**~~ some similarities to CPUs. Basically, it's ~~**like**~~ a mini CPU here. SPEAKER_01: Cool. That's really very well done. And if you're listening in audio only mode at this point and a little confused, definitely check out the YouTube or jump straight to the GitHub project and check out the diagram because it definitely will help if anything is unclear from the verbal description.

01:32:21 
 Okay. You've designed all this. You've got all the code. We've taken the spike from what you would do at the programming layer, how that gets propagated through the hardware. Is it time to go on to how you actually get to visualize and verify that this is working? SPEAKER_00: ~~**Yep**~~. We can go to that quickly. So if you go into the test folder, I'm going to show you quickly how ~~**I**~~ ~~**like**~~ simulated everything.

01:32:47 
 And then I don't know on the tweet, there's also a video of what's actually happening in GPU, but ~~**yeah**~~. So in the test folder, we can just look at, for example, the test in that ad kernel. So literally all we have here is I manually compiled all of those kernels I wrote into the valid machine language for my GPU. So here you see the matrix addition kernel. And then in Python, I wrote this external interface since the memory is external to the GPU design. It's not in the hardware. So I made ~~**like**~~ a simulator ~~**right**~~ there that basically simulates exactly the behavior of the memory. On every cycle, it just checks that basically I've requested the memory from the memory controllers.

01:33:23 
 And if there is, it responds with the data. The reason is because memory is not part of the GPU. It's usually like an external DRAM or ~~**a**~~ ~~**cut-by-endwidth**~~ memory or something like that. So you wouldn't really include that as part of the design. So that's what we're simulating here. In the actual, if I actually tape this out with ~~**potty**~~ ~~**tape**~~ ~~**out**~~, there'll be like an external DRAM similarly like this. And so we load up the program memory here. load up the data memory, as I said, exactly like I was typing right there.

01:33:48 
 And both of these are simulated memory. And then ~~**also**~~ right there, I set the value a little below for the device control register, telling it to execute eight threads. And then ~~**basically**~~ all you do is, ~~**so**~~ I have this setup program, and all that does is it's actually going to take those memory values and load them in. It's actually going to take the threads and load them in. And then it's going to call start. And ~~**so**~~ it's going to set the start sequence to five. ~~**and**~~ it's going to trigger everything. And then we have this little piece of code displaying the data memory just for visualization purposes, ~~**so**~~ you can see what it's starting out as.

01:34:17 
 And then, now you're seeing the exact way to interface with it. So you're seeing~~**,**~~ ~~**like**~~, just wait until the GPU's done signal is set to 1. In other words, the GPU says it's done. So it starts out at 0. We're going to say keep running cycles until the GPU says it's done. And for each cycle, the GPU is going to be processing. And I have this nice piece of code here, which is written in one of the helper files, which is just going to display out the state of the GPU. And so what that actually does is ~~**it's**~~ going to show you every single thread and every single core.

01:34:44 
 What are the registers being read?  What are the current register values? What is the ALU output? What is the LSU output? What are ~~**like**~~ the states of the core? Is it ~~**like**~~ waiting or is it executing or is it decoding or fetching or whatever? Those are really nice to see for the control flow and debugging. And ~~**I**~~ ~~**heard**~~ to anyone who's really curious to see that ~~**that's**~~ ~~**like**~~ a sick learning process.

01:35:01 
 Just go run the matrix edition kernel. There's ~~**like**~~ instructions in the repo and then go look at the log file that gets generated. You're going to see 500 cycles. ~~**Oh**~~, every single thing that's happening inside the GPU to execute this code~~**.**~~ ~~**which**~~ ~~**is**~~ ~~**pretty**~~ ~~**fun**~~, but ~~**yeah**~~, it's pretty simple. And then I did the same thing for matrix multiplication. So you can see the whole execution trace. Then you could see at the end, it prints out the final data memory and you can actually see~~**,**~~ ~~**you**~~ ~~**could**~~ ~~**see**~~ the final values that have been correctly computed.

01:35:27 
 And for anyone curious to see if it doesn't want to run it, there's a video of it, which is in the tweet thread that will probably be linked here. SPEAKER_01: But ~~**yeah**~~. SPEAKER_00: Cool. SPEAKER_01: This is awesome. ~~**I**~~ ~~**think**~~ anything else we want to talk about at the real low level? Otherwise, maybe ~~**I**~~ ~~**have**~~ just a couple high level questions to ask ~~**to**~~, to conclude with. SPEAKER_00: ~~**Yeah**~~, ~~**I**~~ ~~**think**~~ this is great on the low level. People who have questions, they can just DM me or put an issue on GitHub or whatever.

01:35:49 
 So already a lot of people have been contributing to the GitHub more than I expected, but ~~**yeah**~~. SPEAKER_01: ~~**Yeah,**~~ ~~**I**~~ ~~**think**~~ there's definitely a lot of curiosity out there about this sort of thing right now, but it's not easy ~~**to**~~ ~~**,**~~ to go as far as you have. So I'm not surprised at all that people want to follow in your footsteps. Certainly I'm one of them. Coming out of this project. Is there anything that still feels mysterious to you or questions you weren't really able to get a good grasp on things you wish you understood better? Do you feel like you have it all reasonably well understood? SPEAKER_00: ~~**Yeah**~~, so ~~**I**~~ ~~**think**~~ ~~**I**~~ got basically my goals on it.

01:36:22 
 And my goals with anything learning wise in tech, ~~**like**~~ in a technical capacity is usually not necessarily to be ~~**like**~~ super niche deep in every single area, because honestly, it is an infinite time going deep in any engineering discipline. It's more so to get to a level where I understand the entire landscape from an engineering standpoint, and all the key intuitions. And I'm actually deeper than most of the people who are just going in for an entrepreneurial engineering goal. So I don't get ~~**all**~~ ~~**this**~~ ~~**far**~~ with engineers and anything, and I fully understand everything they're talking about. But then I'm mostly entrepreneurially motivated, ~~**like**~~ entrepreneurially motivated in terms of obviously, in this case, there's not really much entrepreneurial opportunity, ~~**like**~~ ~~**like**~~ a super lindy industry because I'm not going to go compete in GPUs or something. But it's just definitely how I approach technical things. And so I feel like I got to the level of intuition where I can now understand anything I need to. And I put at the bottom of the repo readme ~~**like**~~ some of the more interesting, more advanced GPU concepts that I looked into that I didn't implement.

01:37:18 
 So that was fun. But yeah,~~**generally**~~, ~~**I**~~ ~~**think**~~ ~~**I**~~ accomplished my goals. ~~**I**~~ ~~**don't**~~ ~~**think**~~ ~~**I**~~ ~~**would**~~ ~~**want**~~ ~~**to**~~ ~~**spend**~~ ~~**the**~~ ~~**time**~~ ~~**to**~~ ~~**go**~~ ~~**super**~~ ~~**deep**~~ ~~**into**~~ ~~**any**~~ ~~**one**~~ ~~**of**~~ ~~**these**~~ ~~**things**~~, ~~**unless**~~ ~~**I**~~ ~~**happen**~~ ~~**to**~~ ~~**be**~~, ~~**unless**~~ ~~**it**~~ ~~**happens**~~ ~~**to**~~ ~~**be**~~ ~~**interesting**~~ ~~**to**~~ ~~**some**~~ ~~**frontier**~~ ~~**problem**~~ ~~**where**~~ ~~**it's**~~ ~~**actually**~~ ~~**useful**~~ ~~**to**~~ ~~**go**~~ ~~**into**~~ ~~**those**~~. But ~~**that's**~~ ~~**generally**~~ ~~**how**~~ ~~**I**~~ ~~**think**~~ ~~**about**~~ ~~**it**~~. SPEAKER_01: What ~~**do**~~ ~~**you**~~ ~~**think**~~ ~~**this**~~ ~~**tells**~~ ~~**us**~~, ~~**if**~~ ~~**anything**~~, ~~**about**~~ ~~**the**~~ ~~**future**~~ ~~**of**~~ ~~**the**~~ ~~**chip**~~ ~~**industry**~~? ~~**I**~~ ~~**feel**~~ ~~**like**~~ ~~**there's**~~ ~~**a**~~ ~~**lot**~~ ~~**of**~~ ~~**debate**~~, ~~**obviously**~~, ~~**around**~~ ~~**is**~~ ~~**Nvidia**~~ ~~**going**~~ ~~**to**~~ ~~**take**~~ ~~**over**~~ ~~**the**~~ ~~**world**~~? ~~**Is**~~ ~~**AMD**~~ ~~**going**~~ ~~**to**~~ ~~**make**~~ ~~**a**~~ ~~**comeback**~~? ~~**Why**~~ ~~**is**~~ ~~**like**~~, ~~**TSMC**~~ ~~**stock**~~ ~~**not**~~ ~~**up**~~ ~~**nearly**~~ ~~**as**~~ ~~**much**~~ ~~**as**~~ ~~**the**~~ ~~**chip**~~ ~~**designers**~~.

01:37:53 
 And I feel like somebody might say, ~~**geez,**~~ you made it pretty far in two weeks. If you were actually to try to go manufacture something, you wouldn't make it nearly as far in two weeks. That would seem to suggest that there's something a little out of whack where the Like why is all the value accruing to the design side as opposed to the actual manufacturing side? But what's your take on that, if any, coming out of this experience? SPEAKER_00: ~~**Yeah,**~~ ~~**I**~~ ~~**have**~~ ~~**my**~~ ~~**own**~~ ~~**opinions,**~~ ~~**but**~~ ~~**I**~~ ~~**think**~~ ~~**my**~~ ~~**opinion**~~ ~~**is**~~ ~~**so**~~ ~~**far**~~ ~~**from**~~ ~~**being**~~ ~~**credible**~~ ~~**and**~~ ~~**experienced**~~ ~~**enough**~~ ~~**to**~~ ~~**have**~~ ~~**a**~~ ~~**useful**~~ ~~**take**~~ ~~**worth**~~ ~~**saying.**~~ The one thing I'll say is ~~**that**~~ I don't think there's anything wrong with the value accrual to people with massive moats. It makes sense, especially this late on. It's such a Lindy industry, ~~**I**~~ ~~**understand.**~~

01:38:38 
 I think for people curious about that stuff, ~~**I**~~ ~~**would**~~ ~~**say**~~ just read Seven Powers, and Seven Powers will just explain why moats make sense and why, even though ~~**I**~~ ~~**did**~~ ~~**this**~~ in two weeks, realistically speaking, you can't do anything to challenge these people in most cases. And again, disruptive innovation is the place where you get to challenge incumbents, but that's not really probably going to happen much here. And the incumbents are also very competent. Like NVIDIA and TSMC, it's because it's such a well-incentivized industry that the incumbents can't possibly become incompetent because of how incentivized it is across all ~~**this**~~ ~~**busy**~~ driving the world to semiconductor industry. So ~~**I**~~ ~~**think**~~ it's definitely challenging. And there's also ~~**like**~~ a lot of takes, again, ~~**like**~~ ~~**I**~~ ~~**can't**~~ ~~**claim**~~ ~~**to**~~ ~~**know**~~ about if you can challenge CSS here in video, but ~~**like**~~ a lot of employees in video, for example, there's ~~**like**~~ a one viral tweet the other day where the guy who just saw it on video was like, now that I've left Nvidia, I can confirm that you're not catching up to them within the next 10 years. And ~~**I**~~ ~~**would**~~ ~~**think**~~ ~~**that**~~ those ~~**good**~~ factors are probably pretty accurate given the nature of the industry and ~~**like**~~ the economics of it. It sounds ~~**like**~~ it's definitely a steep hill to climb.

01:39:54 
 And now he's trying to build cheap, affordable semiconductor fabs for ~~**initially**~~ the mid-market, like the FPGA market. And there's not really an incumbent there. Like ~~**basically**~~ nobody has ever done it before. The question is not ~~**like**~~, can you beat the incumbents? It's just, is it possible to create it? If he succeeds there, he's going to create a gigantic mid-market. It's what we were talking about here where I was ~~**like**~~, oh yeah, hobbyists knew this stuff. But realistically, ~~**like**~~ most people, if you want to hit ~~**rocking**~~ scale, you just have to hit the incumbent.

01:40:17 
 You just have to use the incumbents. ~~**I**~~ ~~**think**~~ this is something where because he's tackling ~~**down**~~ ~~**market**~~ at first, he's actually legitimately creating a new market opportunity if he succeeds. So ~~**I**~~ ~~**think**~~ that's the coolest attempt in terms of this market. Trying to do something that will make ~~**a,**~~ make an impact on the young person. SPEAKER_01: And the value there is custom designs for relatively ~~**like**~~ comparatively low end use cases. SPEAKER_00: It's not even that. So it'll actually scale probably pretty far, but it's more ~~**like**~~ a current fabric costs ~~**like**~~ $10 billion. And if you want to use them, you have to ~~**like**~~ ship off your stuff and wait ~~**like**~~ a long time to get back test samples.

01:40:53 
 And then there's also the FPGA market, which is a lot faster, but you don't actually get ~~**like**~~ a real design. It's ~~**like**~~ an FPGA. And basically what we're doing is creating fabs that are ~~**like**~~ way cheaper, ~~**like**~~ on the order of hundreds of thousands instead of billions of dollars. And you can get your designs immediately because you just buy a fab and you just have it with you. ~~**market**~~, which is first of all, very obviously going to completely change the forward lending market. But eventually it may actually replace down and mid-market opportunities, which are actually pretty big in terms of you don't even need to go to the huge foundries. You can just use these things, which are way more price efficient. So that could be huge.

01:41:25 
 Yeah. SPEAKER_01: Fascinating. So the idea there would be ~~**like**~~ enterprises would run their own small fab in the same way that today they might run their own cluster or they could go to a cloud provider in the future. They might run their own fab internally because it's been modularized to the point. ~~**Exactly**~~. ~~**Yeah**~~. Fascinating. ~~**Yeah**~~.

01:41:43 
 Do you have any thoughts? One of the paper that has really caught my eye recently was the Microsoft 1.58 bit paper where basically they showed that you could implement neural networks without, there's obviously been a ton of work in ~~**like**~~ quantization of the values within neural networks, ~~**right**~~? We go from full long floating points down to smaller and smaller bit resolution. The smallest that seems to have been demonstrated to work ~~**so**~~ well ~~**so**~~ far is this 1.58 bit, which is just minus one, zero and one are the only available values in a network. And they still show that you can actually get them to work with that. One of the things they said in that paper was that this opens up a potentially totally different hardware future because ~~**Obviously**~~ you don't need nearly as much complication to just handle 1, 0, and minus 1 as compared to all the math if you're doing longer precision numbers. Does this give you any intuition for how that might play out in the future? SPEAKER_00: ~~**Yeah.**~~

01:42:48 
 So huge of true. One of the things that people criticize about the big~~**,**~~ ~~**the**~~ big measurements of compute power nowadays is flops, which is ~~**flowing**~~ ~~**point**~~ operations per second. And that's ~~**like**~~ ~~**kind**~~ ~~**of**~~ the benchmark for compute ~~**towers**~~ versus in GPUs nowadays. And one of the reasons they criticize it is because you can just~~**.**~~ If you have 32-bit floating point values, if you just switch those to 16-bit floating point values, ~~**drastically**~~ reduce the flops because it just takes less time to perform computations ~~**around**~~ ~~**size**~~. ~~**Sorry**~~, ~~**drastically**~~ increase the flops. And that's actually what NVIDIA's been doing. So they just released their Blackwell.

01:43:20 
 One of the things they've been doing, ~~**obviously**~~ they've been boosting their flops in many other ways, but their Blackwell now has smaller floating point numbers, because ML doesn't really need gigantic floating point numbers, unlike graphics. And so by doing that, ~~**just**~~ by nature of doing that, it massively boosted the flops. And it is actually meaningfully faster for the use cases that are less relevant too. And so it's the exact same implication, because if you can use 2-bit, 1.5-bit values, the flops is going to go up ~~**like**~~ crazy. And the important hardware thing is, in order for that to happen, what that actually means is that those register values, ~~**like**~~ in this case, ~~**Again**~~, this is what I meant by it's a bit cyclification, you can explain important things with it. So the register values in this case, they're ~~**like**~~, they're 16-bit register values, ~~**right**~~? They store 16 bits in them. And in real GPUs, they have a whole host of registers.

01:44:07 
 They have registers that store floating point 16 and 32-bit numbers, they have scalar registers, and they also have vector registers. So what this means is ~~**like**~~, in the hardware, you would actually have registers that store 2-bit values or something ~~**like**~~ that, 1-bit, 2-bit values. And that's going to be way faster. And that's how it gets implemented in the hardware. But ~~**yeah**~~, that's pretty interesting. ~~**Yeah**~~. SPEAKER_01: I'm curious to see how much that scales. Seems ~~**like**~~ it could be a big simplification in terms of just a lot of the components too, ~~**right**~~?

01:44:33 
 Like you imagine the sort of arithmetic layer that you had where you're saying it's doing addition, subtraction, multiplication, division all the time. If you're down to just a much more limited set of logical operations that you need to perform at each given step, that can presumably just get a lot smaller too~~**,**~~ ~~**right**~~? SPEAKER_00: Yeah, definitely. Although I'm sure that's what determines the flops increasing, but I'm sure we're grossly oversimplifying a lot of the things because I bet you wouldn't actually just have just two bit floating point registers in your whole GPU. So there's probably a lot of complexity to it. But as a high level intuition, I'm sure this is generally valid. ~~**Okay**~~, ~~**cool**~~. SPEAKER_01: That's all I've got.

01:45:12 
 Anything else you want to talk about? Otherwise ~~**I**~~ ~~**maybe**~~ ~~**just**~~ ask you what you're onto next. SPEAKER_00: Yeah, honestly, this was~~**,**~~ ~~**it**~~ ~~**was**~~ cool. ~~**It**~~ ~~**was**~~ good talking about it too. I didn't expect to get into this much technical detail. ~~**I**~~ ~~**guess**~~ another interesting thing is the place that AI played in this. ~~**I**~~ ~~**guess**~~ there's not too much to say there, except that it's very clear that AI has a huge place in learning now. And ~~**I**~~ ~~**think**~~ part of ~~**like**~~ part of my learning philosophy.

01:45:33 
 In many ways, people may see me, ~~**like**~~ people have seen my stuff now may think of me as ~~**like**~~ a ~~**chick**~~ person ~~**right**~~ ~~**now**~~ ~~**or**~~ ~~**something**~~ ~~**like**~~ ~~**that**~~. But really I'm just trying to learn a lot of stuff really fast based on a completely different high level framework I've been using to decide on this stuff, which I'm going to talk about. But ~~**I**~~ ~~**think**~~ what AI has made possible is my learning philosophy, which is extremely aggressive. ~~**Like**~~ ~~**I**~~ ~~**believe**~~ because of this, a number of different things that I've learned stuff and people can learn stuff ~~**like**~~ a hundred X, 200 X faster than they think. Obviously, as ~~**I**~~ ~~**said**~~ at the beginning with less complete ~~**.**~~ depth, but with the same or more practical knowledge, ~~**I**~~ ~~**think**~~ AI has enabled that in a pretty crazy way. And this has helped me to really feel the truth of that. ~~**I**~~ ~~**was**~~ ~~**like**~~, in many ways, this would not have been possible without AI.

01:46:14 
 So that's one interesting takeaway I have. But ~~**yeah**~~, other than that, I don't have anything specific. SPEAKER_01: Cool. Love it. This has been a fabulous walkthrough of TinyGPU. Congrats on a successful speed run. I'm looking forward to what your next one will be and certainly will be following you for updates. For now, I will say, Adam Majbudar, thank you for being part of the Cognitive Revolution.

01:46:36 
 SPEAKER_02: Thank you for having me.

SPEAKER_01: Did ~~**I**~~ ~~**say**~~ your name right ~~**the**~~ ~~**last**~~ ~~**time**~~? Okay, cool. Fantastic job, man. Really well done. ~~**I**~~ ~~**come**~~ ~~**away**~~ ~~**from**~~ ~~**this**~~ super impressed. ~~**I**~~ ~~**think**~~ you really show yourself to be the real deal. The depth is impressive and the speed with which you acquired it is definitely really impressive ~~**as**~~ ~~**well**~~.

01:46:56 
 If you're doing like 99.1, I'm doing like 90.1 or something. For a lot of these things, ~~**like**~~ I'm not going to go as far as you've gone, but I definitely benefit a lot from this. And ~~**I**~~ ~~**think**~~. People, there's just so much curiosity and it's such a black box to people. ~~**I**~~ ~~**think**~~ people literally stop and I've done a hundred episodes of this and I had the CEO of cerebrus on at one point ~~**until**~~ ~~**I**~~ ~~**would**~~ ~~**bet**~~ ~~**that**~~ most of our listeners. Could say, yeah, obviously paralyzation is super important. And then ~~**I**~~ understand ~~**kind**~~ ~~**of**~~ memory is a constraint for that a lot lately. Beyond that, ~~**I**~~ ~~**think**~~ it's like.

01:47:31 
 really no idea. This will definitely bring a lot of people ~~**a**~~ ~~**lot**~~ closer to up to speed. And ~~**I**~~ ~~**think**~~ you'll certainly have made a tremendously positive impression on them. So let me know if there's anything I can do to be helpful to you in the future ~~**web**~~ events, depending on what you do next, ~~**like**~~ could be another episode. I'm really trying to do more of this sort of thing. ~~**I**~~ ~~**find**~~ ~~**that**~~ ~~**like**~~, CEO interviews are interesting, but we got ~~**a**~~ ~~**lot**~~ more from you than we got from the CEO of Cerebros. And he's ~~**like**~~ definitely a smart guy, but he was bringing much more ~~**like**~~, ~~**I**~~ ~~**sometimes**~~ ~~**call**~~ ~~**this**~~ ~~**podcast**~~ an analogy free zone. And he was ~~**like**~~, when your grandmother's baking cookies.

01:48:07 
 And I was like, We're going to stop you right there. So anyway, I'm trying to do more of this kind of person who is not necessarily ~~**like**~~ at the edge of an industry, but who has found a way to get a really good perspective on something that's really important. And again, ~~**I**~~ ~~**think**~~ you've demonstrated that super well. So if you do it again on something else and it's at all relevant to AI, I would be keen to know about it. And if there is anything I can do to be helpful to you, let me know. It sounds like you've already raised money or whatever, but if you are going to start such a process up again, I could certainly help connect ~~**into**~~ some people that would be interested to talk to you about it. So again, I don't know what~~**,**~~ what help you might need, but if there's anything that I can do, let me know. SPEAKER_00: Yes, I'll reach out.

01:48:48 
 SPEAKER_01: Appreciate that. Cool. Do you know what your next thing is going to be? SPEAKER_00: ~~**No**~~, ~~**I**~~ ~~**wouldn't**~~ ~~**say**~~ ~~**so**~~. ~~**I'll**~~ ~~**keep**~~ ~~**it**~~ ~~**on**~~ ~~**the**~~, ~~**I'll**~~ ~~**keep**~~ ~~**it**~~ ~~**unclear**~~ ~~**right**~~ ~~**now**~~. ~~**All**~~ ~~**right**~~, ~~**cool**~~. ~~**We're**~~ ~~**off**~~. ~~**I'll**~~ ~~**hit**~~ ~~**stop**~~ ~~**anyway**~~.

