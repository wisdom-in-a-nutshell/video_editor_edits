Katya Grace, founder of AI impacts. Welcome to the Cognitive Revolution. Thank you. Great to be here. I'm excited for this conversation. You have just recently completed one of the most comprehensive surveys of the worldviews and expectations of elite machine learning researchers. And as we try to grapple with this AI moment and try to make sense of what's happening and where things are going, I think it's a really, incomparable resource. So I'm excited to just spend, ~~you know,~~ the full hour kind of digging into that and understanding both how you went about collecting this opinion and also ~~what the,~~ what the aggregate opinions are. For starters, you want to just introduce maybe yourself and AI impacts a little bit and give a little bit of the. ~~Uh,~~ inspiration or motivation behind the survey? I am broadly interested in trying to make the world better. And for the last 10 years or so, I've been involved in trying to figure out what's up with the future of AI, since it seems like potentially a pretty important thing to do.

try and cause to go well. I've been variously skeptical about whether it's likely to cause human extinction or not, but yeah, interested in figuring out these kinds of things. So AI Impact is an effort to try and answer questions like this and ~~questions about~~ just various high level questions about what will happen with the future of AI that are relevant to decision making ~~for~~ for lots of people. Is AI likely to cause human extinction? When will, ~~sort of,~~ the big AI thing be? What kind of thing will it be? ~~You know,~~ are there other kinds of changes to society that we should expect and do something about? ~~Though,~~ AI impacts is often answering sub questions or sub sub questions that are relatively in the weeds as input to those things. For instance, ~~like,~~ case studies about other technologies and whether they're ever been slowed down for sort of ethical or risk reasons and what that looks like for a recent example.

So we ~~sort of~~ do a lot of stuff that's ~~not,~~ not directly about AI, but is intended to be part of ~~like~~ a bigger network of questions ~~that,~~ that inform other questions. Cool. For the purpose ~~of a,~~ of a survey like this, Is there any ~~kind of~~ reference point that you can look to where folks have found it necessary even to ~~kind of~~ survey experts to understand the direction of their field? I would think maybe something like biotech might have something like this, or, ~~you know,~~ you could imagine, I just recently read a story about the original nuclear test at Los Alamos and how they had a little betting pool on it, ~~you know,~~ but there was no like big survey. So ~~is there any,~~ are there any kind of other touchstones that you look to? I think the thing that we most look to is just like other surveys in AI over the years where prior to the three that I've been involved with, there were older, smaller ones that often ~~sort of~~ have ~~like~~ a different definition of human level ish AI each time or something.

So it's like a bit hard to compare the results. But as we previously looked for all the things like that we could find and go back to like, I think it was one of the seventies. Yeah. Interestingly compared to over the years, ~~but~~ I guess we're not looking to them that much methodologically. Cause I think ~~they're,~~ they're maybe less careful, ~~um~~ smaller in terms of. things in other fields. I haven't looked in detail at them. ~~I guess~~ my understanding is that there's some sort of thing in climate. Yeah, that's a definitely good reference. There's certainly a lot of guessing as to the future state of the climate. So ~~that,~~ that makes a lot of sense. Okay. So the way I ~~kind of~~ plan to proceed here is just, first of all, talking about ~~like,~~ who are you surveying? Like, who are these people then ~~kind of, you know,~~ what was the survey that they took, ~~you know,~~ a little bit on just ~~kind of~~ the structure of the experience, some of the key.

definitions and framings that you used. And then I think the bulk of the discussion will be like, what, ~~you know,~~ are the takeaways? What do people actually believe in the field? But for starters, who were these people? How many are there and how did you get them to do it? We tried to write to everyone who published in six top venues. So five conferences in one journal in 2022. So as in, in 2023, we wrote to the people who had published in 2022 and We offered them 50 each. We had a little trial at the start where we offered nothing to some of them, and 50 each to another small group. And it looked like the 50 each was doing a lot better. So then we offered it to everyone. ~~We found most of their addresses, so we had to like,~~ it was a decent task to dig up their addresses.

We got their names from all of these papers, and many of the papers have emails for at least one of the authors, maybe several of the authors. We dug up the other ones from other papers of the internet somehow. And I think we found them for a large fraction, like more than 90 percent to get the exact number. So I think this is like in the range of ~~like~~ 20, 000 ish people. And we got about like 2, 700 to respond. ~~And how did you, they,~~ these are people that published in six top conferences. Is there a natural reason that you chose the six conferences? Like ~~why,~~ why not four or ten? So previously, the last two surveys we've done, we just did NeurIPS and ICML, and we want to expand it partly to a wider range of topics. So I think like AI people who are not necessarily all doing machine learning and also just to like cover more of the, top people.

~~Um,~~ so I think it ended up being six because as far as we could tell talking to people around, that was a sort of natural set of them to do. But yeah, I don't have a clear, explicit description of what makes them the ones. Is there a time horizon also on ~~like~~ how recently they published in these conferences? It was everyone in 2022, like everyone who published in 2022. Often people published in multiple of them. Quite recent, yeah. Yeah. Okay, cool. So we got six conferences, universe of 20 ish thousand people, something like a 15 percent respondent rate. Driven in part by a 50 gift card, 2, 700 plus total respondents ~~with that many respondents.~~ ~~Then we can turn toward kind of the experience.~~ First of all, interesting. And there's of course, a whole paper here. You guys have a lot of visualizations of data in the paper.

So Definitely encourage folks to go check out the graphs if they want to do a real deep dive, although we'll, ~~you know,~~ cover the bulk, I hope, of the headline results, but it's kind of a branching structure where, like, not everybody's getting every question, but you have some anchor questions that everybody's getting, and then others, there's kind of a bit of a, not choose your art adventure, I guess, but kind of a Randomly chosen adventure for them, and it takes 15 minutes, which it was kind of surprisingly fast. I would have expected to go slower. Yeah, I guess we haven't actually checked for this one how long it took. It was like an estimate, estimated based on like the previous one. Advertised as 15 minutes. Was there any free response or it's all structured? Like you must either end like select a thing or like enter a number, right?

But there weren't like any paragraph style responses that I could detect. There were some that weren't paragraph length. There's like some brief ones. ~~They're like,~~ for instance, There was one that was, what's an occupation that you think will be automated, fully automatable, pretty late, something like that. So that's, ~~you know,~~ a couple of words, but also ~~for,~~ for I think ~~all of the questions or~~ all of the sets of questions, you could randomly, I think maybe 10 percent of people after each one. There are two different questions you could get about just like, what were you thinking when you answered that? Like, how did you interpret this question? Which are ~~sort of~~ not part of the main results, but for us to be able to go back and if we're confused about a thing, check if everyone's misunderstanding it or something like that, which we have not gotten into yet.

We might do more of. ~~Um,~~ oh yeah, actually, sorry. ~~There, there are another.~~ Yeah, there are various open ended ones. There's another one that was like, what confusions do you think people have about AI risk? Something like that. Okay, maybe we can talk about some of the open ended stuff later. And I'd be interested to hear how you may think about evolving the survey in the future, particularly as you can. Potentially bring language model analysis to open end responses. I ~~sort of~~ expect surveys in general are going to be a an area that will change quite a bit. Certainly I see that in even just, ~~you know,~~ very sort of application centric customer feedback experiences. I'm like, Hey, let's just let them talk. ~~You know,~~ we don't need to ~~like~~ structure everything and ~~you know,~~ a grid or a rubric quite as much anymore. We can, ~~you know,~~ if we were looking for insights to some degree.

You can just, ~~you know,~~ what people sound off, but for these purposes, you're really trying to get to quantitative estimates. And as I went through the paper, there were ~~kind of~~ two super high level framing structures that stood out to me as ~~like,~~ first of all, just very thoughtfully done and worth understanding ~~for,~~ for our audience as well. One is that you have two different definitions. Essentially, I would say of AGI, right? Like ~~this is some,~~ there's ~~kind of~~ two different ways of thinking about like powerful AI, ~~you know,~~ that we definitely don't have now, but we could have in the future. And then there's also ~~kind of~~ two different ways of asking people when they expect certain things to happen. ~~And the,~~ maybe you can unpack first of all, ~~the,~~ the two different definitions of. AGI, if that is, ~~you know,~~ a reasonable way ~~to,~~ to think about it.

Yeah, I think so. I guess I think of them as fairly closely related, but yeah, different people ~~think,~~ think of that differently. So there's high level machine intelligence, HLMI, which is like when unaided machines, so it doesn't even have to be one machine just like ~~Any,~~ any collection of AI can accomplish every task better and more cheaply than human workers. I don't think for any particular task, there are machines that can do it. It doesn't even have to be the same collection of machines that ~~does,~~ does stuff. ~~And,~~ and we're ignoring aspects of the task to which being a human is intrinsically advantageous, e. g. being accepted as a jury member. And, ~~uh,~~ We're asking them to think about the feasibility of it, not whether it's adopted. So this is roughly like when can AI do all tasks better than humans? Then the other definition is full automation of labor, which is sort of similar except that it's asking about occupations.

instead of tasks. So, occupation becomes fully automatable when unaided machines can accomplish it better and more cheaply than human workers. Making all aspects of the occupations for which being a human is intrinsically advantageous is the same. And again, asking him to think about feasibility, not adoption. So, I think there's some chance that people of these things in mind and tend to think of ~~the~~ The automation of occupations as actually about adoption and the high level AI one about it being feasible. So when all occupations are fully automatable, then we have full automation of labor. So I would think of an occupation as probably like a big complex task, or if not composed of many smaller tasks. So I would think that if every task is. Automatable, then that implies that occupations are automatable as a subset of tasks. But, ~~like,~~ in fact, the answers that we see put ~~the,~~ the date for HLMI much earlier than the date for full automation of labor.

So, I think there's a question there about what. people are thinking. And to be clear, ~~like,~~ no one is answering both of these questions. People are randomized to receive one or the other. So you can't say for a particular person, like, why are you so inconsistent? But given that there's such a big gap, presumably any given person is likely to be inconsistent if you ask them the two questions, because it's not just random or something. There's ~~like~~ one other small difference between them, which ~~is~~ We keep in, in order to keep everything consistent from year to year. But I think it's a bit annoying actually. It's for the hl MI one and not the FAL one. We said to assume that like scientific progress sort of continues as normal. Like there's not some giant impediment to that. So ~~those are the,~~ those are roughly the two definitions.

I guess. Actually, sorry for the, ~~for the~~ full automation of labor question though, the question is different in that. First, we describe what full automation of an occupation is, and then we go through a series of questions that ask people about different specific occupations, like we give them four occupations and ask when they think those might be fully automatable. And then ask them to think of ~~like~~ a particularly late occupation and when they think that will be automatable and then ask them when they think everything will be automatable. So I think this, ~~you know,~~ leading them through the process and steps could also lead to a very different answer. And maybe that's what we're seeing. That's interesting. I wonder if it would be useful to just ~~like~~ open up a, like, this is the experience of the survey. Is there any place where people could go and actually just take it themselves or like experience what the, and they may not, ~~you know,~~ qualify necessarily, ~~you know,~~ for your population, but, ~~you know,~~ for folks like me, for journalists who want to understand, ~~like,~~ what actually happened here.

Is there a version where people can go see the actual step by step? I think there isn't actually. Sorry about that. Sorry, you can go and look at all of the questions, but they're all just like in a PDF. I think a difficult thing with just seeing what it's like is that There is so much randomization in it that you're going to get ~~like~~ one particular path through it, which is still probably informative, but you would miss most of the questions. Okay. So we've got high level machine intelligence, which is ~~when you could,~~ when unaided machines can do all tasks. And then we've got what on the surface seems like a pretty similar concept of full automation of labor, but those do lead people to quite different numbers. Let's talk about how the numbers actually get collected. Again, there's two frames here when I think the difference is also pretty interesting.

Yeah. If you want to ask a person about like when a thing is going to happen and you want them to give you a distribution over time, it's sort of going to look like, well, there's a, very low probability of it happening tomorrow and a higher probability that it will happen by the next day and so on. So we're trying to somehow get from them a distribution of probabilities increasing over time. And so ~~I guess there are sort of two natural ways to, well, maybe there are more natural ways, but~~ We wanted to do this by getting three different year probability pairs and then like drawing a line through them to make a probability distribution that we're sort of guessing is roughly close to theirs. And so the two natural ways to do this are to give them years and ask them for the probability that they think ~~there will be,~~ that it will happen by that year, or to give them probabilities and ask them in what year that probability will be met.

~~I guess~~ in 2016, we split people in half and gave half of them one, ~~uh, one probability.~~ set of questions and half the other. And they very consistently give different answers to these two things. We also tried it on ~~like~~ people from Mechanical Turk. ~~It's like, you know,~~ random survey takers for money. And they had pretty similar patterns, where In particular, if you ~~ask, if you~~ give years and ask for the probabilities in those years, you get a later distribution than if you say ~~like,~~ and in what year will there be a 90 percent probability of this? So given that they're pretty consistently different, we keep on asking them in both ways each time because we don't want to just pick one of them and go with it because we ~~sort of~~ know that it's biased. ~~We know one of them is biased or they're both by like, we don't know which one is biased if it's just one of them.~~ ~~So, uh,~~ so we ~~sort of~~ want to do something in between the two of them.

So ~~we,~~ we ask half each way and then ~~like~~ turn them all into probability distributions and then put the probability distributions back together again and average them. So let me make sure I ~~kind of~~ get this working backward. So the idea is at the end, you want to be able to say we have a curve that represents ~~the ML via~~ the elite ML researcher communities, aggregated sense of how likely these different technology phenomena are to happen over time. And ideally, ~~you know,~~ there'd be a nice smooth curve so we can, ~~you know,~~ make sense of it and look up any year and whatever. Now to get there, we have to get everybody's individual take, but to ask people to draw all these Curves is pretty tedious, ~~you know,~~ requires an advanced interface. You can ~~kind of~~ do that sort of thing to a degree. I'm like, metaculous, but it's ~~not, you know,~~ not super lightweight.

So instead you say, well, okay, if we have three percentage year pairs, then that's enough. We can fit a Gamma distribution. You can maybe tell me why gamma distribution versus a different distribution, but we will fit a formula to that, and then we can do that for every user. And then we can basically take the average of all of those fitted distributions, and that will become the final aggregate thing. And then for one more layer of complexity, the pairs themselves could be generated in different ways. You could say, In what year where will there be a 50 percent chance or ~~basically that~~ basically that's the like, what's the over under right? If you're a gambler, it's like ~~I,~~ you give me the year that which you would accept even odds. Exactly. And the flip side of that would be I give you a year. And you tell me a percentage.

That's right. Yeah. And which direction was the bias? I didn't, ~~I didn't~~ catch which one gets the earlier or the later. If I ask what odds do you give in 20 years, that will get you a later curve. So things happening later. So I guess my guess is that it's something like for pretty wild things, ~~I guess you're,~~ you're inclined to put low probabilities perhaps. So if you're given different years, you can just keep on putting lowish probabilities. Whereas if someone says like, when is it 90%? You ~~sort of~~ have to give a particular year. Once you're ~~sort of~~ accepting that it is a particular year, unless you decide that this thing is just not going to get that likely, then. It's not particularly tempting to put it extremely far out. I don't know if that's right at all. That's ~~sort of~~ how I remember which way it goes, because I'm somewhat suspecting that's happening.

Yeah, interesting. Okay. So, going back to the gamma distribution for a second, what can you tell me about that? I'm not very statistically sophisticated. One question that comes to mind is, Does it necessarily reach a hundred percent at the end? ~~Like the overall, you know,~~ I don't really know anything about what is implied by this particular choice of distribution. Yeah. I'm actually also not statistically sophisticated here, which is one reason I have colleagues. ~~Um, uh,~~ yeah, I'm not sure. I think that it doesn't have to go to a hundred. It's certainly a very thoughtful approach reading through the paper. I was like two framings of What it is we're talking about on these ~~like~~ extreme super powered relative to today's AI ~~systems,~~ systems of the future that I think is, ~~you know,~~ healthy, just sanity check. It is notable that there's quite a difference between those that the flipping of the percentages in the years is also really interesting.

And it's definitely just a super thoughtful approach to try to find some way to get like a continuous distribution from relatively. Sparse data across people. So ~~I mean,~~ I came away from it feeling like it was, ~~you know,~~ better than I would do. That's for sure. And of course, all these sorts of things are going to have their artifacts or their weaknesses or, ~~you know,~~ they're Points of question, ~~I guess, you know, what would be,~~ would you give any caveats ~~or sort of, you know, what, what do you think is~~ if somebody needs to say, what's the biggest problem with the way that this data has been gathered and ~~sort of, you know,~~ synthesized into ~~this,~~ this aggregate view, ~~what would it~~ What would jump out to you as the biggest problems with it? I'm not sure about the biggest problem, but one problem I was just thinking of then is like, I feel pretty good about us having asked things in various different ways, because I think in fact, we do see substantial framing effects or different answers for some reason, for what seemed like fairly similar questions.

So I think that's pretty good to get. But ~~I think,~~ I think that I wasn't thinking about when we were designing this in 2016, because we basically had a very similar survey ~~from,~~ from time to time, is that if you ask a thing in four different ways or something, then really opens it up to reporting biasedly. Like you really can choose one of the things and advertise it. And even if we don't do that, other people can do that. And so I think I'm pretty keen on not doing that, but I do end up having to ~~kind of~~ fight a constant battle. to get like the HLMI answers and the full automation of labor answers. I think it's very tempting to just write about the HLMI answers, partly because the full automation of labor answers are so late that maybe they seem like more ridiculous or something, especially to people who feel like AI is happening very soon.

So it's tempting to ~~look,~~ just look at HLMI and be like, Oh, and it dropped a huge amount. You know, this is all happening fast. When, if you look at all of our answers together, It's like quite a strong counter signal saying ~~like~~ people are expecting this in quite a long time. And so I think, yeah, all of this kind of like ~~different,~~ different questions about everything makes that quite hard. And it means that you ~~sort of~~ have to police it yourself, perhaps, and might make it harder for other people to trust that you're doing that well, which I agree with. I try to mitigate by, ~~like,~~ making sure that just everything is online somewhere so people can check. In terms of, ~~like,~~ overall worst problems with it, ~~I mean,~~ I think just the fact that forecasting the future is, ~~like,~~ quite hard, and, ~~like,~~ these people are not forecasting experts even, and, ~~you know,~~ they're often answering it in 16 minutes or something.

There's a lot of questions. The questions are about, ~~you know,~~ really complicated, like, what will the long term consequences for, ~~you know,~~ the world, a pretty complicated thing, be of this technology that we haven't seen at all. I don't expect the answers to be ~~like~~ very accurate, but ~~I think, you know, we,~~ we are having to make decisions about these things. And so I think inaccurate answers that are ~~kind of the best,~~ the best guesses that we can get are valuable. And so it's important to hear what AI researchers think about this, because I also think that just. Knowing what different people genuinely think about it is important for, you know, coordinating. But yeah, that's pretty different from these people are probably right. It probably will be the year that they say. Well, as we get into results, it'll also become very clear that there is a wide range of opinions and, ~~you know,~~ certainly in the aggregate, a lot of uncertainty, which means that The accuracy question is probably less central anyway, because it's not like it's a very tight estimate that we're getting right?

~~I mean, it's a~~ it's a pretty broad range of opinion. I think it's often good to be clear about ~~like what~~ what kinds of updates you might make from this. I think that you can ~~sort of~~ strongly infer that some important things are not ruled out. Like you might think these people are experts. ~~They,~~ they perhaps know that this crazy thought I have is not plausible at all, you know, with some areas that you can get to that sort of conclusion. I think here, the fact that ~~A lot of people,~~ a lot of these AI researchers put some probability on various things ~~is,~~ is worth paying attention to. Yeah. Well, let's get into it then the results are definitely interesting. So I thought maybe we could start a little bit with some general, like higher level characterizations and then work down to some very particular questions at a high level.

couple things that jumped out to me about the distributions and I'll let you ~~kind of~~ expand on this or add your own, ~~you know,~~ commentary as to what has, ~~you know,~~ stood out to you the most, but it seemed that this is pretty consistent, I think, with other things I've seen. that there is kind of a left heavy nature to the distributions. ~~Like the second quartile is, is a lot like smaller than the third quartile. Sorry, for what kind of question are we looking at? Uh, seemingly most~~ like the timing of all the tasks, right? There's ~~like~~ 39 different tasks. You know, when will a AI system be able to do this task? And we can, ~~you know,~~ start to list them in a minute, but across the vast majority of them, I would say the. range of the ~~like~~ first half or the second quartile as compared to the third quartile is just much more compressed.

So it seems like ~~there is~~ a lot of ~~Like~~ people are like, yeah, there's a decent chance that this might happen in the kind of near future. And obviously, ~~you know,~~ there's varying definitions of near for different levels of difficulty, but there's like a pretty decent, ~~you know,~~ mass in the kind of near to midterm future for a lot of things. And then there's ~~like~~ a really long tail that goes like far off into the future. That seems to be a ~~broad.~~ broadly true statement about ~~kind of~~ all the different answers. Is that fair? Yeah, I think that seems right. I guess I haven't thought a huge amount about this, but I wonder to what extent that's sort of what you get if you're predicting any kind of thing where ~~you,~~ you sort of think it will happen soon. ~~And so,~~ and it hasn't happened yet.

So the bit between right now and your kind of 50 percent it will have happened date ~~is,~~ is kind of all smooshed up. But then if ~~it~~ It hasn't happened by then. There is ~~kind of~~ the rest of eternity to spread it over somehow. Yeah, I think that's right. ~~I mean, there's definitely,~~ in everything I've seen like this, this shape is seemingly a norm. Another way to say it is the median is sooner than the mean. And intuitively that makes sense because you have a long tail possibility into the future. ~~You know,~~ you're not going to be predicting the past. So ~~that is~~ fundamental asymmetry, ~~you know,~~ kind of always leads to that. But, ~~you know,~~ basically I would say for folks who are trying to develop a mental picture and you can, ~~you know, again,~~ look in the paper or go look at like some of the questions on Metaculous, you can see actual curves that very much have this shape.

There's like a big bump, ~~you know, in the,~~ in the relatively short and midterm, and then there's a very long tail that goes on. Far into the future. Another big observation for me is just that the ranges are ~~like~~ super wide. I'd say, ~~you know,~~ maybe the headline, a figure in the paper is one that shows the middle two quartiles, that is the 25th to the 75th percentile range of expectations of the timing of all ~~these different~~ 39 specific abilities that you ask about. And the range there is, ~~like,~~ often huge. There's pretty high disagreement, or in aggregate, you could say, basically radical uncertainty about the timing of a lot of these key things. It's not like we're talking, this is ~~like~~ 12 to 18 years out. It's ~~like,~~ this is 10 to 100 years out in some cases. Uh, you said there's disagreement. ~~I think,~~ uh, I think there is disagreement.

Though, note that, ~~like, the,~~ the lines we're looking at here of, ~~like,~~ when milestones will happen are actually the averages of everyone's distributions. So they don't actually indicate disagreement so much as, like, within each person, a lot of different things being possible. So, ~~yeah, like, if the,~~ if there's a very long, line here between 25 percent and 75%. It's that the distribution you get by averaging everyone's distributions together is very uncertain. Okay. That's a key point. So I think there are some other questions that ~~may,~~ may better establish. disagreement on some key questions, but this is just make sure I can repeat this back to you. You're averaging individual distributions. So these ranges reflect the fact that the average individual, so to speak, has provided a very wide range. Yes, I think that's right. I think you could also get this from lots of different individuals providing narrow ranges, but at different times than when they got averaged together, I think you would get a flat distribution, but it at least very well could come from a lot of people having flat ranges.

There are figures of like for HLMI or full automation of labor, what the overall curve looks like and what Some of like a random subset of the individual curves look like in the background. And so I think you can see there that for those at least, the individual curves are all over the place. Like they're not in agreement, but also they're quite often very spread out. So like each person is quite uncertain often for many of them, but also there's a lot of variation between. The people, my guesses, but that's also similar for the narrow tasks. Yeah, it's interesting. I really like that visualization. This is figure three in the paper that you're referring to, right? ~~So it's, you've got kind of,~~ I love this kind of graph. It's ~~the,~~ the bright color line that ~~sort of~~ represents the aggregate and then all ~~the,~~ the faint color lines behind it.

~~And, you know, we could definitely pull this up to, to look at it, to study it a little bit more for yourself, but. for yourself, but.~~ As I'm looking at it now, ~~it is, you know,~~ for the individual predictions, ~~right?~~ It is seemingly fair to say that if you have a very steep line, you are expressing high confidence. If you have a very ~~slow sloping,~~ slow upward sloping line, you are expressing very low confidence. It seems like there are some people that are expressing high confidence. Most of those people seem to have high confidence in the relatively shorter term. You don't see too many lines that are ~~like~~ super steep that are late ~~in the,~~ in the time range, the lines that are. Far out that are rising slowly through time. These are like pretty high uncertainty. It seems like ~~probably speaking, if I were to characterize it, it~~ is a mix of.

more confident people who have shorter timelines and less confident people who have just kind of, I don't know, but it seems like it's not soon timelines. Yeah. I feel like there are a decent number that are like, I don't know. And soon is like about as likely as later, sort of gradually is sort of flat where there are some that are more like flat for a bit and then go up after a while. ~~I mean,~~ I think it would be kind of strange to be like, definitely won't happen for the next hundred years, but they're highly likely to happen in the 10 years after that. But that would be a sort of weird epistemic state to be in, I guess. I mean, you do see a couple of those in the graph, particularly with the full automation of labor, there are two that, ~~you know,~~ who like shoot up in the 2100 to 2150 range.

~~And~~ so, yeah, that is somebody who's saying ~~for their three pairs, right?~~ ~~They must've said something like, you know,~~ when would you give it a 10 percent chance that there'll be full automation of labor? And this person said 2120. And then you said, ~~when,~~ when is there a 50 percent chance? And they said 2125. And when is there a 90 percent chance? And they said 2130 ~~and that does,~~ and it actually might even look a little tighter than that, but so that is strange, but we don't see much of that. Like typically ~~the,~~ the steep lines are, ~~you know,~~ broadly very front loaded where people are like 10 percent in a few years, 50 percent in a few more and 90 percent in a few more. You have a few of these. Oddballs that sort of have a tight range in the distant future or possibly misunderstood the question or something we tried to filter out people who are clearly misunderstanding it, but I think there are some confusing cases probably left leaning.

That's kind of an inherent function of this sort of forecasting dynamic, generally pretty broad distributions and ~~kind of~~ a mix of personas where some people have higher confidence in shorter timelines. Some people have. Very broader timelines and lower confidence, and a few that sort of buck the trend and do something that may indicate that they ~~understood or~~ misunderstood the question. The next big finding is, and this is like from one vintage of the survey to the next. That timelines are broadly coming in. So there's a 2016 version of this with a relatively small sample, comparatively a 2022, which had, I don't know how many hundred, but like a pretty good, ~~you know,~~ sample size, like the kind of thing that, ~~you know,~~ they would do a national presidential poll with, and then this one is like several times even bigger. You want to ~~just kind of~~ summarize ~~the,~~ the general pulling in of timelines?

I think the really notable thing is that between 2022 and 2023 surveys, there was Big drop in these, ~~like,~~ roughly human level performance questions that HLMI and full automation of labor, where it's like the HLMI one dropped by about a decade and the full automation of labor one dropped by four or five decades. And I think, ~~like,~~ that's pretty notable, given that between 2016 and 2022, ~~like,~~ HLMI, I think, changed by about a year. So it's not, ~~it's not~~ just ~~sort of~~ flittering all over the place. And I guess ~~for the,~~ for the narrow tasks, you also see a general drop there. I think on average, it's by about a year there. where there's some dropping by a lot, and even some going the other way, ~~but, uh,~~ but more of them dropping. This is like the year dropping, not like the number of years until the thing happens dropping, which you would expect to get one year less by year passing.

Yeah, so ~~the dates,~~ the actual specific years that are predicted are coming in. Yeah, ~~I also thought, and maybe it's~~ It's worth just reading some of these tasks ~~at this point. So there's,~~ there's 39 different tasks, which each have a short name and then a full description that ~~the,~~ the survey takers get to read. I'll just ~~maybe~~ give a couple of them short name, physically install wiring in a house, full description, given a one sentence description of the task. And given the same information you would give a human to perform this task, such as information about the house. Physically install the electrical wiring in a new home without more input from humans. So that's obviously a pretty challenging task for today's AI systems. Also, I think that definition is like pretty representative of the 39 as a whole, where it's ~~like~~ broadly ~~sort of,~~ here's the task we are interested in.

Can you delegate that task to a AI system with basically the same ease of delegation as you would. Delegate to a human. ~~It's interesting that like, I don't know how you think about that in the context of like~~ the state of AI task automation today is ~~sort of~~ quite different from that ~~and you know, and this is something that I like practice to a certain extent, you know,~~ I can get GPT 4 to do a lot of tasks. But it's definitely harder than in many cases in the setup that it is to ask a teammate to do that same task for multiple reasons, including ~~like~~ they don't have a lot of context. They're not great with context, just broadly speaking for multiple different reasons. So I'm not really sure what to do with that as I try to understand these results more broadly. But I do notice that there is a pattern in the questions that's like.

Can you treat the AI system ~~as a~~ roughly as a human, give them kind of terse instructions, ~~you know,~~ a little bit of context, here's the blueprint go. Whereas ~~as it's happening~~ today, ~~I'm like, I can~~ What I typically tell people is we can probably save you 90 percent time and money. With task automation, but you're going to put the 10 percent in ~~kind of~~ up front ~~or like nine of the 10 percent kind of up front~~ to, ~~you know,~~ set up a system, gather the context, ~~like~~ do validation, workshop your prompt, maybe fine tune the model, ~~you know,~~ whatever. ~~Like,~~ it's definitely not nearly as easy to do that delegation to the AI, but you can still kind of get there. I guess I would have thought that at the moment that it wouldn't be largely upfront. It would be like with a lot of like input along the way, like AI at the moment isn't ~~sort of~~ able to act like an autonomous agent that's trying to do something for a long period of time in a useful way ~~without.~~ without you giving more input ~~or,~~ or ~~like, not even input, just like sort of~~ redirecting it.

Like you're the one kind of directing the task usually. ~~And you're like, maybe it was~~ at least in my experience of doing tasks. I've seen people use it for lots of different things, but ~~like,~~ I don't know. It's like, I'm writing a thing. I'm like, I wonder, you know, other examples of this thing. I'll ask it. And I'm sort of like directing small bits to it. Yeah. I think this is a very important practical distinction in AI task assistance or task automation. The flow that I'm describing there is one where there is some scale at which the task is meant to be completed. And the goal is that you would get the AI performance to a level where you don't have to review every single, you AI task execution once you're ~~kind of~~ satisfied with, you ~~know,~~ the level of performance. So ~~I,~~ I usually distinguish between ~~like~~ ad hoc real time ~~kind of~~ co pilot style usage, which is, you know, I'm writing something.

Can you help me edit this paragraph or whatever? ~~Uh,~~ but you're not doing that task at scale. And, ~~you know,~~ on the contrary, ~~like~~ the classic tasks that almost everybody has, ~~you know, in~~ certainly in business, ~~it's like,~~ Would you like to personalize correspondence at scale? Yes, I'd love to. However, ~~you know,~~ who has time to do that? Well, AI does right now. Can we take your database and understand what kind of personalization matters and set this up and ~~like, you know,~~ take the first hundred results back to your copywriter and, ~~you know,~~ make sure that it's working, ~~you know,~~ on that level and whatever, ~~you know, they,~~ every. Different situation has different kind of requirements, but also a big part of that is just ~~kind of~~ conceiving. How are you going to break the tasks down in the 1st place? Right? ~~That is~~ that is a big part of it.

So I guess maybe that's really the key distinction between kind of what in practice is done today and the paradigm shift. that you're sketching out ~~in,~~ in the survey questions. It's like, who is responsible for breaking down the task into these like subtasks, ~~you know,~~ each of which could be ~~kind of~~ developed and validated, whatever in the survey questions that's on the AI. It's not the human, generally speaking, is not responsible for ~~like~~ getting super granular and ~~like~~ really controlling. ~~It's,~~ it's supposed to be. high level delegation of the sort you would, ~~you know,~~ give to a teammate, not the sort that you would give to ~~like~~ GVT 4 today. I think it varies between tasks, but at least for some of them, they're getting pretty far in that direction. I guess my model of such things is probably ~~like~~ kind of fractal ish ~~or something~~ like ~~you can,~~ you can have some skills that allow you to do ~~like~~ a second's worth of useful work before someone has to redirect it.

~~it.~~ If you have ~~like~~ a bit more skill or some other kind of skill, then maybe you could ~~like~~ put together three different pieces, but still like someone above you has to be like, all right, now's the time for that. And so on, like, it might've been that in the past I could use ~~like~~ a thesaurus and it's like, all right, I know that I want this particular task done. I can check. I'm like now chat. gpt can ~~sort of~~ do several things like that at once without me. Telling it what I want. Like I say, I want you to write ~~this,~~ this letter or something, and it can know that it should think about different words for this place, and know that it should think about how to be polite at this place, or it should think about ~~like, you know, what are the,~~ what are the things I was going to list in the letter or something.

And so it's ~~Kind of~~ putting together several different things, but still it's not able to be like at a higher level directing what happens with this letter writing, for instance, I still need to be like, this is going to need some kind of quality check and someone other than the AI is going to have to do that, I'm going to have to figure out who to send it to, that sort of thing. And so I think I'm imagining Like, my picture of all of this is that you ~~sort of~~ gradually grow toward these things as more and more stuff is able to be done by ~~one,~~ one system. Okay, cool. I'll read a couple others just to give a little bit more concrete color. These are not easy, right? The next one is fine tune LLM given a one sentence description of the task, download and fine tune an existing open source LLM without more input from humans.

The fine tune must improve the performance of the LLM on some predetermined benchmark metric. Okay, cool. That's an interesting one, because ~~I think~~ that's ~~kind of a, a key threshold or~~ one of, ~~you know,~~ a handful of thresholds that I definitely watch out for. Another one is replicate ML paper, given a study published at a leading machine learning conference, replicate the study without more input from humans. The replication must meet the standards of the ML reproducibility challenge. It's a link to the definition of that. There are a few in here that are certainly relatively easy, although most of them I would say are ~~like~~ pretty hard. I think arguably some of them have already been done. Yeah, I wanted to actually ask about that one too. Now this sort of both validates and calls into question some of data, ~~I would say.~~ In my view, probably the easiest and in the aggregate view of the respondents, the easiest task, that is the one that is most likely to happen soonest.

is write readable Python code for algorithms like quicksort from specs and examples full version write concise efficient human readable Python code to implement simple algorithms like quicksort that is the system should write code that sorts a list rather than just being able to sort lists so write code to sort a list suppose the system is given only A specification of what counts as a sorted list and several examples of lists undergoing sorting by quicksort. That one, ~~I would say, yeah, like~~ I went to, ~~to~~ chat GPT and asked it to do a quicksort in Python for me, and I'm pretty sure it nailed it and, ~~you know,~~ so you could say, well, That's like in accordance with the data and that the results there were the soonest, but it still seems like, ~~you know,~~ a full 25 percent of people said ~~that~~ that wouldn't happen until 2029 plus.

And that is one area where I was like, huh, are like a quarter of people, like not aware of chat GPT, or are they like interpreting this differently and sort of generalizing to like other algorithms? ~~Are they assuming that, you know,~~ I mean, ~~quick sort,~~ if I'm trying to like steel man, the case here would be like, well, quick sorts in the training data. Maybe this would, ~~you know,~~ could be understood as like. for new algorithms that you have examples of that are not in the training data. Do you have any sort of way of understanding how that result makes sense? Not fully, but I have some thoughts. ~~Uh,~~ one is I think you're misunderstanding the, ~~it's not,~~ it's not the 25 percent of people thought that it's more, it's the everyone together, average together thinks that there's like a 50 percent chance of this happening.

What is it like a couple of years in the future or something in my experience, it's like demonstrating that one of these things has. Properly being done is surprisingly tricky, especially if. ~~Like, I don't know,~~ maybe that one is pretty easy, but yeah, if it's like, ~~can,~~ can it consistently do this across the board? Can it do it for other things? Basically the same as quicksort, ~~I guess it seems like,~~ I didn't think in the question, we really said ~~like, you know,~~ what fraction of the time it has to succeed at it or something might be like in my use of chat dbt say, My experience is that it's often great at things and then often terrible at things that I would have thought it was great at. ~~Or like,~~ last I checked, it wasn't very good at ~~like,~~ counting things ~~in the, you know,~~ like, how many ones is this in a row?

It's like, not very good at that. And so I could imagine like, without actually going and trying the thing right now, ~~like, well,~~ while you're doing a survey, you're like, well, this seems like the kind of thing it can probably do. So probably sometime between right now and in a couple of years or something. I think also the fact that we included it on the survey, I think they might take as evidence that it can't be done right now. ~~We,~~ we decided not to take any of them off since 2016, partly because it's just ~~like~~ so complicated to figure out if they've actually been done or not. So we decided to not have any ~~opinion,~~ opinion on that and just include them all and maybe ~~like~~ take them off once, ~~once they're done.~~ the respondents start to actually say this has constantly happened instead of like, I don't know, maybe five years.

But I think that might be confusing people. Yeah. Isn't that an option? There's not an option today, right? To say this already exists. There isn't. So yeah, quite plausibly we should add that and that would make things less confusing. Zooming out again on 35 of the 39 tasks, we have a 50 percent chance that it will happen in 10 years or less. So here's the four just to ~~kind of~~ calibrate on the ones that were greater than 10 years. One was the installation of the electrical wires. That was estimated 50 percent at 17 years. The ML paper, there's a replication one that I read that was estimated at 12 years, 50 percent chance of at 12 years. And then there's also a research and write. In other words, do your own ML paper from scratch. That one is 19 years. And then the farthest ones out were around math.

Interestingly, there's some ~~like~~ interesting proof points there as well of late, but prove mathematical theorems that are publishable in top mathematics journals today, 22 years out. And solve longstanding unsolved problems in mathematics, such as the millennium prize problem, 27 years out. So these are ~~like~~ the hardest ones with the longest timelines, things like Python, ~~you know,~~ things like playing angry birds at a human level, ~~you know,~~ the large majority of these things are under 10 years, 50 percent chance. I would say another kind of observation, I wonder how you would react to this is it seems like the timelines here while they have come in. And while like most of these things are, ~~you know,~~ More likely than not to happen inside of 10 years, ~~you know,~~ privy aggregate judgment. It seems like the timelines here are still longer than the guidance that we're getting from the heads of leading labs.

Like, I think ~~if,~~ if Sam Altman, if Dario, if Demis and Chainleg took your survey. I think their numbers would be on the shorter end of the aggregate. That would be my guess. I think also just from many people working in AI who I know here in the Bay Area. Yeah, I don't think we have three year percentage pairs from the heads of the leading labs. I can't tell if we do or not. Well, ~~I mean,~~ here's things that I've seen recently. Sam has said AGI is coming soon, but it might not be as big a deal as you think. ~~Dario,~~ he also said at a Y Combinator Event that startup founders now need to build with AGI in mind, which is certainly an interesting app design challenge. Anthropic, we don't tend to hear quite the same thing too often, but there was the, I think, credibly sourced pitch deck that they had, where they said that in 25, 26, the leading model developers might get so far ahead of others that nobody will be able to catch up because they'll have their own kind of feedback loops where they can use their current models to train the next ones.

That certainly sounds like, ~~you know,~~ a scenario where things are happening faster than the aggregates from the survey. And Shane Legg from DeepMind recently said that, ~~you know,~~ he's basically had like a 2029, ~~you know,~~ median timeline for like 15 years. I think it's interesting to note that ~~like~~ having had that timeline for a while and similarly that was for some of these other people. I think you could wonder like hearing. very optimistic timelines, or, well, optimistic or pessimistic, depending on how you feel about this, but like very soon timelines. You might wonder whether that's from ~~like,~~ seeing something right now ~~that,~~ that suggests that it will be very soon versus being kind of selection effect, where people who think that AGI is soon work on AGI. And I think where we observe that people already had these views some time ago, ~~you know,~~ that support for the selection effect explanation of the difference in opinion there.

Yeah. So for comparison, the survey gives a 10 percent chance Of high level machine intelligence by 2027, and for a 50 percent chance, you have to go out to 2047. So that's pulled in significantly from just one year previously. In 2022, it was 2060 that you had the 50 percent chance. So from 2060 to 2047, 13 years in, but that's still ~~like~~ quite a bit farther than certainly what we're hearing from all the leading labs. Like ~~what the,~~ what the survey says is sort of. a 10 percent chance. It seems like the leading lab heads ~~are kind of like,~~ think it's like more likely than not on a similar timeframe. I imagine it's by impression. Yeah. Yeah. So, ~~I mean, you know,~~ who do we believe? Obviously, ~~uh,~~ very hard to say. Yeah. One thing that you've done that I think is pretty interesting is that I might even actually spend a little more time on this is.

You've published all the results in ~~kind of~~ cleaned up anonymized form. So folks can go do their own data analysis on it. So ~~could I,~~ would there be anything stopping me from taking the results and saying, I'm going to go filter out anybody who ~~like,~~ thinks that the Python thing is ~~like~~ not close and, ~~you know,~~ rerun the analysis with ~~like~~ those people filtered out. ~~You know,~~ it's hard for me to put a mental model on. I don't know how many people I'd filter this way, but there's gotta be a fraction, like a meaningful fraction that might be ~~kind of~~ skewing the tails. where if it's like you published in the leading conference in 2022, but you are saying that, and I won't be able to write a quick start for a long time. Like ~~I,~~ I am kind of confused about that. Maybe I should just filter you.

I could do that. Right. With the data that you've published. I'm thoroughly encouraged that I would love to see more people. Use it. And ~~I mean,~~ I think there are a lot of interesting things you could ask about. And it would really only touch the surface with giving the basic answers to each question and ~~like~~ some amount of how does this relate to demographics ~~or something.~~ But yeah, I think ~~how,~~ how did the answers relate to each other? A lot of interesting things there. I think if you filtered out the people who said that quicksort wasn't possible for, ~~you know,~~ at least 15 years or something, I'm not actually sure. what you would be getting there. ~~I think,~~ I think you might be filtering out a mixture of people who didn't understand the question or ~~like~~ just made an error on that one, or you have some sort of complicated philosophical take, like it's not doing quicksort.

It's doing matrix multiplication or something. ~~It's not, yeah, it's not,~~ it's not doing real quicksort. It's just doing sort of stochastic period imitation of quicksort. ~~But~~ I don't know, and then I guess if you filter out those people, I don't know if ~~you're, I don't, like, I don't know if~~ they're more likely to be wrong overall about other things than they are, but I feel like they're more likely to have thought about things, probably. We did actually just ask people how much they'd thought about different things, so I think a very natural thing to do would be to see what just the people who said they thought the most about things said. Things think there. Yes, that's also tricky because of the selection effects with people who are more concerned about things thinking about them more. So I think, in fact, the people who thought more do seem to be more concerned, but I'm not sure what you should actually take away from that.

I think we'll return to that correlation question in a second, just carrying on through a few more headlines. It was definitely striking to me that people are ~~sort of~~ expecting the unexpected. Here's a quote, a large majority of participants thought state of the art AI systems in 20 years. would be likely or very likely to find unexpected ways to achieve goals. More than 80 percent of people expect that in 20 years. Be able to talk like a human expert ~~on more topics,~~ on most topics. Again, more than 80 percent expect that. And frequently behave in ways that are surprising to humans. Just under 70 percent of people expect that to be the reality for AI systems 20 years from now. ~~I mean,~~ I have to say, I probably agree with all those things, but that is definitely a striking result, right? That people are ~~like,~~ expecting ongoing surprises from AI systems as ~~kind of~~ their default.

~~You know,~~ that seems, ~~that's~~ like the closest thing probably to consensus in this survey is that everybody ~~sort of~~ expects these things to be ~~kind of,~~ unwieldy and surprising, even as they become like quite a bit more powerful over a 20 year period. Yeah, that does seem like, ~~you know,~~ one of the more consensus y things, at least. Yeah, I guess I don't know ~~how,~~ like, how sinister to hear some of these things as, like, ~~sort of~~ finding unexpected ways to achieve goals. At some level, that's just like, ~~you know,~~ what you expect if you are getting someone else to achieve goals for you, you know, you're not going to figure out all the details of how to achieve it, they're going to do it and you'll be like, Oh, nice. You figured out a way to do that versus being quite surprised and being like, well, that was a norm violation.

~~I,~~ I thought I should put up more barriers to prevent you doing that. But, um, apparently you did. Yeah. Similarly for the sort of frequently behaving ways that are surprising to humans. I feel like that, ~~you know,~~ Could be more terrifying or more just what you expect. So I would have liked it if we'd been clearer with those questions a bit. Yeah, maybe there's a way ~~to~~ To tease that out a little bit through ~~kind of~~ correlating ~~with~~ with the next headline result, which is ~~for my~~ to my eye The group is only slightly more optimistic than Neutral. Basically, you've got, ~~you know,~~ a few of these kind of classic questions where you're like, ~~you know, sort of~~ one to five, like extremely bad, bad, neutral, good, or very good. And just ask people, ~~like,~~ how likely do you think this is to be, ~~you know,~~ in ~~these,~~ these various buckets To be clear, this is like, ~~how,~~ how good or bad is the long term future as a result of HLMI in particular?

Yeah, each person puts 100 percent between the five buckets. and they're ~~looking at, like, pretty,~~ I would say this is like the clearest sign of, ~~like,~~ radical uncertainty ~~or like very broad disagreement~~ because all five of the buckets, ~~you know,~~ have significant. percentages and they are skewed slightly positively.? ~~You, you said like there's a lot of disagreement. I,~~ it looks to me like a fair amount of agreement, ~~but agreement~~ on extreme uncertainty. There's a good bulk of this graph that is ~~You know,~~ people putting a chunk of probability in each of the five buckets and just a little bit at each end, where people are either very confident in good or very confident in bad. ~~But each of these columns is a particular person. Gotcha. Okay, so I think I was maybe, again, misunderstanding a little bit how the question had been asked. I was thinking you had said, pick one of these five. for your expectation, but instead~~ and this is a question that everyone in the survey got, and everyone had to ~~answer it by putting it, had to~~ add 100 percent and they had to answer it in order to get to the next question.

~~Um,~~ so basically everyone answered this and ~~it's, it's just like,~~ yeah. ~~So each,~~ each, ~~uh,~~ column is one person, ~~but 800 of them randomly assured in this.~~ ~~And, well, at least the one that I'm looking at, because they don't all fit in a way that you can see the color.~~ Yeah. Okay. And this is figure 10 in the paper, ~~right? That you're referring to. Yeah.~~ This is definitely a cool visualization as well. ~~It's a little bit hard to describe, but basically~~ it's a ~~sort of~~ sorted list where each person ~~basically~~ gets a vertical pixel and then you can see how each person distributed their expectation from extremely good to extremely bad. And because it's sorted, you have ~~like~~ an extremely good region on the one end and an extremely bad region on the other end. And in the middle, you can see that, yeah, like a lot of people have given non trivial weight to all five ~~of the~~ of the buckets and the middle sort of two thirds of people probably have agreement ~~on~~ on radical uncertainty.

And then there's like maybe a sixth on either end that are sort of the. Optimists and the pessimists. Even like most of those are like pretty uncertain. ~~Like,~~ even if you look at ~~like~~ the big black chunk of pessimists at the right hand side, ~~like~~ still like most of their area is not, ~~you know,~~ extremely bad. It's a decent chunk of extremely good in there. Yeah. You got a few bimodalists. I don't know if I quite do call myself a bimodalist. ~~I~~ sometimes I do, but I'm ~~not,~~ not quite ready to. I'm not gonna stick my identity on it, but there is definitely a band there where ~~it's like~~ you only see the two colors and it's the extremely bad and the extremely good. I think I'm also seeing like a 2080 band like that. But if it's that extremely bad, it's extremely good. Okay, I like seeing that like how ~~it's sort of~~ Polarized this isn't ~~like,~~ I think this kind of thing can feel polarized.

I, people are ~~sort of~~ talking about doers and so on, but it's, I think very not. Yeah, totally. ~~I think, I mean, certainly the online discourse is problematic in, in many ways and definitely feels like people are often. You know, sniping back and forth at each other, whatever. Nobody needs me to recount the ills of Twitter. But this~~ definitely suggests a very big middle of people that are just ~~like~~ very uncertain as to what to expect. And, ~~you know,~~ those people are probably for many obvious reasons, ~~like~~ not the most vocal online. But I think ~~that's~~ a huge part of the value of this work is demonstrating that there is no single consensus. There is no, ~~like~~ Dominant view, ~~you know,~~ there's no single number that we can put on this, but really ~~like,~~ for me, the headline is just ~~like,~~ there is radical uncertainty expressed like any number of different ways ~~by the,~~ by the community at large, Radical uncertainty is in some sense, a consensus about something like, like I think here, you could say there is more or less a consensus that there's like non negligible probability of extremely bad outcomes and from a sort of action perspective, ~~like if the,~~ if the question is.

Yes or no, is this ~~like~~ a risk worth paying attention to? Then it ~~sort of~~ ends up being sort of like a consensus for ~~like,~~ yes, worth paying attention to because uncertainty is like, yeah, there's some chance of it. And you'd actually need to be pretty confident on something to say, no, ~~not,~~ not an issue. Yeah, I think that's a good point. The headline that I've seen from the, ~~you know,~~ coverage of the survey broadly has largely focused on ~~the,~~ what you might call the PDoOM point estimate. And I've seen this most often reported as a majority, and it's a slight majority, 51%. Believe that there's at least a 10 percent chance that AI could lead to human extinction or similarly severe Disempowerment. Two things. I think the thing ~~that I,~~ that I've most heard at least is the median 5%, but also I think that 10 percent would be wrong here because what we asked this question about value that we're just talking about, then we asked three other questions that were quite similar about human extinction in particular.

So table two. Shows the three different questions that we asked that are all quite similar about human extinction or similarly permanence and severe disempowerment of the human species. ~~Um,~~ one of them is just straight up asking about that. One is asking about that as a result of inability to control future advanced AI systems. And one is asking about it within the next a hundred years in particular. And so the medians for those were like 5%, 10%, 5%, and I guess the means were quite a bit higher. So that headline is cherry picking. The highest median of those three. It's interesting too. ~~I mean,~~ this does show some of the limitations of both surveys and interpreting surveys. ~~I mean,~~ I think it's very interesting framing, but ~~you,~~ when you really start to stare at it, you're like, well, there's definitely a sort of conjunction fallacy at work here.

Right? Like ~~the,~~ the first question says, what probability do you put on future AI advances causing human extinction or similarly permanent and severe disempowerment of the human species? median answer, 5%. The next one is essentially the same question, but with an added detail of inability to control. And as far as I'm seeing here, every other word is the same and the percentage chance doubles, right? So ~~if,~~ if you're like logically consistent entity, presumably The second one would be like lower, right? I think it's not necessarily coming from the conjunction fallacy, though I think it may well be. But things to note here are ~~like, these are,~~ like, people are randomized into each of them, so it's not like the same person answering them, so it could just be some variation from them being different people. But I think often, a lot of people put 5%, and a lot of people put 10%.

So whether the median is 10 percent is down to like, ~~you know,~~ exactly how many put. where the middle person lands. They're ~~kind of~~ like a bunch of people putting five percent, a bunch of people putting ten percent, and exactly where the middle is changes it from five to ten percent. But ~~it's,~~ it seems wrong to be like, ah, they doubled it. ~~It's kind of like~~ the distribution of people thinking things is ~~kind of~~ similar. Last year, I remember 48 percent of people said ten percent extremely bad. So it's like, if it got up to 50 percent, then the median would be ten percent. But because it was 48 percent, the median is five percent. Gotcha. So these are bucketed answers. In this case, we're not doing a distribution or a free response point. Right. We're asking each person, what is the chance of this? And in fact, people just never put numbers between 5 percent and 10%.

Like they're sort of rounding it themselves or not never, but like quite rarely. So I think in practice for these questions, the median jumps fairly easily between 5 percent and 10 percent rather than. hitting intermediate values. I do still think that probably some amount of conjunction fallacy is going on here, partly just because we saw the same pattern last year. Again, to attempt to summarize the headline, pDoom point estimate is 5 to 10 percent median estimate. With certainly, ~~yeah,~~ again, a kind of left heavy distribution and the mean being higher than the median. ~~And,~~ and that's presumably driven by a minority of people who have high estimates, but you can see that again in ~~the.~~ Any other figure as well? Well, that's definitely, ~~you know,~~ something that's pretty consistent. I would say with my own, ~~I wouldn't,~~ I don't spend a lot of time trying to narrow down my P doom.

I thought Dennis did a pretty good job answering this question. ~~He,~~ he did the New York times hard fork podcast recently, and they, ~~you know, kind of~~ asked him what his P do is. And he was like, it can be a subtle distinction and people want to collapse the distinction, but He's like, we really just have no idea. ~~You know,~~ it's not like we have a, ~~like there is a~~ process that we ~~sort of~~ have established that has any. ~~You know,~~ number attached to it that we're now going to execute and ~~like,~~ find out on the contrary, ~~like~~ we really have no idea what the nature of the process is that we're going through. And so it's just like super unknown. I typically say either ~~like~~ five to 95 percent or 10 to 90%, something like that, ~~depending on just kind of how I'm feeling.~~ And, ~~you know,~~ certainly the consensus ~~is like,~~ at least the low end of that range, ~~you know, it~~ is like, not objectionable to the large majority of people.

Pretty in line with him. I disagree with you, or I agree it's very unknown, but I think that's what probability is for. I'm a big fan of putting numbers on it anyway, and I think that ~~at least, I don't know, lets you,~~ even if you can't do anything except ~~like,~~ guess a number, it at least allows you to ~~sort of,~~ compared to other numbers you're guessing or like make consistent decisions over time. I practice guessing numbers about lots of things and then I can check how well that's going and so whether to listen to the numbers that I make up about this. ~~I've also tried to do a more, you know, elaborate thing, which is not going to be very good.~~ It's very uncertain, but I think it's worth trying. Yeah. I ~~I mean, I think they're, I think both certainly have value.~~ I certainly find a lot of value in this work, just ~~in,~~ in grounding the idea that, ~~you know,~~ this is clearly not a fringe.

Question. ~~You know,~~ it is clearly worth thinking hard about ~~adding it. You know,~~ the Overton window is wide open and it should be wide open. If the elite researchers in the field think that there's a five to 10 percent chance of extinction level, bad outcome. So I think that's like definitely really true. I guess the way I tend to think about these numbers is, ~~is a little bit more like,~~ what can we shift the true probability to, as opposed to how can I become more accurate in my. Estimate today and. ~~I mean,~~ I think both of those are ~~like, you know,~~ worthy pursuits, but especially, ~~you know,~~ for somebody in Devin's position, he's like, what I need to do is ~~like,~~ create an agenda that ~~moves the, you know, that like,~~ collapses the uncertainty and hopefully moves ~~the, you know,~~ the distribution. ~~Also,~~ I agree that like, ~~I think~~ trying to put numbers on such things does potentially get you into a headspace where you feel like you're not controlling the number.

~~Um,~~ and I do think that's bad. Yeah, because ~~we don't have,~~ we're not like about to spin the roulette wheel, right? Like ~~it,~~ there is no roulette wheel that we can like spin now with these numbers. I think ~~that's kind of,~~ that's the main thing that I think is worth keeping in mind as a caveat, right? We're still building the wheel and we have a lot of uncertainty as to what the roulette wheel looks like and the more, ~~you know,~~ concretely it comes into focus. Then presumably the distributions will start to narrow, although maybe not, ~~you know,~~ it certainly ~~wouldn't, it~~ wouldn't shock me at this point if we have ~~kind of like~~ just super high level of uncertainty and disagreement right up until a phase change hits and then it's like, well, we just found out and it wasn't ever very clear, ~~you know,~~ until relatively shortly before it happened again, it's just ~~another,~~ another dimension of pretty radical uncertainty.

I think when you're talking about ~~like,~~ like the probability of a thing conditional on us following some particular path, ~~it's,~~ it's a reasonable way to do it. Like not P doing very high, we're going to die or something. It's more like ~~if,~~ if we, the humans are building this particular thing, I think that particular thing will be quite bad. Maybe we should build this other thing instead. It was maybe a more like proactive way of thinking about it. Unfortunately, one kind of downer result is Interpretability breakthroughs are not expected in the short term. Most respondents, 80%, considered it unlikely or very unlikely. That users would be able to understand the true reasons behind A. I. Decisions in 2028. Pretty logically, I would say there's a strong agreement. ~~This is probably the other other main consensus is that a large percentage seem to agree~~ that A. I.

Safety broadly. ~~Everybody seems not everybody, of course, but it~~ should be true. More prioritized than it is today. Very few people saying it should be less prioritized than it is today. Also, ~~I would say this one, if I'm reading correctly, is more of a portion of respondents, not like a waiting thing, but~~ should AI be going faster or slower again? ~~Like very sort of.~~ Only 5 percent of people said it should be going much slower than it is today, but then a healthy, ~~you know,~~ relatively even 30 percent on somewhat slower, 27 percent on current 23 percent somewhat faster and 15. 6 percent much faster. So ~~am I,~~ am I right to read that as high agreement that we're not expecting interpretability breakthroughs, high agreement that more AI safety work would. would be good, but high disagreement as to whether we should be slowing down or going faster.

I think that seems right to me. I think one complication in interpreting that last one is that the central option, I think, was like relative to the current speed. So the most Natural, literal interpretation is like, whatever the current speed ~~is,~~ is good, but like, if things were to accelerate, you would want them to go slower, and you might think they would naturally accelerate. My guess is that people weren't thinking about it for that long, and what they actually mean is more like, the current trajectory is good, ~~which,~~ which is perhaps a good thing. Accelerating, but yeah, I think before you read too much into that one, it would be nice to, ~~you know,~~ ask people some related questions again or something and get a bit more clear. So let's summarize this whole thing ~~and then we can just spend a couple minutes on. How you might evolve and you know, what, what new types of things you might look at in the future, and then, you know, a little bit of kind of your personal commentary on things as well,~~ I guess, high level summaries, people do not dismiss existential risk on the contrary point estimate is like five to 10%.

People have. pretty wide uncertainty about exactly when AI will be able to do various things, though those estimates are broadly coming in. Also, a lot of them are relatively soon. More than 80% Of the tasks had a 50 percent estimate of happening in less than 10 years. So no, no breakthroughs expected in interpretability, more emphasis on AI safety and disagreement or confusion on faster, slower. It all seems right. As a single headline, ~~it's like,~~ we really don't have a, ~~you know, a~~ confident take as ~~a,~~ A community, I shouldn't even say we, because I've not published in these conferences. I wouldn't be qualified to complete the survey. But if I'll, ~~you know,~~ flatter myself to be included in the community, then we as a machine learning research community do not have a confident view of what's going to happen. On the contrary, we have a pretty radically uncertain view of What is going to happen, how long it's going to take to get to different things, whether it's going to be good or bad, whether we're going to be able to control systems or even, ~~you know,~~ whether we'll be here at some point in the future, like all of those are very.

live questions with ~~like~~ non trivial weight on ~~like~~ all the options. One question we didn't talk about where I think it was ~~sort of~~ interesting that there was more consensus perhaps is the one about different scenarios and how much concern they warranted. Not clear that they are concerned, but they think the thing deserves concern from society. ~~So I think like~~ the very top one there was making it easy to spread false information, eg deep fakes. I think ~~like~~ more than 80 percent of people thought it was worth either substantial concern or extreme concern. That's an unusual degree of agreement. Yeah. So these scenarios, just to read a couple of them, AI makes it easy to spread false information, e. g. deep fakes, AI systems manipulate large scale public opinion trends. Authoritarian rulers use AI to control their population. Other comes in as the fourth most concerning scenario.

AI systems worsen economic inequality by disproportionately benefiting certain individuals. AI lets dangerous groups make powerful tools, e. g. engineered viruses. Bias in AI systems makes unjust situations worse, AI systems learn to discriminate by gender or race in hiring processes. All of those that I just read had more than 60 percent of people saying they are either substantially concerned or extremely concerned and For all of those, fewer than 10 percent of people said no concern. It's not really about what will happen, but ~~like~~ what could happen enough that it's concerning in some sense for that to be the only thing that's really clear consensus about is maybe supporting your hypothesis. They don't know what's going to happen. Yeah. None of these are dismissed. That's for sure ~~60 percent plus, again, with substantial or extreme concern and under 10 percent of people with no concern across all of those. Except maybe other, but I think that shouldn't really count here. Apparently a few people wrote in other to be not concerned about that. Still under 20 percent of people had no concern in other, but yeah, other ones, the other is a little bit hard to parse.~~ Okay, cool.

Well, I love the work because it's just so methodical. ~~It's, you know, as,~~ as we've, ~~you know, kind of~~ established through a couple of my misunderstandings, ~~like~~ it's pretty nuanced, it's pretty granular and, ~~you know,~~ you can pick it apart in a lot of different ways, but ~~I think people would be hard pressed to take like a, you know, a huge leap forward in terms of the clarity that they could get out of a survey like this.~~ ~~And~~ it does seem that it's ~~just like~~ very well established by this result that there is a lot of uncertainty about what we're in for. There's a lot of uncertainty about, ~~you know, again,~~ even the highest level questions of ~~like,~~ whether it's net good or net bad, whether we are going to survive it or not. It is not like a doomer community, but it's not a community that dismisses The tail risks either.

And, ~~you know,~~ so with that, ~~you know,~~ we ~~sort of~~ head into a very cloudy future. I think for the future, I assume you guys are planning to run this again. ~~Do you have ideas? I mean,~~ a ~~few,~~ few things that kind of came to mind for me, ~~I could,~~ I could throw at you, but before I do that, what are you considering doing differently in the future, or ~~what do you,~~ what do you think are the sort of natural evolutions from here? I am reasonably likely to run it again. I think the most natural thing to do is to just run it identically to the past, because, ~~you know,~~ it's nice to be able to compare these things. This time we added various questions that we hadn't had before, like this one about the concerns and the one about the different traits that AI systems have in the future.

But it's like a fair bit of effort to add the questions. And so, ~~you know,~~ the project can be a lot more contained if you just ~~sort of~~ have a survey and send it out ~~every,~~ every now and again. And so it's somewhat tempting to basically do that. I think also it's quite hard to cram more questions in here and still have it reasonably short, give each question to fewer people and randomize and so on. And so ~~I it's,~~ it's very tempting ~~to just like,~~ if we want to do more surveying, run more surveys. I've been thinking a bit about having something about policy, but I haven't thought about it that much. I guess people send us various questions they're interested in. I guess if there was one thing I could ask for, and it's because multiple people have asked me, ~~I've, you know, I've been asking people recently, what could I do to be more valuable to you?~~ ~~Right? I'm, I've been candidly, not that strategic in the approach to making this podcast. I've largely just tried to, you know, You know, with apologies to Tyler Cowen, like I have the conversation I want to have and just followed my curiosity and people have, you know, kind of appreciated it. So that's great.~~ ~~But now I'm like, well, geez, there's enough people that are listening and there's high enough quality audience. Like, what can I do to be more valuable? And, you know, by the way, that's a invitation to send me a message if you have a take on that, but the most common thing that I've heard so far has been.~~ ~~that~~ people don't know how to understand affordances.

There is a ton of investment over the last year that has gone into a ~~kind of~~ the application layer, connecting models to databases, of course, ~~you know,~~ for rag type implementations, connecting them to APIs and other tools. ~~You know,~~ now we've got like the memory scratch pad, a great paper on this is cognitive architectures for language agents, where they ~~kind of~~ survey, ~~you know,~~ the full literature and, ~~you know,~~ try to put a taxonomy on all these things. I do think it's pretty interesting to consider what happens when, with all this scaffolding, as it's often called in the application world, or, ~~you know,~~ assortances as it's otherwise called, when all that is built. And it already exists. And it's like working okay, you know, or even maybe pretty good with a current model, what then happens when a core model upgrade happens? And, ~~you know,~~ all of a sudden it's like a drop in replacement to a structure that has really been like built up to try to compensate for all the weaknesses.

And now maybe those weaknesses drop by like an order of magnitude. To what degree does that create a situation where, ~~you know,~~ you essentially are like flipping a switch and. A lot of things go from ~~really sort of working to like really working or, you know,~~ sort of working to working really well. And ~~like, And like,~~ how does that play out? I do see a lot of potential for that, but people ~~are, you know, I've~~ been asking me for ~~like,~~ well, what are scaling laws for scaffolding? ~~You know, could we start to put anything together there?~~ And I just find I really don't know. I have this kind of. Broad mental picture of a lot of work has gone into it and a model upgrade is going to make a lot of existing systems that don't work that great yet, ~~like~~ work quite well, ~~but how to model that in a better way,~~ I'm not even sure what questions I would ask, but ~~yeah,~~ I would definitely love to get a community sense for.

the relationship between core model power and affordances or, ~~you know,~~ scaffolding infrastructure that's already ~~kind of~~ out there. Where affordances here means something similar to scaffolding. Yeah, basically synonyms. Yeah. ~~What,~~ what tools do you have access to? What information do you have access to? It's all the things that are outside of the weights. that the model can use at inference time. You want to predict what will happen when the new model comes out ~~with the,~~ with the scaffolding. ~~You want to be somehow measuring the scaffolding over time?~~ Yeah, there's a latent potential there. It is a tricky one ~~to, to wrap, you know, I'm certainly modest in my confidence about like how well I could just, you know, the question you.~~ ~~Even,~~ but yeah, ~~I think the,~~ the key thing that I'm wondering about is to what degree should we be expecting a step change in the actual AI impacts when a new model gets plugged into existing enabling infrastructure ~~that,~~ cause that seems like a fundamental difference between AI and almost every other technology is that ~~like~~ all of the distribution infrastructure already exists.

And the compliments are getting built now and people are ~~like~~ very aggressively and eagerly building all these compliments in anticipation of the next upgrade. And they're ~~kind of~~ tolerant of the fact that like it doesn't really work now because they fully expect that there's going to be an upgrade that's going to make it work. But then that has potentially very unpredictable consequences when all of a sudden everything turns on at once. ~~I mean,~~ I don't know a lot about the details of this kind of world. ~~I think~~ I would have thought that there are things a lot like that in other kind of technologies that get updated and have a ~~like, you know,~~ Widespread reach, ~~like especially sort of software things, like I might imagine that if a new operating system comes out, it's sort of, it slides into a lot of systems everywhere and other there's other software that interacts with it.~~ ~~And maybe when that other software was written, it was written with the possibility of a, of an operating system update in mind. Yeah, don't know much about that either, but I'm curious if that seems like a.~~ Yeah, I think so.

Although ~~I, the difference that feels most salient to me right now is that typically most software is shipped in some working form.~~ ~~So it's not like, you know, and you get it, you may get a new computer. It may run a little faster. It may be a little better or whatever, but. It's not like the capability of the system step changes when, you know, you, when you have an application on your computer and you upgrade the operating system, it's not like that application all of a sudden can like do a lot more stuff for you than it previously could, or at least like very rarely.~~ ~~Maybe a few, you know, I don't know there, I'm sure we could find some, some small examples of that, but in the,~~ in the AI application world today, ~~you have just tons of people trying to build agents for all kinds of things. And they're basically all building toward to varying degrees with varying, you know, probably with varying forecasts in their own minds for like how soon, you know, how and how big the, the updates are coming.~~ ~~But~~ there's a ton of building that is going into essentially the kind of ad hoc delegation that a lot of the survey questions ~~sort of~~ anticipate as well.

~~You know,~~ people are like, ~~well,~~ it'd be sweet is if I didn't have to ever write a sequel query anymore and I could just ask my sequel agent to, ~~you know,~~ look at my database schema and figure out what's what, and then, ~~you know,~~ break the thing. And, ~~you know,~~ the same thing for, ~~like,~~ web scraping and the same thing for, ~~you know,~~ answering phone calls and making phone calls. And, ~~you know,~~ it just goes on and on, right? ~~I mean, there's,~~ there's ~~a flurry and~~ research assistance and, ~~you know, I mean,~~ programming assistance. So it just goes to ~~kind of~~ every corner, ~~you know,~~ you could find a agent type product. Legal work, medical research, diagnosis, research for yourself, and they're all ~~kind of~~ like. Well, GPT 4 is not quite there, but ~~it's like,~~ it's good enough that I can ~~kind of~~ build a system and ~~like~~ track how well it's working.

And even if it's not working that well, then I know that there's like a countdown clock on to the next thing. And that's where ~~I do,~~ I feel like ~~there is a, just. Kind of a crazy, it's like~~ we're building a new capability overhang, right? There's been the notion for a long time of ~~like,~~ well, if the chips are all out there and nobody trains the models, then someday somebody could come along and, ~~you know,~~ have just like massive advance. But here, it's like on the other end, it's all of the compliments and ~~sort of~~ guardrails and access to data and access to run times and, ~~you know,~~ the loops and the ability to cache skills. And all of these things ~~are,~~ are getting built. ~~And it's like,~~ but the model isn't that good at using any of them yet. And so if it goes from ~~like that~~ one level to another, how does that kind of cascade through the overall system?

And do we suddenly move from an era where nothing is really working super well so everybody right now can ~~sort of~~ assume that they're essentially acting in isolation? ~~Like, I think most,~~ most everybody's worldview right now is like, the world is the world. I'm ~~like,~~ applying AI here. Nothing else is changing. That's ~~kind of~~ the implicit assumption. You're thinking like, that's happening under the Yeah, everybody's doing that. And ~~because it's not really working, like,~~ is so far true that like nothing else is really changing. But I think there's at least potential for a model upgrade to suddenly put us in a different regime where it's like, now my thing is working. Much better. Now I'm actually going to go use it a lot more and send it out, ~~you know,~~ potentially semi autonomously to do stuff. But at the same time, everybody else's autonomous systems are also getting unleashed.

And now we have ~~like every, you know,~~ all these like autonomous systems ~~kind of~~ going off into the world at the same time and potentially encountering each other. And ~~like,~~ we're definitely just not at all prepared for, or even ~~I would say like~~ thinking about ~~that, you know, we're not even, we're not even close to prepared for.~~ What that might look like. It could be not a huge deal if the next upgrade isn't that big, but my sense, ~~you know,~~ and this goes back to the Sam Altman comment of ~~like,~~ you should be building with AGI in mind. Like my sense is that there is ~~a couple more,~~ at least one more round of like substantial upgrades ~~and probably a couple more.~~ And so, ~~you know, do we,~~ do we have this kind of phase change that happens? Suddenly. It could happen before the next survey can get run.

Yeah, often technologies change ~~kind of~~ gradually, even if there was a kind of big insight or something. But that's often because you're like, ~~I think, sort of~~ building stuff to make use of it afterwards and takes a while. And so you're saying, ~~like,~~ maybe if we build all this stuff to take advantage of it ahead of time, ~~because,~~ because we already ~~had this, like, we~~ knew it was coming and we had this lesser version of it to play around with, that it could be ~~more,~~ much more discontinuous than most technologies ~~when,~~ when that bit gets swapped out. ~~I'm serious.~~ Pretty interesting theory. I guess I wonder how much we've seen that with past upgrades like this. ~~Like,~~ I think that my explanation here for why AI would be different to like any other technology in this regard would be just that it's like quite general. So you are ~~sort of~~ using it across the board and there is the potential to suddenly upgrade a thing everywhere.

But then, ~~you know,~~ you might expect that also to apply to, ~~so, you know,~~ the change between GPT 3 and GPT 4. GPT 2 and GPT 3. Maybe those were not useful enough to be building stuff around yet. That's my sense. ~~I,~~ I feel like only ~~with maybe, you know,~~ with ~~like~~ chat GPT 3. 5, certainly with four, but probably not with three. ~~Was there a,~~ and ~~you know, then~~ that whole timeframe is also like fairly condensed, ~~right? I mean,~~ it was only. ~~Four months, less than four months,~~ like three and a half from 3. 5 first release to four first release and chat GPT dropped it basically the same time. So it's like three and a half months from chat GPT to GPT four. Yeah. I just don't think before that there was really much, ~~you know, the, the,~~ the gold rush or the sort of, ~~you know,~~ everybody goes stake out ~~their, you know,~~ their place in the AI app sphere.

seems to have been, ~~you know,~~ a last 12 to 14 months phenomenon at most. And like, certainly there was stuff going on before that, but I would say there's been a phase change of how much activity there has been. You're saying this would be an interesting thing to have a survey about? ~~Or just like, this is a different area where we might add value?~~ Yeah, I mean, ~~I guess~~ I'm very uncertain about it. Quality people have asked me about it. So I have the strong sense that among, ~~You know,~~ a ~~kind of~~ thought leading population, ~~you know,~~ that's somewhat in some cases adjacent to like policy decision making. There is a lot of uncertainty and a lot of questioning around that particular question. To what degree should we expect discontinuity due to the plugging in of new models to existing infrastructure? ~~How does that become a serving one? Maybe that that wasn't terrible.~~ You know, maybe we could use that as a first draft.

But yeah, ~~I do. I I've gotten a lot of~~ that's been the number one assignment that I've gotten from some of my, ~~you know,~~ highest value correspondence ~~where I would be like, if I can answer this person's question, I would love to, but I don't really know.~~ And I'm ~~like,~~ struggling ~~to~~ to model it ~~or, you know, how seriously I should take it.~~ So That's my case for a bonus question next year. Okay, thank you. I guess other areas that could be of interest, ~~like,~~ would you look at like military or, ~~you know,~~ US China chip restrictions? ~~How many, you know, sort of,~~ is Sam going to get his seven trillion dollars? ~~I feel like those kinds of like,~~ maybe that's like outside the scope of ~~like~~ what ml, ~~you know,~~ elite Publishing folks should be consulted on. That is also like, ~~I don't know, what do you,~~ what do you do with those answers?

What does someone usefully learn from these things? ~~Or,~~ or ~~like~~ do differently as a result? I ~~do think that,~~ well, ~~I mean,~~ escalate or deescalate with China ~~would be, you know,~~ maybe one key question. I don't know that, ~~you know.~~ our executive decision making is necessarily going to be ~~like~~ super evidence based in the near term, depending on who's in charge. But I do feel like that's another question where I have dramatic uncertainty. ~~And, and I think the population~~ About what ought to happen or what will happen. So is the chip ban going to work? Will it meaningfully slow or, ~~you know,~~ will there be like a multi year gap between US and Chinese Frontier models as a result of the chip ban. I am hearing confident takes on both sides of that issue. I've had people tell me who I think are very smart, ~~like,~~ Oh, it's definitely working, ~~you know,~~ they're not going to be able to keep up.

And then I look at ~~like~~ certain models and I ~~like, you know,~~ managed to go as far as. Trying Ernie 4. 0, and it seemed pretty good, ~~like, you know,~~ at a couple of queries, ~~you know,~~ head to head with GPT 4, it wasn't like, obviously, dramatically inferior, in fact, it seemed pretty comparable, and there's a lot of Chinese speakers, ~~you know,~~ at these conferences, obviously, too, so that would be interesting to know, ~~kind of,~~ do they think that this sort of policy will even be effective? If so, then you could say, well, ~~if, if,~~ if the machine learning researchers think that a chip band will in fact slow Chinese progress, at least, ~~you know,~~ that could be something that ~~the,~~ the executive could ~~sort of~~ take into account, but on the other hand, if they think it's not even going to work, ~~you know,~~ then it's like, well, geez, now we're just sitting here escalating ~~and, you know,~~ and the elite researchers, many of whom are in fact, ~~you know,~~ originally from China, ~~like~~ don't even expect it to have an impact.

I could see that being perhaps. Policy relevant at a super high level. Yeah, ~~I guess I,~~ I think if I was going to run a survey about that sort of thing, maybe I'd want to talk to chip experts rather than AI experts, like you can just run a survey of any experts to answer your emails. ~~I think~~ also maybe I have ~~like~~ some kind of hesitation around that sort of thing or I'd have to think more about it. Partly just, ~~like,~~ a lot of what AI Impact does is ~~sort of~~ on topics where we feel broadly, ~~like, I don't know,~~ just more people knowing what the situation is like in more detail will make things better. I think once you get close to things that are adversarial, you have to be more careful. Like, ~~you're,~~ you're probably helping one side or another. ~~You know, I,~~ I don't know enough about the topic to have a strong view on what should be happening.

But, ~~uh, yeah. You know,~~ don't want to be contributing to arms race. Prima facie. ~~I mean,~~ that's a tricky one for sure. I don't have a strong view as to whether it's going to work or not. My bias, not knowing if it's going to work, is that it seems ~~kind of~~ too risky. The current course is definitely an escalatory course. And, ~~you know, it's like~~ if you worry that the most likely flashpoint in the world is that China will blockade Taiwan or something, then, ~~like,~~ preventing China from getting any of the chips from Taiwan certainly seems like it may make them more likely to do that, right? Because What do they care ~~if that, you know,~~ if chip production is disrupted, if they aren't getting them anyway. ~~So, now, whatever,~~ that's fairly crude analysis, certainly could be disputed. But if you could inform and say, Hey, by the way, the machine learning research community doesn't even think it's going to work.

~~I mean, I think in some sense, there's like only an opportunity. Well, this is maybe, that's a hyperbolic statement.~~ It seems like new information, I Given how escalatory the current politics are, probably is much more likely to serve, if anything, as a reason to de escalate, just because, ~~like,~~ we're already in this sort of cycle of escalation, so you could ~~sort of~~ come with a finding that's like, yeah, your escalation is potentially going to achieve your policy objectives, and then, ~~you know,~~ people would ~~kind of~~ stay the course. But ~~there's, you know, I mean,~~ I would say that it's probably more likely that you could have a deescalatory impact ~~if in fact, you know, the results turned out a certain way, which I don't know what they would be. I'm not confident at all. The default right now seems to be escalation. So if you could, you know, in the,~~ in the sort of gambling analysis of.

~~You know,~~ maybe I'll find something that supports escalation, but escalation already exists. Maybe I'll find something that supports relative de escalation, ~~you know,~~ maybe that is in some ways, ~~you know,~~ more positive expected value just because the current course ~~is,~~ is what it is. Yeah, that's fair. But yeah, also, I don't know enough about it to be confident that I should want de escalation, but yeah, as a baseline, I don't love escalation. That's another big question. I do think that it could be clarifying there too. I could be convinced if I was ~~like~~ really sure it was going to work. Then I would be more likely to support it. You know what? The worst scenario is a highly escalatory move that doesn't even achieve it's ~~like~~ intended goals. So, ~~you know, if it's like, well,~~ it's at least going to achieve its goals, then like maybe some escalation, ~~you know,~~ could be worthwhile if you believe in those goals, but even if you believe in the goals, if it's not going to work, ~~you know,~~ then it's just like a total net negative.

So anyway, ~~I don't know, we can both be very, you know, candid with our high level of uncertainty here, but~~ food for thought for possible additions to the survey next year. Anything else that you want to make sure people don't forget about this work or any other kind of commentary you want to bring us to a close with? ~~I don't think, like, I don't know.~~ I mean, it's quite a lot of uncertainty here. I think ~~like~~ the chance of things just staying pretty similar and nothing coming of this to like drastically change people's lives seems quite low. So ~~it's like, you're,~~ even if you're not very directly involved with AI, it seems like a pretty important thing to be keeping an eye on and trying to have accurate opinions about what the public thinks about these things ~~will,~~ will. affect what policies happen, and I think that, ~~you know,~~ could change the rest of the future forever, a lot for better or worse, ~~so.~~ Does that suggest that you'll ~~be doing, or would~~ be exploring some public opinion investments as well?

I'm not sure, ~~I think there's been like,~~ Some other people are working on that. ~~Yeah,~~ I think public opinion being important is somewhat different to ~~like~~ it being important for us to know what public opinion is. I guess the public knowing what public opinion is, ~~I think,~~ is potentially good for moving toward whatever view it's moving toward, hopefully a more reasonable one. ~~Like, I think~~ if you start to get evidence for a thing and at first you're like, ah, nobody else thinks that. And maybe I'll just be quiet about that. It's ~~like, um,~~ helpful to be polling. What do people actually think along the way? Yeah. ~~I don't know that that would be, I mean,~~ this work is so in the weeds and, ~~you know,~~ works so hard on the definitions, obviously polling the public, you would have to have a very different set of questions and a much higher level sort of gut check, ~~you know,~~ for many people.

But I do think the common knowledge is probably pretty valuable. And I do think also, For just ~~kind of~~ reassuring policy makers that they're not ~~like~~ out on a crazy limb if they want to do something, ~~you know, that,~~ that the public is ~~like~~ kind of with them by default, I think is probably pretty good to establish, ~~but I think that is gradually being established.~~ ~~So I don't know that that's like a, you know, highly neglected area.~~ Also among the tech people, there's like a, I think under appreciation of how skeptical the public is. I'm not sure that the Silicon Valley set. is really realistic about what, like my mom's friends think about AI skeptical in the sense that like they, they don't like AI. Yeah. ~~Uh,~~ there's a lot of people out there that are just like ~~my,~~ my mom's one friend who I've known for, ~~you know,~~ my whole life ~~pretty much said, uh, and this is, she said this to me, you know, we'd go back a long way. So she can be pretty familiar with me and it's not like a big deal, but you know, I do nothing but work on AI.~~ ~~I'm kind of broadly. You know, an enthusiast, I have my safety concerns and you know, the longterm worries are very real to me, but like, She's not even really thinking about that. She's just knows that like Nathan works in AI and~~ her reaction to me was like, it creeps me out.

I don't want to have anything to do with it. Full stop, ~~you know,~~ like not curious, not looking to automate some tasks, ~~you know,~~ not looking to get help on an email draft. I don't want to have anything to do with it. ~~Now you might have, you know, go back,~~ you could say like, Hey, they might have said the same thing on Facebook and have it on there too. So I don't know that that's the final word. It is creepier than Facebook. Yeah, no doubt. I do think there's just a lack ~~of,~~ of grappling with, ~~you know,~~ just how widespread that sentiment is. Anything else on your mind? This has been a great conversation. You've been very generous with your time. I really appreciate that. And love the work. So I definitely hope to see a 2024 edition as well. Thank you for having me. Cut to grace.

Founder of AI Impacts, thank you for being part of the Cognitive Revolution.