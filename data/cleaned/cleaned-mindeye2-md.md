Paul Scottie. Welcome to the cognitive revolution. Thanks for having me. I'm excited for this conversation. So you are the lead author on a recent paper called mind. I too. Shared subject models, enable FMRI to image with one hour of data. A long time listeners will know that we had a, an earlier episode with your teammate and the author of the original mind I paper just nine months ago. And I thought this one was worth following up on because what multiple reasons, ~~I mean,~~ in general, ~~the, um,~~ the fact that we have brain reading technology in the world today, and that you can literally. Use a machine to see what somebody is seeing is ~~like~~ a pretty striking reality that ~~I think~~ a lot of people are ~~kind of~~ missing amidst all the other noise and developments that are happening, ~~uh, but~~ also ~~that~~ the techniques under the hood are really, ~~uh,~~ interesting.

So I'm excited to get into a lot of the technical details and learn how you've made this, ~~uh,~~ modern Marvel happen. ~~You want to start by just, um, I guess, you know, folks can go to the original episode to hear the full deep dive. Uh,~~ you want to ~~kind of~~ maybe just set up ~~like~~ briefly where the first one left off ~~and,~~ and, ~~you know,~~ what ~~kind of~~ motivated the part two of this project? ~~Uh, yeah,~~ yeah. To give a bit of history to this, ~~I guess~~ back in 2022, ~~um,~~ I joined Lion discord server, and I joined basically to. Get better acquainted with machine learning methods, ~~kind of like~~ learn how they are publishing really high quality work as a open community. And I appreciated their open science approach to things. And as I was scrolling through that discord, I found that there was a channel dedicated to reconstructing, ~~uh,~~ images from brain activity.

And the person who was leading that was Tanish. ~~And~~ so I basically introduced myself and said, Hey, I'm already kind of doing this. So I was a postdoc at the time, ~~um,~~ in Ken Norman's lab. And this is ~~kind of~~ the direction that we wanted to go more so regarding memory. But ~~you know,~~ to reconstruct memory, you first need to reconstruct vision, right? So that's the direction we were going. And at the time there was not as much work being done in this space. At least the quality of the results were not the quality that. Now these papers are able to show and it was actually like a perfect timing because there is a new data set that came out the natural scenes data set and this was really high quality data from eight participants being scanned for 30 to 40 hours each a very intense ~~kind of~~ data collection effort from the nasal RSNK labs.

Thank you And ~~so~~ this was an amazing opportunity to use better data quality, and there were new open source models being released. ~~So~~ for instance, CLIP and Stable Diffusion were ~~kind of~~ very great open source models that you could tap into. ~~And~~ so everything was there. The pieces were there to get a new pipeline working for state of the art results. ~~And~~ so we teamed up and made this an open science project with MindEye 1. When we released MindEyeOne, ~~I think~~ there was a great reception, and there were also many other papers that were doing the same sort of approaches. ~~Um, so,~~ for instance, Furkan Ursulic had a brain diffuser paper that was also showing really good results using similar approaches. And ~~so,~~ the constraint with MindEyeOne and the similar papers was that all the models are being trained independently. So you have ~~like~~ 40 hours of data from one person and you train the entire pipeline on just that one person.

It's not being trained across multiple people. So if you have a new person that you want to apply these models to, you would basically need to put them back into the MRI machine and scan them for ~~like~~ 30 to 40 hours. And ~~obviously like. I don't know exactly how much it costs.~~ It's like a thousand dollars an hour to scan somebody. So it's really not realistic for followup work. So if you want to apply this kind of stuff to new directions, like memory, mental imagery, dream reconstructions, biomarker diagnosis, ~~Um,~~ have it usable by people in hospitals, like clinicians, ~~you, you,~~ you're just not going to get the ability to apply these models. So that's the ~~kind of~~ focus we are working towards. And MindEye 2 is that step in that direction that we now have output. ~~So~~ it basically allows you to. With a totally new subject, collect one hour of data and get good, not as good as if you had 40 hours, but a lot better than you would have had with only one hour.

So it's like near the same level of quality with 2. 5 percent the amount of data as before. Yeah, there's a couple of really interesting things here about the relative magnitudes of data, and ~~I think~~ there's a couple of interesting connections also to ~~like~~ some recent, ~~um,~~ like a recent episode we did on robotics. So let me just try to highlight a couple of things and you can react to these. One of the challenges, obviously, as you said, for. This sort of problem is scarcity of data, right? And that's again, true in robotics as well. There's just a limited amount of sort of how does a robot go about solving this problem? In this case, there's like ~~a very limited amount of available FMRI, excuse me,~~ a very limited amount of FMRI data available to train models on. So even in the first mind, I paper, one of the big ideas was, We can combine this like relatively modest data set that we have with these foundation models in a clever way to tap into this kind of general purpose understanding of images that they have already learned from their ~~like~~ web scale training.

And we're going to find a way to tap into this with a dataset that's ~~like~~ comparatively ~~much,~~ much smaller, but is still big in the FMRI sense. And now you're ~~kind of~~ taking another. Order of magnitude or, ~~you know,~~ close to two order of magnitude step in terms of how much incremental data is required to start to unlock this capability for a particular person. ~~So I think that's,~~ that's really interesting because ~~these,~~ these sort of foundation models that embody a sort of a world model and obviously ~~that~~ that topic is like hotly, ~~um, you know,~~ debated in terms of exactly how we should understand it. But it is. Increasingly clear with all these different applications that it has a lot in there that if you figure out how to augment it in just the right way, you don't necessarily need to bring a ton of new data to the table to make really cool things happen.

~~So I think~~ that's just a really profound takeaway from this work. Can we talk about the data set itself for a second? First of all, do I understand quickly that the mind I one and mind I two papers are using the same data set? Is that right? Yeah. Okay, cool. ~~So the, and~~ let me just run down a couple of things to make sure I understand it. You can correct me if I have any misconceptions. I believe there are eight patients that participated in this collection set. They participated between ~~like~~ 30 and 40 hours ~~in, in,~~ in an FMRI each. And ~~they were shown,~~ there's one table in the paper that I wasn't quite clear on, which is the in figure two, where there's this, ~~um,~~ list of subjects and how many hours they were in there. And then the number, is that the number of input output pairs for each ~~person?~~ I think that's the number of voxels that get fed into the model to put them in a shared space.

Gotcha. Okay. ~~So,~~ Let's come back to voxels in just one second. So they're in there for 30 to 40 hours each. Their experience during that time is. What they're getting shown an image every how many seconds. ~~Do you know? Yeah, that looks like~~ every 4 seconds. So ~~it's, um, not everybody.~~ Everyone was requested to be scanned for 40 hours. Not everybody got through the full amount, but ~~so,~~ for instance, for subject 1, they were there for 40 separate sessions, 40 visits to the MRI, and they saw 750 images per visit, so per hour, basically. Okay. ~~And~~ so it's three seconds of looking at an image, then one second of blank period, and then another three seconds of a different image. And the task is to press a button inside of the machine. If they, ~~uh,~~ ever see an image again. So every image was shown, ~~uh,~~ three times throughout the entirety of the 40 sessions.

Interesting. So they, and they are identifying a total of like 40, 000 images that a person sees 30, 000 to 40, 000 images where every image ~~gets, uh, Uh, it~~ gets shown three times. Gotcha. Okay. Interesting. And ~~their job is that,~~ is there something that, ~~um, that~~ that sort of clicker data is used for, or is that primarily just to ~~like~~ keep them engaged and make sure that they're actually still looking at, ~~you know,~~ thinking about the images that they're seeing. Yeah, we didn't use it, but I think it could be used. ~~Uh, like~~ there's been some other papers that try to do the opposite of what we're doing. Like you try to predict the brain data instead of predict the image. And the state of the art for that is you use all the behavioral information that you have possible to, ~~uh,~~ better predict the brain data.

~~So it,~~ that shows that it can be useful. We tried to get that to work. We weren't having much success, so we ultimately didn't use it. Gotcha. Okay. ~~So,~~ Then let's talk also about just what is measured. So every four seconds, they get a three second view of an image, a one second blank. During that time, when eyes are on a particular image, we're taking, is it just one data point or multiple data points of the brain? Activity during that three second window. ~~Uh,~~ yeah. So I think it's taking a snapshot every second. I might be wrong. ~~There's like a,~~ this is the so called repetition time of the MRI machine. So you're taking a full picture of the entire 3d brain. Every X seconds where X is the repetition time of the machine. So it's not a continuous. measurement. It's like every second you get a full picture of the brain.

And then they use a general linear model to basically convert it from this time series, like every second measurement to a single measurement per image. Gotcha. Okay. Cool. ~~Um,~~ and then in terms of ~~the,~~ the actual, ~~like,~~ biological function that is measured and sort of the space that is measured, the fMRI is measuring blood flow, if I understand correctly, and I had to look back at my notes with Tanishka on this one, but, ~~uh,~~ he described it as a two millimeter cube resolution. So ~~yeah,~~ the way I thought of that was if I kind of imagine my visual cortex being at the back of my brain and the number of voxels from this chart is like in the sort of the lowest number is ~~like~~ 12, 000 and some the highest number is 17, 000 and some, ~~it would seem to correspond to.~~ if a single voxel is two millimeters cubed, then you have five ~~by five~~ by five with a centimeter cubed, which would give you 125 voxels per cubic centimeter.

And then to get to those numbers, you'd ~~kind of~~ need to have a 10 by 10 like layer ~~of, uh,~~ so it's 10 centimeters by 10 centimeters. With these sort of rice grained two millimeter cube bits, ~~each one measuring~~ with a single number for each one, ~~right,~~ that measures the blood flow to that small little region within the broader, ~~um, I assume this is all~~ visual cortex. ~~Yeah. Yeah.~~ Does that all sound right? Yeah. Yeah. ~~So, I mean,~~ basically just kind of restate, ~~I guess,~~ like with fMRI, you are processing these images and the parts of your brain that are more involved in processing the image are going to consume more resources. And basically the brain needs to resupply those parts of the brain with more blood. ~~When,~~ when it's been used, and the machine itself is measuring the oxygenation changes that are, ~~you know,~~ associated with increased blood flow to those parts of the brain.

And so we are measuring that oxygenation, blood level, oxygenation change in the corresponding 1. 8 millimeter cubes of the brain where every voxel is ~~like~~ hundreds of thousands of individual neurons. ~~Um,~~ so you have a whole brain that's completely divided into all ~~of~~ these voxels. We only take the voxels from the visual cortex specifically and feed ~~in~~ those into the model. Okay. ~~So each person, um,~~ first of all, that's remarkable that, ~~you know,~~ it's a pretty crude measure in some sense, right? It's like a, ~~um,~~ it's both. ~~I guess,~~ like, at least one step conceptually removed from what you would presumably think of would be ~~sort of~~ the ground truth of, ~~like,~~ what neurons are firing it's aggregating. Also, as you said, like, the hundreds of thousands of neurons being compressed into 1 data point. I'm also a little curious about how the different individuals end up with different numbers of voxels.

Is that just a reflection of ~~like~~ anatomy, like people have different brain sizes and you're kind of ~~segmenting?~~ Yeah. ~~I mean,~~ some people have bigger brains than others. Some people have differently shaped brains and also people have different. topography of just how they represent things, like the functional topography. So ~~like,~~ you're looking for voxels that are receptive to visual processing, right? And so where you cut off where the visual cortex ~~is,~~ is going to be different for every person, and it's going to be differently shaped. And so you'll have different numbers of voxels that are corresponding to the visual cortex. But in addition to that, Just in general, people will have different numbers of voxels and not every voxel has like a correspondence to the same voxel in somebody else's brain. So that's why it's ~~kind of~~ difficult to have shared subject modeling. Because there's all these variations in how people represent things and how the brain, like the input data itself is a different dimensionality, right?

Yeah. Interesting. So is that something that. A clinician or a technician is ~~sort of~~ doing based on their ~~like~~ familiarity with fMRI scans. Is somebody actually like sitting there and drawing a line around it or is there some automated process for determining what the region is that will be considered for an individual patient? ~~Uh, well, for,~~ for kind of large scale. ~~Uh,~~ determining of where the different regions are ~~like, uh,~~ you can use some automated software for that. There's ~~like,~~ parcellations that you can use that are based on ~~like,~~ large scale data sets where generally speaking, you can look at the different. ~~Um,~~ like cell sign gyri of the brain and figure out where it should be, ~~uh,~~ cut off for more specific brain regions. There's functional localizers. You can do retinotopic mapping. Basically, the occipital cortex has its own special way of, Representing where it should correspond to where in your visual field, your inputs from the retina are coming from.

So there's some more nuanced stuff that you can do for it. But for just like visual cortex, you could rely on automated software. Yeah, it's not like you're subjectively looking at someone's brain and drawing, ~~uh,~~ where ~~the,~~ the different regions are. So did I say incorrectly that the one technique that you're describing there would basically amount to ~~like~~ calibrating a system by. showing, ~~you know,~~ a light in one corner of the visual field and then ~~kind of~~ looking for where that lights up and ~~sort of~~ creating a map of ~~like,~~ okay, this is how this person seems to represent the visual field based on the fact that ~~we,~~ we know what we showed them. And we ~~kind of~~ know that ~~like,~~ there's going to be a surge of activity corresponding to that. Am I getting that right? Yeah, pretty much. ~~Yeah. It's,~~ it's, ~~uh,~~ like You basically are just looking at where in the brain are you getting visual inputs and essentially you have these rapid repeated images, right?

You could directly quantify what parts of the brain are receptive to, ~~uh,~~ these visual images because you expect that if you have a repeat that the same. ~~Uh,~~ the brain regions responsible are going to look similar. They should look similar if you look at the same thing, right? But ~~if the,~~ if there's no sort of correlation between image repeats, then probably that information is not what you want for perception at least. So ~~like,~~ that's one way that you could quantitatively figure out the brain regions that you want, but also just We know that visual cortex is probably the most important part of the brain to use for this. We did try using the whole brain, but it didn't work out as well. Interesting. So this data set actually contains a lot more voxels than are being Specifically used for this project. Yeah. Yeah. You get the ~~whole, uh,~~ whole brain for every snapshot, but we only, ~~we~~ subset out the visual cortex.

Interesting. Okay. ~~Um, all right. Fascinating.~~ So we've got something on the order of 10, 000 ish images. ~~You said~~ each kind of shown three times. ~~Um,~~ each measurement of activity corresponds to essentially a vector of ~~like~~ 12 to 17, 000 numbers, which indicate activity in ~~these,~~ these tiny little regions, 000, because everybody's anatomy is different. And so you're on the order of ~~like~~ 10, 000 times 10, 000, like a hundred million numbers for an individual patient. ~~Um,~~ and that's all drawn from this, ~~like,~~ 30 to 40 hours ~~in the,~~ in the fMRI machine. So ~~that is still like,~~ that's not nothing. ~~Um,~~ but unto itself, let's like, ~~you know,~~ still not enough to, ~~uh,~~ train this sort of image reconstruction. So maybe give us a little bit of the kind of motivation for, or ~~the, you know, uh,~~ maybe the intellectual history of ~~like~~ how you guys realized that there was this opportunity to tap into, ~~um,~~ even bigger scale, ~~you know,~~ webscale foundation models that had already been pretrained and available to.

The key thing, ~~I guess,~~ is that you don't want to map the voxels to pixels, right? It would just be ~~kind of~~ an insane task. The number of parameters would explode and ~~like,~~ you're not relying on ~~sort of~~ rich, ~~um,~~ representational information like semantics of what is the image. That's the kind of space that you want to map the brains into. You want to map from brain to some more compressed Rich space. And that's why we mapped to clip space in particular. And because clip has this kind of rich semantic information. ~~And~~ if you use even better clip models, it even has basically low level information, you can ~~kind of~~ exactly reconstruct the image from the latence. ~~Um,~~ so that was like ~~kind of~~ the key insight, ~~I guess,~~ map it to clip. And that'll allow you ~~to,~~ to really allow yourself to get high quality reconstruction results.

~~I think~~ the past work would do stuff like Gabor filters and ~~kind of like~~ CNN based approaches where the key inside, ~~I guess~~ here is just do whatever you can to map into this frozen pre trained space. Yeah. ~~I kind of think of this as~~ we've recently covered, ~~um,~~ UNETs in a bit of depth on a recent episode, specifically ~~like~~ a Mamba variation on a UNET. So I think of this as ~~sort of~~ Analogous to mapping onto the bottom of the unit, you're mapping to the conceptual space where the image data has been ~~kind of, you know,~~ convolved if it's ~~a, if it's~~ a convolutional network and ~~sort of~~ abstracted away from the lowest level. raw pixel inputs and to a state where it is fewer channels, but deeper dimensions, more meaning per channel. And then as the unit kind of sweeps up the other side, it is gradually unpacking that into pixel level data again.

And similarly, ~~like,~~ that's ~~kind of~~ what the. Diffusion models are doing right. They're starting with this like high meaning, but low precision and then unpacking that into something that, ~~you know,~~ you can actually render again ~~as~~ as pixels and see what it looks like. ~~Um, I think that I don't know if you have any other kind of intuition, uh, drivers for this sort of thing that you could share.~~ ~~I think.~~ This is a really interesting point and one of the big things I try to do with this show is ~~like~~ help people develop their sort of conceptual intuition because there's so much technical stuff. There's so much notation. ~~Um,~~ is there anything else that you could ~~kind of, uh,~~ offer about the nature of the space into which you are mapping the brain data? Yeah, ~~I think, um. You know,~~ these are foundation models that have uses beyond, ~~you know,~~ just image and language.

~~I mean,~~ clearly you're able to map brain data into clip and you could see that as like another modality that you're appending to a frozen model. Basically my take on it is that as AI. Foundation models get better and better. The ability for medical applications will also get better and better. ~~So like,~~ for instance, we used Stable Diffusion XL for this new paper, whereas we used Versatile Diffusion for MindEye 1. And, ~~you know,~~ the better the image generators get, the better we'll be able to tap into that. And the better the representations get, for which we map the brain data into, The better the results will be. Yeah, ~~there's a, um, well,~~ let's come back to that in a second. So it seems like there's ~~kind of~~ two big drivers of why this project, ~~you know,~~ ultimately, ~~you know,~~ at the end of the day, ~~right.~~ Each project has a figure and the figure from mind I won is ~~like,~~ here are the images that the people saw, and here's what we are able to recreate.

And some of them are ~~like~~ incredibly good where you're like, wow, ~~that,~~ that is ~~like~~ crystal clear brain reading. And then ~~I think, you know,~~ as you get a little deeper into the data set, like some of them are, ~~you know,~~ not so good or ~~kind of~~ uncanny valley. And, ~~you know,~~ mind I too, ~~you know,~~ at the simplest, highest level analysis, like the images look even more like what the person saw. And ~~so, you know,~~ the reconstruction is ~~like,~~ at ~~sort of~~ just a glance, it's ~~like kind of~~ obviously improved. It seems like there's two main, ~~um,~~ drivers of this. Improvement. One is that you've figured out a way to not have to train a single model for each individual person, but instead mix their data together. ~~Um,~~ this is, ~~you know,~~ conceptually super important as that, as you mentioned at the top, ~~like,~~ allows you to.

Collect a lot less data for a person, which means like to the degree that there are going to be clinical applications for this. You can actually, ~~you know,~~ get there in a reasonable amount of time ~~and~~ and effort and expense. And then the other thing ~~is,~~ you're ~~kind of~~ hinting at just now is the ~~you.~~ image generation models themselves are getting a lot better. ~~And so~~ there's ~~kind of like,~~ some of this stuff ~~is like~~ coming ~~kind of~~ for free because ~~like, you know,~~ rising tide ~~of,~~ of just general performance is ~~kind of~~ lifting all of ~~the, you know,~~ the different project boats tortured metaphor. ~~Um,~~ how would you ~~kind of~~ break those down? ~~Like,~~ what do you think is important and ~~how, um, you know, where,~~ what is the kind of relative contribution of those two conceptual. Advances to the ~~just~~ overall headline view that like, oh, my God, the reconstructions are looking really good.

Yeah. So ~~I guess, yeah,~~ there's 2 points to this. 1 is specifically if you isolate the shared subject part and. Discount the fact that we're using different models, like ~~we're,~~ we're mapping to a different clip space. We're using a different image generation model. So if you only look at the shared subject part, we do an ablation in the paper and show ~~that~~ that is, ~~you know,~~ a notable increased performance. So just by that by itself using shared subject modeling. Even if you're using the full data set, you know that you'll get ~~where we get~~ state of the art results. So we focus on the one hour use case here, but if you use the full amount of data, then you get state of the art, ~~uh,~~ reconstruction and retrieval results. The second part, the clip model. So you can take out the shared subject part and use the new models and everything.

And that also Gives you notable improvements in performance. So ~~they're both, I can't say which is like more important than the other.~~ They're both giving obvious improvements, ~~um,~~ for both of these. And also another part of the paper ~~is that we don't really focus that much on.~~ It's not the focus of the paper. ~~Um,~~ but we trained our own unclipped model for this. So it's actually. ~~Uh,~~ state of the art unclip, ~~like,~~ clip image embedding to image, ~~uh,~~ state of the art performance in that domain by itself. So we fine tuned Stable Diffusion XL to be able to, ~~like,~~ reconstruct the image from ground truth, ~~uh,~~ clip image embeddings. In a way that preserves both the high level meaning and the low level ~~kind of~~ image details. And that raises the possible ceiling of the ~~kind of~~ quality of results you can get. Because the previous models that most of these papers were using was versatile diffusion.

And basically, if you give it an image, say, People cooking in a kitchen or something, and you convert that to clip image embeddings. And then you undo it back to the original image. You don't really get the original image. You'll get like something that maybe resembles a kitchen and a chef, but you're not going to get them in the right spot. They're not going to be wearing the right things. Lighting's going to be different and all that. So we specifically had to train our own thing to raise the performance of that. And so we tap into the new space allowable by the. Ability to unclip better than before. Okay, ~~so there's, yeah,~~ I want to break this down and get a little bit more technical on each side. I'm looking at figure two. We can maybe overlay this on the YouTube version and maybe put this on the link to this image in the show notes as well.

~~Um,~~ there's ~~kind of~~ ~~two, you know,~~ two main parts of the overall pipeline, right? There is the part that ~~works on the image data. Or sorry, it~~ works on the brain data. And there, the big advance is creating a single model that combines all of the individuals into a single shared latent brain space. And then the other side is, okay, now we're figuring out how to tap into the power of these foundation models by ~~like~~ figuring out the best possible way to convert the latent into an actual good looking image. ~~At the,~~ this wasn't maybe entirely clear to me, but you're saying that the image portion ~~in the, in this,~~ in this figure, figure two in the paper, the brain side is shown as ~~with,~~ with fire emojis, which means that those parts are actively trained. And then the image parts are shown with the snowflake indicating that they're frozen.

Yeah, you're saying that they were first fine tuned and then frozen for the purpose of ~~like~~ doing each individual, ~~um,~~ persons. Model. Is that right? ~~Like you, but there is some kind of customization.~~ There's a difference between the flames and the snowflakes is basically like ideally all of these models that are in the snowflakes would already be out there and we would just like tap into them during inference to get the final results. And like the actual mind, I to model itself is only training the parts that are indicated with the flames, like the MLP and the diffusion prior stuff. ~~The.~~ The nuance that I was talking about is that when you've converted the brain into clip image space, and then you want to undo it, AKA use an unclipped model. There didn't exist a good unclipped model. So ideally someone else would have done it already and we would just use theirs.

But since a good one didn't exist, we fine tuned SDXL to make a better unclipped model, and then we use that. Okay, cool. That's helpful. ~~Um, Well,~~ let's just go into a little bit more detail then on each half of this. ~~Um,~~ maybe start with the image one, because it seems like that's ~~kind of~~ the first part of the work. And once that's solid and working, then you can go over and do the, ~~uh,~~ the brain to image mapping portion. ~~There's, I think~~ you said earlier that ~~the,~~ if you take a clip latent and try to unclip it, you get something that is semantically aligned with whatever ~~the,~~ the input was, but may look quite different. And this that we take to be a reflection of, ~~you know,~~ the original clip. Conceptually was aligning text and image space and driven by going back to your lion project.

The lion 5B, right? It's like the 5 billion images pulled off the Internet that are captioned with obviously a lot of noise, ~~you know,~~ in those captions, a lot of ~~like.~~ Not super specific. ~~You know,~~ the ~~typical~~ typical web caption is not like describing in, ~~you know,~~ specific detail, like which elements are in which portion of the image. It's not ~~like~~ containing all these sorts of layout type details. It's just ~~sort of~~ saying high level. ~~Like,~~ what is this ~~right~~ at best? And it might even be, ~~you know, kind of~~ worse than that, ~~you know,~~ and just off topic. So there's a lot of noise in the underlying data set for aligning text and image. The captions are not really meant to allow you to reconstruct. They're just meant to ~~kind of.~~ Summarize and give a sense for what the image is. So it makes sense that when you have this.

Aligned space, and you try to reverse out of the space into image space that you would ~~sort of~~ have ~~like~~ a fairly wide range of possibilities, all of which would correspond to the meaning, ~~you know,~~ like there is a, ~~uh, you know,~~ a tiger is, ~~uh, whatever,~~ it's a tiger, ~~um,~~ be a tiger in a lot of poses, ~~you know,~~ doing a lot of different stuff and with a lot of different backgrounds, whatever. So all of those are ~~kind of~~ possible from this. ~~You know,~~ the, ~~uh,~~ the diffusion prior or ~~the,~~ the latent that, ~~uh,~~ represents a tiger that could go in a lot of different directions of images. That's, ~~you know, also, that's, that's~~ cool, right? Because that's why you can get, ~~like,~~ different images every time from the same prompt, ~~uh, out of a,~~ out of a diffusion model. There's like just a lot of ways you can go.

You don't want that flexibility in this use case, right? You want to actually get back very specifically to the thing that the person saw. So there's ~~kind of~~ a complicated mix of techniques. You want to describe the mix of techniques that augment this ~~Just~~ semantic content with ~~like~~ more, ~~you know,~~ low level visual detail. And there's also the captioning portion. I'm interested to hear how you conceive of all that working together. Yeah. So basically to get back to the idea of like clip is semantic, right? So with unclipped models going from clip back to image. The typical way that these models work is that you take the final output of ~~the,~~ basically the classification token, ~~right?~~ The pooled output of the contrastively trained clip model, trying to align the image captions with the original image. And then you feed that through some model that gets you back the pixel image.

And that is going to be highly semantic. And people like that. Because it gave you this kind of creative flexibility. It was called image variations models. So like ~~you give it,~~ you give the model an image and then it gives you ~~kind of like~~ a creative re representation of that image. We didn't want that. Like you said, we, we wanted exactly the original image, which at first glance might seem kind of. contradictory. Why do you want to be able to input an image and then output the same image? Like, why, what's the point? But the point is that you're able to then cut off the beginning, just feed in the latent, and then get back the original image. And so ~~that we, that,~~ that's ~~kind of~~ the basis for training our own unclip model. We don't use the pooled output. ~~Right.~~ We actually use the kind of like last hidden layer.

So all of the, ~~um,~~ outputs from each of the tokens that were represented in the vision transformer. So it's a much higher dimensionality. Then just the final output of the clip model. And what's actually pretty interesting in its own right is that you can basically get back the exact original image from the clip latent, if you take it from that point. So even though it's only trained to contrastively match the captions with the image, the clip model somehow learns to represent ~~basically~~ the entirety of the image itself. ~~Right?~~ So you can get back. Basically the original image from the clip latent. Now this wasn't possible with MindEye 1. Which meant that we needed to have a total low level pipeline where we are predicting a blurry representation of this image that doesn't care about semantics at all. And then we use that as the initial starting point for the denoising diffusion process.

So instead of starting from noise, we start from this kind of separately obtained. Blurry reconstruction and then go to the finished product from there and the blurry representation was not using a clip model. It was using, ~~uh, like~~ other models, like the V. A. E. of stable diffusion and it had no semantics. To it, but it was able to more or less ~~like~~ have a darker region where the person should be, and maybe it has the sky is blue and ~~that~~ that's the extent of it, but at least it had this kind of low level representation to it. So, with mind, I, too, we consolidated that low level pipeline into the full pipeline. So ~~it's not that we, we don't,~~ we don't have to train 2 separate pipelines anymore. And also we. Realize that ~~the, the,~~ this new clip latent with the unclip basically does have the low level properties.

Now, it's not as important to have the low level pipeline at all. And the ablations that we use for the paper Like we don't even need to start ~~from,~~ from the low level reconstruction and use that as a starting point. It actually does worse when you do it like that, the low level. There are a couple of points to the figure two, which we do, which kind of makes it look complicated, but you could actually remove and it doesn't really affect the performance that much. So the low level stuff. Getting this blurry reconstruction from my night to it. You really don't need it. It's more ~~of~~ like ~~it,~~ it improved the low level statistics ~~in the,~~ in the, ~~uh,~~ like table to get the state of the art results, but subjectively speaking, it didn't really do that much to it. We also have this kind of captioning module that.

Is trying to predict from the brain, the caption associated with the image to reconstruct. And we use that to further guide the final output. So maybe the caption is a cat on a table. And so we feed that caption into the refinement process to get the final image. That also didn't really matter that much. Like it's cool to show that you can predict the caption from the brain. But honestly. The vast majority of the information here is contained in that clip laden, you could throw out the captioning and the low level and it will do nearly as well. Okay. That's fascinating. Let me try to restate some of that back to you and make sure that I understand it. ~~And then hopefully everyone will understand it.~~ In the earlier version, going back to MindEye 1, the foundation model that you were able to use was semantic, but not structural in terms of what it represented.

~~So,~~ this is why you would have this kind of, ~~you know, Um, you know, like,~~ like ~~you see~~ when you go use stable diffusion, you can say, ~~you know,~~ a tiger and they'll give you many different tigers because there's a lot of different ways to represent that. So to get something that actually looked like the image that the person was using at that time, you had to start the diffusion process from essentially, ~~uh,~~ structural hint. So you're combining two things. Saying, here is what is represented, this is where we want to go, and here's ~~kind of~~ a loose hint starting point of ~~like, kind of~~ the regions of the image, so that you can fill in appropriately, and at that time, that was ~~Sounds like~~ quite necessary to get things to work reasonably well. Now, if I understand correctly, it's largely because the, ~~this is a really,~~ this is a really interesting, ~~uh,~~ and potentially quite subtle point.

Maybe you have clarity on it, but I don't quite yet. ~~Um,~~ it seems like it's because the foundation model is so much better that essentially now you can Just take this, ~~uh,~~ diffusion prior, which is what the second half, which we'll get to in a second, the mapping from the brain data to this space, what that outputs. is now so rich in, in meaning that it basically works straight away and you get images that look like the original image. And then you also experimented with a couple of these other things to, ~~you know, like~~ do ~~that hint,~~ that visual hint suggestion again. And then now you also have the caption hint suggestion and those sounds like kind of boost ~~Uh, kind of,~~ let's say, ~~like, um,~~ Yeah, it gives, ~~like,~~ a slight boost to the metrics, but overall it doesn't actually make that big of a difference.

And, yeah, ~~we're allowed,~~ we're able to get, ~~like,~~ really good, ~~uh,~~ caption predictions from the brain. But, ~~you know,~~ more or less, ~~the,~~ the bulk of the model is explained by mapping brain into a frozen, pre trained clip image embedding space. Thanks. So ~~even the, so I guess~~ the very practical interpretation of that is ~~like,~~ if you remove these additional hints and just run ~~the,~~ the down the fairway, simplest version of this, then the images you get out still look like what you expect them to look like. And as a human looking at them, you're like, this is working. And then you add these other things and it improves these scores, which are ~~like,~~ obviously more objective, but perhaps Not ~~like~~ necessarily super meaningful, ~~I guess. I mean,~~ these are like pixel level, ~~you know,~~ comparison type. Yeah. It's like you compute the correlation of the original image and the reconstructed image at where you like feed them both through a frozen network and you compare how similar the intermediate parts of the networks are.

And honestly, we had some complaints about the metrics themselves, which we hinted throughout the paper saying that these metrics are not very good. So. Like, you can have reconstructions that, to me, are obviously better, and they'll score worse on these metrics. And ~~so,~~ we actually did some human testing. ~~Like,~~ we showed people online, human raters, the different images, and had them pick the ones that they prefer better. And it's like, clearly there's a disconnect between These quantitative metrics that are being used in all these papers and subjective preference for what people think is the better reconstruction from perception. Yeah, okay. ~~I mean, that, um,~~ we're hitting a lot of frontiers, ~~I think,~~ at the same time, generally across ~~the, you know,~~ The whole vast field of AI, ~~we're~~ like, that is becoming a very live issue. These sort of robotic scoring methods that we've used in the past are ~~kind of~~ hitting their limits.

And all of a sudden it's like, ~~so, I mean, is there a, I didn't see this in the paper.~~ ~~I don't know if this would, you know,~~ there's a lot of noise, obviously in human evaluations as well. ~~Um,~~ is there a, just like human, like, how good is this score? ~~Like,~~ did you guys just try doing ~~like, uh,~~ everybody rate this one to 10 or something? We had ~~like~~ a alternative forced choice. He showed two things and then they pick the one that they think better, ~~um,~~ resembles the, ~~uh,~~ original image, which, ~~you know,~~ could have its own problems. Like it might be more biased towards looking. Like a naturalistic image rather than retaining low level properties. That was, it's ~~kind of~~ a not perfect solution, but, ~~um,~~ yeah, I'm not quite sure what the best way is to measure the quality of these kinds of results.

Because for instance, the low level modules, the blurry stuff, like we get really good results with the low levels just by doing a simple Averaging process with the blurry reconstruction. It's very crude, but it works really well for the final evaluations, but subjectively, I think it looks worse. So I don't really know what to make of that, but ~~you know,~~ for these kinds of papers, you have to show that you're able to get the state of the art tables with these metrics. Yeah. It's tough. ~~I mean,~~ one of the things that I've done that has informed my thinking about a lot of these metrics, ~~um,~~ perhaps more than anything else really is trying to get a system together that would do an effective aesthetic evaluation of images. This is something that in my company, which people have heard me talk about many times, Waymark, we'd make videos for small businesses, blah, blah, blah.

We need to choose from their image library images that both semantically correspond to what they're talking about in the videos that we're generating. And also ideally the ones that look good and that looking good part is ~~like~~ a real challenge. And part of the reason it's such a challenge is that people just have. A lot of disagreement as to what looks good. The Ava dataset that came out six, seven years ago, maybe at this point has ~~a, I think~~ about a hundred data points per image. Like they went on mechanical Turk or whatever, and just had people rate these images, but they, you would think, ~~well,~~ geez, why do you need a hundred? Images are 100 ratings per image in this data set. And the answer is that there's so much variation that they ultimately report a mean and a standard deviation for each image, as opposed to a single score.

Because at the end of the day, ~~it's just like, man,~~ it's just too much noise in ~~sort of~~ how people think about these images. And we can't really reduce that in, ~~um, you know,~~ for any practical purposes. ~~So.~~ The evaluation is like across the board is, ~~you know,~~ becoming a very interesting topic where it's like we're hitting ~~the end of, or~~ certainly the limits, if not the end of ~~like~~ what robotic scoring can do for us. But then when we ~~kind of~~ look inward and say, ~~well,~~ what do we think about this? We're confronted by the fact that we often disagree. ~~Um,~~ I would also be really interested to see research as to how individuals disagree with themselves over time. There probably is some of that in other settings. ~~Um,~~ but definitely across people, ~~you know,~~ we disagree quite a bit. And I would suspect even, ~~you know,~~ an individual ~~will,~~ will be subject to quite a bit of drift over time.

So, okay, here's 1 thing I don't really understand. And ~~maybe we can,~~ maybe you answer this in part with the description of more of the FMRI side of this equation. ~~Maybe not, I guess I'm not sure. It depends on the answer,~~ but. Where is the semantic information coming from? I'm understanding that we have these voxels in the visual cortex. This corresponds to blood flow. My sort of very naive version of, ~~you know,~~ what's going on ~~in their understanding of what's going on~~ in the brain is that the visual cortex is ~~Kind of~~ like a convolutional network and it's ~~like, you know,~~ working up from like low level, ~~you know,~~ edge detection type stuff into higher order concepts, but I'm not really sure how far that goes. And I'm not sure if, ~~like,~~ in my visual cortex, I get to the point where, ~~like,~~ back there, it's understanding that it's a tiger or, ~~you know, sort of~~ these high level Concepts need to be sent forward to, ~~you know, uh,~~ even higher order processing that would be like, okay, that's a tiger.

~~Um,~~ but you're mapping this stuff from the back of the brain to the latent space. ~~You're,~~ you're saying that this both the, what I would expect is that the, like the visual, the structural data would be there. If you just said, hey, ~~like, we can, you know,~~ we can get, ~~like,~~ what colors people are seeing and what regions and ~~you can kind of from this, you know,~~ from this noisy thing, we can back that out to an image. I get that. But ~~when you,~~ when it's. I can also get a caption out of that. I'm like, huh? ~~Um,~~ is that because that semantic data is ~~in the,~~ in the data pulled from the fMRI? Like it's represented by the blood flow? Or ~~is,~~ is this something that the CLIP model is doing for us because it has done the work of doing the the, the, ~~the timeline?~~ ~~Yeah. Like semantic connection has~~ ~~the~~ both of these, like I'm including higher level processing areas of the brain in the visual cortex, right?

~~So, like,~~ the very back of the brain, ~~like,~~ primary visual cortex, you'll get V 1 V 2. These are just representing, ~~like,~~ orientation lines, maybe colors as you continue the visual hierarchy, but you get to, ~~like,~~ the inferior temporal cortex. And this area of the brain has specialized semantic processing. ~~Like~~ one example would be the fusiform face area. It's very specific to processing human faces. So ~~I mean, there are,~~ there's a lot of rich semantic information in the brain that is being fed through the model. Now you could feed only the primate visual cortex into this model, in which case it's not going to have the semantic stuff and it's not going to do as well in mapping to clip space. Gotcha. ~~Is that,~~ that sounds like something that you could do by segmenting. Basically, you would have just fewer voxels to work with and you could ~~sort of~~ say, ~~and~~ we have ~~a,~~ a good enough understanding of the regions of the brain to, I apologize for being relatively ignorant of, ~~uh,~~ the biology ~~and the,~~ and the neuro science under this.

Would you say ~~like,~~ there's a, ~~you know,~~ a general consensus as to ~~like~~ what regions you would. Not include if you were going to try to just work with the lower level like content representations as opposed to the semantic understanding. Yeah. ~~Well, I mean,~~ if you only cared about low level, then you can just isolate to the Like v1234, primate visual cortex, but like part of the reason this is an exciting field of work is because, ~~um,~~ you're tapping into someone's representation, like their cognitive representation of what they are looking at. If you only cared about getting an exact reconstruction of the image, then just ~~like~~ tap into your retina. Or, ~~like,~~ put a camera in front of your eye. Like, ~~what's, what,~~ where is the importance to these kind of works if you're just getting a one to one representation back of what you're looking at?

What you actually care about is the nuance behind what I am representing. Versus what somebody else is representing. ~~It's,~~ it's ~~kind of~~ like looking into the brain and seeing specifically what is leading to your specific representation. What is your like past history, your biases, all of the parts of your brain that have led to your current functional topography and how you represent things. That is ~~kind of~~ the thing that we care about and what will ultimately be relevant for follow up work. Like, comparing clinical populations and stuff. Yeah, okay. So ~~the,~~ this is another kind of key theme that ~~I feel like~~ comes up all the time, where the space that we're ultimately mapping into was originally created by aligning text and image. And with that initial work, ~~like,~~ you could get into that space either by giving it text or giving it an image, right?

You could get into this sort of shared align text image space either way, but. ~~You couldn't,~~ you couldn't necessarily get to the spaces that we're actually finding here with the brain data, because this is essentially bringing ~~kind of~~ both, right? It's bringing both the, ~~like,~~ what did I see, ~~you know,~~ in the raw low level kind of data, and also how did I conceptualize that and finding a space that you could presumably not access exclusively with text or exclusively with an image and. Representing both kinds of information there, ~~um,~~ at the same time. That's definitely really interesting. ~~Yeah,~~ Yeah, one thing to highlight on this is that, ~~you know,~~ what people ~~I think~~ get away from this paper is we can get really high quality reconstructions of what you're looking at from the brain. What I've found that people, especially neuroimagers, like the neuroscience community seems to value more, is actually the retrieval component.

So, this is mapping brain space to basically a third modality of CLIP. This is a retrieval module in the paper. ~~So,~~ trained with contrastive loss, ~~right?~~ And that allows you to do stuff like find the nearest neighbor in clip space, it allows you to basically have a brain latent that you can compare to any other clip latent you want and see where you are along that axis, stuff like that. And this is where we. Show that it has such a fine grained representation. One of the criticisms that I've been seeing is like, okay, it knows it's an elephant and ~~like,~~ whatever, but you're like, we show that you can, you have a really fine grained nuance here. You can determine that it is this exact zebra image out of. Like 20 zebra emissions, like you can get perfect matches with pretty high accuracy if you are relying on ~~like~~ this retrieval sub module.

And there's a lot of applications that can be done from that. Knowing that you're having such fine grained information. It's not just like the overall category of the image. It's not just a picture of an animal, ~~you know?~~ Yeah. Okay, ~~cool. Um, This I do find this stuff to be incredibly, uh, thought provoking and quite profound.~~ Let's talk a little bit about the details of just ~~the,~~ the fire side ~~of the,~~ of the figure two, where the voxels are being ~~kind of~~ merged into the same space and then ultimately mapped. In some ways that might be more intuitive for people. I guess ~~the~~ The basic strategy is just like, we need a single latent space, but we have different dimensional data because people's brain sizes and atoms are different as we talked about, so there is a per patient adapter that is trained to map each individual's brain size.

Data into this shared space and then from that shared space, everything is mapped into the, ~~uh,~~ the diffusion prior space and then you ~~sort of~~ systematically hold one out, train a model on the other 7 patients from the original data set, get something that's shared across those 7 and then see, okay, what if I just use a little bit of data from this 8th person, ~~you know,~~ do I now have ~~enough of a, of a shared, you know,~~ rich enough representation, robust enough representation in the shared space that I can quickly, ~~you know,~~ Map this new person onto that successfully. ~~Um, anything I'm missing there? What, um,~~ how would you refine my description? ~~No,~~ yeah, that's all right. Okay, cool. How big are those? How long do those take to train? ~~Um, would it be, would we expect that?~~ Obviously, this is ~~like~~ a ~~kind of~~ unique data set where there are these, ~~like, you know,~~ unusually long time spent in the fMRI for an individual person.

So, ~~you know,~~ if I instead said, hey, I don't want to do that again, but I went and got, ~~you know,~~ 240 or 1000 people to sign a release for the 1 hour that they spent. ~~Um,~~ Would you expect that to work? ~~Like how, I guess, how, um,~~ how do you think this would work with ~~sort of~~ a little bit of a different data set ~~and,~~ and how much compute, how many kind of parameters ~~give me,~~ give us the specs on that kind of stuff? Yeah. ~~Well,~~ so you're referring to ~~like,~~ instead of having eight subjects scan for ~~like~~ 40 hours, ~~like a lot of~~ like hundreds of people scan for maybe an hour or something, I'm just thinking about ~~like~~ what data is just ~~kind of~~ already sitting out there, ~~you know,~~ that wouldn't take such a long time. Heroic effort now might take a legal heroic effort to unlock some of that stuff.

~~Um,~~ but, ~~you know,~~ there's a ton of people that have spent 1 hour, ~~not necessarily. Yeah. Yeah. I guess~~ 2 points on that. 1 is this is ~~kind of~~ relating to the idea of precision neuroscience. Which is the kind of trade off between, ~~like,~~ shorter scanning sessions with a lot of people versus a lot of scanning with a single or just a few people. And there are ~~sort of~~ advantages to both. Obviously, the NetShow Scenes dataset did the latter. And the advantage you get is higher quality data. The ability to really get subject specific data. ~~Um,~~ results and then the former is more so like you can really figure out how one person's brain is different from all the other people's brains. But the overall quality of the data is worsened. So maybe you'll never really get the kind of high quality results. So you would get with something like the natural scenes data set the.

point about like you have a lot of neuroimaging datasets currently out there that could potentially be tapped into to create ~~kind of~~ like an overarching model space that is something we are actively working on so we're currently training A foundation model on fMRI, taking basically all possible data from the entirety of the publicly available datasets that we can get our hands on. And the potential there, I think is huge because you're essentially creating a new avenue towards getting the signal that you care about from a brain scan. ~~Right.~~ You're able to get a generalizable latent signal. Convert the raw brain scans into a generalizable latent trained across, ~~like,~~ hundreds of thousands of people's brains. ~~Uh,~~ that's been ~~kind of~~ imbued by the heuristics of how brain patterns typically work. So I'm really excited about that. That is a definite potential, ~~uh,~~ that hopefully ~~can,~~ can work out.

Yeah. It seems like foundation models are coming to, ~~uh,~~ every modality ~~and,~~ and all modalities, ~~uh,~~ all pretty quickly. So ~~these, uh,~~ I asked cloud three to help me figure out how big the models were on the fMRI to. ~~Uh,~~ diffusion prior side of this and it estimated 940 million parameters. ~~I,~~ I don't know if that was, ~~um,~~ I didn't find that myself in the paper. ~~I dunno if it found it or if it calculated that. Well, that was, that was mind I, one,~~ that was the parameter count from mind. I one. Okay, so ~~tell me how,~~ give me ~~kind of~~ a sense of the order of magnitude of parameters and compute for this work, and then I'm really interested to hear also, like, for your upcoming work, how much data is out there, like, how many hours of fMRI data are you able to tap into, and what are the ~~sort of You know,~~ compute budgets.

You're going to need to do that sort of work. Yeah. ~~So, um,~~ for my day to the models that we trained were probably larger than they needed to be, because ~~you know,~~ if you're going to do ~~every~~ everything for a deadline, for a conference, just like maximize the compute that you currently have to just train it and get the results. ~~Right.~~ So that was like 2 billion parameter, ~~uh,~~ for my day to models. But since then, ~~uh,~~ I've looked into reducing the parameter account ~~and you can.~~ Like more than half of that parameter count and still get basically the same quality of results, ~~but~~ also ~~like~~ 2 billion sounds huge, but actually it's not that big because it's basically coming from a single NN dot linear. Part of the model. So ~~it's~~ basically over a billion of that is only coming through the residual MLP backbone.

And MLPs are a lot more computationally efficient than ~~like~~ a diffusion model, for instance. So it's actually not that slow, like computationally inefficient, even though the parameter counts so big, it's literally just this one linear layer that leads to this ~~kind of~~ over a billion parameter count situation. Okay. And that is primarily the layer that is mapping the shared space to the diffusion prior. ~~It's,~~ yeah, it's like in figure two, ~~it goes,~~ it's like this, ~~uh,~~ shared subject latent space where every person's brain gets compressed to 4, 096 and then ~~goes,~~ going from 4, 096 to the clip latents, ~~uh,~~ dimensionality, which is 256 times 1, 664. And that's flattened. So it's ~~like~~ half a million, I think. So it's that nn. linear of 4, 000 to, ~~uh,~~ half a million that leads to the parameter count. Gotcha. ~~Okay, cool. Yeah,~~ that's really helpful. ~~Um,~~ so ~~how,~~ what happens next?

~~Like, are we talking a,~~ how many orders of magnitude more fMRI data would the foundation model be trained on? ~~Um,~~ and. What does the compute budget look like to make something like that happen? Yeah, ~~so right now we are working with,~~ well, first I'll say the neuroscience community is amazing in the sense that there is a bit of a standard towards releasing your data that you collect as a lab, open source. According to standardized format, specifically it's called BIDS, Brain Image Dataset Specification, I think. So, ~~that gives,~~ that's amazing that there's already ~~kind of~~ this push towards standardization and easily accessible datasets. ~~So,~~ that, ~~uh,~~ is available publicly. There's also the human connectome project, which was a massive large scale neuroimaging data set. There's the UK biobank, which is another one, although you have to pay to get access to that one. But basically, ~~you know,~~ we're talking about millions of hours of people in machines and trying to ~~yeah,~~ use a foundation model to compress all of this data into some sort of generalizable purpose latent that can then be fed for downstream applications, ~~like.~~ One example would be better reconstructions, but it doesn't have to be that.

It could be anything, right? Yeah. Okay, so ~~that is~~ We're going from hundreds of hours to millions of hours, which is obviously Quite a jump. Yeah, we haven't fully gotten all the data sets out. So I don't know an exact number Maybe it's less than a million, but it's ~~like, you know~~ a lot more data. So what do you think? This is all cool. Going, ~~I mean,~~ you may have some interesting, ~~uh,~~ views of the future. We're gonna do another kind of somewhat companion episode to this, where we look at, like, all of the brain computer interface technologies that are out there, ~~you know,~~ for reading increasingly, there's some that are attempting writing, ~~um,~~ or at least steering, ~~you know,~~ the brain activity from the outside. And ~~I guess, you know,~~ there's like interesting progress across a lot of different, ~~you know,~~ hardware modalities and, ~~you know,~~ different kind of signals to try to pick up on and so on and so forth.

Where is this going in the big picture? ~~Like,~~ do you have a view of what is going to become possible? Maybe, ~~you know,~~ in a medical setting is one question, but also in ~~sort of~~ just a general daily. ~~You know,~~ consumer device, like daily life ~~sort of, um,~~ view. ~~Like,~~ it seems like ~~we're not too, you know,~~ you guys are going to have your way with this foundation model. I fully expect that you're going to come up with something, ~~um,~~ that's going to be quite interesting, but then what are we going to do with it? Yeah. ~~I mean,~~ ideally everything works out. The foundation model allows us to have such rich latents and then follow up work could do multimodal stuff. So like you could take inferior. Non image, non invasive imaging tech and map it to a better quality late in space and ~~like~~ the dream would be that you can take smaller data sets that are higher quality, maybe even invasive data sets, and you can just ~~kind of~~ merge all these different modalities together, ~~um,~~ to be able to make use of that kind of higher quality data.

~~Um,~~ FRI right now is the best noninvasive tech, but the downside obviously is that it's very expensive. And it's also very, ~~um,~~ difficult to use, like just from a patient's perspective, you can't move at all. ~~Like~~ if you're someone who gets claustrophobic, then you can't get scanned basically, especially for these kinds of like hour long experiments. ~~Right. Um,~~ so that's not ideal. ~~So Uh,~~ but it is the best that we have in terms of invasive stuff. Like you can get electrodes planted in your brain already. If you're a seizure patient, then people will put electrodes across your brain and try to like deduce, localize where the seizure is coming from. And you can get really high quality data if you have invasive electrodes on your brain, like much better than fMRI even. But the downside is that it's not spread out across the entirety of the brain.

~~So, like,~~ Neuralink is specifically in the motor cortex, right? So ~~you're not going to be able to do, um, like,~~ you're not going to be able to make use of the entirety of the brain ~~unless you've put these,~~ unless you've completely taken out your scalp and put these electrodes over the entirety of your cortex. So you're going to be constrained in terms of localization, but you benefit from the higher quality data. in terms of non invasive stuff like EEG. You're not really able to localize at all. So, ~~you're,~~ you don't get a 3D brain like you do with fMRI. ~~You get~~ basically, you put some electrodes in certain positions and then they bounce along the skull and eventually you get some changes in electrical oxidation and you ~~kind of like~~ deduce things based on that. But you don't really know where the, initial neurons that are leading to that change in electrical activation is coming from.

This is a localization problem. ~~Um,~~ now I think there's a lot of new tech that's coming out that's really promising. So there's ultrasound, there's ~~like~~ near infrared, infrared ~~kind of, um,~~ proxies to what you would get with fMRI that seem very promising. ~~Um,~~ currently the tech is not there yet, but it's clear that it's in the right direction that something's going to come out in the foreseeable future that allows us to get hopefully Close to fMRI quality results with noninvasive technology, or we figure out a way to, ~~yeah,~~ get like a multimodal thing working that can leverage higher quality data with worse inputs. So then what's my life going to be like? ~~Do you,~~ do you have a vision for, ~~you know,~~ is there ~~kind of~~ a, ~~you know,~~ Jetson's, ~~uh,~~ level future that you envision for the general public with, ~~uh,~~ Brain reading technology on ~~like~~ a daily basis or, ~~um,~~ like ~~what,~~ what is ~~sort of~~ your core reason why ~~is it,~~ is it medical?

Is it daily life? ~~You know,~~ what do you think are the most exciting like midterm applications? Let's say, Oh, yeah. ~~I mean,~~ I don't know about ~~like~~ Jetsons. ~~Like, this is sort of, um,~~ the first step that we're going to, ~~um,~~ Is get these complicated machine learning things that are currently only able to be done in an academic setting with a huge data set to work with very limited data, hopefully do things in real time, be able to do things with very constrained data sets and still get high quality results and furthermore, be able to generalize this. To different kinds of outputs that are more relevant than just perception, ~~right?~~ So for instance, we are collaborating with the University of Minnesota to try to do mental imagery reconstruction, like visualizing an image and recreate that. You can imagine that there's a lot of different pipelines that would be relevant here, like memory decoding, stuff like that.

~~Um,~~ furthermore, something that we didn't really talk about was ~~like,~~ The shared subject space stuff shows the potential for this kind of approach to work, where maybe you have some subjects that have a lot of data, but you can ~~kind of~~ tap into this shared space with very minimal data. And that approach by itself is not specific to reconstructing images. So basically, there is a lot of potential here, ~~I think,~~ in Being able to get the cutting edge machine learning pipelines that are currently only reserved for kind of academic basic science research to more real settings that are actually feasible for clinicians to use, for instance, ~~um. I mean,~~ beyond that, when it comes to ~~like~~ BCI tech, I would love to be involved in that space. Obviously, I don't know what ~~kind of~~ the potential is for consumer applications or things like that. Yeah. ~~And so~~ I feel like in general, we're ~~kind of~~ all groping really, ~~you know,~~ I asked these questions and ~~I,~~ in general, the answer is ~~like,~~ nobody really knows where this is going, which I find to be ~~a, a~~ sort of.

Fascinating. And also, to be honest, somewhat disconcerting reality. And that's a much broader statement than, ~~you know, sort of~~ brain, ~~you know,~~ data reading and interpretation, but just ~~kind of like, what's,~~ what is the sort of positive vision for where all this AI tech is taking us is one that I think everybody would do well to spend a little bit more time thinking about. ~~Um,~~ so the memory thing is really interesting. Fascinating. ~~I guess, you know,~~ are there any sort of surprising lessons from this work that people should know about? I'm thinking, for example, like oftentimes bizarre failures, ~~you know,~~ of the model can be ~~like~~ an interesting unpack. I wonder if you, ~~you know,~~ have seen any examples where you're like, whoa, it got this wrong, but in a revealing way where we were able to learn something, ~~um,~~ from it. Or I wonder if you have any kind of updated theories of ~~like,~~ Cognition or like, how similar or different people's perception are.

~~I mean,~~ these sorts of questions have been like, age old philosophical questions. Like, when you see an apple and it's red, ~~like,~~ what does that look like to you versus me? ~~So,~~ yeah, I'm kind of curious as to ~~if you,~~ if you feel like this work is. Informing the way you think about what once were sort of limited to philosophy questions, but now are maybe starting to be penetrated a little bit by science. Yeah, ~~I guess~~ kind of related to this. ~~Um,~~ so the data set itself, right? People were actually looking at totally different images. So usually how people would approach shared subject modeling is everybody looks at the exact same emissions and then, ~~you know,~~ Okay. Basically, the correspondence between I saw this and you saw the same thing, but mine is different than yours. And then you can look at that individual difference and help ~~like~~ extract that out.

And then you have this sort of shared subject component and the individual component, right? We didn't have that here. I mean, there was a small subset of images that everyone saw, but that was the test set. So ~~we,~~ we didn't have access to that at all for the training of the models. And so ~~I think~~ it's pretty revealing that the approach that we used. is literally, it's a lot simpler than it seems. It's just ridge regression. It's linear regression, ~~right?~~ So you just take, no matter what the input was, ~~you know, you take,~~ you take my brain, you take your brain, we looked at different images, you put them in the same batch, and then you get your own linear mapping, I get my own linear mapping, and that just can get the dimensionality to be the same. And account for differences in our voxels, how we represent things, then everything else after the model is the same for both of us, ~~right?~~ And the simplicity of this and the fact that it works so well is very interesting.

~~Like,~~ this was the first thing that we tried. It's like, okay, how do we do shared subject modeling the most obvious way? Just do a linear mapping. To a shared space and then everything else is the same, ~~right?~~ And then the first try and it works great and it's like, why is nobody else doing this? ~~Why,~~ why is everyone doing fancier things of like manifolds and complicated neural networks that are shared between people? Like the very easy solution of just linear mapping for my brain and your brain to the same space works so well. So I mean there has to be, ~~I guess,~~ some shared representations that can support that by one. And you're able to extract that kind of variability in a pretty simple manner. And so just the fact that it works so well, it tells us something about the similarities in how people's brains represent things, even though the inputs are different.

You can get the same kind of underlying relevant signal out, and, ~~you know,~~ the individual specifics of things can be further elaborated by fine tuning the complicated part of the model. In terms of, ~~like,~~ failure cases, ~~um,~~ there are some obvious failures when it comes to the reconstructions, and ~~I think~~ that usually happens when the images are more complicated. ~~So,~~ if you are just looking at a simple image that has maybe one object in it, Then the reconstructions will typically be quite good. Like if it's a single draft, you can recreate the draft. It'll look the right way in the image. It'll basically be in the right setting and it'll be great. But if you have a complicated marketplace scene with multiple people and there's complications like that, it doesn't quite do as well. And ~~I think~~ part of the reason for this is simply the datasets were collected in a way where the images come from Microsoft Coco and.

The majority of the images that people saw are simple scenes. And so the farther away you get from that distribution of images, the less the model is going to be able to account for that. Yeah, that sort of suggests that somebody like an open AI is in a really good position to do a version of this. Because, ~~you know,~~ reading their Dolly 3 technical report or whatever they officially called that, ~~right,~~ there was like a huge, Sort of data enrichment process where they were able to get much, much better results on the image generation, like much more controllable by 1st doing super detailed captions of the images that they used for training. ~~Right? And~~ and kind of getting past the ~~sort of a~~ bootstrapping process where. These ~~like~~ captions that have come off the web are, ~~you know,~~ not meant for this purpose for one thing and just ~~kind of, you know,~~ noisy and general as we've talked about, but they were starting to caption these things in ~~like,~~ really granular ways, like detailed descriptions, and then that allowed them to create that same, ~~you know, kind of~~ corresponding level of control on the other side, ~~it seems like the it.~~ ~~This,~~ this sort of shows the way to potentially ~~like~~ even way more, ~~you know, kind of~~ powerful and high fidelity versions of this.

If you simply just had like better data, better foundation models, ~~um,~~ one way that I'm ~~kind of~~ thinking about that is like how much of the compute in this project was incremental to the. Original foundation model effort ~~versus, um, you know,~~ and if that ratio is really small, then, ~~like,~~ that would suggest that, ~~you know,~~ these sort of mega projects potentially become, ~~like,~~ even more valuable than we may be naively thinking, right? Because then if it's just a little extra compute to do to unlock all these other use cases on top of these foundation models, then, ~~like,~~ the best foundation model wins, maybe, ~~um.~~ So do you have a number, I again asked Cloud3 to analyze this, and ~~um,~~ it guessed that the incremental compute was somewhere between 0. 1% ~~Uh,~~ and even less than that, ~~like~~ at the high end of its range, it guessed that it was essentially one one thousandth additional compute relative to the stable diffusion Excel foundation model training.

~~Yeah. I don't know if you've analyzed it in that way, but does that seem directionally right or wrong?~~ So, for instance, ~~like~~ we train ~~the full,~~ the full thing. Of ~~like~~ the pre training and then the fine tuning, it's like less than a day on one, each 100 node for the pre training. And then also less than one day for each fine tuning. And then for the Stable Diffusion XL unclipped, we trained it for less than one Epic, like fine tuned it from the base SDXL, so yeah, ~~it's, it's,~~ it's still on that kind of scale in terms of GPU hours, ~~it really is like, you know,~~ the foundation model. ~~Is,~~ is quite important. It makes a big difference in terms of ~~like~~ the starting point for which you can add all these other things. And also the applications that you get out of it are potentially bigger than you would initially think.

~~So,~~ for instance. ~~Like, uh,~~ back when we were trying to get the unclip to work, ~~uh,~~ Tanish reached out to Robin, the ~~like~~ head of Stable Diffusion team. And Robin was like, yeah, ~~you know,~~ if you want to get exactly the original image back from the clip image embedding, just use the open clip. ~~Uh,~~ unpooled embeddings and yeah, you'll get it perfectly. Like here's the kind of results that you can get. And we were like, what, why can't you release this? This is amazing. There's a lot of applications for that, but I don't think that Robin understood the potential of having that. So ~~like,~~ he was basically like, ~~you know,~~ I don't see the potential in it. So then ~~we had,~~ we went and basically followed up and actually fully fine tuned and got the thing working and everything. But ~~like,~~ Yeah, there's probably more of a scope than people initially think that is possible from foundation models.

I don't think that when people trained clip that they would think that people would add a brain modality to clip. Yeah, ~~um, that does sort of, I mean,~~ this is again, another kind of theme across a bunch of different topic areas that we've explored, but the. idea that a foundation model has like often, ~~you know,~~ with GPT 4, even just in prompting it better, we've gone like a year and change now and people still continue to come out with ~~like~~ better ways. just to prompt it to get, ~~you know,~~ better and better performance. And then here, ~~you know,~~ with ~~the,~~ the foundation models being open source, ~~like~~ you're able to ~~sort of hack in a million, I don't know, hacking is the wrong word, but sort of, you know,~~ come up with all these ~~like~~ interesting, clever, ~~you know,~~ ways to map spaces to spaces and connect, ~~you know,~~ spaces to spaces that were never really intended and really don't actually end up requiring all that much compute.

I wonder if you have other, ~~uh, Like~~ areas in mind where you think ~~that~~ that might work. I was kind of trying to map this process onto something for like protein discovery, or, ~~you know, kind of~~ novel protein generation. And it seemed like a lot of the parts kind of line up with, ~~you know,~~ obviously with different models and different data sets, but ~~the,~~ this approach seems like something that I would expect to see a lot more of, do you have other things in mind that you think people should start to bring this, predict the diffusion prior approach to? Yeah, I don't know about, like, specifically predict the diffusion prior approach. I think, ~~you know,~~ there might be better ways, ~~um,~~ to do these kind of things. And, ~~you know,~~ the ability that we had to tap into the, ~~like,~~ clip image space and image generation models was possible because our goal was to output an image.

When you're outputting images, Like proteins or other kinds of things. It's probably a bit more difficult to get the ~~like~~ alignment of using foundation models that were trained for other purposes, although it's probably still possible. ~~Um,~~ and ~~I think~~ at that point, it's like. The potential of foundation models is so apparent for other use cases that training medical foundation models is going to be very important. ~~So like,~~ that's why we're trying to do the FMRI foundation model, for instance, because it would unlock so many new applications, even outside of FMRI, like with multimodal possibilities. So how big of a project is this next one? ~~I, I feel like we, you know,~~ we covered ~~kind of~~ The amount of data is orders of magnitude more. Presumably the compute would be orders of magnitude, more orders of magnitude, more than one day. I'm one H 100 still isn't necessarily that big.

Is this, ~~um, is this~~ Foundation model project, something that is kind of moderate scale compute that you guys are getting. I'm interested to ~~kind of~~ hear how, because Tanishq is the founder of MedArk and you guys are both on the paper affiliated with MedArk and with stability. So I'm ~~kind of~~ interested to hear ~~like~~ a little bit about how that relationship works. ~~I think~~ people are very curious, ~~you know,~~ for many reasons right now about how stability works. ~~Um,~~ but I'm ~~kind of~~ curious, like, First of all, ~~the, you know,~~ the nature of that relationship, but also is this next thing going to be the scale of compute that can ~~like~~ easily come out of a stability budget or is it going to be big to the point where, ~~you know,~~ it becomes like either something that creates scarcity at stability or requires, ~~you know,~~ other resources to be brought to bear?

~~Like,~~ just how big is it? ~~Yeah, so, I mean,~~ yeah, first off, ~~yeah,~~ Taneesh, head of MedArc, he's been instrumental in helping to organize various projects, even outside of these neuroimaging projects where I'm the lead of the team for NeuroAI. And ~~so, like,~~ Ahmad was kind of like the instrumental person in supporting many different open source ecosystems, ~~right?~~ ~~And~~ he was very generous to basically allow us to use the compute. That stability had for external teams and for open source projects and medical domain that I believe, ~~you know,~~ these projects do have profitability, but it's more of a longer term profitability, ~~um,~~ to be aware of. ~~Right?~~ So it is a very generous use case of stability, ~~um,~~ employing me into niche. We're not getting paid through MedArc and stability, giving access to compute, not just to the employees, but to the open lab team that we cultivate across both academic institutions and an open source, an open science discord community.

And I believe ~~that~~ that's a very unique situation. I'm not really aware of other places. that have cultivated that kind of environment. And it's also very crucial to being able to publish quickly and have a diversity of opinions and background to, ~~like,~~ keep aware of, ~~like,~~ the literature and what papers might be relevant to get ideas from. ~~And~~ so that's been great. And also, interestingly, you would think that that would be kind of Prohibitively expensive of like onboarding multiple people that are not employees into a cluster and using all of this, but honestly, so far throughout everything, we've been able to get away with only using a few nodes, ~~um,~~ across the whole team, which involves more projects than even the ones that we've talked about right now. ~~Like,~~ basically, we're able to make use of large teams of people across multiple projects without that much compute.

At least in, ~~uh,~~ relation to the amount of compute that like the stable diffusion team would use. So that's been great, but we haven't yet gotten to the scaling up period of the foundation model, for instance. So I don't really know. What kind of scale of compute we'd actually need and whether or not we would have access to that kind of compute as an external team like this is ~~the,~~ the sort of reality of things. Hopefully we're able to show more promising results and petition for more resources ~~and that kind of thing.~~ ~~Um,~~ but yeah, so far things have been good with little compute, honestly speaking, which is ~~kind of~~ interesting. And hopefully we can petition to continue to grow the project out. Yeah, the embodied compute. I sometimes think of it as ~~the,~~ that these models, ~~um,~~ you know, make kind of portable to other projects is again, and just another fascinating.

Thing to ponder, ~~uh,~~ in the context of your work. So ~~I think~~ this is a super interesting, ~~uh,~~ line of research, and it's going to be very interesting to see, ~~you know,~~ just how far ~~the, um,~~ the foundation models can go. One of my theories right now is that I don't know ~~how,~~ how fast superhuman. Capabilities are coming in language models, but I'm pretty sure they're coming quite quickly when it comes to all of these other things that are like non human modalities. ~~Right?~~ I mean, ~~we're just~~ we have just no evolutionary basis for interpreting ~~the, you know,~~ the voxel level activity of the brain. We have no evolutionary basis for ~~like,~~ Reading DNA sequences that are super long and ~~like~~ making connections, ~~you know,~~ across great distances in the genome and beginning to predict how those things are going to interact, ~~you know,~~ as they ~~kind of~~ work up the layers ~~of,~~ of activity in a cell.

And so the foundation models in these areas are headed for superhuman status, like basically, ~~you know, the,~~ on the first go, and then the question is going to be ~~just like,~~ just how powerful. Are they, but it seems like they're going to be quite powerful. ~~I mean,~~ we're already decoding the brain on small data, ~~um,~~ to think what might be possible with several orders ~~of,~~ of magnitude scale up on top of that ~~is,~~ is really quite remarkable. ~~Do you,~~ do you have any thoughts ~~on kind of, I mean,~~ this is obviously a very complicated question and there's like a, both pros and cons to it. ~~And I, I don't, um, have like a highly ideological perspective on it myself,~~ but. How do you think about open sourcing things that are so different than anything that's been out there in the past and that clearly have this ~~like~~ ability for people to come along and ~~like~~ add on a little bit to it and unlock ~~like~~ something that you didn't even expect, ~~you know,~~ obviously, there's a lot of people who are super pro open sourcing and just say, ~~you know,~~ it's the only way.

And then there's a lot of people that are ~~like,~~ I'm ~~kind of~~ afraid of what, ~~you know,~~ somebody might do if you train something, ~~you know,~~ at 10 to the, ~~you know,~~ 20, whatever, ~~uh,~~ flops and then put it out there and people can do something with ~~like~~ one, one thousandth of that incremental, how do you think about that? Do you guys have like a framework for, ~~you know, if you,~~ if you do train this next foundation model successfully, is there even a way ~~to.~~ to structure thinking on ~~like,~~ weather and how to create broader access to that? Well, ~~I mean, I think~~ the point of the foundation model is to make it broadly accessible to allow new research ~~to be,~~ to expand from it. So ~~like,~~ The point of kind of sharing how this works is to improve, ~~you know,~~ science as a whole, right?

~~Like~~ to allow this kind of progress to be made to, ~~like,~~ make an actual difference, allow for novel clinical diagnosis approaches, allow for novel basic science advances that would be possible from ~~kind of~~ a massive, ~~you know,~~ Training ~~of,~~ of models. ~~So like,~~ usually people are not going to have access to the kind of compute that might be necessary to train a foundation model, but after it's trained, if you just use it for inference, ~~like,~~ yeah, you can get that to work probably. And you can just map into that latent space and make use of that. ~~However you,~~ however you want, you can fine tune it or do all sorts of things with it. So ~~like,~~ I'm very pro in terms of ~~like.~~ Making sure that it's broadly accessible, ~~um,~~ and the, ~~like,~~ open science component of, ~~like,~~ getting People to collaborate with you to kind of expand the frontier of what's possible with these data sets.

~~Um,~~ there's just so many amazingly talented people that are not affiliated with an academic institution or have ~~like~~ a PhD or, ~~you know,~~ Are like actively working at big tech that, ~~you know,~~ there's just so much interest in the field and there's so many people who want to get involved and who are very talented that it's just such an untapped like Avenue, in my opinion, for creating fast paced, high impact work as a team. ~~So.~~ Yeah, and ~~like~~ open sourcing everything is great ~~if there are like practical issues of~~ like, ~~you know,~~ all the data that we're using is already public. So it's not like there's a privacy thing. People consent to share their data. So I don't think ~~that~~ that's an issue. ~~Um,~~ like sharing how things were trained, I think is really great if there are ~~like~~ profitable avenues for applying downstream applications from foundation models.

I think that that is very ripe and there would be a lot of interest after it's shown to be effective as basically the new way to process set first fMRI, but then hopefully additional modalities after that. Yeah. ~~That seems clear that there is a. Significant. I mean,~~ from what we've seen so far, all this activity has broadly been like, very positive. And, ~~you know,~~ I'd be pretty confident saying there's like a 10 to one, maybe even a hundred to one ratio of ~~just like~~ tapping into this, ~~you know,~~ talent that otherwise wouldn't have access and creating all these new things. And ~~the, you know,~~ the creativity and the fact that people will build things that you didn't think to build on top of the foundation, as you said, ~~like~~ is the point. And then I'm also like, man, some of this stuff is so wild in terms of ~~like~~ what it might enable that I do ~~kind of~~ wonder if we might one day wish that we ~~had~~ had a more structured approach.

~~I mean, in, um,~~ in the safety, you may be aware of this kind of, ~~um,~~ terminology, but in some corners of the safety world, there is the notion of structured access, which is like trying to find this middle space between. Open sourcing everything in a way where you like, can't take it back if you ever wanted to, but still like, trying to create the kind of access that people need to be able to do, ~~you know,~~ the good work that you want to enable, ~~um,~~ any, ~~I don't know if you have any thoughts on that, you know, it might just be hard.~~ ~~It might be, um, you know, kind of, you know, Anathema to like what stability is all about. Um, but~~ I wonder if you have any thoughts on a structured access approach to this kind of foundation model. Yeah. ~~I mean,~~ to an extent it depends on how well things work.

~~Like~~ if it's clear that it's working orders of magnitude better than anyone expects, then maybe there are reasons to be more diligent about how, like what exactly you're sharing. But, ~~you know,~~ for fMRI, there are so many. ~~Um,~~ limitations that I'm pretty confident there's not going to be like a huge problem with sharing these kind of foundation models, ~~you know,~~ as things get more invasive and the quality of data gets better and better, then there are going to be privacy concerns. ~~Um,~~ and then you need to be very careful about what is being released and who has access to it and like what companies are like, maybe going to benefit from it. You need to be very careful and make sure that. ~~You know, people are not,~~ people's brain data is not being taken advantage of. ~~Right.~~ Yeah, ~~I mean,~~ it doesn't seem like there's ~~a,~~ a major barrier to like, ~~I mean,~~ there's a lot of little barriers, not to say it's not gonna be a lot of work, but it seems like lie detection, ~~you know,~~ type of technology would pretty predictably get to the point over the next couple to few years where you could imagine ~~like~~ the state, ~~you know,~~ like just putting a headset on people and being like, you're going to answer these questions and we're going to know if you're telling the truth or not.

And I would. Be a little concerned about that future. ~~I mean,~~ it's like, on the one hand, you can imagine a very positive version of that, too, where people might, ~~like,~~ voluntarily wear these things and ~~sort of, you know,~~ demonstrate to others that they are being honest, because, ~~like,~~ look, my brain readout shows that I'm being honest. And, ~~like,~~ maybe that creates a high trust future that is, ~~like,~~ really beautiful in a lot of ways. But ~~it also, you know, all these,~~ it's just always so striking how dual purpose a lot of these things are. And especially when they are still ~~kind of~~ noisy and it's ~~like, you know,~~ we've seen these issues even with just face recognition sometimes where, ~~you know,~~ police like show up at somebody's door based on nothing but ~~a,~~ a face match. It turns out it was a false match. And, ~~you know,~~ now they're kicking somebody's door down who literally had nothing to do with, ~~you know,~~ whatever was the issue.

So ~~it's,~~ it's very, I find fraught. ~~It's like, I, I,~~ I personally, I'm just like super excited about it. And at the same time, ~~like,~~ yikes, ~~what, um,~~ what might happen here as this stuff gets. Really good, so at a minimum, ~~it's~~ it's good to hear that you are, ~~um,~~ thinking very actively. ~~So, I mean,~~ to be clear, there's no way that, for instance, lie detection or, ~~you know, kind of~~ applications by law enforcement would be relevant to the fMRI domain. ~~Like,~~ the quality of the data is not good enough. To do these kinds of things. You cannot access things that the person is not ~~like~~ directly thinking about. If you move around more than a millimeter, the data gets distorted. Like you're not going to be able to get the kind of signal to noise that would be at all relevant for ~~like~~ practical applications that you're talking about.

But it can get good enough for very important medical purposes of like comparing clinical groups, identifying biomarkers, trying to assess, ~~um,~~ like mental imagery stuff. You have to be concentrating on the task and be, ~~you know,~~ very still in a scanner to get any of these fMRI approaches to work. And I don't want to err on the side of ~~like~~ being too careful. In this current stage of knowing what the quality of the data looks like, because I know that ~~like~~ we want ~~you want~~ to make advancements first that are going to practically make a difference in terms of helping people with their disorders, understanding basic science of how the brain works. It would be much later on. That there's the potential, ~~I think,~~ for the kind of technology you're talking about, there have to be some big innovation that happens where the quality of the data gets so good.

You're able to access it even with the person, not really ~~like~~ wanting it to be like access. ~~Like~~ this is much farther down the line. At which point, if that technology does become accessible, that would open new questions. Yeah. But at this stage, I don't think it's as important to really focus on like, All the nuances around and I'm not even sure my concrete stances on everything. I'd have to think more about it. Yeah, I think that's an important commentary. It seems like we're maybe at sort of a GPT 1 or GPT 2 phase of this line of research where it's like. Just entering into the zone where it can start to become useful, but it still ~~like~~ takes a lot of effort to actually get real value from it. And there are probably like, at least a couple generations to go before it gets to the point where it's ~~like,~~ just really easy to use and ~~like~~ broadly applicable.

~~Um,~~ I often say about GPT 4 that we're ~~kind of~~ in a sweet spot right now where it's ~~like,~~ on the one hand, super useful and on the other hand, like not so powerful as to be like serious risk of, ~~you know,~~ becoming a destabilizing force in the world. And I think maybe we have ~~like, you know,~~ another generation to go before that happens. At some point, ~~like~~ it does seem clear, ~~like~~ we're headed for something that's like powerful and potentially destabilizingly so, but it is good to calibrate on all these different lines of research. It's just like, are we? Just entering that sweet spot or we're getting like to the late stage of that sweet spot. So to say that ~~we are, uh, you know,~~ we have a couple of years at least to ~~sort of~~ really allow the research to mature and continue to figure out like what the plan is as it gets into that, ~~um,~~ that sweet spot of like easy broad use across a lot of use cases is a really good frame for us to all keep in mind.

~~Um,~~ this has been an awesome conversation. I appreciate the technical depth and the philosophical engagement as well. Is there anything that we haven't covered that you want to make sure we touch on a little bit? ~~Um, I guess~~ I want to give a shout out to the other people responsible for the work, right? So there's clearly Tanisha, she was the senior author on the paper, both for Mind Eye 1 and Mind Eye 2. Mahir Tripathi, Cesar Tariqo, Rhys Neeland, Ken Norman, All very instrumental. There's even more co authors, ~~but~~ there's like a dozen people involved. ~~Uh,~~ you know who you are. But yeah, ~~there's, it's,~~ it's a very widespread effort. We're still working, for instance, with Princeton to collect new data. We're trying to have more collaborations. With academic institutions and the broader open science community. And in that sense, we are open for people to help out.

~~Like~~ if you think you can help us with these projects, we have several projects going on right now on the MedArc discord. So if you just go to MedArc. ai, you can find the discord link. ~~And~~ join our server. We have weekly meetings for different projects. We just ~~like~~ meet on zoom once a week and, ~~uh,~~ work with public get hubs for most of these projects. So it's an opportunity to, ~~uh,~~ work on some cutting edge research together. And yeah, I encourage anyone who's interested to look into it. Cool. So that's medarc. ai is the website. Definitely. ~~Um,~~ at a minimum, everybody's going to want to check out some of these figures and see just how high fidelity the reconstructions are of ~~what the, uh,~~ what the mind's eye is seeing. And, ~~um,~~ I'll definitely join the, ~~uh,~~ discord ~~and,~~ and lurk a little bit, at least, ~~um,~~ I'm sure you'll get a couple other, ~~uh,~~ new entrants to the conversation as well, ~~uh,~~ for now.

Cool. Again, fantastic conversation. Thank you for sharing all this, ~~uh,~~ research and perspective with us, Paul Scotty, thank you for being part of the cognitive revolution. Thank you so much.