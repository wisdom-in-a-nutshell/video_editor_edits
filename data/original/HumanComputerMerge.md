# Merge


## Introduction to the Cognitive Revolution

[00:00:00] Dean W. Ball, Research Fellow at the Mercatus Center and author of the Hyperdimensional Substack. Welcome to the Cognitive Revolution. Thank you, Nathan. Great to be here. I'm really excited for this conversation. You and I are kindred spirits in that,~~ um,~~ we both ~~kind of ~~have had our day jobs and then we've had our obsessions with different areas of technology.

[00:00:19] We connected online after a couple pieces of writing that you put out that ~~I thought~~ were excellent and,~~ uh, kind of learn~~ as I learn more about you, I was fascinated to see just how much time ~~you~~ and energy you have put into learning all about the brain computer interface space. 


## Exploring the Brain-Computer Interface Space

[00:00:35] And so today we are going to do a survey on all things neurotech, and I'm really excited to learn from you in this domain ~~that I really don't know a ton about~~ and to share that with our audience.

[00:00:45] so much. Awesome. Yeah, me too. 


## Dean W. Ball's Journey into Neurotech

[00:00:46] So you want to start by just maybe introducing yourself. ~~I mean, ~~you've got ~~kind of ~~a varied,~~ uh,~~ set of experiences and background interests, and then we can dive into your kind of high level thesis and dig into ~~the, ~~the technologies that will take us there one by [00:01:00] one. Yeah.

[00:01:00] ~~Well, ~~as the title of my sub stack suggests, I have ~~kind of ~~a varied background, a lot of different brands going into it. ~~Um, ~~I've spent most of my career in the think tank world. ~~Uh, ~~so about a decade,~~ uh,~~ working in state and local economic policy for the most part, housing, infrastructure, things like that.

[00:01:17] ~~Um, ~~but I've always maintained a very strong interest in technology. ~~Uh, ~~I started open source forum software. If there's anyone who remembers PHP BB from way back in the day,~~ uh,~~ I was,~~ uh,~~ doing commits to that and ~~writing, ~~writing technical documentation,~~ uh,~~ for that at a young age. ~~Kind of ~~the coding bug hit me at that point.

[00:01:35] ~~Um, ~~and I never went into it professionally, but it's always stuck with me and just a general interest in technology and that morphed as the deep learning revolution unfolded in the last decade,~~ uh,~~ into following the M. L. literature pretty closely. ~~Um, ~~and once Chakchidiki came on the scene, I realized that this was a perfect moment for me.

[00:01:59] ~~Uh, ~~[00:02:00] because I have a background in policy and there's a lot of interest ~~in, ~~in the societal and policy implications of this technology. And at the same time, ~~I've got, ~~I've got a pretty good ground truth technical sense of what's actually going on, which is often lacking in the policy space. ~~Um, ~~so I try to bring that to everything that I do.

[00:02:17] I love that.~~ I think, you know, ~~I often say that we really need to figure out what is before what ought to be done about it. And there's, ~~you know, ~~all too much jumping to what ought to be done about it that is not based in a good ground truth understanding of what's going on. What technology actually ~~is, you know, ~~exists today, what can be done with it,~~ um, you know, ~~where that's sitting in the near term and so on.

[00:02:37] So I appreciate that perspective and I'm excited to get into the details of what you think is going to happen in the big picture of the possibility of a merge between man and machine. 


## The Merge: Man and Machine Integration

[00:02:50] Yeah, so~~ I guess~~ I'll explain a little more about how I came ~~to this, ~~to this particular issue. ~~Um, ~~I am a longtime fan of J.

[00:02:57] C. R. Licklider, the DARPA [00:03:00] senior researcher,~~ um,~~ who wrote an article back in 1960 called Man Machine Symbiosis. It's not quite the physical merge,~~ uh,~~ that we're going to be talking about. It anticipates the development of something like AGI, as a lot of people did in the 60s, being like way easier. ~~It was sort of the, ~~the thesis ~~of that, ~~of that paper is AGI is going to be here in the next five.

[00:03:19] To 10 years and,~~ uh,~~ who knows what happens after that? Probably it'll become super intelligent and replace us. But in the meantime, we'll have a lot of fun. That's basically the thesis of this senior government official. At that time, it shows you how much things really changed in terms of technology, similar quotes from some notable technology leaders.

[00:03:40] More recently as well ~~from ~~from the technologists for sure ~~But ~~but coming directly from the DOD is ~~just~~ interesting and it shows you~~ just I mean ~~how ~~kind of like~~ greenfield everything felt with computing at that time and what's exciting about right now is I do feel like we're at a similar moment where A lot of things are possible But I've always thought about this issue of [00:04:00] symbiosis and ~~kind of~~ thought about it in a fairly broad way in the last You Few years ~~as, as, ~~as the,~~ uh,~~ the concept of artificial super intelligence has become maybe a little closer than any of us thought possibly eminent.

[00:04:14] If you believe some people on Twitter, the concept of the merge just keeps coming up, ~~uh, ~~prominent technologist, as you say, ~~you know, ~~Sam Altman. ~~Uh, ~~has a famous blog post called the merge where he essentially describes it as an inevitability. ~~Uh, ~~Elon Musk is not just called it an inevitability, but is in fact working,~~ uh,~~ on technical solutions.

[00:04:36] ~~Uh, ~~so he is probably taking this more seriously than anyone else. And just in general, in my conversations ~~with, ~~with people building in the AI space at advanced levels, there is a. ~~A, a~~ very common sense that this is something that will happen. And if artificial super intelligence is even remotely eminent, ~~uh, ~~then the merge must be not too far off either if that's going to [00:05:00] happen.

[00:05:01] And so my fundamental question is what does the technology really look like? ~~Uh, ~~and that's what I sought to answer,~~ uh, in, ~~in some articles that I've written. And,~~ uh, I guess~~ to do that, I first looked at. The actual,~~ uh, you know, ~~scientific technologies that exist right now and ~~sort of ~~like, where is the literature?

[00:05:19] ~~Uh, ~~but also thought about it in a broader sense. ~~Um, ~~because in a lot of ways,~~ um, ~~computers already are neural interfaces. You don't control them directly with your brain, but they obviously interface with your brain and you are using your brain, ~~you know, ~~via,~~ uh, uh, ~~touch control to manipulate them. ~~Um, ~~and ~~I think that technology~~ throughout the history of technology, one of the things we see is that information technologies don't just change the way information propagates.

[00:05:44] They change the way we think. ~~Um, and ~~so that was ~~kind of ~~my starting point of almost seeing the merge as something that started a long time ago, a path that we are still on and that will get more vivid and more visceral over time, [00:06:00] but not a fundamentally new. Yeah. That's interesting. Certainly I feel like I've been through a couple of waves already of technology changing how I think.

[00:06:08] ~~People, you know, it's, ~~it's feels so quaint now, but there was the idea of ~~like, ~~we don't have to remember anything anymore because we can just go search for it on the internet. And certainly that is true, ~~you know, ~~or even, ~~you know, ~~GPS, right? Just, we don't have to ~~kind of ~~make spatial maps of our, ~~you know, ~~general environs because ~~we,~~ so many of us just default to using the GPS.


## The Impact of Language Models on Thinking and Coding

[00:06:30] ~~I think ~~for me right now, I wonder if you have experienced something similar with the current wave of technology. I do find myself getting into some pretty deep habits, at least, with how I relate to language models. ~~I think ~~coding tasks are a pretty good example of this, when I reflect on how I used to code versus how I approach coding tasks now.

[00:06:51] Probably making a mistake in the past, ~~you know, ~~because I've never been an outstanding coder, but I feel like I often used to go into the editor first, ~~you know, ~~and maybe [00:07:00] start writing some comments and trying to ~~kind of ~~break things down or starting to scaffold out the classes. But very often I was jumping too quickly into code.

[00:07:08] And one of the things I've learned from using the language models is ~~like, ~~I'm really rewarded for taking the time first to think ~~like, ~~what do I actually want here? ~~You know, ~~can I define that in Conceptual terms first, before I start to, ~~you know, ~~diffuse as it were into ~~the, you know, ~~the finer grained structure of the code.

[00:07:24] And in general, I think ~~that ~~that's starting to happen to me. ~~I've, you know, ~~we recently did an episode with shortwave, the email client as well. That has a really good email AI assistant. I see an evolution there too, where I ~~like~~ used to remember keywords to get to navigate my, ~~you know, ~~vastly overflowing and unmaintained inbox.

[00:07:41] And now. I don't have to be so keyword oriented anymore. I can ask like questions. ~~Um, ~~and that's definitely very helpful for me, but ~~I ~~also ~~do. ~~I'm starting to feel already a little bit like my keyword search ability is beginning to atrophy. So certainly I recognize and,~~ um, you know, ~~experientially feel the [00:08:00] idea that ~~the, ~~the ways of technology have changed how I thought.

[00:08:02] I don't know if you have any,~~ uh, ~~similar stories,~~ um,~~ and this probably pales compared to,~~ uh,~~ what might be ahead of us still. Yeah. No, absolutely. And~~ I think~~ it is important to think about ~~that, ~~that kind of thing. For me. ~~Kind of ~~in a similar vein, ~~I find~~ LLMs just fundamentally changed the way I ask questions and made me much better at asking questions.

[00:08:23] ~~Um, ~~oftentimes I remember it's about a year ago,~~ uh,~~ since my first experiences with GPT and 3. 5 was ~~not quite as, not, not, not, ~~not quite as vivid in this way. But when GPT 4 came out, I would ask it a question and I would find this isn't quite what I wanted. ~~You know, this isn't, ~~this isn't quite what I was looking for, but I know this model must be capable of giving me what I look for.

[00:08:44] So the problem must be with me. And so I kind of refining my question and getting down to what is it that you're really asking? And so often in fields of intellectual inquiry. That is actually the more important thing. Finding the right question [00:09:00] is 90 percent of the intellectual journey. And so something that accelerates that, that makes us better at that, ~~uh, ~~accelerates all kinds of intellectual endeavor.

[00:09:10] ~~Um, ~~and I am excited to see things like Clawed 3 Opus, which it seems that the qualitative difference. ~~in something like~~ in that model versus GPT 4 is really at the frontier of all kinds of different fields that people that are ~~kind of ~~doing middle of the bell curve activities. ~~Um, in, you know, in four or, or,~~ and not even coding necessarily, but just like various kinds of questions on science,~~ uh,~~ history, whatever it may be that those questions, the middle of the bell curve stuff, the difference between GBT four and cloud three open is not necessarily all that much, but that when you're asking things that are more at the frontier, flawed three really shines GPT four.

[00:09:49] I can definitely tell you having used. LLM's a lot to help me accelerate this process of researching these articles,~~ um,~~ on neuroscience. That's definitely been [00:10:00] the case. ~~Uh, ~~with Claw, it's just,~~ uh, uh, ~~so much better at answering those questions,~~ uh,~~ than GPT 4, where GPT 4 would get answers that I had enough context to know, like, Heh, this is kind of B.

[00:10:11] S. here. This is ~~kind of ~~just filling in the blank. ~~Um, ~~whereas Claw 3, ~~you know, is, ~~is more capable. ~~So, so, ~~I think ~~as the, ~~as ~~the, ~~the LLMs get more capable,~~ um,~~ that will continue ~~to, ~~to be. ~~Uh, ~~very relevant. And then, ~~I guess, ~~on another level,~~ uh, ~~obviously, anyone paying attention ~~to, ~~to this,~~ uh,~~ Space is asking themselves philosophical questions, ~~at least I have to think so,~~ about what is the nature of cognition?

[00:10:35] What is the nature of intelligence? What distinguishes my intelligence from the kind of intelligence that I'm seeing here exhibited on this screen? And ~~I think that that is a kind of,~~ it's actually a journey that we should all go through anyway, regardless of whether LLMs exist. Because asking, ~~you know, how you,~~ what special value do you bring to the table?

[00:10:55] ~~Um, ~~and ~~like, ~~what unique perspective do you have is a question that everybody should be seriously [00:11:00] asking themselves all the time and not enough people do. And I think LLMs are kind of forcing function for a lot of people,~~ uh,~~ for labor market concerns ~~more than anything, you know, ~~just as much as anything else.

[00:11:09] ~~Um, ~~to ~~kind of ~~just really probe at that. So in a way, ~~I think, um,~~ for me. ~~My,~~ my experience as I look back on the last year using, ~~you know, ~~advanced LLMs,~~ I think ~~it's made me more reflective and just more inquisitive. And that is Excel. That has all kinds of compounding benefits. And obviously the other thing is that once so much about the history of technology is not how it impacts one person or one building or one place.

[00:11:37] ~~Um, ~~it is instead about the nonlinear compounding effects when everybody has access to that same capability. And I think we're in the very early days of that. And that's part of why I~~ just~~ find this to be such an enormously exciting time to be alive. Yeah, it's crazy. ~~ I mean, I, I,~~ 


## Navigating the Complexities of AI and Society

[00:11:54] you hit on something there that I often think about as well, which is just that the future [00:12:00] dynamics of the world at large as AI ~~kind of ~~starts to infuse into everything.

[00:12:05] Are dramatically under theorized. When I go around, talk to everybody in AI today, largely their implicit world model seems to be. The world is the world. I'm applying AI here. Therefore, we're going to have the world plus my AI intervention. And that'll be~~ like~~ sweet because we'll get this productivity gain or whatever.

[00:12:23] And that is, ~~you know, ~~not wrong, but it sort of misses the fact that everybody else is doing this at the same time. And therefore there's going to be just a lot of ~~sort of~~ In my view, unpredictability to how all these new arrangements interact with each other at ~~sort of ~~a higher level. ~~I think it's also, yeah, go ahead.~~

[00:12:40] I think about that all the time. And I think that we're in for just ~~a, ~~a ~~sort of~~ order of magnitude complexification of the world beyond what we've already seen. If you think about, ~~you know, the, the, ~~the world has just become denser in the last 30 years. ~~You know, ~~you think about ~~what was. ~~What was New York City like 400 years ago?

[00:12:58] The island of Manhattan,~~ uh,~~ [00:13:00] versus what it is today. ~~It's just a, ~~it's the same geographic landmass. ~~I mean, ~~they landfill parts of it, but basically the same thing. ~~Um, ~~and the difference is that it's just dramatically denser. ~~Uh, and, ~~and more complex. And I think that we are in for,~~ uh,~~ perhaps not geographical densification, but,~~ uh,~~ another substantial round of conceptual and cognitive densification.

[00:13:21] ~~Uh, ~~and one of the things I worry about, and another thing that attracted me to this issue area, is the question of how we're all going to keep up with that. I worry now that people like you and me are already in a bubble where ~~we're ~~We're extrapolating, ~~you know, sort of ~~multiple points out on this LLM on the dynamics ~~of, of, ~~of AI agents being all over the world.

[00:13:45] ~~Um, ~~other people haven't even contemplated the idea of AI agents that have probably, ~~you know, ~~asked chat GPT 3. 5, one basic question 18 months ago, and haven't thought about it really that much sense. And so I worry about people being ~~kind of like~~ shocked and left behind. ~~Um, ~~and I think that [00:14:00] maybe neural technology is our way.

[00:14:03] At least I hope, ~~um, ~~for us all to just keep better pace ~~with, ~~with the complexity that I suspect is coming. Yeah, there was a recent study just this week that came out that said that 30 percent of young people,~~ I don't know exactly what the age cutoff was, but 30 percent of young people~~ are ~~like~~ using chat GPT for work or whatever.

[00:14:16] And it is a good reminder that, ~~I mean, that's a, ~~that's a lot, cause it's only been a year and change since the first version. And. ~~You know, ~~a year and just a few days since GPT 4, but it's also not that many yet. And, ~~you know, ~~the tools are still certainly relatively basic compared to where it seems quite clear that we're headed in terms of, ~~you know, ~~going along for the ride, which is,~~ uh,~~ an Elon Musk,~~ uh, ~~framing on this.

[00:14:38] ~~It seems like, And I want to just, you know, now get into the meat of the technology and get, you know, really dig into these details.~~ And this may be a little shocking for people that aren't aware of how much progress has already been made because one way I think about it is like the way we use.

[00:14:47] Computers has, first of all,~~ it's, it's currently a, you know, it's, ~~it's a bi directional relationship, right? We put information into them, they give us information back. The way in which we have entered information into the computers historically [00:15:00] has been this sort of finite action space of like, you can type, ~~you know, ~~on the keyboard and you can, ~~you know, ~~click the mouse and you can do a lot with that.

[00:15:09] ~~The, ~~but ~~it is sort of a, you know, ~~at any given time you have ~~like ~~a finite and often quite, ~~you know, ~~Prescribed set of actions that you can take. And outside of that,~~ like,~~ just nothing happens. ~~So, ~~and that's ~~kind of ~~also, ~~it's~~ interesting. That's one of the reasons that they cite for why the web agents historically didn't take off.

[00:15:22] They tried to do reinforcement learning on the web and the reward signal was just too sparse. Agents couldn't get anywhere and they, ~~you know, ~~they couldn't chain enough. Success together to get any reward. And so the whole project, ~~you know, a few years ago, kind of ~~went nowhere for that reason. Of course, the signals~~ that we've been~~ that we get back have been getting richer and richer over time, like higher resolution, ~~you know, ~~more realistic graphics, et cetera, et cetera.

[00:15:43] Now, with this language moment, we have the ability to speak in natural language to the computers. And this really opens up, ~~you know, ~~a sort of richness of communication for what they can understand. In terms of our mental states ~~and, ~~and desires, ~~I think~~ Ilya [00:16:00] from OpenAI once and possibly still from OpenAI,~~ um,~~ put that ~~well ~~when he said,~~ like,~~ the thing that's most incredible about this is I speak to the computer and I feel I am understood.

[00:16:10] And ~~I think~~ that's a really good ~~just, you know, ~~baseline reality check of ~~like, ~~what makes this different than before. But now, ~~you know, ~~recalling ~~your, ~~your substack title hyperdimensional,~~ we're,~~ we're moving around the verge of potentially moving from this language mode of communication, which has just opened up for us to talk to computers in our own natural language.

[00:16:28] That is obviously a richer space, ~~you know, ~~much more wide open space than you can click on these particular buttons within an interface, but it is still, of course, the compressed form, ~~you know, ~~token by token that we're ~~kind of ~~reducing our internal thoughts to this output stream, ~~you know, ~~that we can encode in language.

[00:16:48] And. One big way, ~~I guess, ~~to think about the set of technologies that we're going to talk about today is that ~~it, ~~it ~~kind of ~~goes up yet another level in terms of the richness of ~~the, ~~the~~ sort of ~~dimensionality of the [00:17:00] space in which we can communicate with computers. Yeah, absolutely. 


## The Future of Neural Technologies and EEG Insights

[00:17:05] So let's get into it.

[00:17:06] I think at a fundamental level, all of this, ~~you know, the, the, ~~the neural technologies, it's really a story ~~of, ~~of very advanced sensor fusion and signal processing, which is the story of so much technological innovation over the last hundred years. We don't think about it in that way. ~~Uh, but, ~~but ~~that's, you know, from, ~~from Bell Labs on, it's been a story of signal processing.

[00:17:25] In this case,~~ uh,~~ the signals are the biomarkers. Thought is fundamentally mediated by electricity and magnetism. And so you can measure electrical and magnetic fields generated by brain activity. That's the primary way that this is done, or at least it's ~~sort of ~~the most straightforward way. Let's put it that way.

[00:17:45] There are other interesting ways ~~that ~~That we'll get into.~~ I think~~ the most common that you see ~~for, ~~for neuromonitoring, monitoring brains, not so much modifying them,~~ but, but, ~~but understanding what a person is thinking. ~~Uh, ~~is called EEG [00:18:00] electroencephalography,~~ uh,~~ and,~~ uh,~~ it has a closely related cousin called MEG magnetoencephalography,~~ uh,~~ which is measuring the magnetic fields.

[00:18:08] ~~Um, ~~so kind of same idea. The difference is that,~~ um,~~ EEG can be done with tiny electrodes placed on the head. Magnetoencephalography requires pretty substantial equipment that I don't think is going to make it out of the lab anytime soon. The thing that ~~just~~ a lot of people don't know,~~ I think~~ even in the tech space is that,~~ um,~~ it's not just Neuralink in this field, shipping products for more than a decade, there have been neural technologies,~~ uh,~~ using EEG, especially because it's relatively cheap, ~~um, ~~to do.

[00:18:38] All kinds ~~of, ~~of~~ things, uh,~~ practical things. ~~Uh, ~~and it's obviously also been used ~~in, ~~in a lot of lab settings. The difference between really ~~a, ~~a lab, EEG and a consumer EEG is just the number of channels that you get. So consumer EEG might be stereo, just two,~~ uh,~~ electrodes,~~ uh,~~ up to, yeah, maybe at the maximum 32.

[00:18:59] That would be a [00:19:00] lot. 32 would be like. ~~Kind of ~~a whole helmet that you're wearing. ~~Uh, ~~something like 816 is more common in,~~ in~~ the consumer setting. ~~Uh, ~~lab can go all the way up to 256 and this is just taking an enormous amount of data. ~~Um, ~~that's something that ~~I think is, ~~is also not appreciated. It's just that the brain generates a torrent of data, a lot of EEG.

[00:19:19] You can change the sampling rate,~~ but,~~ but the sample rate is usually around a thousand Hertz. So a thousand samples per second. ~~So. ~~N8 channel EEG in a consumer headset is going to be generating 8, 000 data points from your brain per second. ~~Um, ~~so you quickly get into millions if you're using it ~~for, ~~for any substantial period of time.

[00:19:36] ~~The problem, though, is that ~~fundamentally,~~ actually,~~ the problem with all of these technologies really is the human skull. ~~Um, ~~the skull was designed to protect your brain and it does a good job of that. ~~Um, ~~And it is,~~ uh, uh, ~~not particularly,~~ uh,~~ conductive,~~ uh,~~ so it tends to attenuate signal coming out of the brain ~~and, ~~and going in.

[00:19:54] There's other things too, there's cerebrospinal fluid,~~ um, there,~~ there's other mediating layers between the [00:20:00] actual surface of the cortex and the brain, and the scalp, but the skull is the most important one. And so modeling that, modeling the skull, which also varies between people,~~ um,~~ in thickness,~~ Uh,~~ density is very difficult.

[00:20:13] It also varies not just between people, but across your skull. ~~Uh, ~~so your skull is not homogenous in terms of its density or thickness. ~~Um, ~~and so it ~~kind of ~~has to be modeled ~~in,~~ in real time for an ideal reading. ~~Um, ~~And that is where I think we're ~~kind of at, ~~at the frontier of these technologies, but I'll talk a little ~~just~~ about ~~sort of ~~what has been done so far, particularly with EEG.

[00:20:36] EEG devices can do things like screen for cognitive, for signs of cognitive decline. ~~Um, ~~they can read ~~for, ~~for example, Parkinson's. ~~Uh, ~~there are biomarkers of mental illness that can be picked up ~~in,~~ in EEG devices. Generally, you don't see that on the consumer side of things. And the reason for that is that,~~ um, uh, ~~relates to FDA regulation.

[00:20:57] We can get into that later. ~~Uh, but, uh, but, ~~but for [00:21:00] consumer devices, you'll often see,~~ uh,~~ devices to help you meditate better. ~~Um, ~~the brain waves, they fall into,~~ uh,~~ a few broad families. ~~Um, ~~and it is actually kind of intuitive. Your brain ~~Uh, you know, the, the, ~~the electrical fields that your brain is generating oscillate at higher frequency when you are thinking more intensely.

[00:21:18] ~~Um, ~~that is ~~kind of ~~an intuitive relationship. So when you're in deep sleep, ~~uh, ~~the frequency ~~is, ~~is very low. And when you are focusing or, ~~you know, ~~in a panic, the frequency is very high. And so those kinds of crude measures of, is this person sleeping? Is this person in a deep meditative state? Is this person.

[00:21:38] attentive and focused, or are they scared? These are the kinds of things that can be read right now in consumer devices, still fairly crudely,~~ but,~~ but it can be done. Very practical, ~~you know, kind of ~~rookie level questions about exactly what we're measuring and exactly what we're doing with it [00:22:00] here. ~~The, ~~the electrodes that you put on your head to measure these electrical signals.

[00:22:07] ~~Are they, ~~you said the frequency with which they can take a measurement is about a thousandth of a second. A thousand hertz is a thousand cycles a second. So ~~it's, ~~it's a one, one thousandth of a second measurement. The rate at which a neuron can fire, ~~I believe,~~ is also roughly a thousand times a second.

[00:22:22] Is that right? It can be quicker than that. Neural activations can be quicker. So I should actually ~~yeah, I should I should ~~just step back. First of all, we're talking here about non invasive technology. There's a whole school of invasive stuff, which we can get into later, but on the non invasive side. Yeah, that's right.

[00:22:38] And E. G. is also, ~~um. ~~useful ~~because, ~~because of ~~that, that,~~ that temporal resolution, as it's called in the literature, is excellent for EEG. As a comparison, the kind of gold standard for spatially imaging the brain is fMRI, and that, at its best, can take one image every ~~two seconds. So, ish, roughly~~ two seconds.

[00:22:59] So in the [00:23:00] space, That, ~~if you're fMRI,~~ if you're using fMRI, you're in a lab, because,~~ uh,~~ fMRI requires superconducting materials,~~ so, um, ~~you're not wearing that on your head anytime soon. If you're wearing, for example, let's just say a 64 channel EEG, ~~uh, ~~then in the space that an MRI can take one image of your brain, the EEG has generated more than a hundred samples.

[00:23:21] ~~Uh, from, ~~from the brain. So the temporal resolution is great. Not quite at the neural activation level ~~from a, ~~from a temporal perspective and far from it, ~~uh, from a, ~~from a spatial perspective. So an electrode ~~kind of ~~at its most precise can see millions of neurons still, ~~you know, ~~down to a spatial resolution ~~of, of, ~~of centimeters ~~or, or, or, ~~or millimeters in some cases.

[00:23:42] ~~So, ~~I want to understand a little bit better what the electrodes are actually measuring like what their output is and then how that gets translated into this sort of spatial model of what's going on in the brain. Is it accurate to say that an electrode creates one number every [00:24:00] thousandth of a second that would represent ~~like potential or~~ sort of the strength of the electrical field?

[00:24:03] A point in time, yeah, exactly. ~~So, ~~and ~~that is, ~~it seems like there's ~~sort of ~~Fourier transform kind of math that must be happening here, where ~~what is actually,~~ this is ~~kind of ~~similar to, like, how cell phone towers work from what, ~~you know, ~~a limited understanding I have ~~of, ~~of this,~~ um, ~~branch of physics, but basically we have all of these.

[00:24:20] neurons firing off, ~~you know, ~~with these sort of spikes, right? And there's a very short duration, hot, relatively high voltage signal that is getting sent very locally. Stop me if I'm wrong about this at any point, cause I'm really not very expert in this at all. ~~Um, ~~because that's now happening in all these billions of locations,~~ uh,~~ concurrently throughout this whole region of the brain that then sends out this ~~sort of, ~~Massive signal to the outside world.

[00:24:46] Much like a cell phone tower is ~~like ~~sending out a massive signal that is. ~~You know, ~~communicating with ~~kind of~~ all of the phones in the area at once. And then ~~in this, I guess, ~~in a similar way,~~ if it's similar, the analogy is going to break down. So forget the analogy for a second. The, ~~all these electrodes are receiving this sort of messy signal [00:25:00] where it's like, okay, I'm here and this is what I feel right now.

[00:25:04] And that is the aggregate of all of the signals that have been created from these individual neuron firings. And then there's A real computational challenge after that to say, okay, we've got 64 different signals because we got 64 electrodes ~~on the ~~on the head. What does that translate into? So can you tell us a little bit more about how that is happening?

[00:25:25] Like, how does that sort of 64 numbers get translated into a spatial understanding of what's going on inside? Yeah, this is a great, ~~it's a great ~~question. ~~Um, ~~first of all, yeah,~~ your,~~ your intuition here and ~~what, ~~what you sketched out ~~is, ~~is basically correct. ~~And, ~~and yeah, a lot of the basic analysis that's happening here is,~~ um,~~ your standard Fourier transform,~~ uh, uh, ~~that basically takes a composite signal and breaks it down into the intensity of the signal at different frequencies over time.

[00:25:56] ~~Right. ~~So you have some wobbly signal that you're [00:26:00] measuring. And this says, okay, ~~how would you, ~~how could you re represent that signal as the sum of intensities at certain frequencies throughout the frequency spectrum? Yeah, ~~I mean, ~~the way I think about it ~~is, ~~is almost like if your brain is playing a chord, the Fourier transform separates it into individual notes.

[00:26:21] And so ~~you can, ~~you can see in,~~ in, in~~ that way ~~and, ~~and ~~that's, yeah,~~ that's essentially ~~what's, ~~what's going on,~~ um, with, with, ~~with EEG signal processing, I would just add that there's competing electrical signals, right? Your eyes, when you blink are generating electrical fields, all of the muscles in your face,~~ uh, are, are, ~~are doing the same thing.

[00:26:37] The device itself that you're wearing, Is generating electrical fields,~~ uh,~~ so there's all of this competing,~~ uh,~~ electrical activity around the head,~~ uh, which has to be,~~ which is being a sample. The electrodes are going to sample some of that. That's got to be processed out. And then there is what I was alluding to earlier that the electrical field itself from the brain is going [00:27:00] to be attenuated in different ways.

[00:27:03] ~~Uh, ~~dynamically,~~ uh, by, ~~by the skull ~~and, ~~and other sort of,~~ uh,~~ intracranial,~~ uh,~~ media. ~~So, ~~difficulty ~~of, ~~of ~~sort of, like, ~~processing that, that's where there have been a lot of traditional techniques, traditional statistical analysis techniques that have been used,~~ um,~~ and ~~sort of ~~older,~~ uh, Uh, ~~ML techniques, convolutional neural nets for current neural nets, all that kind of stuff that have shown some promise, basically everything that I'll talk about actually,~~ um,~~ is really using those kinds of older forms of statistical analysis to break this apart and figure out what's relevant and figure out what it means.

[00:27:35] ~~The, uh, uh, ~~there hasn't been a lot. ~~of, ~~of transformer based work and other types of things. And that's where I think there's just a lot of low hanging fruit. And it's actually still an open question to me ~~why there hasn't been more. I mean, ~~the transformer has been around for a while now. There are some papers that show EEG transformers,~~ uh, and, ~~and other types of neural signal processing,~~ uh,~~ with transformers,~~ but,~~ but why aren't there much more?

[00:27:56] ~~Um, ~~is it the conservatism of academia?~~ I don't think. It's a, ~~my first intuition [00:28:00] was,~~ well,~~ maybe it's a data problem. 

[00:28:02] 

[00:28:02] ~~ Uh, ~~these things generate a lot of data. So I really don't know. I'd love to know if any of your listeners have,~~ uh, I mean, ~~insights, I'd love to hear, yeah, ~~a couple of, I mean, ~~I have a couple of intuitions, which may or may not be right.

[00:28:11] And,~~ um,~~ people can correct me on this as well, but we just did an episode on the first 90 days of Mamba literature. And one of the things that ~~I think~~ is really interesting about this, new mechanism,~~ the,~~ the selective state space mechanism is that it does have different strengths and weaknesses compared to the attention mechanism, both in terms of ~~like, you know, ~~how much memory it consumes, where the attention mechanism is quadratic ~~in the, you know, ~~in the length of the input.

[00:28:41] And that might be, by the way, one of the reasons, like just as you talk about, like a torrent of data and, ~~you know, ~~a thousand. samples per second. ~~If, ~~if that were to be naively translated to a thousand tokens per second, then, ~~you know, ~~very quickly you're getting to a level of tokens that we have only very recently reached [00:29:00] with frontier grade transformers,~~ right?~~

[00:29:02] ~~I mean, ~~it was only With GPT 4 a year ago that the public first got to see a quality 8, 000 token transformer. And, ~~you know, ~~before that it was like, it's just a couple of months where ~~they had,~~ we had just seen the 4, 000 and before that ~~it was 2, 000, you know, ~~as of like 18 months ago, 2, 000 tokens was what you could really get from like the open AI API.

[00:29:22] So just the sheer volume of data may not lend itself super well to the transformer. ~~Um, ~~but also another interesting thing is that ~~they, ~~when they break down these. micro tasks and look at what the transformer can do and can't do. One of the things it really struggles on is the hyper noisy environment.

[00:29:42] There was an interesting result in this one Mamba versus transformer comparison paper, ~~which really I should, you know, mom is such a great name, but I really should be. ~~It's more about the selective states based mechanism and the attention mechanism. Those are really the 2 things that ~~I think are ~~are more dueling it out than the higher level architectures.

[00:29:56] And they're not even doing it out because they actually work best together. Spoiler. But ~~the. ~~In the super [00:30:00] noisy environment where what actually matters is quite rare in what you're signaling, then the transformer sometimes has a hard time converging. And the intuition I've developed for that is because it's ~~kind of ~~changing all the weights at the same time across like the entire range of the input, It may be that ~~the, ~~the gradient is ~~sort of ~~often dominated by noise and has a hard time converging on the signal.

[00:30:26] Whereas when, ~~you know, ~~I don't want to make everything about ~~the, ~~the selective state space model. ~~Um, ~~though I do have an obsession about this as folks know, it is updating per token. And so it seems like it has a more natural mechanism when the actual signal hits. To say, oh, ~~you know, ~~and this is where I,~~ uh, ~~start to violate my no anthropomorphizing,~~ um, ~~policy, but ~~it, ~~it has an ability to recognize when the signal hits and update in a more focused way on that one thing that really was supposed to matter, whereas a transformer is ~~like~~ updating everything, ~~you know, ~~all across ~~the~~ [00:31:00] it's considering everything at once.

[00:31:01] And so it seems like ~~the, ~~the signal can get lost in all that noise, the recurrent nature of the selective state space mechanism. allows you to kind of zero in and do the gradient on the signal when you have the signal. And then of course, ~~all the other noise, you know, ~~there's still a lot of noise, but that maybe can get separated from the signal because of this ~~kind of~~ bit by bit level processing and updating.

[00:31:23] I'm not a hundred percent confident in that theory, but ~~it does,~~ it is consistent with all the evidence that I know of so far. So we'll see how that,~~ um,~~ evolves through time. ~~Well, ~~then we might be in for an exciting couple of years 

[00:31:34] ~~ ~~

[00:31:34] ~~for~~ if that's true. Yeah, that's a great point. ~~That's a great point.~~ It very well.

[00:31:37] ~~Um, very well.~~ Maybe that.

[00:31:38] ~~Um, I guess I should, like, talk about sort of some,~~ I've talked a lot about the downsides and ~~sort of, like, ~~what's hard ~~and that's, I guess, a part of it.~~ Once you get into this literature, what ends up happening is you just admire the very, very challenging problems,~~ uh,~~ that there are and just ~~how, um,~~ how complicated the brain is.

[00:31:49] ~~Um, ~~but there is a lot that you can do. And what's shocking about EEG Is that it works as well as it does non [00:32:00] invasively, ~~um, ~~especially when you consider that the only electrical signals really that can reach the brain, or that can reach the electrodes on the scalp, ~~um, ~~from the brain are ~~at the, ~~very much on the surface, very much, ~~you know, ~~cortex, maybe, One to three millimeters below at the most.

[00:32:18] Everything else just gets totally attenuated. So any deep brain structure, there's just nothing ~~that, ~~that EEG ~~can, ~~can really read. So everything we're talking about is stuff that's coming from just the surface of the brain. And that can do things like,~~ uh,~~ you predict seizures with nearly 100 percent accuracy up to one hour before.

[00:32:36] ~~Um, ~~that was demonstrated,~~ uh,~~ in fact,~~ I think~~ about eight years ago,~~ uh, in a, ~~in a lab setting. And there are devices that do this ~~for, ~~for people with epilepsy. So the telltale signs, ~~which ~~during which, ~~you know, ~~the patient feels absolutely nothing. There's no external sign for the patient that a seizure is coming, but there are brain patterns that can be interpreted,~~ uh,~~ by consumer [00:33:00] EEG hardware that can predict.

[00:33:02] With almost 100 percent accuracy, whether a seizure is going to happen,~~ uh,~~ same thing ~~with, ~~with ~~Parkinson's ~~Parkinson's,~~ uh,~~ early onset Parkinson's has a pretty distinct neurosignature that can be read. ~~Um, ~~that's something like,~~ I forget exactly, but that's~~ 90 plus percent accuracy. ~~Um, ~~so already. The potential of a medical device.

[00:33:19] I mean, I'm wearing AirPods right now. In fact, Apple has a patent that they've explored of putting electrodes on AirPods to do this exact thing. Just simply something, a pair of AirPods ~~that can, ~~that can monitor for important medical conditions like that, and maybe help me relax. ~~Uh, you know, ~~that's already something that could be quite useful and seems like it will happen within the next few years.

[00:33:43] ~~Um, ~~maybe not exactly in the AirPods form factor,~~ but, but nonetheless, um, you can,~~ when I said, you know, make you relaxed, that refers to something called neurofeedback. So rather than a direct modulation by hardware of your neural state, neurofeedback is basically giving [00:34:00] you the data and allowing you to change your patterns of thought in response to the data.

[00:34:06] So ~~sometimes a lot of,~~ there are neurofeedback devices that have EEG,~~ uh,~~ sensors on a headband that you might wear and it pairs with a phone app. ~~Uh, ~~and the phone app will show you your neural activity in real time and maybe give you a game or some other kind of cognitive stimul stimulator ~~to,~~ to play with.

[00:34:22] ~~Um, ~~and it will tell you how focused you are. And you can ~~kind of ~~get focused more. ~~And ~~the idea is that,~~ um,~~ you can actually train circuits in the brain to become more focused. There are devices that do this for sleep, ~~um, ~~without you having to do anything. A lot of devices, for example, ~~will, if~~ there's one ~~that,~~ called the BIA, I believe, that resembles like a sleep mask.

[00:34:42] You wear it around your head. ~~Uh, ~~and ~~I, ~~I've never tried this, by the way. I don't think the BIA is actually shipping yet, and I'm certainly not endorsing it,~~ uh,~~ just to be clear. But you can ~~kind of ~~wear it like a sleep mask, and it will play music. And that music is ~~sort of ~~dynamically tuned in response to the EEG signal, and meant to ~~sort of ~~bring you into a more [00:35:00] relaxed state.

[00:35:01] ~~Um, ~~and ultimately, I don't think there's a lot of scientific literature backing this up, but there is general scientific literature to support the idea that neurofeedback is a thing. Your brain will develop circuits to get into a more relaxed state more easily on its own ~~with, with enough, ~~with enough practice at that.

[00:35:17] So that's the kind of thing, ~~uh, that we're, ~~that we're talking about ~~in, ~~in consumer devices. There's also motor control. ~~Which is a, ~~which is a whole different, interesting field that maybe you want to go into. 


## Advancements in Motor Control and Brain-Computer Interfaces

[00:35:26] But first, ~~I guess, ~~if you have any questions on the ~~first set of, ~~first set of things. Yeah,~~ it seems like we're working our way up through~~ this sort of same paradigm seems to happen all over the place.

[00:35:34] There's like echoes of this going, ~~you know, ~~with the increasingly rich signal, then we also have these sort of increasingly meaningful, States that we're able to identify, right? ~~I mean, ~~within the transformer, this is ~~like, ~~pretty well studied now where in these sort of late middle layers, ~~you know, the, ~~the inputs have been worked up to these rich concepts.

[00:35:52] And you can,~~ like,~~ even, ~~you know, ~~identify the direction in activation space that corresponds to, ~~you know, ~~justice or fairness or, ~~you know, ~~love or these kind of [00:36:00] things. You're like, wow, how did this, ~~you know, ~~AI learned to represent that when, ~~you know, ~~all it's doing is next token prediction. And here there's like a somewhat analogous thing where.

[00:36:10] As the ability to read the signal gets better and better, we're able to see like, ~~okay, you know, with your notes, you've got like, ~~okay, it takes ~~two channels,~~ two electrodes on the brain to detect if you're sleeping or awake. With four, you can get to ~~like, ~~are you stressed or are you relaxed? With eight, you can get to general emotional state, like fear, happiness, disgust, anger, and then With 16 and 32 more advanced things are starting to happen.

[00:36:35] So I am actually really interested in the motor thing, because that seems like ~~an, you know, ~~an interesting lens into just like, how much resolution do we already have in the ability to decode this stuff? ~~Um, ~~And then I also wonder if you have thoughts about ~~the, ~~the limits ~~of, ~~of that with this technology, of course, but we're going to discuss a lot of technologies too.

[00:36:53] Yeah. ~~So, ~~so motor control, it really does turn out that when you think about moving something. ~~Uh, it is, ~~it is [00:37:00] tantamount to sending the electrical impulses to actually move that thing, and that can be translated,~~ uh,~~ for people that are unable ~~to, ~~to make that motion themselves. So, there's demonstrations, ~~you know, you'll see these on, at least maybe, ~~maybe it's just my algorithm, but I see this on X all the time,~~ um,~~ of, uh,~~ uh, You know, ~~a person wearing a helmet ~~using, uh,~~ controlling a robotic arm, for example, and oftentimes the motions are ~~kind of ~~jerky or slow, or you'll see sped up footage.

[00:37:24] ~~Um, ~~and those are ~~kind of ~~the signal processing issues that we have now. ~~And those are also probably. You know, ~~if you see someone wearing a whole helmet, that's going to be a lot of EEG signal that they're reading. ~~Um, ~~but you can interpret,~~ uh, uh, ~~some basic motor control,~~ uh,~~ from the cortex, from cortical activity.

[00:37:39] Now, a lot ~~of, ~~of motor control is coming from deeper brain structures. ~~So, ~~the limits on that At least to me as someone who's not a neuroscientist, unclear exactly, but I actually think,~~ uh,~~ it's important to say at this point that some of the highest quality datasets ~~for this kind of, uh, for, ~~for this kind of application are [00:38:00] datasets that combine EEG readings with fMRI or PET scans.

[00:38:06] Now PET scans and fMRI, again, not the kind of thing~~ that,~~ that are going to make it into,~~ uh,~~ consumer use cases. Maybe ever, certainly not anytime soon in lab settings. You can record that. And then you have an interesting relationship because the in particular is a 3 D. ~~Uh, you know, ~~voxel ~~as, ~~as it's called in the literature of representation of brain activity uses blood flow to infer, ~~uh, uh, uh, ~~blood oxygenation to be precise,~~ um,~~ to infer brain activity.

[00:38:36] ~~Um, ~~and then you can connect that with the signal. So all of a sudden, that's an interesting AI application, because you've got a much richer, but more sparse signal coming from the fMRI, and then a super noisy signal coming out of EEG. And ~~there's, ~~there's an interesting connection, ~~you know, what actually~~ What are subtle differences that we might not notice subtle patterns that we might not ~~notice ~~notice [00:39:00] between different states of mind that we can easily see on the FMRI that we can't see on EEG, but perhaps ~~they can be, ~~they can be pulled out of the data.

[00:39:07] I think that's a ~~very, ~~very rich area of research. ~~Um, ~~and,~~ uh, we'll, ~~we'll talk later ~~about, ~~about some companies that are using models exactly of that kind to do interesting things. ~~Um, ~~but that could really advance, especially on ~~the, ~~the motor control. ~~Um, ~~but non invasive EEG based brain computer interfaces already exist.

[00:39:26] And ~~I think~~ that's to understand. Are they super useful? No. That's ~~like, ~~not really. ~~Uh, ~~there are four enthusiasts of this kind of thing, and they're not cheap. But. You can,~~ uh,~~ today, buy, ~~I believe it's~~ a 32 jam channel, EEG headset, ~~um, ~~and pair it. With software,~~ um,~~ that you can use to manipulate objects on the screen of your brain.

[00:39:48] ~~Um, ~~it requires some calibration. Can't just do that out of the box. ~~Uh, but, um, but it can, ~~but it can do it. ~~You can sort of, you know, move. I think~~ it's like they have ~~like ~~a block that you can move left and right and things like that. Nothing like the cursor control that Neuralink has [00:40:00] displayed,~~ uh,~~ has shown.

[00:40:00] ~~But, ~~but ~~that, that,~~ that is possible in digital environments. And then, ~~um. ~~Certainly also it's possible to move robotic devices and there are prosthetics that work in this way already. One question I've been pondering here is, what is the kind of breakdown? ~~You know, ~~we talked a little bit about ~~kind of ~~traditional signal processing and Fourier transforms and whatever.

[00:40:21] And then, ~~you know, ~~I'm also reminded of Elon Musk saying on the Neuralink show Intel day a year and change ago now, ~~I think ~~that it turns out the best thing to decode a neural net is another neural net. And so I'm ~~kind of ~~wondering ~~like ~~what parts of the interpretation process, ~~you know, ~~you've got like a thousand, ~~you know, ~~numbers coming off each electrode per second.

[00:40:43] And then ultimately that needs to be translated into something, right? A classification of your state or like a direction in motion space that you're going to try to move, ~~you know, your, ~~your robot arm or whatever you're trying to do. 


## Exploring the Balance Between Neural Nets and Traditional Methods

[00:40:55] And between there, there's, ~~you know, ~~you could imagine like decoding, ~~you know, with, ~~[00:41:00] with all traditional methods, you can imagine an entirely neural net thing that just like takes in these raw numbers and translates that to an output.

[00:41:08] With, ~~you know, ~~basically no principled approach other than just, ~~you know, ~~throw a model and a bunch of data at it and let it figure it out. Do you have a sense for ~~sort of ~~what the right balance is there ~~and, ~~and whether ~~like~~ today's balance. Is likely to hold, or, ~~you know, ~~are we headed for another bitter lesson where just all of this gets thrown into neural nets ultimately?


## The Shift Towards Neural Networks in Various Fields

[00:41:25] ~~I mean, ~~it does ~~kind of ~~feel that way,~~ uh, just based on, you know, we,~~ we see problem after problem falling to the unprincipled application of neural nets. ~~I mean, ~~obviously it's more complicated than just throwing data into architecture,~~ but,~~ but basically, ~~right. I mean, we, ~~we've seen that over and over again.

[00:41:39] ~~Um, ~~And every time it happened, the experts and the scientists who were specific to that field told us that will never work. It's impossible. And then it works. ~~Uh, and, um,~~ so~~ I, uh, ~~I've learned to be humble in this regard,~~ um,~~ and ~~I, ~~I wouldn't want to hazard too much of a guess. My intuition right now is that the [00:42:00] field.

[00:42:00] ~~Um, ~~frankly ~~is, ~~is probably relying on a little bit, ~~um, ~~more primitive methods of doing the signal processing than it could. ~~Um, ~~and that sort of moving in the direction of a ~~kind of~~ fully neural net,~~ uh,~~ is at least worth experimenting with. And I think part of the reason ~~that, you know, ~~that the scientific community doesn't do that is because you want comparability with studies that have come before.

[00:42:26] ~~And so ~~there is this ~~kind of~~ like innate. conservatism in this field of science that,~~ uh, I think ~~a fresh crop of startups focusing on this will be entirely uninterested in. ~~Um, ~~and ~~so I do think that part of it is just that we need ~~I think we're at the stage where we need to get this research of scientific labs and more into,~~ uh,~~ at least corporate R&amp; D labs.


## The Role of Big Tech in Advancing Neuroscience

[00:42:46] And obviously, something that's, ~~you know, ~~important to understand here is that,~~ uh,~~ every one of the big tech companies employs neuroscientists who work on things that are at least adjacent [00:43:00] to this. Apple's got a lot of patents in the field. ~~Um, ~~Apple's exploratory design group,~~ uh,~~ is, ~~you know, ~~probably looking at this.

[00:43:08] That's their kind of skunk works operation. Meta has been more public about it. ~~Um, ~~Meta's public reason. ~~And, and, ~~and in fact, Mark Zuckerberg,~~ uh,~~ it's worth noting in his review of the Apple Vision Pro, which we should talk about by the way, because that's like an interesting kind of half step,~~ uh, towards, ~~towards all this.

[00:43:23] But Mark Zuckerberg and his review of the Apple Vision Pro said. Basically something along the lines of, ~~I guess~~ the eye tracking interface they've built is okay until we get the neural interface hooked up. Basically viewing it as an inevitability. ~~So, uh, yeah, I mean, I, I think, I think that~~ there are probably right now inside ~~of, ~~of both startups and large corporate R&amp; D labs, fresher approaches to this being tried than at least what I have seen in the academic literature.

[00:43:47] Though, in fairness. ~~Um, ~~academic literature doesn't always go into a ton of detail about exactly how the signal processing is done at the ~~kind of ~~statistical analysis level. ~~But, um,~~ but yeah, that's my general sense. 


## The Challenge of Data Availability in Neuroscience

[00:43:58] Availability of data also may [00:44:00] be a real issue for some of these things. It seems like the way that this has progressed in neuroscience has, ~~you know, ~~I'm ~~kind of ~~backfilling a narrative here, but ~~I, ~~I feel like it is.

[00:44:11] ~~You know, ~~the brain is the ultimate black box, right? ~~Even, ~~even more messy and black boxy and just hard to get into, obviously,~~ for,~~ for, ~~you know, ~~all sorts of medical and ethical reasons as compared to a digital neural network, which you can be into with clarity and, ~~you know, ~~shop parts off and, ~~you know, ~~see how it works with, ~~you know, ~~different permutations.

[00:44:29] So there's been just a huge initial challenge of figuring out in rough terms. what is going on and how does it work? ~~Right. ~~And so people are doing these ~~kind of ~~very small sample sizes of ~~like ~~looking at people in FMR eyes and trying to deduce~~ like~~ what brain region does, what and like what frequency of waves seem to correspond to what, and ~~this, ~~this ~~like~~ super low data regime figuring out ~~in general terms, ~~in general terms, what's going on then seems to naturally lend itself to, okay, now it's ~~like.~~

[00:44:59] Try to [00:45:00] apply, ~~you know, ~~principled approaches to identify when that's happening versus I can see this shifting very quickly and then probably would happen ~~in, ~~in private companies first. ~~Um, ~~just because if nothing else, they're going to ~~like ~~see the opportunity and really invest in scaling up the data.

[00:45:15] Just having the data to say, ~~you know, ~~what were you trying to do when we recorded that brain state? What were you trying to do? ~~Uh, ~~with your robot arm or, ~~you know, ~~with the cursor on the screen. Like that data just has never existed. So you can't really.~~ have the, you know, you can't ~~take the better lesson approach until you have the scale of recorded data to make it go.

[00:45:34] And it seems like we've only recently ~~kind of ~~realized that ~~like, ~~this is going to work for everything and we need that scale of data. And ~~so we're probably You know,~~ nobody has really collected it globally. 


## Innovations in Brain State Interpretation and the Importance of Data

[00:45:42] ~~Um, ~~we just did this episode with,~~ um, ~~Paul Scotty, who is the author of the mind, I two paper, they are beginning to work on a multimodal brain state interpretation model, which would take in different kinds of signals ~~and, ~~and try to output different kinds of, ~~you know, ~~predictions and.[00:46:00] 

[00:46:00] It sounds like that's really only getting underway. Like, oh, there's, you know, data is super fragmented. It's all kinds of different places and forms. And ~~so I guess~~ that also seems like a big part of why this hasn't happened yet. Would you agree? Yeah. ~~I, I do.~~ I do. And I think ~~we're, ~~we're at a very primitive state when it comes to the data.

[00:46:18] I don't think we have a good sort of population level modeling of the variation in neurosignatures, even things like skull thickness. ~~I mean, ~~right now it's ~~kind of ~~like the literature is ~~kind of like, ~~well, everyone's skull is different. And ~~it's like, ~~well, that's probably not true. ~~Like, ~~it's probably not literally true that ~~like, ~~it's like a snowflake, right?

[00:46:38] Like my skull is, ~~I mean, it, it might be, ~~it might be unique at a ~~very, ~~very, ~~you know, ~~granular level of detail. You can probably model population dynamics. For this sort of thing,~~ um, that would make, and, and, ~~and that would make your imaging challenges,~~ um,~~ substantially easier. Same thing goes ~~for, ~~for the neural activity.

[00:46:54] ~~I think it's, ~~it's definitely ~~quite, you know, ~~quite likely that,~~ um,~~ high dimensional thought in [00:47:00] particular is probably, ~~you know, Um, ~~pretty unique signature, ~~you know, my way of ~~my associations that I have with the concept of flawed three opus or Mozart or something are ~~very, ~~very different from yours. And it's not entirely clear.

[00:47:13] That we're going to be able to get to the point of like, Oh, Dean and Nathan are both thinking about Mozart. ~~Uh, like~~ that seems hard, ~~uh, ~~to be able to reach, but there's a lot lower level thought ~~that it seems like you,~~ that is still interesting that it seems like you could model that's also more, ~~you know, ~~that's higher level somewhere in between Mozart and, you know, I'm scared of this tiger.

[00:47:35] I'm trying to run away from it. We can read that pretty easily. ~~Like~~ we know what that looks like. That has a pretty common signature ~~that ~~that's easy to pick up on. ~~But, um, ~~the my guy paper actually ~~did have an interesting, you know, ~~may shed a little light on this because ~~they,~~ first of all, the data set is only from 8 people that they work on.

[00:47:50] ~~And, um, I think this, I think that will probably be out before this one. ~~So people, ~~you know, ~~it should be right before this on the feed. Basically what they're doing is looking at your brain state as measured in that case by fMRI and reconstructing [00:48:00] what you were looking at. They had an earlier version of this where they created a custom model for each of the eight people that are represented in this one open source data set.

[00:48:10] Each person had to spend like 30 to 40 hours. In an MRI machine over, ~~you know, ~~presumably a bunch of different sessions, and they're being shown an image every few seconds. And then they just had to sit there and click a button~~ if they had ever~~ if they recognized that they had seen that image before, just to kind of make sure that people are engaged in the task on an ongoing basis.

[00:48:33] So, ~~What was interesting,~~ jump between MindEye 1 and MindEye 2 was that instead of creating a single model trained on all of the 30 to 40 hours of data per individual, they were able to train a single model Based on seven of the eight individuals data, ~~and then they could create a little, and each of those,~~ by the way, each of the brains is quite different in shape, they report the number of [00:49:00] voxels per individual voxel being ~~a~~ roughly speaking a centimeter or a two millimeter cube.

[00:49:06] ~~They're~~ the lowest number of voxels from one person is like 12 ~~and this is the ~~how they choose the number of voxels is by like the anatomy of the brain. So they're looking at basically the visual cortex segmenting that off and then just splitting it~~ into. ~~into voxels and the resolution of the voxel is constant.

[00:49:21] So how many voxels you get depends on the total space of the portion of the brain that they identify as your, ~~you know, ~~the relevant visual cortex for this purpose. So ~~1200 and some, or ~~12, 000, ~~sorry, ~~and some is the low end. And the high end is over 17, 000. So ~~you have, ~~you have a 12, 000, a couple in the 13, 000 range, a couple in the 14, 000 range, a couple in the 15, 000 range.

[00:49:42] And then one over 17, close to 18, 000 is the highest. So you see like a roughly 40%, ~~you know, ~~difference from the lowest number of voxels to the highest. And that create requires then a little bit of a bespoke [00:50:00] adapter portion of the model for each person because they just have literally different numbers of input number, ~~you know, ~~the vector that is measured is ~~like~~ a different length for each of these individuals.

[00:50:09] So the adapter to then get to the shared latent space has to be a bit different for each individual. But once they create that shared latent space. Then they're able to do an additional person with just one hour of data. So the key finding there is that they go from a, like a previous technology. Same data set, interestingly,~~ um,~~ same raw information, ~~same, you know, ~~these are like the same sessions that were recorded, but they're able to go from ~~a, ~~a version where it works at 30 to 40 hours, which is obviously prohibitive for ~~like, you know, ~~most usage down to now they can get it to work with one hour of data because they're tapping into this like shared latent space that they've created from other individuals.

[00:50:52] So I think that is pretty interesting. I asked him a question about, ~~you know, how similarly~~ can we say anything about how similarly people perceive the same [00:51:00] thing? And he said, unfortunately, in the data set that they were working with, there's very little overlap between the different images that people saw.

[00:51:08] So on the one hand, you would say, like, in some sense, that kind of maybe suggests that there is like a high level of generality because They're able to get this shared light and space to work, even when people mostly didn't see the same images on the other hand, it ~~gives us a ~~kind of creates a limitation in terms of, ~~you know, ~~can we say what,~~ like,~~ the inner product of, ~~you know, ~~my cloud three conception and your cloud three conception is like, unfortunately, in this study, people just didn't see the same thing enough, ~~you know, ~~for them to be able ~~to, ~~to do that sort of analysis.

[00:51:36] But I did find it quite interesting, ~~you know, ~~truly profound, quite interesting as an understatement. It's a profound observation that you can get with only seven individuals whose brains vary in size by up to 40 percent to a shared latent space that is general enough that you can just come plug another individual onto it with basically [00:52:00] one hour's worth of calibration data from an fMRI.

[00:52:03] Bye bye. Yeah, 

[00:52:04] 

[00:52:04] no, ~~that's that's, uh,~~ that's wild. It really is. 


## The Potential of Non-Invasive Brain-Computer Interfaces

[00:52:07] And,~~ uh, I think, um,~~ I think we're just scratching the surface there. ~~Like, ~~like I said, ~~you know, ~~I don't know that we're going to get to my Mozart and your Mozart ~~or, or, or, ~~or whatever. But I also think that. There's a really wide space. There's a lot of surface area,~~ um, that you can, ~~that you can reach.

[00:52:23] So yeah, ~~I think, I think that the, and,~~ and anything that makes it easier to collect data is particularly appealing because having to, ~~you know, ~~have people go through these things for 30, 40 hours ~~is just, is,~~ is an unreasonable collection. So I suspect that we will see an acceleration of all this,~~ um, ~~pretty soon.

[00:52:40] ~~I think, I think~~ it's going to happen pretty soon. And,~~ um, I think that, you know, ~~I don't know that EEG will be the technology specifically. There's one other,~~ uh,~~ approach called ESMER's,~~ uh,~~ functional near,~~ uh,~~ infrared, ~~uh, ~~spectroscopy, spectroscopy ~~that can, it kind of, I mean, ~~you almost think of it as like fMRI, ~~but non invasive or not non invasive,~~ but that you can wear around in a consumer device.

[00:52:59] ~~Um, not as~~ it's not going to give you the [00:53:00] 3D depth that fMRI is, but it's doing the same thing where it's blood oxygenation to measure brain activity. ~~Um, ~~it is actually pretty simple. ~~Like, you can.~~ It's pretty comparable to ~~in terms of, ~~in terms of what it can do, and in terms of its costs and things like that.

[00:53:14] So it could be that, ~~but, ~~but I think that there are devices that I can imagine existing that are not. 


## Exploring the Capabilities and Future of Neural Interfaces

[00:53:21] The full brain computer interface that we're all dreaming of or like what Neuralink is doing But it gets you to some pretty interesting directions And by the way, just as a side note, what Neuralink is doing is really kind of just an invasive version of everything We're talking about here.

[00:53:38] So instead of putting the EEGs or the electrodes on your scalp They're putting a lot of them ~~very, ~~very high density,~~ um, in, you know, ~~implanted in the brain,~~ um,~~ with some silicon on board that can do some of this ML sensor processing,~~ um, right, ~~right there,~~ uh, for, ~~for latency purposes. ~~Um, ~~so what you saw, ~~you know, ~~With ~~the, the, ~~the guy playing chess on a computer, ~~um, ~~with an implanted device, ~~it's actually like, ~~it's very impressive ~~and it's, it's amazing to see.~~

[00:53:59] And it's [00:54:00] amazing to see some awareness of these issues ~~coming, ~~coming up. Also, not shocking. You want to describe that ~~a little bit, ~~just in case people haven't actually seen it? Yeah. ~~That is,~~ there I go thinking that everyone's, ~~you know, ~~as engaged in this as I am. So yeah, this is,~~ um,~~ the first patient of Neuralink.

[00:54:16] Is ~~a, a, ~~a young, ~~uh, ~~quadriplegic. A very sad case, he had,~~ I think,~~ a diving accident or something like that. ~~Um, ~~doesn't have any use of his arms and ~~he had this,~~ he had ~~the, ~~the Neuralink implanted,~~ um,~~ and was able to manipulate,~~ uh, you know, ~~standard Windows computer. If you've ever used Windows, this is exactly the same interface.

[00:54:34] ~~It's not, ~~it's not some special software. He's just using Windows to move the cursor around, to play chess. ~~I believe he, uh, ~~the first night that he got all this hooked up, he's, Stayed up all night playing Civilization VI,~~ uh,~~ which I can relate to,~~ uh,~~ and,~~ uh, uh, ~~yeah,~~ um, so, ~~so ~~kind of ~~just ~~this, ~~this amazing,~~ uh,~~ ability to more or less fully use a computer.

[00:54:52] ~~Um, ~~and ~~I should say, uh, typing~~ with devices like this implanted, typing has been demonstrated,~~ um,~~ not in a way that a consumer would ever want to use, but typing [00:55:00] has been demonstrated with non invasive technology. ~~You can, ~~you can do that,~~ um,~~ with non invasive EEG. ~~Um, ~~but being able to ~~sort of ~~just fully manipulate a computer and live at least a, ~~you know, ~~a digital life, surely with the brain,~~ um, was, uh, was, was, was, uh,~~ was what he was able to do.

[00:55:13] And again, amazing. ~~Uh, ~~but not something that would come as a galloping shock to anyone who's been paying attention to this. We've seen stuff like that before. And a lot of what Neuralink actually has done,~~ uh,~~ In kind of a classic Elon Musk fashion, one of the most important innovations that they have done is the automated surgery device, the robotic surgeon that can do this more or less without human intervention.

[00:55:37] ~~Um, ~~there's Elon Musk thinking about, ~~you know, how do I~~ not just, how do I bring. Innovative technology, but how do I change the cost structure? ~~Um, and, uh, you know, he, he, ~~he's thinking along those lines. So he's obviously thinking about wide scale deployment or something like this. Okay. 


## The Impact of Consumer Devices on Neuroscience Research

[00:55:50] So can we summarize, or maybe there's a couple other ~~like~~ high level data points that could ~~kind of ~~bring all this into focus, ~~it seems like to, ~~to zoom out ~~and, ~~and, ~~you know, ~~give us the sort of survey [00:56:00] view of where we are in the taxonomy, ~~we are, yeah.~~

[00:56:01] So ~~far we've focused on, you know,~~ it's ~~kind of ~~two directions in which information can flow. We have focused on reading of the brain states, not yet the ability to. change the brain states, except in as much as very sort of a feedback thing of like you measure and then you show that to the user, then they can ~~kind of ~~get into a certain rhythm and react to the measurement that they're seeing.

[00:56:20] ~~Um, ~~but that neurofeedback is not direct, ~~you know, ~~manipulation of the brain state. And then within this reading half of the equation, ~~you've kind of broken down,~~ there's a lot of different technologies. We've got stuff that's outside the skull is subject to just Really ~~kind of ~~noisy signal. A lot of challenges with that, but with the EG, you do get high frequency of signal, then you've got the FMRI, which is.

[00:56:44] A million dollar machine or whatever, certainly not something you can wear around, gives you a much better spatial resolution. You can really see what's going on at a finer grain level, but less fast. There's some idea that these two [00:57:00] modalities might be merged and there could be some really interesting~~ like ~~generalization.

[00:57:03] ~~Um, ~~based on that, you can maybe unpack that a little bit more. And then in terms of ~~like ~~what we can do with it, it's like with just a couple of electrodes on the head, you can do basic stuff like, are you asleep? Where are you awake with a lot of electrodes on the head? You can do. Reasonably advanced stuff.

[00:57:20] Although it's still kind of clumsy and slow. Like you can type,~~ uh,~~ with your brain neural link then is going inside the skull and that gives them a much cleaner signal and gives them the ability to have higher bandwidth. ~~I mean, ~~that's kind of the whole value prop that,~~ uh, you know, ~~Alaska has talked about over time.


## Advancements in Brain-Computer Interface Technologies

[00:57:34] Are there other ~~sort of very like. ~~Memorable kind of striking demos or products that you think people should understand that are coming out of all this work that they could, ~~you know, ~~go check out or watch a little demo videos of, yeah, I would say,~~ um, uh, ~~robotic control is definitely. Always worth looking at and you can find ~~if you just ~~if you just google EEG robotic control,~~ um,~~ there aren't really devices They're not like marketed devices to do this But you can find a lot of [00:58:00] laboratory settings ~~where ~~where it's been done beyond that,~~ uh, you know, there ~~there are ~~Uh, uh, ~~all kinds of interesting headbands, things like that, that you can wear sometimes, like headphones too, ~~sort of ~~over ear headphones is very common with a band over them, and they integrate the EEG into the band that goes over your head.

[00:58:16] ~~Uh, so, ~~There's a lot of products like that out there and, ~~you know, these are not, ~~they're not fantastically expensive. They're not super cheap. Usually somewhere between five hundred and a thousand dollars would be roughly my estimate. In terms of ~~the, ~~the brain computer interface though, the company that I know of that is furthest along in this regard is a company called Emotiv.

[00:58:34] ~~Um, ~~E M O T I V. And they are the ones that I was referencing earlier ~~that sell headsets that you can sort of, ~~The little DIY that you can buy them and,~~ uh, uh, you know, ~~use them at home ~~and, ~~and pair it with software ~~that, um,~~ that allows you to ~~sort of ~~do some very low level manipulation of objects. ~~Um, ~~you mentioned though, like ~~where,~~ what might be the next sort of level of generalization that we reach as we get better data and we apply better model architectures?

[00:58:58] Obviously, I don't [00:59:00] know. ~~One thing that's interesting to me that we've seen some, ~~there's been some interesting work on this~~ in, uh, in, ~~in the lab is,~~ uh, ~~better decoding of language. So ~~sort of, I mean, ~~the best way to put it is mind reading. ~~Uh, ~~so we've seen that with image. ~~Uh, ~~there've been a lot of, you know, MindEye is an example.

[00:59:13] ~~Uh, ~~Meta has had some research in this regard. There's a lot of people that are basically decoding images that you're imagining in your mind's eye,~~ uh, you know, ~~into,~~ uh,~~ using AI,~~ uh, into a predicted, ~~into a predicted digital image. ~~Um, ~~basically doing the same thing with text, word by word at first, but eventually latency,~~ uh,~~ that seems achievable.

[00:59:33] That seems very interesting, particularly when you think about it in the context of communicating with an LLM, ~~uh, you know, sort of ~~prompting an LLM with thoughts.~~ I think,~~ as we were saying at the beginning of this conversation, the challenge, ~~uh, ~~but also sort of opportunity ~~with a, ~~with a frontier LLM is how good is your question?

[00:59:52] And the better your question, the more precise and tailored your question, the better an answer you will get ~~out of, ~~out of these systems. ~~Uh, ~~[01:00:00] so some people just don't have the communication capability to ask exactly the question that they want to, but they can surely think of it. So can that start to be translated into questions that ~~they could, ~~they could prompt in an LLM?

[01:00:16] I don't know, maybe. Certainly it seems to me that basic motives. Could be. So I think about something like this, this device that came out at CES this year, the Rabbit R1, which uses that thing that they call a large action model, ~~um, ~~an LLM that ~~sort of~~ translates what you want into ~~like~~ intense, basic, intense.

[01:00:39] ~~Um, ~~I also think about something like,~~ uh,~~ the iPhone, ~~you know, and, ~~and Apple devices in general have these,~~ uh,~~ automation layer called shortcuts. where you can make little ~~sort of~~ modular pieces of software to do a simple thing. Could we start to translate thought into basic actions like that? ~~So~~ maybe not, ~~you know, ~~my thoughts on, ~~you know, ~~a [01:01:00] painting by Picasso, very complex set of thoughts, but maybe I want to see what my schedule is for today.

[01:01:06] I want to see what the weather is for today. I want to turn the bedroom lights off, things like that. Can we translate that and then use existing AI infrastructure, sort of automation infrastructure,~~ uh, or, or~~ currently being built~~ to,~~ to actually just perform that. ~~Um, ~~and essentially, ~~I mean, ~~that might feel shockingly like telepathy.

[01:01:23] ~~Uh, ~~if you can think, I want to turn the lights downstairs off and they will turn off. That might feel shockingly like telepathy. So it, in a certain sense, ~~something I think about with this technology and something that like, I feel like what I've said during this conversation, ~~the really stone cold reality of where we are, but everything feels like it's converging to this point where a real qualitative leap is possible.

[01:01:42] At least it seems to me like it could be in the relatively near future. ~~I think~~ to your point. To get to the brain computer interface that we all really want. ~~Um, ~~you need right access, not just read access. ~~So, uh, that end, uh, you know, maybe do you want to move on to the neuromodulation or do you have any other questions on?~~


## The Future of Brain-Computer Interfaces and Neuromodulation

[01:01:54] Yeah, ~~let's, um, let's just, I think so, but let's, ~~let's sketch out a little bit more, the, ~~how does this, ~~how does this [01:02:00] tip over the next couple of years? It seems like the model is basically. You know, this is a bit hackneyed now ~~in the, ~~in the AI space to ~~sort of ~~put everything in terms of ~~like, ~~what GPT level are we at, but it seems like we're ~~kind of ~~at ~~like ~~GPT 1 on this, ~~you know, ~~maybe between 1 and 2 where it's like GPT 2 was just barely starting to be.

[01:02:21] useful when fine tuned, you could do, ~~you know, ~~some interesting things with it. ~~Um, ~~but you couldn't do much with it, right? It ~~was not like, ~~certainly wasn't doing much in the way of reasoning. It didn't have the few shot learning, ~~you know, ~~ability ~~that,~~ that emerged with GPT three. And so you could do ~~kind of ~~classification type tasks or, ~~you know, ~~something like sentence completion perhaps, but very~~ kind of~~ limited application.

[01:02:45] And it seems like ~~we're in a,~~ we've sort of managed through mostly basic science to get to the point where. We've got a similar level of capability and ~~you know, ~~you've got links here in the show notes that ~~we can, ~~we can include,~~ um,~~ so people can go check out ~~like~~ in [01:03:00] visual form, like what these devices look like, but there's, ~~you know, ~~eight or so different consumer devices that have, ~~you know, ~~seen enough there where they're like, somebody's going to want to buy this.

[01:03:08] Let's get started building a company on it. And where I see this really changing is ~~kind of ~~akin to robotics to the amount of data that is going to be generated as the products hit the wild. Even with the early adopter set, it seems like it just goes vertical compared to what has existed before.

[01:03:28] Keeping in mind that MindEye 2 Is eight patients, ~~you know, ~~a total of 200 and some hours ~~in an ~~in an MRI across eight individuals. Now you've got things that are going consumer, so you've got orders of magnitude more data flowing in, and you have people that are. Actually attempting to use them. And so you're going to start to get feedback on ~~like ~~what is working, what is not working.

[01:03:50] And so the regime, the data regime is just totally changing. So~~ if~~ there's ~~kind of a,~~ ultimately a very simple story, it's like ~~we've gone from the basic science. ~~We've gotten through enough basic science. To get to the point where [01:04:00] there's just this kernel of utility, which is going to tip us into a ~~kind of~~ much faster bootstrapping dynamic, where we're going to soar through orders of magnitude of data available that of course, ~~you know, ~~we've already got architectures that can probably decode that signal if there is enough data to learn from.

[01:04:19] ~~And so we can, you can start to see unless there's like something very fundamentally different about this as compared to other domains where Machine learning, you know, ~~the same machine learning techniques are working everywhere. So unless there's some very odd reason ~~where they,~~ why they won't work here, the data is about to come online and that's going to be the big unlock that's going to allow for just tremendously more utility.

[01:04:32] And we might hit some limits around, ~~you know, ~~just how good of a signal can we ultimately get. But it seems like at a minimum, we are headed for this sort of stored procedure triggering that you mentioned for, ~~you know, ~~image decoding is ~~like, ~~obviously, ~~you know, ~~what you're seeing is already quite decodable.

[01:04:50] What you're thinking about and turning that into verbal form seems like very achievable. And then even control seems like it likely gets refined to the point where [01:05:00] It's ~~like ~~practically useful,~~ um,~~ if not like super smooth, just based on collecting a ton of data and applying known machine learning techniques to those signals.

[01:05:11] Is that basically your world model or how would you refine what I just said? I think that's pretty much spot on. The GPT 2 comparison ~~is, ~~is an interesting one. I recall back in my,~~ uh,~~ housing policy days, ~~you know, ~~kind of YIMBY adjacent build more housing is what YIMBY means,~~ uh,~~ build more housing and make it cheaper,~~ uh,~~ to put it simply.

[01:05:29] ~~Um, ~~I,~~ uh,~~ tried to use GPT 2 to analyze municipal zoning codes, segments of municipal zoning codes. To ~~kind of ~~give me just a thumbs up or down on how restrictive was it? How not restrictive was it in an automated way? And the answers that you could get out of it compared to now are just so simian, ~~you know, ~~it was like good, bad kind of thing.

[01:05:49] ~~Um, and, ~~and you look at where we are now. ~~Um, ~~the difference of course, is that language is a kind of ground truth. ~~Um, ~~that, ~~that. You can, ~~you can refine your [01:06:00] understanding of overtime. And the question really is, ~~you know, ~~how consistent is the language of thought across people? ~~Um, ~~I think that's the fundamental barrier, potential barrier is how calibrated does this have to be?

[01:06:14] And ultimately, if I have to go get an fMRI scan. To make any of this useful to me. ~~Well, ~~then that kind of really changes it a lot. ~~I mean, ~~maybe I'll do that. They're like the craziest thing in the world, actually. ~~Especially if it's only an hour. Yeah. I mean, ~~how long does it take to go buy an Apple vision pro about half an hour?

[01:06:31] ~~If you're going to~~ demo is 25 minutes. Yeah. ~~Yeah. Yeah.~~ Yeah. ~~So, I mean, you know, right. ~~How hard would it be to put an FMRI in the back of an Apple store or something? I don't know, ~~but, um,~~ but still ~~it, ~~it changes it for sure. ~~Um, ~~it, the level of calibration to me is really the big open question here. ~~Um, ~~both on the interpretation side and the imaging or the skull side, but I have to think that substantial progress can be made and that it is possible to at least translate impulses, desires, basic [01:07:00] desires.

[01:07:01] And that can be fed into various kinds of AI architectures that can take actions on your behalf. That just seems super possible in the near term. I would be shocked if that didn't happen in the next five to 10 years, maybe less, maybe a lot less. That basically is my model. Yeah. ~~The fact that all these,~~ I see a parallel also between the fact that all these devices exist and the current state of AI agents broadly, where It's like, everybody ~~kind of ~~sees where the technology is going.

[01:07:34] In the case of the agents, it's like, yeah, ~~you know, ~~GPD four wasn't quite trained to ~~like,~~ there's this weird juxtaposition where it's like closing it on an expert level on things like medical diagnosis, but then it ~~like~~ sometimes can't, ~~you know, ~~click the right button on a very simple user interface.

[01:07:49] ~~And it's like, why is that? You know, ~~or it gets stuck. It gets into like loops that it sometimes can't break out of. Like. Why is that, ~~you know, ~~disconnect there? Presumably it's because there wasn't really the sort of task completion, ~~you know, ~~mid length [01:08:00] episode data available to train the first version of GPT 4 on.

[01:08:04] I strongly believe now that,~~ like,~~ the big tech leaders are investing heavily in creating that sort of mid length episode data. And that, in all likelihood, ~~you know, the next big shift is going to be I, at least from,~~ I would bet, ~~you know, ~~reasonably confidently that the next big shift is going to be that ~~those start, ~~those sorts of things are going to start to work.

[01:08:19] Even if like the MMLU score doesn't go up that much in the immediate term and ~~here. And so, ~~so to close that thread, when that happens, then all these agent products, ~~you know, ~~start to work dramatically better ~~kind of ~~all at once. And I can suspect to that dynamics question we talked about earlier. On the brain computer interface side, it does seem similar where people are ~~like ~~developing all these different form factors.

[01:08:41] You've got glasses, you've got helmets, you've got, ~~you know, ~~headbands, and they all kind of don't work that well because there wasn't enough of the right kind of data. To train them on, but you also had to get these things out there to get that data.~~ I'm not,~~ I don't know that open AI necessarily had to have all these agent products created.

[01:08:59] I think they probably could [01:09:00] have created their own data, but ~~in this, ~~in this field, ~~like~~ you actually need readings off of a lot of brains. And so it seems like we're in this kind of similar space where the hardware is starting to ship. It doesn't quite work, but again, it's going to collect a lot of data.

[01:09:12] And then you can imagine a lot of these things ~~kind of ~~turning on relatively quickly and being just a lot more. Advanced. Perhaps without even necessarily needing,~~ like,~~ major upgrades. To the hardware, you already have a 32 channel thing that may well be enough for a lot of use cases if you can decode the data, ~~you know, ~~effectively, so maybe it'll take more than 32, ~~but it sure seems.~~

[01:09:33] ~~I mean, ~~do you know ~~how many?~~ What is the number of electrodes that the guy from the early patient had? ~~You know how many channels his signal is? ~~That's a great question. ~~Uh, ~~I don't know exactly, but it's a lot. They're very dense ~~that it's, I, I think it don't quote me on this, but well, I guess it's possible to podcast, but, um, uh, it might be thousands, uh,~~ it might be like a thousand plus.

[01:09:43] I'm not actually 100 percent sure, but~~ I, ~~it's quite a bit because of,~~ um, ~~the,~~ uh, ~~sort of,~~ um, ~~fibrous way ~~that they, that they, that they're, ~~that they're doing the implant,~~ uh,~~ it is like,~~ Very,~~ very dense and fine. Google, both traditional search result and generative search result comes back with [01:10:00] 1024 electrodes.

[01:10:01] ~~Yeah, that was, that was my, that was my recollection.~~ Yeah. ~~Um, ~~so it's a lot, ~~it's a, it's a lot that they're, that they're, um,~~ that they're, yeah, the problem though with that obviously is that it's a lot in a very specific place and there are~~ like ~~long term issues with that. Invasive is very promising in terms of capabilities, but.

[01:10:16] You gotta have them all over the place. ~~You know, you can't just,~~ the EEGs, they do have the same problem ~~that the, ~~that the,~~ uh,~~ non invasive ones have ~~when they're, ~~when they're planted in the skull. They can't read everything in the brain. They can read in very local areas. They can read down to the neuron level invasively.

[01:10:30] ~~But they, ~~but they can only read locally. ~~Um, ~~so same problem. And also obviously like the idea of connecting an EEG in my brain to the internet is ~~like~~ absolutely terrifying, right? Just from a cybersecurity perspective. ~~Um, ~~so ~~like there's, there's, ~~there's obstacles there too. Yeah. But ~~no, I think, I think that tracks.~~

[01:10:43] ~~And I think one,~~ one other thing that I do think about though, is,~~ um,~~ in as much as these various platforms have, ~~I mean, the, ~~the problem with~~ the agents ~~Foundation model agents is that ~~they are, you know, their, ~~their test environment is the real world. The actual full, ~~you know, ~~they don't have a baby version of the internet or computing [01:11:00] environment that they can interact in.

[01:11:01] They have to use our computing environment, which is. Weird and has all kinds of history to it and affordances that are for us that they don't necessarily need that probably ultimately serve to confuse them and complexify the environment for them. ~~Um, ~~it is why I mentioned the shortcuts idea ~~from, from, you know, from, ~~from Apple's platforms because what they have kind of done is modularized the functionality of not just their whole operating system, but,~~ um,~~ developers can plug into this too.

[01:11:27] So ~~like~~ third party apps are ~~like~~ modularized too. ~~So. And, ~~and Android, I believe, has something similar. ~~Uh, ~~it's not just Apple. ~~So, ~~that's the kind of thing that, again, it could, like, take off pretty quickly. ~~Uh, ~~and it could actually be local, so it could be fast. ~~Like, ~~you could do this inference locally in theory,~~ um,~~ if you had a sufficiently powerful, ~~you know, com~~ compute on board, but it wouldn't be like, you don't need like an H 100 to do this,~~ uh,~~ kind of thing.

[01:11:50] ~~So, um, ~~yeah. ~~Uh, and, ~~and that the other shocking thing is just how little compute has been applied to this problem. Yeah. ~~Is, is there, is. ~~There's low hanging fruit. ~~I am no,~~ I have no doubt about it. So yeah, ~~I mean, I guess~~ that takes us ~~kind of to the, ~~to the [01:12:00] right section of this, which from a technological perspective is actually pretty simple for me to explain in a narrative way because ~~it uses, I mean, ~~it's the same basic principle.

[01:12:11] We want to manipulate electrical and magnetic fields. ~~Um, ~~the problem is that everything I described in terms of getting signal out of the brain, In a clean way, those problems are magnified substantially for getting signal into the brain, because that's really what your skull is, ~~you know, ~~intended to do.

[01:12:30] It's not so much to keep things in, it's to keep things out. ~~Um, ~~So ~~there's, ~~there's direct current and alternating current stimulation. Transcranially, ~~this is, ~~this is non invasive tech,~~ uh,~~ transcranial magnetic stimulation would be~~ the, you know, the, ~~the magnetic field equivalent of that. ~~Um, ~~the problem is that there's been some promises.

[01:12:47] There's been some stuff that's been shown in the lab, but it's either very expensive hardware. ~~That's not,~~ it's not writing that good hardware curve, like the electrodes are, by the way, which are getting denser, cheaper all the time. ~~Um, ~~And the signal [01:13:00] just diffuses, so it's just erratic,~~ uh, it's, ~~it's impossible to focus, and it's very,~~ um,~~ varied between people.

[01:13:08] So, I don't really see the magnetic or electric field manipulation as being a problem. all that promising in the long term for neuromodulation. Some people disagree with what's called transcranial magnetic stimulation. Some people disagree and think ~~that ~~that is going to be the path if we can just stick the cost down.

[01:13:28] But right now the cost is very, very, very high. ~~Um, ~~so it's just not what I'm focusing on because I'm interested in things that seem like they could happen in the relatively near future. There is a. technology though. This really was my exclusive focus when it comes to neuro, non invasive neuromodulation.


## Transcranial Focused Ultrasound: A Promising Non-Invasive Technology

[01:13:46] It's called transcranial focused ultrasound. It's a very old technology. It goes back a hundred years,~~ uh,~~ but really its rebirth ~~is, ~~is in the past 10 or 20 years. What this is, it's firing very high frequency sound [01:14:00] waves at the brain, far outside the range of human hearing, far outside the range of the hearing of dogs and cats, though there are animals that can hear it.

[01:14:08] ~~Uh, I think ~~rats. ~~Uh, ~~bats, I think you can hear it too, but that you can fire at a sufficient frequency and amplitude and pulse to target pretty precisely into the brain and deeper than anything else can. You can't get to deep brain regions. You can't write to the hippocampus, which is, ~~you know, ~~memory,~~ uh,~~ from,~~ uh,~~ with this.

[01:14:31] But, you can go a few millimeters, as much as a centimeter,~~ uh,~~ into the brain, and you can target it very precisely. ~~You can,~~ so it's a few millimeters to a centimeter of depth, and then millimeter level. resolution, spatially. So, it's a very promising technology. Also, it's cheap, and also, much like the electrodes, ultrasonic transducers are on a good hardware curve.

[01:14:56] ~~Um, ~~they are getting cheaper and denser, ~~you know, ~~year by year. The weird [01:15:00] thing about TFUS, as it's called, it's not very well understood how it works. ~~Um, it's, it's, it's, ~~and ~~it's weird, right? I mean, it is,~~ it's just weird in principle that firing high frequency sound waves into the brain, ~~like ~~makes you, like changes your thought anyway.

[01:15:13] ~~It's like, ~~that's just weird, but it does. ~~The theory, there's some, ~~there's some who theorize that ~~it, um, ~~it changes the electrical,~~ uh,~~ receptivity of, brain tissue in certain ways that just make electrical signal flow more readily. ~~Uh, ~~there's a theory that it just stimulates neurotransmitter release in the targeted regions of the brain, but no one's really found any safety problems with it at reasonable dosages.

[01:15:39] At high dosages, It has a thermal effect. ~~So, as, ~~as ~~you know, ~~we see with other ultrasound, ~~you know, it can, um,~~ at a very high dosage, ~~it can, uh, um, ~~it can cause brain tissue to heat in a way that you probably don't want, ~~um, ~~and the other thing from a safety perspective, of course, is that all of these things are done, ~~you know, ~~one time in a lab, ~~um, it's not~~ nobody has modeled what the long term [01:16:00] effects are of using this, for example, every day.

[01:16:03] ~~Um, ~~but it's very promising. The FDA says it's safe below a certain level, and it's been shown to do some shocking things in my mind. So we've talked a little bit about improving bandwidth, maybe ~~like~~ the single most interesting finding to me of this whole pandemic. Chain of research that I've done is this finding that with TFUS, at the sense, at the somatosensory cortex, fired at the, its specific regions ~~of, ~~of basically your tactile sensation, the part of your brain ~~that ~~that measures tactile sensation.

[01:16:35] ~~You can, uh,~~ you can perform tactile discrimination tests. So basically you ~~sort of blind the ~~cover the user's eyes. ~~Um. ~~and rub a pin on their hand and then rub two pins ~~really close together, ~~really close together, and ~~you see ~~if they can distinguish between the one pin and the two pins. And TFUS was shown to improve that when it was being fired at the brain.

[01:16:57] And also had some [01:17:00] offline effect too, an offline and versus an online effect. The online effect is, do we observe an improvement in the desired cognitive activity when the treatment is actually being applied offline is, do we observe it afterwards? ~~Um, ~~and 40 minutes afterwards. The discrimination attenuated a little bit, but we're still there.

[01:17:19] So that had a long term effect on what it would seem to me is the brain's channel bandwidth,~~ uh,~~ to the hand. ~~Um, ~~so that is very interesting. There's a lot of other things that it's done too. ~~Um, ~~mood, focus. ~~Uh, ~~visual acuity and visual discrimination tasks in the same way that I described with the tactile discrimination, you can actually induce complete changes to the visual field, or at least noticeable changes to the visual field by,~~ um, ~~inducing these, what's called phosphenes.

[01:17:48] If you imagine,~~ um, ~~when you shut your eyes and you lightly press with your fingers on your eyelids, Those kind of blobby shapes you see, those are [01:18:00] phosphenes. You can induce that in a person's visual field using TFUS. Auditory discrimination also goes up. ~~People, uh, can, or, or~~ another finding that interests me too is the ~~people can feel, ~~people can discriminate between vibrations at different frequencies that are very close together.

[01:18:16] They can do that discrimination, not substantially, but marginally better. With TFUS and ~~this is applied~~ all of these studies for the most part are TFUS applied at fairly low dosage levels, ~~uh, ~~well below Where that thermal effect is seen that I mentioned. So ~~there might, ~~there might be an interesting range of capabilities ~~that you could, ~~that you could obtain if you were to go beyond that dosage level.

[01:18:41] And ~~I know~~ I've talked to both of the leading scientists in the world who ~~are, uh, kind of ~~have had ~~the, the, ~~the top TFUS papers in the last 10 years. And they've both said, we are very confident that you can increase dose. Safely at this point, ~~um, ~~that's TFUS and~~ I think, you know, ~~can it write thoughts [01:19:00] to the brain?

[01:19:01] Like absolutely not. And I don't even want to suggest that I think that's like remotely close. I think that's really hard. That's my intuition. But this idea of being able to improve not just mood, but things like channel bandwidth ~~in the sensory areas,~~ in the senses ~~rather, ~~that's really interesting to me.

[01:19:21] Yeah, any questions, ~~Nathan? I can't hear you. Sorry, my bad. Um, there was a little noise here. So I muted myself for a second. Oh, gotcha. ~~So you had said that the skull is a huge barrier, ~~you know, in the ~~in the most obvious way to getting these signals into the brain. It seems like that's also a problem on the way out.

[01:19:32] But we sort of compensate for it by just like having a bunch of electrodes all around and ~~kind of ~~collecting the mess and then trying to Yeah. ~~You know, ~~clean up the mess, but on the way in where the precision really matters, that becomes a big barrier. Is there ~~like~~ an evolutionary story for why the brain would be good at this?

[01:19:49] Or do you think this is just like an accident of biological history that the brain blocks these electrical signals? ~~I think it's probably an accident of biological. ~~I think it probably has to do with the fact [01:20:00] that it's probably useful for all kinds of other reasons for bone just in general to be low in conductivity.

[01:20:05] ~~Um, ~~and the skull is made of bone, and ~~so, you know, you're,~~ you ~~kind of ~~just benefited there. I have no idea, though. That's an interesting question. I'd love to ask an evolutionary biologist, ~~um, ~~that question. ~~Um, ~~I will say, though, with TFUS, the exact same imaging problems are present. ~~Um, ~~weirdly enough, in certain cases,~~ uh, I don't have a good model for understanding when this is the case versus when it isn't, but sometimes ~~the skull can actually have a beneficial effect on getting ultrasound signal ~~into, uh, in, ~~into the brain.

[01:20:29] ~~It can have like a lensing effect. Uh, if you, if you hit it just right, and you're, you know, if the child, ~~if the kind of signal you're trying ~~to, to, ~~to create,~~ uh,~~ and the specific region of the skull is just right,~~ um,~~ it can actually have a lensing effect. But in general,~~ it's still, you know, ~~you still have to deal ~~with, With, uh,~~ with signal processing, ~~you know, ~~and sort of can you modifying your signal based on a dynamic model of the skull.

[01:20:45] You still have to do that. ~~So, I mean, ~~it is crazy that ~~this, ~~this ultrasound is basically just vibration, right? ~~I mean, ~~sound is vibration. This is just high frequency vibration. And I don't know if you know ~~the, like, ~~the frequency off ~~the, ~~the, You Top of your head that this operates at,~~ um,~~ like middle a or [01:21:00] middle C ~~on the, you know, ~~the tuning note in the orchestra is like four 40, right?

[01:21:03] So this would be like, ~~you know, ~~you go up several octaves. We can definitely still hear up into the range of ~~like ~~a few thousand Hertz it seems. And then I guess this would be like North of 10, 000 maybe~~ In the range of, uh, like, I,~~ I think it's ~~like ~~five to 10, Usually in that range, I'd be 10, 000 Hertz. ~~So, ~~yeah, it's weird, right?

[01:21:23] ~~I mean, this is, this basically at a, ~~at a neuron level, this basically amounts to shaking the neuron, right? It's like, this thing can sort of fire off its electrical signal a thousand ish times a second, or sometimes even faster, as you said earlier. Now we ~~kind of ~~bring in ~~like, uh, a lyrical,~~ a literal physical shaking that comes in at an even higher frequency than that.

[01:21:44] Okay. 


## Exploring the Dual Nature of Stimulus Technology

[01:21:45] And that seems to kind of wobble things loose, ~~I guess. ~~And ~~like, ~~just ~~kind of, ~~this is stimulus, right? ~~Is there a, ~~is there a suppressive effect that can be achieved this way? Or is this purely a stimulus technology? ~~Uh, it has usually been,~~ most of the literature I've seen ~~is, ~~is for [01:22:00] excitatory, ~~you know, ~~signals,~~ but,~~ but no, ~~there, ~~you can do inhibitory,~~ uh,~~ effects as well.

[01:22:03] So yeah, ~~you can, ~~you can block certain things too. And ~~is there,~~ do you have a. Sense of what is the difference? ~~Like if I'm, ~~if I'm imagining, ~~you know, uh, ~~shrinking myself down to neuron scale and sitting in this region of the brain, ~~like~~ it seems that the vibrations coming through, like, ~~how would they,~~ how would I know whether they're supposed to make me do stuff or not do stuff in my particular local area?

[01:22:25] Yeah, ~~that's a, ~~that's a very good question. ~~Um, I will say my general sense. ~~I'd love to ask ~~one of the, one of my,~~ one of the scientists who helped me with this research. I'd love to ask them this question. That's a great practical question for them. My general sense is that most of the inhibitory stuff is not, ~~it's, there's not~~ that much inhibitory stuff ~~that, ~~that actually, ~~you know, ~~gets done.

[01:22:43] Let me actually,~~ I'm just going to, uh,~~ quickly pull up my notes ~~and, um, ~~I'll get you an answer to that question,~~ uh,~~ on ~~like ~~what specifically has been inhibited at least. Thanks. ~~Uh, ~~because ~~that's like, I think that,~~ that might ~~kind of ~~answer the question. ~~Um. Hold on a sec. Okay.~~ Yeah. 


## Diving Into Inhibitory Effects and Pain Attenuation

[01:22:52] So ~~I'm not sure that inhibitory, uh, like use of this has, I don't know, reduced reaction time in a motor cast.~~

[01:22:53] ~~Yeah. Like you can actually do that. Um, yeah. So it's been, that's, that's right. Um. I'll restart. Um, ~~the inhibitory,~~ uh,~~ TFUS, ~~most of, ~~most of it,~~ uh,~~ has been used for pain attenuation. So [01:23:00] it's ~~kind of ~~looking at the parts of the brain that are,~~ uh, like that, that are, that, that are~~ sensing pain and are targeting that, ~~um, ~~and that seems to work for whatever reason.

[01:23:09] I don't think that is well understood at all. I don't know for sure, but that also has the side effect of reducing motor time,~~ uh,~~ reaction time for~~ like ~~various kinds of motor tasks and can reduce some of the things we were talking about. 


## The Brain's Self-Regulating Mechanisms and Potential for Inducing Thoughts

[01:23:23] Yeah, it seems like there's also, even if it was purely stimulating activity in a particular region, it's an ~~important note to keep in mind, too, that, you know, more than a footnote, um, but an~~ important fact to keep in mind that the brain is self regulating in all sorts of ways, right?

[01:23:34] So~~ there is this sort of, you know, Um, you know, ~~we see this ~~in in sort of the ~~at the cellular level to ~~write. There are~~ there is a gene that gets expressed to suppress the expression of another gene. And at the brain level, there are regions that activate to suppress other regions. And so you can imagine, even if you could only ~~kind of ~~turn things on.

[01:23:49] In many ways, that might allow you to turn things off if you can ~~sort of ~~figure out the indirect pathway to get there. It doesn't sound like that's what's going on here, but conceptually ~~It might well be.~~ It might [01:24:00] well be what they're doing. ~~Uh, ~~that could very well be the case. ~~Um, ~~it is definitely inhibitory in terms of activity, though.

[01:24:06] And I don't have a good model for why that works. ~~The, the, ~~the excitatory is at least ~~like~~ intuitive. If anything,~~ it's, I mean, ~~it's a little like, well, if you shake the TV set, ~~you know, ~~it generally fixes the signal. There's a primitive ~~part of the~~ part of us that can understand how that works. ~~Uh, but, ~~but yeah, the inhibitory ~~is, ~~is a bit more of a puzzle to me as to why that works.


## Envisioning the Future of Brain-Computer Interfaces

[01:24:26] So this question of, Writing thoughts. ~~I mean, ~~I'm not sure we ever would want this exactly anyway. ~~Um, ~~but putting that aside, ~~the, ~~I can start to imagine how you might close the loop here. Maybe there are some barriers that I'm not immediately seeing. I'm sure there are, ~~you know, ~~plenty of challenges that would have to be overcome.

[01:24:43] And I'm not seeing any fundamental barriers. The resolution on this technology was down to a couple of millimeters. As well, right? ~~That's right.~~ That's right. Yeah. And so ~~it's~~ that's notably like the same scale as the voxels that come out of the FMRI. And so if I [01:25:00] connect this to the. Mind eye paper again.

[01:25:03] And I'm like, okay, there are 12 to 17, 000 voxels per person back in ~~the, you know, ~~the visual cortex. ~~This is,~~ and~~ I'm, you know, far from, I got a little,~~ I got a mini lesson on this,~~ um,~~ in that episode around, ~~you know, it was kind of like how, ~~how much semantic information is encoded in the visual cortex. ~~You know, ~~is that just like, raw sensory, ~~you know, ~~work up until kind of lines and edges.

[01:25:23] And then the front of the brain does the sort of, that's a tiger type stuff. ~~And, ~~and ~~he said, you know, this was ~~Paul,~~ uh,~~ the author of this paper, he said, no, ~~there is ~~still in the back of the brain, ~~you know, ~~as part of the visual cortex, there is ~~like~~ a lot of semantic information that understands what it is that, you know, and actually recognizes conceptually what it is you are looking at.

[01:25:41] So from these, ~~you know, ~~12 to 17, 000 voxels, two ish millimeters cubed. They're able to extract both a general sense of ~~like ~~the image that you're looking at, like what are the colors and ~~you know, ~~what are the regions of the image, which part is dark, which part is light, whatever. But then [01:26:00] also, straight from that reading, ~~they can take,~~ they can predict a caption for the image ~~from the, ~~from the brain state.

[01:26:07] And that's because there is enough semantic information there that not only do you have this kind of, ~~you know, ~~blurry, ~~you know, ~~purely visual information, but you also have this conceptual information. So anyway, the point there is that you can read that at ~~like ~~a two millimeter cube level. And now you're saying that you can also focus this signal down to that same kind of scale.

[01:26:29] It seems like the possibility at least conceptually exists to create a feedback loop where you might say, okay, I want to send some signal. How do I know that I sent that signal? I also need to then read the signal back. So ~~could you, you know, what, what would you, I don't know how, you know, ~~you can focus the signal down to a couple millimeters region, but maybe you can't do that, ~~you know, ~~10, 000 wide, maybe you would have to do that, ~~you know, ~~10, 000 box holes at a time ~~to kind of create meaningful higher order concepts, you know, ~~to induce meaningful higher order concepts in the brain, but it does seem like you at least now start to have some ability [01:27:00] to make a perturbation and then also read it on the other end.

[01:27:04] And if you have a sense for what it is you're targeting, ~~you know, ~~I want you to be thinking about cloud three. ~~Um, you know, ~~there seems to be at least some ability to start to be like, okay, I'm going to send a signal and then decode what states arise from that. And was the person thinking about cloud three?

[01:27:19] ~~Like, ~~yes or no. Then I ~~sort of ~~update ~~my. You know, ~~my network that is deciding what signal to send in based on the signal that is later read out. And, ~~you know, ~~this seems like a leap, but given what we have seen work, it doesn't seem too crazy to think that you could start to close this loop with, was I able to induce what I was trying to induce?

[01:27:41] ~~You know, ~~maybe the reward is too sparse. Maybe you need to have like broader regions, ~~you know, ~~subject to receiving a signal at the same time to really get anywhere. If you can only target one, two millimeter cube, ~~you know, ~~maybe that just gets lost in the overall broader state of the brain. But I'm starting to at least [01:28:00] imagine how you can close a loop and begin to reliably induce certain things because you can read those things.

[01:28:08] And that gives you some ability to ~~kind of ~~correct, or, ~~you know, to, ~~to gradually learn how to do the inducement in the first place. I'm not sure ~~what other, ~~what are their ~~like ~~big barriers there would be to doing something like this. But ~~it, ~~it also seems like there's plenty of possibility that things could just be quite odd. And that ~~like, ~~maybe you don't actually need that, ~~you know, if you could really figure it out, ~~if you really could figure out how the brain works, like all these sort of thoughts arise somewhere, ~~right.~~

[01:28:29] And they presumably arise ~~like sort of ~~locally first, ~~right. ~~There's ~~like ~~some sort of trigger. That, ~~you know, ~~ultimately propagates through broader regions of the brain, but, ~~you know, ~~starts with some sort of input or some sort of signal that, ~~I guess, like, ~~becomes dominant in the moment. ~~Right. ~~And ~~sort of ~~ultimately rises to the level of consciousness.

[01:28:48] It seems like it's not crazy to think that maybe you could find ~~sort of. ~~The key levers through this process of attempting to now, would we do this on humans? Maybe not, but ~~you know,~~ you could definitely do it on monkeys and I bet we're similar enough to [01:29:00] monkeys. We could even create like a shared latent space with some monkeys at some point to get to the point where you could ~~sort of ~~say, yeah, I'm just going to run this loop a ton.

[01:29:08] I'm going to try to induce these things. I'll measure, ~~you know, did I,~~ how close did I come? I'll update on that. And even if I don't have the ability to stimulate that much of the brain at a single time, potentially I can find ~~like the, you know, ~~the key that unlocks the lock to induce the states that I want to induce.

[01:29:23] Possibly crazy, but ~~what are the, you know, ~~what if anything would say ~~that ~~that can't happen?~~ I think ~~the main thing is That we don't really understand at all, as far as I know. We don't have a good understanding of how information ~~is coded,~~ is encoded, in the brain. And ~~so, ~~it can be read back when you're doing things like the mind eye, ~~you know, the, ~~the fMRI, and especially when you're pairing it.

[01:29:45] Like ~~in, in the visual, when you're, ~~when you're looking at the visual cortex, you can kind of like extract out of it. But actually,~~ like,~~ The right process, like how do you do that? Where does the right happen? Probably more than one place, right? Like ~~there's the,~~ the thing that distinguishes brains from even ~~the, ~~the most [01:30:00] sophisticated, ~~you know, ~~neural networks is just how much crosstalk there is and how much weird resonance ~~and, and, ~~and just all sorts of feedback that's constantly being conveyed.

[01:30:10] And I don't think we have a good understanding of that at all. ~~So, Like, ~~would I be shocked if you were able to induce a simple thought in someone's mind by ~~like, ~~firing ultrasound at V2,~~ the, you know, ~~the visual cortex? ~~No. I wouldn't be. But,~~ well, no,~~ I, I, I would be, ~~I would be pretty surprised if that worked,~~ like,~~ it wouldn't be a galloping shock to me, because of the informational aspect of that.

[01:30:28] I'm not sure that,~~ like,~~ high dimensional information can be, first of all,~~ like,~~ encoded in instructions to ultrasound, to ultrasonic transducers. And then successfully transmitted into the parts of the brain, probably several where it would need to go in order to actually write information, particularly of any I'm thinking of the moment in the matrix where Trinity downloads the instructions for how to operate a helicopter.

[01:30:57] I'm thinking of that ~~right? Like, ~~as [01:31:00] being ~~sort of ~~your North star. ~~Um, That's,~~ that seems hard ~~to, to, to, ~~to encode in,~~ in, in~~ all the various ways they would need to be encoded. ~~Um, ~~but I wouldn't rule it out at the very least though, ~~activity,~~ like brain activity is possible to stimulate and it is possible to change the mood of the wearer, ~~uh, ~~the level of focus that they have, things of that nature, all possible.

[01:31:22] ~~Um, ~~we don't know how much and how high dimensional that can get. Obviously like a good mood. Is a pretty general concept ~~that, ~~that could be refined quite a bit. 


## The Potential of Ultrasonic Technology in Enhancing Conscious Experiences

[01:31:33] There's a few TFUS devices that are on the market. ~~Um, ~~there's a company called prophetic AI that is,~~ uh, uh, ~~based in New York. They've had a lot of buzz.

[01:31:45] ~~On, ~~on X and other social media, ~~uh, with, ~~with ~~some of their various, um, ~~some of their various announcements. And there's ~~like ~~a couple of things that are really interesting ~~about, ~~about what they're doing.~~ I think,~~ cause it is actually kind of the convergence of a few of the things that we've talked about in this discussion.

[01:31:58] First of all, they have [01:32:00] developed what they call an ultrasonic transformer, which is ~~a ~~a transformer based system trained on EEG and fMRI data which takes as its input EEG readings from the headband that they're going to sell and outputs instructions to the ultrasonic transducers also on that same headband so it's inferring your brain state And then it's turning that into instructions ~~for, ~~for neuromodulation.

[01:32:32] And their ambition is to do this with lucid dreaming. ~~So the idea is that ~~it's actually a pretty easy signature ~~from, from a,~~ from an EEG perspective, this is not hard. Like you can absolutely recognize that somebody is lucid dreaming while they're sleeping because ~~you, the,~~ the neurosignature of sleep is pretty consistent.

[01:32:50] Pretty easy to recognize even for low channel EEG. ~~And then, ~~and then ~~there's, there's, ~~there's a spike of,~~ uh,~~ gamma waves ~~in the, ~~in the prefrontal cortex. It [01:33:00] recognizes that spike and then ~~it just keeps. It, ~~it directs the transducers to just ~~kind of ~~keep firing to maintain, ~~uh, ~~that brain state and keep you in that lucid state.

[01:33:09] ~~I'm not, this, they, they actually, they're, ~~they might have slight corrections to that description, but the basic idea is that it can keep you inside of a lucid dreaming state. ~~Um, ~~they also have the ambition to, to do quite a lot of other stuff. Conscious experiences,~~ uh, sort of all, ~~of all kinds. ~~Uh, ~~much kind of higher dimensional conscious experiences.

[01:33:27] ~~Um, ~~they want to start with focus and a positive mood. ~~Uh, ~~but ~~I think that ~~they have ambition to go beyond that. A lot of other people that are operating in the space have ambition to go beyond that. And again,~~ uh,~~ you're writing a positive heart record. ~~So, ~~so just like you are with Eden. Yeah, the ~~how, ~~how high dimensional is thought is, ~~I mean, ~~it's really, I'm thinking about the representation engineering work that recently came out of case and other, there's been a bunch of authors on that paper, but they identify these high order concepts through a~~ bunch of kind of~~ clever, but honestly, it's ~~like ~~not even that crazy of a technique where they [01:34:00] create a bunch of different contrasting pairs that ~~sort of~~ show like the presence and absence or the positive and the negative of a concept of interest, and then create enough pairs of those and then run them all through and then look at these intermediate states and then take the averages.

[01:34:14] and are able to basically say, okay, this appears to be the general direction, ~~you know, ~~trying to kind of let the noise cancel out that represents, ~~you know, the, ~~the direction from unfairness to fairness or the direction from sadness to happiness. And then they can start to use that vector direction to detect those states in, ~~you know, ~~further,~~ um,~~ downstream inferences.

[01:34:37] Blind, right? Just looking at the activations and saying,~~ like,~~ is this a happy state or is it, ~~you know, ~~an unhappy state and then they can also start to layer those in and shape behavior based on ~~just inserting that kind of just, ~~just adding it to the, ~~you know, ~~whatever the thing is doing at a given time, if you just add on the happiness direction, then you can see that you actually ~~steer the direction you ~~steer the downstream model behavior.

[01:34:59] ~~In, uh, you know, ~~in a [01:35:00] reasonably intuitive direction where it seems like, okay, now it is actually generating~~ like~~ happier outputs. So that, ~~you know, ~~is way, way easier to do in a neural network than it is to do in the brain. And the dimensionality of that is pretty high. ~~Um, ~~so it might just be, ~~you know, ~~an engineering challenge that is like, at least in a noninvasive way, just too crazy to get to, but.

[01:35:22] It does seem like we have something pretty similar going on, ~~you know, where, um, and, ~~and ~~you're, ~~you're setting all these examples where it's like, you're doing that at basically a low dimensional, very crude sort of way. And as we get into these, ~~you know, ~~higher and higher level concepts, I also think about the entropic,~~ um, ~~sparse auto encoders with this, where they show that basically.

[01:35:45] The. From these high dimensional states, you want to identify these human intuitive concepts. And the more space you give the sparse auto encoder, the more concepts it will find. They call this ~~like~~ feature splitting. So if you don't give it [01:36:00] that many, ~~if you, ~~if you give it like a limited number of concepts that it can, ~~you know, ~~differentiate its activation space into, then you'll get ~~high, order, you know, high, ~~high level, kind of more general concepts out.

[01:36:11] And if you give it a lot more space, then the features will split and you'll start to see these ~~like~~ very granular level representations. And it seems like we're just ~~kind of ~~operating right now at a very, very, ~~you know, um, ~~general level ~~where the, like the features~~ we have not yet got to the level of resolution where the features can split.

[01:36:26] And so we're ~~kind of. You know, ~~making ~~like ~~these very basic modifications to mood, but ~~it, ~~it seems like it's really a question of resolution more than anything else over time. Yeah. ~~Uh, ~~resolution and ~~maybe ~~it might also just be data, ~~you know, I mean, it might just be that, that we don't have a lot of, I mean, ~~the same data problems that we talked about earlier.

[01:36:44] And then obviously ~~like ~~with TFUS in particular, that's even more limited. ~~Um, ~~of a data set. ~~So, uh, ~~it might be some combination of all those things. And yeah,~~ but,~~ but ~~it, it, it also is, ~~I don't know how far it can go. ~~Right. ~~In principle, any conscious state ~~is, ~~is inducible, but ~~you know, how, ~~how far can that go with [01:37:00] non invasive?

[01:37:00] My guess would be somewhere between exactly what I'm experiencing right now at the high end. And. ~~You know, ~~being happy at the low end. I would suspect you can do more than that, more than being happy, less than what I'm experiencing right now. ~~Um, ~~but where exactly? ~~I mean, ~~I think ~~we, ~~we just have to find out.

[01:37:17] It is though possible. It takes some time to, to even get your head around what that would be like. ~~I think ~~the closest thing is probably drug use. ~~Right? Um, ~~yeah, ~~I mean, ~~I really don't want to say that this is like using a drug, ~~uh, ~~because I think that this might be, ~~you know, ~~much more than that. ~~Um, but, and,~~ and I don't even necessarily mean illicit drug use.

[01:37:39] I just mean, ~~you know, uh, um, ~~something that's meant to enhance your mood ~~because, but even then I've never used, I have no experience with, with, with like, you know, things like that. Um, yeah.~~ But even then, my guess would be that because of the sort of slower mechanism of action that most,~~ like,~~ pharmaceutical mood drugs have, it's gonna feel like it's coming from inside you more.

[01:37:53] Whereas something like TFUS, you will feel good or focused. ~~Kind of ~~[01:38:00] suddenly ~~and then kind of not, uh, you know, I don't know.~~ I don't know exactly how long ~~the ~~the feelings persist I would be shocked if it were like a steep drop off like a completely like,~~ you know ~~cliff But it is also probably fairly quick that it drops off.

[01:38:10] ~~So yeah, I think All that is there's a lot to think about ~~and certainly ~~Yeah, you know, ~~I would guess that the Transcription by CastingWords Experience of using something like the prophetic headband to lucid dream. ~~I think~~ if you use that every day, it would be like taking LSD every single day and you would eventually go into ~~like ~~a Sid Barrett do wimp doom loop would be my guess.

[01:38:28] ~~Uh, but, ~~but yeah, how far can you go? I have no idea. ~~Uh, ~~it's very early days. ~~This is, this is the kind of thing that like,~~ this is very much at the frontier at the same time, immediately ~~into~~ integratable ~~into, ~~into consumer hardware, not expensive. And FDA approved below a certain limit. ~~And that might actually be a good segue.~~

[01:38:42] ~~I mean, I'm, ~~I'm happy to explore this more. 


## The Ethical and Policy Implications of Advanced Neural Technologies

[01:38:44] That might be a good segue into some of the policy. ~~I, ~~I am a policy person. ~~Uh, so I have these kinds of questions too. I wouldn't, I would hesitate to offer policy. What are the basic questions we should be asking, which are probably on everybody's mind right now, uh, in a certain, in a certain sense, ~~before we go into policy, let me just ask one more kind of technical intuition question.

[01:38:57] If I had to. Make a high level guess based on [01:39:00] everything we've talked about. It feels like I would put my money on, we're going to have pretty good brain rating in consumer devices over the next few years because the hardware is. already ~~kind of ~~there and the cost curve is good and the data is about to explode and the best thing to interpret one neural net is another neural net.

[01:39:21] On the other end of ~~the, you know, ~~the right or the modification to brain state side, it seems like resolution and just, ~~you know, the, ~~the overall ability ~~to send,~~ to accurately send the signal that you want to send is, limited and seems like it's probably going to continue to be limited relative to the bandwidth that you would actually need to create ~~like~~ really rich,~~ um,~~ input signal to the brain from outside the skull anyway.

[01:39:51] And so we're probably headed for something that is like fairly crude and sort of valence level, but ~~not like, you know, ~~we're not going to be [01:40:00] invoking, ~~you know, ~~or reducing thoughts of cloud three Opus,~~ um,~~ with precision with those sort of non invasive techniques. And so if those things are to be done, it seems like they would require invasive techniques.

[01:40:15] And at that level, then you can ~~like~~ actually send the signal where you want ~~the obvious ~~at the obvious cost of having to implant thousands or, ~~you know, ~~potentially many thousands of electrodes into the brain. ~~Um, ~~but ~~if, if you imagine sort of like, Is there a, you know, what would, ~~what would the path be if there is to be a path to something like the matrix moment where you could randomly download skill sets or knowledge bases or whatever, or, ~~you know, ~~even just create the sort of mind control, ~~you know, ~~to get people to act in whatever way you want them to act, ~~that simply would have to be with anything, you know, ~~with the current technology landscape, that seems only plausibly feasible through invasive methods.

[01:40:45] Yeah, I totally agree ~~that intuitively. I completely agree. ~~The only thing I'll say, just ~~as a, ~~as a caveat is that,~~ um,~~ the spatial resolution of TF us and the spatial resolution of EG are basically the same. ~~Um, as far as I mean, there might be, ~~there might be practical differences in spatial resolution that I'm not thinking [01:41:00] of the obvious 1 being that.

[01:41:02] ~~Like, you know, you can't ~~with EG, you can't differentiate between ~~all the. ~~all the neural signals, ~~you know, ~~between individual neurons, but you aren't picking up ~~the, ~~the collective aggregate result of their electrical activity. ~~Um, ~~whereas with TFUS, ~~you know, ~~maybe it is the case that you need neuron level or close to neuron level targeting to create really rich experiences, but I don't know.

[01:41:28] ~~Uh, ~~at the same time, ~~like~~ neural networks, Tend to have ~~like a, ~~like a fair amount of redundancy. ~~Right. Uh, would, would you say, ~~would you say that's like ~~a, ~~a fair observation? ~~Um, ~~that ~~like there's, there's, ~~there's, ~~you know, ~~possible that you don't need to get down to the level of the individual neuron~~ to, um, ~~to do quite a bit.

[01:41:41] So Yeah. That,~~ but,~~ but in general, I agree with your intuition ~~that that's, ~~that's about who I am too. Cool. ~~Well ~~then let's get to, ~~I think. ~~Policy. 


## Personal and Societal Impacts of Emerging Neural Technologies

[01:41:46] And also, ~~you know, ~~I wonder ~~if you could sketch like maybe, I don't know if it, you know, who knows what, what order things happen in, I'm also kind of interested in like, do we have, ~~do you have any intuition for sort of, as this technology maybe follows its natural course, how does life start to change?

[01:41:54] ~~You know, like, have you, ~~have you started to game out dynamics at all? And, ~~you know, ~~policy will be a part of ~~kind of ~~shaping those dynamics. ~~But, you know, we've had,~~ [01:42:00] I used to ask people all the time, if the Neuralink had reached a million patients. And was generally considered to be safe,~~ like,~~ would you get one?

[01:42:10] And Interestingly, ~~I got very different, you know, ~~for people that are like very much on the frontier of AI, I got very different answers there, but some of the interesting answers that I got were rooted in the idea that,~~ well,~~ I'd have to, ~~you know, how, ~~how else would I keep up? So I feel like there's some, ~~you know, kind of ~~game theoretic aspect to this and, ~~you know, ~~policy can, Shape the sort of game theoretic environment, perhaps, and maybe can do other things, too, including~~ discouraging or ~~encouraging or discouraging the development of different kinds of technologies.

[01:42:38] But I'm interested in both kind of how you think this, ~~you know, ~~in the absence of,~~ like,~~ governmental intervention, how it evolves and then, ~~you know, what ~~what questions government can and should be asking and ~~what, you know, ~~what maybe the most likely things. Hard to have it in that respect. Yeah. One of my favorite anecdotes, I'll go back to the 1910s for one second.

[01:42:55] ~~Um, ~~the interior design of the average American's [01:43:00] house before, and you can just ~~imagine, ~~imagine like an old house, right? Victorian house, whatever. Beautiful houses in Detroit, dark red, dark green walls you often think about,~~ right,~~ those kind of like burgundy, Why? Why were all the walls dark back then? ~~Um, ~~it seems weird.

[01:43:15] ~~Well, ~~the reason is that interior illumination was provided by kerosene lamps, which stained walls. So you had dark colored walls to hide the stain. White walls are a luxury enabled by electricity. My point is, Only is that it is ~~very, ~~very hard to predict,~~ uh, the, ~~the outcomes of what happens when these things are at scale, obviously like ~~the, the, you know, the, ~~the things I can predict ~~are, are, ~~are going to pale in comparison to, ~~to what's, um, you know, like~~ what will actually happen,~~ uh,~~ that being said, a couple of things do seem apparent to me, I always try to think about the legal system first, and this is an area that interacts with the legal system, I think in really interesting ways.

[01:43:56] ~~Uh, ~~the concept of being under oath changes if we want [01:44:00] it to. And that's ~~kind of ~~the question. And ~~I think ~~a interesting way of thinking about the next 20 years of technology in general is will we want to impose artificial constraints? On various aspects of social life and technological development, ~~um, you know, ~~and I'm not saying that ~~I think ~~we should, ~~uh, I don't have, you know, that that's not ~~I don't have a strong model of that yet, but it does occur to me that if we're headed towards the Nicholas Bostrom, ~~you know, ~~solve the world.

[01:44:29] Which I don't think we are, by the way. But ~~if we are,~~ if he's right, then ~~one of the, ~~one of the things you will need is artificial constraints. Because my strong intuition is that a solved world quickly devolves into hell. I really do believe that. Can you unpack ~~the,~~ what exactly you mean by solved world?

[01:44:47] And then,~~ like,~~ You're probably going there anyway, but more kind of granular specific questions. I take it that you're asking,~~ like, would we work? You know, ~~you can imagine a lot of different regimes, but would we require you to wear one of these headsets to testify? [01:45:00] So we can also look at your internal brain states in addition to your like, spoken testimony.

[01:45:04] That's the sort of thing that you. are getting at, right? Yeah, exactly. ~~Um, so, I mean, the, ~~the solved world is a concept ~~from, ~~from Nick Bostrom,~~ um,~~ essentially, ~~you know, ~~post singularity, ~~utopia,~~ techno utopia style thinking ~~where, ~~where every conceivable good is available and in enormous abundance and humans have godlike powers to assemble atoms and whatever.

[01:45:27] Way that they find desirable or something does some form of conscious like that. Maybe not us. ~~Uh, ~~but,~~ um, you know, I, I, ~~I'm not a big, ~~um, ~~subscriber ~~to any of, ~~to any of that sort of techno utopian thinking, but ~~it does, you know, ~~yeah, ~~I mean, ~~ultimately. It's a very different kind of society if we can actually know at a biological level whether or not you are lying, for example, right?

[01:45:51] ~~Like, ~~that's just a profoundly different society. Do we want that? Is a society with no deception actually a desirable thing? Is an AI model ~~with a deception,~~ with [01:46:00] no deception ~~rather? ~~Actually a desirable thing. I'm not confident the answer to ~~that question,~~ either of those questions, is yes. In fact, I'm like, pretty confident that the opposite is the case.

[01:46:08] I'm pretty confident that some degree of deception is an important part of life. I think some degree of deception is probably an important part of judicial life. ~~Right? Like, ~~in court, we assume that there will be some degree of bending of the truth. ~~And you could probably model that. You could probably model someone telling a white lie.~~

[01:46:24] ~~You could probably model all different nuances of deception. And do we really want to do that? Um, I don't know.~~ And~~ then of course also when it comes to the judicial case,~~ I think that thinking about court is a fascinating way to think about ~~society, ~~how society will digest this. ~~Um, ~~it's not the only way, but I just think ~~it's a, ~~it's a very concrete way to think about it.

[01:46:34] What are the limits on something like a warrant or a subpoena in a world where varying degrees of dimensionality into human thought is recorded ~~and, ~~and, ~~you know, ~~in theory, something that you ~~can, ~~can be examined for a court case. ~~All that is, you know, I, I have a, ~~my instinct on that kind of a question is that.

[01:46:51] ~~Uh, ~~it's probably beneficial to a certain point, but there is an extreme where ~~like if you,~~ if everyone really has, ~~you know, ~~Neuralink and really,~~ like,~~ truly [01:47:00] every thought you have can be recorded ~~in a, ~~in a ~~high dimensional sort of, you know, uh, Uh, compression, probably~~ high resolution compression ~~rather probably don't want that.~~

[01:47:03] ~~Right? Like, ~~you probably don't want that ~~to be able to ~~to be available to certainly not ~~for I mean, ~~for advertising purposes. That's another, ~~you know, ~~saying, I think about,~~ um,~~ within limits. It seems fine. At the extreme, it seems. ~~Kind of ~~dangerous to have to figure that one out and ~~I think we will write like I'm~~ my general Sense is that from a policy perspective?

[01:47:22] Sure. I mean if this exists, the Europeans will regulate it, right? Can we know that and half of America at least will? What to regulate it, whether or not they'll be able to is a separate question. ~~Um, ~~actually, it's probably worth talking about just for a moment,~~ like,~~ what the current regulatory state of all this stuff is.

[01:47:39] ~~Um, ~~so these are not considered to be high risk medical devices by the FDA, by and large. ~~Um, the, ~~they ~~kind of ~~fall into this category to this mid level of risk, ~~um, ~~but there's an exception to that, which allows you to ~~sort of ~~evade most of the FDA's regulatory processes if you are [01:48:00] marketing a general wellness device.

[01:48:01] So ~~you will, like,~~ I remember ~~when the, ~~when the Apple watch came out with its, ~~it had that. Um, Yeah. ~~Blood oxygen sensor a few years ago, ~~Apple was, and ~~it was during COVID. It was like in 2020 ~~that ~~that watch came out. Apple was extremely careful. ~~And, ~~and blood oxygen ~~is, you know, ~~is an important measure ~~for, ~~for COVID.

[01:48:17] Apple was very careful to say ~~like. ~~This has nothing to do with measuring for COVID. This is purely telling you your blood oxygen level. Because if you connect the device to diagnosing or treating any specific medical condition, then all of a sudden you're in a whole different world from a regulatory perspective.

[01:48:38] Neural technologies have Generally gone for this general wellness exemption from the FDA. The FDA has not actually been clear that that exemption applies to them. They've been asked to make that clear and they have refused to do so, which is a common thing that the FDA and other aspects ~~of, ~~of,~~ uh,~~ American bureaucracy tend to do, anyone who knows cryptocurrency will also [01:49:00] be familiar with this, that like a regulatory tactic is actually uncertainty.

[01:49:04] That is a tactic. That's a form of regulation that regulators use is creating uncertainty and creating gray areas. ~~Um, ~~so that's ~~kind of ~~where most of these neural technologies exist right now. It's ~~kind of ~~sitting in a gray area. They're being marketed, ~~but, ~~and the reason that ~~the FDA and~~ the FDA does that, I suspect, ~~I mean, I'm not in their heads, but the reason that they do that ~~is maybe they don't know.

[01:49:18] Maybe they don't know how they feel about it. That could be true. ~~Um, ~~but also for sure, they probably want to preserve the optionality ~~to, if, to, to, ~~to cut back on this stuff if they want to, and to remove all the general wellness devices from the market in a heartbeat, but at the moment, ~~I think ~~If you could make a non invasive brain computer interface that can induce all kinds of conscious experiences,~~ um, ~~and as long as you're not trying to treat ~~or, ~~or diagnose a specific medical condition, and as long as you're staying below certain thresholds of, ~~you know, ~~DFUS and other things, like there's certain safety guidelines, but as long as you're mechanically underneath those things, then ~~like, ~~you can just sell this on the general market.

[01:49:55] That's the way, that's how it currently applies. ~~Um, ~~and ~~I guess, ~~The final part of your [01:50:00] question was about ~~sort of ~~long term dynamics of adoption. And yeah, ~~I mean, ~~I think if these things are cognitive enhancement devices,~~ uh,~~ then there will be ~~sort of ~~evolutionary incentives. There are evolutionary incentives to do all sorts of things right now that most people do not do, right?

[01:50:21] Like, we have enough data to know that I have an evolutionary incentive to go on a jog after this podcast. Will I do that? Probably not. That's the theory, right? ~~Like, ~~or that I should only eat whatever Andrew Huberman recommends. ~~Uh, ~~like, all that stuff is true, and yet,~~ like,~~ We don't do it, right? We don't do it all the time.

[01:50:37] We shouldn't be drinking alcohol. ~~Everyone, not everyone,~~ but a lot of people drink alcohol. So, I don't know. ~~Like, I think it is a mistake that a lot of people,~~ and I see this in the AI safety community in general, a lot of people think that ~~like, sort of ~~legible Darwinian evolutionary impulses. Are going to drive ~~technological adoption of society, or sorry, uh,~~ technology adoption in society.

[01:50:51] And~~ I just sort of think that that's like, ~~I think it's more complicated than that. I don't think it is a straight, ~~you know, Darwinian, ~~Darwinian evolutionary algorithm that's being applied here. [01:51:00] So ~~that's, ~~that's ~~kind of ~~my read on that. ~~Um, ~~And,~~ uh, ~~I also think that there's a flip side. I think the impulse that people have a lot of the time is to think, Oh no, if there's an incentive to use this thing, then everybody will, ~~um, ~~have to use it.

[01:51:18] And what about the people that don't want to use it? I go there too, right? I have sympathy for those people. I think about the Amish. ~~Right. ~~And I do think that there's probably going to be forms of digital Amish in the future that we need to be thinking about at the same time, the people who want to enhance their cognition.

[01:51:33] Also should have the liberty to do that. And we should want there to be more cognition in the world, especially human cognition. ~~Like ~~there's a part of me that says, my God, like the idea that we would want there to be less human cognition in a world where GPT 5 is right around the corner,~~ uh, ~~I am not sure from what world model ~~that is,~~ that impulse derives.

[01:51:55] But it is a role model that I could poke holes at. Not that it's wrong, but I could certainly poke [01:52:00] holes at it. ~~Um, ~~so that's like, my mind is not at all made up on any of this, but that is ~~kind of like, ~~I know you're,~~ um,~~ potentially just a couple doors down the hall from Robin Hanson, who's also a recent guest on the podcast.

[01:52:13] And ~~I think~~ he makes a extremely compelling case that we are in a, what he calls strange dream time between. What are almost sure to be much longer eras both before and after us in which evolutionary dynamics and just ~~the, you know, ~~the practical constraints of like available resources ~~are, ~~are in fact,~~ like, you know, ~~the dominant drivers ~~of ~~of how things unfold and we're in this weird moment now where we've ~~sort of.~~

[01:52:39] Created, ~~you know, ~~way more capital per capita and ~~like, ~~birth rates are down and, ~~you know, it's just like, ~~it seems like the sort of strangeness probably can't last ~~in the, ~~in the course of ~~like, ~~evolutionary time scales. ~~Um, ~~then he also, it's funny you mentioned the Amish too, because he also has ~~a, ~~a part of his near term world model, which I think is less compelling,~~ um,~~ personally, but,~~ uh,~~ you can debate that with him.

[01:52:57] Over lunch, perhaps,~~ uh,~~ where he [01:53:00] thinks ~~the, because of the higher birth rates, like, we may be headed for a period of, or ~~because of the lower birth rates in general, but higher among the Amish, we may be headed for a period of technology, stagnation and Amish domination. So ~~that, um,~~ that gets a little fine grained in terms of like, how precise the crystal ball has to be ~~to, um.~~

[01:53:12] To advance the theory like that for me. ~~Yeah.~~ But,~~ um,~~ there's definitely a couple of,~~ um,~~ interesting,~~ uh,~~ lunch debate topics~~ for, uh,~~ for you now at the Mercatus Center with him. ~~Um, ~~well, for sure. And ~~I, I mean, I, ~~I think part of his point is like, Not to put words in his mouth at all, but we have a lot of artificial hyper parameters on the way society works, and those things are called laws and policies,~~ uh,~~ and that's what I study for a living.

[01:53:34] ~~Uh, ~~and,~~ um,~~ yeah,~~ they've,~~ they've created all kinds of unintended weird effects and that's not to say they're bad or good, or really any to say that they exist and to say that things don't proceed according to mathematical models. ~~Uh, ~~of, ~~you know, ~~the way that history, nature has unfolded in the past, for those reasons.

[01:53:52] ~~Um, ~~because, ~~you know, ~~they didn't have occupational licensing a million years ago. ~~Uh, ~~and that does change ~~the way that, uh, ~~the way that evolution works at a, ~~in a~~ certain level. ~~Uh, ~~at least the evolution of [01:54:00] society. ~~Um, ~~and ~~I guess, ~~yeah,~~ like,~~ like this. One other point ~~about, ~~about the kind of digital Amish thing and just in general, my read on,~~ uh, sort of, you know, there's, ~~there's some polling that gets put out about AI and things like that.

[01:54:12] I don't put a lot of stock in issue polling personally. And ~~I think that those polls are not motivated.~~ I don't think they're coming from organizations that are motivated to find the truth. I will just put it that way about what Americans think. I think the questions are along the lines of someone's going to make how 9, 000.

[01:54:27] How do you feel about that? It's coming to kill you. It's gonna be here next year. What do you think? 98 percent of Americans are opposed to, ~~you know,~~ like, okay,~~ uh,~~ that's not that useful of a question. But issue polling is also not that useful of a field, unless you do it ~~really, ~~really carefully. But I do think ~~that this is going to,~~ that these things, adoption of technology ~~are going,~~ is going to start to have a political valence to it.

[01:54:48] ~~Um, ~~and whether that is coded as AI or whether that ends up getting coded in terms of these neural technologies or, ~~you know, ~~the Apple vision pro is the Apple vision pro a political statement [01:55:00] of a sort. Yeah, I see it as one already. I think a lot more people will in the future. So I don't know exactly how to model that, but ~~I do, ~~I do think that we should expect this all to become more political, not just about who gets to control the existing platforms, but about.

[01:55:15] Sort of whether you are interested in these technologies at all. Yeah, unfortunately, ~~I mean, ~~I would love to see the discourse around AI and technologies like this remain ~~kind of ~~Separated from ~~you know ~~day to day political discourse as long as possible just because it seems like everything gets worse once it gets cast through the Lens of certainly partisan politics.

[01:55:36] ~~I would agree with ~~I'm not sure I agree with ~~your ~~your view on polling and as much as I would say like My read of a lot of those answers is that ~~they, ~~I certainly would agree that they're subject to framing effects majorly. ~~Um, ~~so definitely by that argument, my sense of, ~~you know, ~~just the people that I talk to in life and ~~kind of~~ their, ~~you know, ~~outside of the bubble, when I get outside of the bubble, their kind of gut reactions to things is that it is ~~kind of ~~[01:56:00] negative by default.

[01:56:01] ~~Um, ~~and arguably that has been shaped by culture and fiction and the Terminator and ~~you know, ~~whatever. But I do think there is something quite real there that those Answers are getting at, though. I also do think they're coming largely from a position of ignorance, ~~you know, ~~certainly when you're just doing general public opinion polling.

[01:56:17] So I always advise people, ~~you know, ~~I don't think we don't have too many,~~ like, you know, ~~chachapiti novices listening to this show. But again, when I get outside the bubble,~~ uh, you know, ~~and present like business leaders or whatever, ~~you know, ~~I'm always just like, okay, first thing you got to do. Spend some actual time with the technology.

[01:56:31] Like you need to develop your own experiential understanding of what this stuff is. You can't just have it all be filtered through the media for you. ~~Uh, ~~because the surface area is just so vast, ~~you know, ~~and then the range of different use cases and ~~you know, ~~it's just so big compared to ~~like ~~what you can ~~sort of ~~get~~ in a, ~~In an article or whatever.

[01:56:49] So you really do need to get hands on with it, feel it, mess around with it, give it your data, ~~see how it re, you know, ~~see how ~~it, ~~it reacts to stuff that is really personal to you ~~and, ~~and if that's useful to you [01:57:00] and ~~you know, ~~that also gives you a great ~~strengths~~ sense of its strengths and weaknesses in doing that.


## Navigating the Future: Recommendations and Closing Thoughts

[01:57:03] Maybe in closing, I wonder if you could recommend some things that people might do to start to get themselves. acclimated to this merge,~~ uh,~~ technology tree. ~~We are, you know, ~~again, a lot of people follow this show will be very familiar with the language models. They'll have a sense for where AI is and, ~~you know, ~~some sense of where it's headed.

[01:57:25] I would guess that most people have Never worn any of these devices, ~~you know, ~~even starting with an Apple vision pro, I would still guess that ~~like ~~some 5 percent of the audience has even done the demo at the Apple store at this point. So what would you suggest? Is it like going and watching the demo videos ~~on, you know, ~~on the company websites that we can put links to these companies in the show notes?

[01:57:46] Is it going and trying an Apple vision pro is that, ~~you know, what, ~~what do people do to start to Orient themselves to this and start to develop their own, not just like through the media, not just by listening to you and me, but how do they get their own sense for how they should [01:58:00] start to feel about this technology?

[01:58:01] Yeah. ~~Um, ~~first of all, I would just say in response to the polling thing, I totally agree with you. ~~People, ~~people are pessimistic about this. ~~That's like a, that's, ~~that is a fact. ~~Um, ~~I think that ~~the, ~~the techno optimists like myself, ~~you know, ~~we have an uphill battle. The fight. ~~Um, ~~so ~~I always want to,~~ I don't want to suggest that the polling is like manipulating the reality.

[01:58:17] ~~Um, ~~I think it's exaggerating a tendency that already exists. That's how I would put it. Anyway,~~ um,~~ it's a very good question. I've never thought exactly about ~~sort of ~~what other people should do. I guess my somewhat myopic reaction is like the path I took ~~was, ~~was useful enough for me. There's a good book about all this,~~ uh,~~ that came out recently called The Battle for Your Brain by Nida Farahani.

[01:58:45] She's Done some great~~ report and sort of more ~~reporting almost ~~about this, ~~about this kind of stuff. ~~And, ~~and,~~ uh, I would, ~~I would recommend reading that. I actually would recommend trying on ~~a,~~ an Apple vision pro. And the reason for that is that you would be shocked how close your [01:59:00] gaze is to a kind of neural interface.

[01:59:04] And it is something we didn't talk about at all, but it is probably the case The ideal form factor for,~~ uh, uh, ~~any of this kind of neural technology we're talking about is probably something that wraps around the head, much as ~~the, ~~the MetaQuest and the Vision Pro do. And so when you start to combine the idea of, well, we've already got eye tracking and we've got hand tracking, And now we're adding in the neural technology.

[01:59:30] Maybe the neural signal doesn't need to be that good to do something really interesting, right? That's ~~another, it's~~ a whole different angle to approach. This is a sensor fusion. So that's just~~ one, ~~one note, but yeah, I would try a vision pro is by the way, a breathtaking experience. I was really floored by just the demo and I haven't bought one yet.

[01:59:50] Only because I'm not sure how much content there is and how much, ~~you know, ~~time I would really spend in it, but it was definitely an eye opener for [02:00:00] me in the sense that it was like ~~kind of ~~akin to a GPT 4 type experience where I have the Oculus 2 and, ~~you know, ~~when I first Got the GPT 4 access. I had been using a lot of GPT 3, but ~~the, ~~the step up in terms of the quality and the like, Oh my God, like this is just an arrestingly different experience.

[02:00:22] It is a similar, like you ~~kind of ~~have to experience it to feel the difference. ~~You know, you, I could, ~~I could tell you how much better GPT 4 is versus GPT 3, but the best way is to get your hands on. I would say the same thing. ~~If ~~even if you have done like relatively recent VR, but not done the Apple Vision Pro yet.

[02:00:37] I would say you do kind of owe it to your own worldview, if not necessarily to buy it, but at least to get that sense of ~~like, ~~what this thing can do, how high resolution it can be, how immersive it is, just how compelling the overall sensory experience of it is, because it is genuinely next level. And it, ~~you know, ~~absolutely feels to me like part of the future.

[02:00:58] We haven't quite figured out how to use it yet. The [02:01:00] experience of it is ~~like, ~~yeah, this is definitely going to be a thing. Yeah, no,~~ I,~~ I totally agree. ~~Um, ~~and I think that ~~it, ~~it's not entirely a neural interface, but it's also not, not a neural interface. ~~Um, ~~so I would say like reading that book, trying an Apple vision pro.

[02:01:13] ~~Um, ~~and beyond that, unfortunately, my answer is boring, which is always try to proceed from ground truth ~~of, ~~of actual empirically demonstrated knowledge,~~ um,~~ and not narrative. Yes. Because ~~I think~~ the narrative surrounding this technology in particular is likely to be quite toxic and quite misleading in a variety of different ways.

[02:01:33] So model it for yourself. That is ~~like~~ my best advice. Don't rely on someone else's model, including mine. ~~Well, ~~in the future technology scouting business, it is important to maintain always a high degree of epistemic humility. And I think that's a great note,~~ uh,~~ potentially to close on. ~~Is there anything that we didn't talk about that you think is important that, um, you don't want to bring up now, or we, you know, we could even maybe as an aside, we could insert something.~~

[02:01:53] ~~You know, earlier in the conversation, we don't, it doesn't have to be presented in the, you know, necessarily fully linear order that we had the discussion. Anything else on your mind? I mean, no, I don't think so. We, we talked, I, I was hoping, I was thinking we would go a little more into the vision pro, but I don't think, I think we, we, we actually went more substantively into the neural stuff and I think that's probably better for the purposes of this podcast.~~

[02:01:53] ~~So, um, yeah, no, the only thing is, uh, I definitely don't want to like, be mean to the AI policy, whatever group it is that does those polls, they annoy me because like issue polling, the problem with issue polling is that you don't get a, you don't get a sense of priority. So it's like, okay. I think AI is bad.~~

[02:01:53] ~~I'm mad about it, but I'm more mad about 5, 000 other things. And that gives you a sense of how important it is. Um, and like, you don't see it when you look at issue polls that like prioritize Americans, it's like, what issues do Americans care about? AI is not in the top 10. Um, so, you know, right now it's 98 percent because people don't really even understand what it is.~~

[02:01:53] ~~Um, and, uh, I mean, another way of putting it, like, the, the questions, the equivalent questions that I would ask, like, if I wanted to do with what I think, frankly, that that organization is doing, I would call people and ask things like, do you think the government should regulate your iPhone camera? And 98 percent of Americans would say no.~~

[02:01:53] ~~And then I would say 98 percent of Americans are opposed to AI regulation. Uh, but I wouldn't do that because I'm not interested in that kind of thing. But, uh, but like, I just don't know how much it advances the epistemic ball. That's all I'm going to say, but I don't want to be mean. So maybe don't include that.~~

[02:01:53] ~~Well, you'll have the option, um, to review if you want to as well before we, um, before we publish, maybe I'll, uh, give our classic closing. You ready for the closing? Let's do it. ~~Dean W. Ball, research fellow at the Mercatus Center and author of the Hyperdimensional Substack. Thank you for being part [02:02:00] of the Cognitive Revolution.

[02:02:01] Thank you, Nathan. 