# TX State – presentation for current college students


[00:00:00]  What~~ I thought ~~we would do today is. 

[00:00:02] ~~ ~~

[00:00:02] **GMT20240411-230847_Recording_1920x996:** ~~Basically~~ go through 

[00:00:03] ~~ ~~

[00:00:03] **GMT20240411-230847_Recording_1920x996:** a list of things that ~~I think~~ you guys should be thinking about as ~~I think ~~the workforce is in for some significant change and it is probably going to impact you the most ~~because of reasons I'll get into. ~~

[00:00:14] ~~And~~ so I wanted to explain why~~ I think ~~that's the case and then also explain what ~~I ~~you guys should do about it to give yourselves a competitive Advantage early in your career, which also happens to be early in the AI era. ~~Again, welcome to pipe in questions anytime. I don't really plan to spend too much time on my company, but if you have questions about that at the end, happy to get into it more.~~

[00:00:29] ~~But~~ this was ~~if you're thinking about it a little bit,~~ what I thought would be. The most useful for students today. ~~Sound good. Alrighty, here we go. ~~So I've titled this 10 super practical AI tips for current college students, or how to stand out in a world of infinite AI interns. So my first couple of tips are really around just calibrating yourself to what's out there today.

[00:00:49] I'm sure you guys have all used chat GPT ~~and stuff.~~ So forgive me if some of this is a little bit. Obvious or repetitive. I speak to all different kinds of audiences and you would be amazed by how many [00:01:00] people who, are actually very smart, accomplished leaders have remarkably little curiosity about this technology and little awareness.

[00:01:08] About just how far it has come already. So I always like to just set a little bit of a baseline by talking about ~~like~~ where the technology is today. So my first recommendation in terms of 

[00:01:20] ~~ ~~

[00:01:20] **GMT20240411-230847_Recording_1920x996:** what to do is understand current state of the art. This is a recommendation ~~that I give~~ to just about everyone.

[00:01:25] And, obviously a lot of ways to describe what the state of the art is, but the high level description that I tend to give is that, AI is closing in on human expert performance on routine tasks and the word routine is really doing a lot of work. AI is getting really good at things where we know what good looks like.

[00:01:50] And so a few examples of that, ~~I'll skip over this because you guys are probably well aware of that. Is that~~ we're starting to see this is on a benchmark called NLU, which is a set of exams from the [00:02:00] college and graduate school level from all disciplines. A typical human might get 35 percent of those questions, right?

[00:02:08] An expert in the domain of interest gets 90 percent right. GPT 4 gets~~ like~~ 86%, right? So it's closing in on human expert performance at these ~~like~~ discrete tasks where we know what the right answer is. Here's another one. This is a paper out of Google. Out of ~~a series of papers that are broadly called ~~the MedPalm series.

[00:02:27] This is MedPalm two in particular. This is performance on medical licensing exams. So it's again, another one of these ~~like ~~test like environments to pass is 60 percent score on the licensing exam, which by the way, should give you some pause as to ~~like ~~just how accurate your human doctor is going to be on a, any given topic that you might have for them.

[00:02:46] MedPalm one. got to 67 percent correct, and MedPalm2 blew past~~ the,~~ what you needed to pass, and again, closing in on that expert level, 86%. This task is specifically [00:03:00] medical question answering, and here they asked a bunch of human doctors, the same questions that they asked the AI. And then they asked human doctors to evaluate whether the AI answer was better or whether the human doctor answer was better.

[00:03:17] The top chart are the good things. These are like the desirable traits, ~~like~~ better reflects the medical consensus, better reading comprehension, et cetera. This is how often the AI was judged to have done a better job by human doctors as compared to other human doctors. So you see that overwhelmingly the AI is getting the higher marks.

[00:03:39] This is how often the human doctor was judged to have done a better job. And this is in between where it was considered to be a tie. Then these are the bad things. So these are the things that you don't want to have. This is the only category that the AI lost. And that is basically on hallucinations.

[00:03:54] I'm sure you guys are all familiar with the problem of hallucinations. Here they describe it as more inaccurate or [00:04:00] irrelevant information. And the AI did that more compared to the human doctor. But all the other bad things, the things that you don't want to happen in your medical question answering, the human doctors did more.

[00:04:10] So ~~the,~~ as judged by human doctors, the AI is beating human doctors on eight out of nine evaluated criteria. This is basically happening across the board. This is another version where they're doing diagnosis. Again, it's beating humans. There's like~~ just ~~tons and tons of examples like this. I know there's a lot of CS students in the room.

[00:04:34] Coding challenges are another area where AIs are now outperforming the majority of people who participate in coding challenges. They'll enter AI into coding challenges alongside humans. It won't be the number one contestant, but it will be above average. These are, pretty striking results.

[00:04:52] Now, one thing that the as are not really able to do yet, or at least very infrequently is come up with [00:05:00] genuinely new, really high quality ideas. ~~So~~ I call these Eureka moments. ~~And I would say,~~ I used to say there were no Eureka moments coming from AI. Now we've started to see a few of them. So I now say there are precious few Eureka moments.

[00:05:13] Here's one example of a Eureka moment. This is actually from a paper called Eureka. Oddly enough, I used to say that. And then they titled the paper Eureka. So it was like a very sort of meta thing. I was like, wow, they reading my mind's here. The task in this case is writing a reward function for a robot to learn how to do a certain task.

[00:05:33] So to unpack that means if you're trying to do reinforcement learning to teach a robot, to do a task. You need a reward function to evaluate how well it's doing on the task so that you can then train it to maximize that reward. A problem, though, is at the beginning when the robot is just fumbling around and making very little progress, you have the problem of what's called sparse reward.

[00:05:53] So that is to say, If the robot is coming nowhere close, then it doesn't get any reward, then it doesn't have [00:06:00] anything to learn from, right? It's so far off that we can't even identify oh, that was good, do more of that. It's just totally fumbling and flailing around. This is actually why, a couple years ago, OpenAI had a project where they were going to try to get AIs to browse the web.

[00:06:13] And they were going to try to do this reinforcement learning, but they abandoned that project because they found that ~~basically~~ it was making no progress. It was getting no reward. And so it had nothing to learn from. So when you're doing a project like this with robotics, you have to oftentimes write a custom reward function.

[00:06:30] ~~So~~ that is to say okay we know that you can't, in this case, it's like a robot's trying to twirl a pencil in its hand. Now, robot starts no ability to do that and it's not coming close,~~ right? ~~So you need a reward function ~~to say,~~ to at least proxy, to try to give some sense of like ~~how, ~~how can we detect even like the flicker of you getting close so we can reward that so we can hopefully bootstrap you into, learning this task.

[00:06:53] ~~You might,~~ if you were to sit down and try to come up with a custom reward function, you might think~~ like ~~maybe angular momentum around, a certain axis of the pen or [00:07:00] whatever, it could be ~~like~~ a leading indicator of. of possibly this is headed in the right direction. ~~It turns out in, ~~so humans do this, ~~right?~~

[00:07:05] This is~~ like~~ how it's done in robotics. ~~Like ~~humans will sit there and try to think what would be close and I'll write a reward function for this. GPT 4 is actually a lot better at writing reward functions for these tasks than humans are. And this is something that really only Experts do right like you don't have a lot of amateur reward functions If you go approach a person on the street and say hey Can you write a reward function with this robot?

[00:07:26] They'll just look at you like I have no idea what you're even talking about So it's not you can't even compare to an average person This is all compared to specialized people who like have expert, or at least specialized skills NGPT4 is perfect beating them and coming up with better reward functions, better able to train the robots.

[00:07:43] But you still see very few of these things where the robot is outperforming genuinely expert people at ~~like~~ new, Novel tasks. So that's important just to understand what can AI do today? Again, to summarize it is closing in on expert [00:08:00] performance on routine tasks, but still precious few Eureka moments, precious few genuinely new insights coming out of AI systems.

[00:08:10] When they do happen, they make news. Cause they're still very rare. Okay. Next understand the tail of the cognitive tape. This is why is it happening that way? And there's a lot of dimensions to this, but I'll try to go through it quickly. I break down all these different aspects of cognition and then~~ just~~ give each, a human expert and an AI as core.

[00:08:28] Again, this is not an average person, this is an expert that I'm evaluating here. So for breadth, obviously AI has ~~a huge advantage over ai. It's, or~~ a huge advantage over humans. It has read the whole internet, right? It's read all the books. ~~It knows ~~it can score not 86 percent on some of these exams, but all of the exams across all the subjects.

[00:08:44] It can speak all the languages, et cetera, et cetera. Humans in comparison and individual human is like super narrow humanity as a whole, it's more on the level of the AI, but that's one area in which the AI is like already pretty clearly superhuman. On the other hand, depth is still an [00:09:00] advantage for, ~~The human AIs or sorry,~~ the human experts, as opposed to the AIs.

[00:09:03] They're getting pretty good, but they're definitely not at the level of command of a subject like a human expert has. Breakthrough Insight, that's the same thing we just talked about with Eureka Moments. That is really, ~~I would say,~~ humanity's biggest edge right now in comparison to AI systems. So I only give the AI a 1 on that dimension.

[00:09:20] AI is a lot faster. It can typically generate content faster than we can read it. So that's~~ a, like~~ a pretty notable threshold. We can't even keep up reading what it's able to write. And it's also way less expensive than a human. And~~ I typically say ~~you can expect it to be like at least 10 times faster and probably at least 10 times cheaper.

[00:09:40] It's also super available and parallelizable, meaning~~ like~~ it's on 24, seven. It can just sit there and do nothing until you come back to it and say, Hey, I have another question for you or whatever. And it doesn't cost anything to do that. It only costs something when you're using it. If you want to, you can also spend up 10 or 100 at the same time, which obviously you can't do for humans.

[00:09:59] So these [00:10:00] are probably AI's biggest advantage. Memory~~ is a, ~~is as much as our memories fallible and imperfect. Our memories are definitely still way better than AI memories. We have this sort of integrated sense of who we are, what we're doing, what our purpose is, what our long term goals are, how what we're doing now fits into the big picture.

[00:10:20] That's all very intuitive for us. The more I study AI systems, the more impressed I am with human memory because Today's batch of AI systems don't have that. They, at best, can like search on the internet and find stuff. Of course, they know a lot internally. They have a good ~~like~~ long term factual memory in some sense.

[00:10:38] But if they need to go query information on the fly, it's a ~~rough, it's a~~ brittle process. And they also have ~~like~~ finite working memory. They're pretty good in that working memory. But outside of that, things start to get tough. I'm sure you guys have seen systems like RAG, Retrieval, augmented generation, ~~tell me if that's not a familiar term and I can go into that a little bit,~~ but that's just one of the ways that people are trying to improve AI memory and it's very much [00:11:00] a work in progress right now.

[00:11:01] Technology diffusion speed. This is going to be a big focus of the talk. Actually, ~~I think~~ it's a big part of what you guys should be thinking about. Because humans are not great at learning new technologies, especially people who are mid and late career. They are going to be pretty reluctant in a lot of cases to embrace a new way of working.

[00:11:24] Of course, everybody's heard the saying, you can't teach an old dog new tricks.~~ I think ~~that is actually key to a lot of the opportunity that I see for young people entering the workforce to take advantage of AI in a way that their, more senior co workers may be, for whatever reason, not inclined or not able to do.

[00:11:43] AI, on the other hand, though, does take advantage of this stuff really quickly. When somebody figures out a new way to do whatever, whether that's a new memory technique or a new optimization, something that makes it faster, something that makes training more efficient, whatever. These things spread super quickly because a lot of times it's like, Oh, okay, [00:12:00] cool.

[00:12:00] You found a way ~~to opt, ~~to optimize the learning process. So it works 40 percent faster. I'll plug that into my system. Boom. Now everybody can take advantage of that. Broadly speaking, most of the research has been published. These days we are in a period of closing. So due to the fact that.

[00:12:15] The human researchers are actually now sharing fewer of their breakthroughs, especially out of the top labs, like your open AIs, your, anthropics and deep minds. The pace of diffusion in AI might be actually slowing a little bit, but it's still pretty fast. And here's one paper. ~~I can share this presentation.~~

[00:12:30] ~~Of course,~~ if you want a sobering read, you might consider natural selection favors AIs over humans. ~~That's a~~ that's like an academic paper. Okay, bedside manner is another one. People often think you could never have an AI that could match the, the warmth or the empathy of a human.

[00:12:44] On the contrary, that's not really the case. The AIs are extremely patient. ~~They are, ~~you can get them to behave weirdly, but by default, they're ~~like~~ actually quite nice, quite patient, quite understanding, very willing to explain things to you. And indeed, like we see this in the medical system, doctors are overworked, they're [00:13:00] stressed out, they don't have time, but the chatbots, they'll explain things to you 10 times and they'll be polite, every single time it's,~~ this is ~~maybe a little bit of a, radical position for me to give the AI a higher mark on bedside manner than humans.

[00:13:13] But certainly, at least in some ways, it is true. And then this final one, maybe alongside Breakthrough Insight, is probably the biggest weakness of AI relative to humans. We are~~ like, ~~broadly, pretty robust to crazy stuff. If a crazy person comes up to you on the street, you get this quick sense of wait a second, I think I'm dealing with a crazy person here.

[00:13:34] If somebody's trying to scam you, the alarm bells go off pretty quick. And something about this doesn't seem quite right. Your guard goes up. And, you're scrutinizing these inputs at a different level. AI systems don't really do that very well. So they're much easier to trick.

[00:13:49] We see all these ~~like~~ jailbreaks, we see them divulging information. They're not supposed to divulge. We see, chatbots on car dealer websites, agreeing to 1 car sales. All these things are [00:14:00] because AIs are not very adversarially robust. ~~This even extends to pretty advanced.~~ This is not just a chatbot phenomenon.

[00:14:06] I'm sure I've heard of AlphaGo, the AI that, is~~ like~~ the world record best Go player in history. It's broadly considered to be superhuman at playing Go. ~~And yet ~~I recently did a podcast episode on this. If you want to learn more about it, it's from a group called FAR AI. They found a way to create an adversarial attack on AlphaGo and consistently defeat it.

[00:14:28] With a strategy that a human would never lose to. I don't play Go, so I don't know a lot about that, but they basically found that~~ there,~~ it has major blind spots, even these superhuman Go players. When they're able to adversarially optimize against it, they were able to find ways to beat it. And again, ways that a human will look at that and be like, You lost to that?

[00:14:47] That's crazy. But the AIs just have these big blind spots. They are not adversarially robust. ~~I think~~ it's really important to keep these strengths and weaknesses in mind, because, You want to play to your strengths as a human, [00:15:00] and you don't want to be competing with the AIs in the ways that they are superhuman.

[00:15:05] A lot of people cash this out~~ to, ~~and this is~~ like~~ the kind of advice that people, mid career are oftentimes getting these days, and including from me. You can think of an AI as a day one employee who's~~ like~~ pretty bright, eager, hardworking, but doesn't know anything about your business.

[00:15:22] Totally lacks context, ~~right? ~~And can also make really weird mistakes. You gotta be super careful to spell everything out for them, give them super clear directions, show them what good looks like, all these sorts of things. But ~~basically,~~ if you do that, then you have infinite interns. Or in a coding, CS type of context, people will say ~~you,~~ you have infinite entry level software developers.

[00:15:44] So that is ~~like~~ a radically different work working environment that you guys are entering into as compared to anyone in human history, ~~right? Like~~ never before did any software firm have anything where they could say Oh yeah, we have infinite junior coders, or we have infinite [00:16:00] interns. That's never been a thing, but it is now starting to be a thing.

[00:16:03] And I was just ~~at a,~~ at an event not too long ago where people were like, yeah, we're not hiring as many junior coders anymore because we're really just focused on making our senior people more productive. And we think that with all these tools and ~~the, ~~the infinite interns that we can give them ~~that, that's a more, ~~that's a more advantageous strategy for us.

[00:16:20] It's going to be higher ROI for our business. So I'm definitely not one to ~~like ~~sugarcoat things. I think this is a real challenge for a lot of people entering the workforce. You're now competing against infinite interns, infinite entry level coders, and you want to make sure you are angling toward human strengths and away from AI strengths.

[00:16:42] Okay, three, understand the modes of AI production. This ~~is ~~I think there's just a ton of confusion. And so I like to try to clarify this for people. There's basically three ways that I see people. Working with AI systems today. One that's like very familiar is often called co pilot mode.

[00:16:58] Microsoft uses that [00:17:00] term for their product, but this is basically the chat GPT experience where you as the human are doing your thing. You might be doing your, coding work.~~ You might be doing whatever you're doing.~~ You might be writing a letter. You might be, putting together ~~a,~~ an analysis.

[00:17:12] You might be brainstorming a list of things, but at some point you think, Oh, what I could do is I could ask AI for help. And then you go over to it and you put in some instructions and it gives you something back and you look at it in real time and you think, Oh yeah, that's good, bad, whatever.

[00:17:24] I can use it. Maybe not. There's a couple of good ideas there. This is the real time back and forth mode where you are the pilot. It is the co pilot. And ~~I think~~ this is definitely something, as you'll see one of my later things, like you want to get good at this. This is only one of the three modes though.

[00:17:40] The other mode, which I think is actually going to be even ~~more ~~more important in some ways in business context, especially ~~like~~ big business contexts is what I call delegation mode. You could also call it task automation mode. And it is the idea where you're not just interacting real time ad hoc, haphazard.

[00:17:57] Instead, you're saying, let's find some bottlenecks in [00:18:00] our business. Let's find some things that right now we have to put a lot of time and energy into. And, ~~we really, ~~maybe we really have to manage people very carefully to get the quality and consistency where we want it to be. And maybe this ~~like~~ inbox is overflowing and~~ we, ~~what are we going to go hire a bunch more people to do this thing?

[00:18:16] Or maybe we would like to 10 X what we're doing, we just don't have the resources to do that. Identifying these sorts of tasks and then setting up a system. For AI to do that and to do it consistently in the broader context of the business. That's what I call delegation mode, and the key thing there is you want to get to the point where you are not evaluating every single AI output anymore.

[00:18:39] When you're doing co pilot mode, you have to evaluate every single output, ~~right? You're, nobody's gonna, ~~you would be very unwise to just take the output of chat GPT and turn it in as your paper or turn it in as ~~your, ~~your coding assignment. There have been examples, I'm sure you've seen them, where lawyers have done that.

[00:18:51] They thought the chat GPT didn't make mistakes or whatever. And so they turned in a brief to the court. And some of those guys, ~~I think,~~ have lost their license because That's just [00:19:00] outright malpractice. You have to review what you're getting out of copilot mode. But if you do your setup right, then you can get to the point for many tasks where the AI can consistently do the task, and then you don't have to evaluate every single one anymore.

[00:19:16] You can get to the point where you actually can trust it. There's a trade off there where you're dialing in, you're narrowing the scope. You're zooming in a very particular problem. You're setting it up. You're controlling what the inputs are going to be. You're working through it. You're ~~like~~ testing a bunch of inputs to make sure that the outputs are what you want.

[00:19:30] And then at some point, depending on ~~like~~ how important it is, how high risk it would be if it did make a mistake ~~at some point,~~ you can say, actually, this feels~~ like~~ good enough now that we can actually use this as a process in our business and not have to supervise it. Every single time. So that's delegation mode, and that's going to come back again in a minute, too.

[00:19:47] And then in the middle, ~~we have the sort of missing middle, is ideal, ~~the best of both worlds would be like, if you could delegate in real time, like you do in copilot mode, but you could trust the results, like you can get to with work in delegation mode, this would be ~~like~~ the [00:20:00] dream of agents, right?

[00:20:00] This would be like saying, hey, I just had this idea. Oh, by the way, can you go out and find 20 Websites of businesses that offer whatever, accounting services in Detroit, Michigan, where I'm based and, go look at the rates that they have and then put those into a spreadsheet and come back, write an analysis of all that and come back to me when it's done.

[00:20:22] The sort of mid scale problem that you would give to a person, if you can do that on the fly and actually get good results back. Now we're into agent mode and that's not quite there yet, but it is coming quite soon. The general consensus in the field is that the next big open AI release, whether that's GPT 4.

[00:20:45] 5 or whatever, is probably going to power a lot of those ~~like~~ multi step, multi app Agent use cases, but that's not quite there yet. But ~~I think~~ it's pretty safe to say it's coming. Okay. So those are the modes. So just getting clear ~~on,~~ on what [00:21:00] mode you're actually using and being able to talk about that and ~~have, ~~have conversations where you're helping other people understand what's going on.

[00:21:05] Having this conceptual framework in your head, ~~I think~~ it's super useful to do that. Very simple tip, but super important. Definitely use the best available AI's if you're using free chat GPT, ~~like ~~Not good enough. It's absolutely worth the 20 a month. And if you have to go do a Upwork project, ~~to~~ to get the 20 a month to do it, like again, it's absolutely worth it.

[00:21:25] There's no The reason really to use anything other than one of the top tier models. Top tier models today are cloud three GPT four. Actually, when I wrote this two days ago, cloud three had taken the top position. I would now say GPT four, which just released a new version of this goes to show off quick.

[00:21:42] The leaderboard can change now. GPT four latest version is probably top again. Cloud 3 is great, it's very good for writing, and Gemini Advanced, Gemini 1. 5 is also extremely good, and Google is very much a live player in this game. There are a couple of open source options [00:22:00] that are decent ~~even very good~~ they're all very good compared to what we had even a year ago, but, I tend to stick with the very best tools and I don't mess around too much with anything else.

[00:22:09] I basically use nothing other than these three tools. If you're going to go into the open source, you're into hobbyist land. You also have to figure out where you're going to run it. Like the very best open source models are also big or you can run that on your laptop. It's just a pain in the butt.

[00:22:22] I would generally advise sticking with the best tools. This is, by the way, also my advice to ~~like~~ business owners. It's buy the best tools for your team. Don't cheap out. Don't be a cheapskate. I would say for you guys, don't work at a place that isn't willing to buy you, the best tools either.

[00:22:37] Alright, so now we get into the modes mastering co pilot mode. I think you guys are probably all well on your way here. ~~I have a slide on should have actually shown this slide before, ~~everybody has different frameworks for prompting best practices. But they largely end up being the same.

[00:22:51] OpenAI has published their official guide to prompting. Anthropic has their official guide to prompting. I have my official guide to prompting. But they're basically all saying the same thing. It's [00:23:00] you want to make sure your instructions are very clear. One of the great rewards, especially in a software environment of working with AIs, is that it forces you to take a breath before just, and I used to do this all the time, and I'm sure some of you do too Oh, I'm going to code code.

[00:23:14] Just immediately I'm starting to type code. Class, whatever. Have I even thought about what I'm trying to do? I personally struggled with that in the past. Working with AI helps me there because I can't expect the AI to do anything for me until I've articulated what I want. And I have to do that pretty clearly or I'm going to get something in the general direction of what I want, but not really what I want.

[00:23:34] So everybody always, is going to say you're going to need clear, accurate, unambiguous instructions. That's an art in and of itself, definitely something worth practicing, probably pretty clear to you guys already. The next big thing that is super important or super useful is, especially if you have examples, some things are easier shown than told.

[00:23:55] So give it examples of what good looks like. In some ways, this is like the thing that has [00:24:00] unlocked the current AI era, the GPT three paper title was large language models are few shot learners. Few shot learners means that if you give it a couple of examples, it can pick up on the task and it can begin to do the task.

[00:24:12] Even if you didn't describe the task, just based on the examples that it sees, that's a pretty profound thing. Like no AI system before GPT 3 could ever do that. Certainly not in a general purpose way. So we're only~~ like~~ two years into the, Few shot and learning era. And it's a huge advantage for many tasks instead of trying to tell it exactly what to do, if this, then that do this, then that so often you can just be like, here's the sample input.

[00:24:37] Here's what a good output looks like. Give it a few of those and you'll get, much better results. Again, I think this is probably fairly obvious. So I don't want to waste too much time on prompting. You can also get good results by telling it what role you want it to play. That can be like a professional role.

[00:24:51] You're~~ my, you're~~ the senior software engineer supervising my work. Give me feedback on it that, your job is to do a code review of my work, that kind of thing. You can also [00:25:00] do~~ like~~ specific names. Sometimes when the AI is right in a very~~ like ~~verbose kind of flowery, wordy way, I'll tell it, I want you to think Einstein Hemingway.

[00:25:09] That's my phrase. And then sometimes I'll also, so Einstein Hemingway that's your role. It's this mashup. I don't want super smart, but I also want terse, crisp, clear, simplest possible language. Sometimes I also say, we want to demonstrate our intelligence in part via economy of words.

[00:25:24] You'll get a very different style of writing out if you say you want Einstein Hemingway versus just letting it write in its normal default way. And I personally strongly prefer the Einstein Emmy way. Other simple things I'm sure you've seen before, labeling your data giving the model time to think.

[00:25:39] That's ~~like~~ chain of thought, explain your reasoning. These days they tend to do that by default. You can also tell it ~~like~~ exactly how you want your answer formatted. Use the format and literally give it a template for how you want it to return information to you again I imagine you guys have seen this sort of thing So everybody has their different framework on prompting you want to be good at this when you get a [00:26:00] job Likely your boss will not be good at this Possibly they will most of the time they won't so even though this stuff is like fairly basic.

[00:26:09] It is a huge advantage relative to not knowing how to do it. So ~~just, ~~a relatively simple crash course on this kind of thing can be worth a, huge amount in terms of the ROI, ~~return on.~~ And not just money, but time, energy,~~ like~~ you'll just get way better results from AI.

[00:26:25] Now, AIs are also starting to get pretty good at this. This is the latest thing from Anthropic. They call it the Metaprompt. Basically what you do here is go to a collab notebook that they provide and give it a prompt and it will give you~~ like~~ a way better prompt based on your initial prompt. So it's ~~like~~ a funny, meta thing where the AIs are already starting to help us prompt.

[00:26:44] Crazy, but I've used this and it definitely really helps. It flushes it out for you and you can read ~~like~~ all the ~~detailed,~~ more detailed instructions and you can start to refine them. You'd be like, Oh, that's not exactly what I meant. Tweak. Yeah. Very useful

[00:26:55] also in co pilot mode, definitely getting used to these coding assistants. I'm sure you guys have all [00:27:00] used some, or maybe all of these things. GitHub co pilot is ~~like ~~the most common one. Codium AI,~~ I think~~ is a really interesting one. That's lesser known, but they specifically focus on code integrity.

[00:27:10] So they help you generate unit tests. They do all these sort of type checking things, all the things that ~~like~~ you get dinged on in your homework, that you might also get dinged on in a code review in your job because like somebody with a stick up~~ their ass~~ is~~ like~~ telling you, oh, you got to do this every time, whatever.

[00:27:23] And you're like, oh, do I really care? Whether that really matters is debatable and definitely depends on context, but that's what Codium is there to help you do. And it's quite good at it.~~ I personally believe.~~ I've never been great at that stuff, and so I really like Codium because it comes around and cleans up my mess and reminds me of the things that I should be doing that I wasn't doing.

[00:27:40] And Cursor is like the next, evolution on Copilot, where it's ~~like~~ an AI first coding environment. How many of you guys have used Cursor, just out of curiosity? No Cursor users? Oh my god. Okay get Cursor. Definitely try it. It blows people's minds with how helpful it is. Okay, cool.

[00:27:56] And ~~that's~~ if you get nothing else out of this, you should all go try cursor. [00:28:00] Obviously you're going to want to watch out in co pilot mode for hallucinations and other mistakes. AI makes mistakes, makes a lot less mistakes than it used to. It's probably going to continue to make fewer mistakes in the future, but it does still make mistakes.

[00:28:10] Okay, next one distinguishing yourself with delegation mode. I have a whole presentation here on AI task automation 101. This is something that ~~I think ~~young people entering the workforce can literally blow their bosses minds with. And even crack into places that you might not otherwise be able to crack into by doing a little project like this and demonstrating to them what's possible.

[00:28:35] ~~Cause people don't, these,~~ the reason these are tips, cause people do not understand the state of the art. They do not understand the relative strengths and weaknesses. So if you have that and then you can bring some task automation to them and show them what's possible. A lot of times ~~they will,~~ there'll be literally mind blown by what is possible.

[00:28:49] And the fact that you can do it, they're going to look at you. Like you have magic powers. So we don't have time for the whole task automation thing, but it's often done with no code platforms. ~~If you guys, ~~many of [00:29:00] you are in CS, like you shouldn't have any trouble using these no code platforms, but one thing to really understand about organizations, like how businesses really work is that a lot of times their processes Are not documented and not really formal at all.

[00:29:15] There is a way that work gets done, but nobody ~~like ~~actually sat down and designed that with a flow chart. So this is why I say in this presentation, these process diagrams do not exist. You guys may remember ~~the,~~ this person does not exist. It was~~ just~~ like a classic. One of the ~~early ~~early ~~AI ~~AI things you can go to this person does not exist.

[00:29:32] com and it just makes a new person for you on every page load. This is from ~~I think ~~a GAN, ~~like~~ a generative adversarial network that just makes headshots. And all these people are fake and it's~~ like a ~~mind blowing, wow. ~~This is the style GAN too, is making all these things. ~~So whatever that's an aside, this isn't ~~a~~ a sort of reference to that, but the key thing to understand is businesses have implicit processes. Nobody really designed it in a lot of cases, nobody really documented it, but just one person knows I do this and then I hand it off to them and then they do something and then it [00:30:00] goes wherever. And people know where they fit and they know what their responsibility is.

[00:30:03] They don't necessarily have command of the bigger picture. So a lot of what you have to do to be successful. Is map out these processes they exist implicitly, but ~~there's~~ nobody's ever really gotten specific about like, how actually do we do this? So just understanding that that's the state of play and you're going to probably have to go figure that out, map out this territory and figure out What are the inputs?

[00:30:25] What are the outputs? What's happening in between? Whose responsibility is it? I use these terms, input logic and outputs for the AI portion. But then there's also when does it happen? What causes the process to start? What happens at the end of the process? ~~All these things are, ~~you have to answer these questions.

[00:30:40] If you do that, and then you find one of my other little best practices is prompt before app. What I mean by that is the first thing you want to do is Understand what this core task is and demonstrate that the AI can do it. Work on the prompt, make sure the inputs and outputs are working, work with [00:31:00] whoever kind of owns the process to say give me 10 inputs and 10 examples of what good looks like.

[00:31:04] And let me see if I can match that with the AI. And if I can do that, and possibly, by the way, you might just take their 10 examples and use them as few shot examples and just say Hey, AI, here's nine. Can you do the 10th? And just see maybe that's enough. It can often be quite simple. Sometimes it takes more work, each case is different.

[00:31:20] But if you can get that working, Then you can do all this other stuff with the no code platforms and the triggers and the automations and Zapier. And maybe you have to write a little custom code at some point in the Zapier zap to get it to work. But if you can get that core thing to work, then you can build the process around it.

[00:31:35] You can pipe it into where it needs to go. You will blow people's minds. I think ~~this is going to be~~ one of the most common. New AI jobs is just automating existing processes with AI. And these are not like sexy things. A lot of times they're things that nobody really wants to do, write the first draft.

[00:31:52] And sometimes there may still be a human in the loop as well. Maybe it's just we get a ton of customer service tickets. Nobody really enjoys answering those tickets. [00:32:00] Can you write the first version of the response to the tickets? Here's~~ like ~~a hundred examples of what, we've done in the past.

[00:32:06] If you could get that to work though, in an environment where nobody else knows how to do that, you are immediately a difference maker. And that can open doors into context where you're going to have remarkable access to expertise, because these people do have something, that they're bringing to the table, but it's not AI task automation.

[00:32:23] And they will love you for it. If you can actually bring this. And make it work for them. And one of the reasons they're going to love it is it's going to be a lot faster and it's going to be a lot cheaper than having a human do it. ~~So I'll put while I'm thinking about it, I'll just drop these links into the chat.~~

[00:32:32] ~~And my chat go, there it is. So there's that presentation and then this one as well.~~

[00:32:32] ~~All right, cool. ~~So there's more, question, please. ~~Hear from here. Okay. It is coming through because I'm like far away from the mic, so that's why I'm asking. It's a little choppy. Yeah. Speak up. But yeah, that's what I was asking, like whether the audio is fine, but from back there but~~ I ~~did~~ have one question.

[00:32:36] ~~What? ~~A lot of the time, people ~~in ~~in their degree in CS right now, they're doing a bunch of data structures type stuff, and it's oh, what is a stock? What is a queue? Like, all that sort of thing. And ChowGBT and, AI tools are really good. They understand a lot of that already.

[00:32:49] And I feel like there's this gap between what most students are spending a lot of their time on in classes and, ~~like,~~ all the stuff that you're talking about is all new. ~~It's like how to, ~~but none of this is in university. ~~I guess~~ my question to you is, [00:33:00] Given the fact that ~~you've talked,~~ you've spoken with people in industry, what do you think the right balance is?

[00:33:04] Is it like, just stop just do the bare minimum for school? Or is it like, 50 50? ~~Does that kind of make sense? ~~Yeah, that's a good question. School's a little fucked up, to be honest, in a lot of cases. ~~In the real world, there's no, I'm a ~~simultaneously I'm a believer that there's no world more real than the one you're living in right now.

[00:33:20] It's not there's some like mythical real world out there that's totally different, but. In the, work for pay world which is definitely different from school. Nobody cares, about using chat GPT. There are maybe a few ~~like~~ idiosyncratic people out there who are purists or who have extreme data security issues.

[00:33:39] There are a few legitimate reasons to think that you should never use a certain tool like this. Even then ~~you can, ~~I would say. Data concerns are fairly overblown. And there are ways to use these tools that protect your data privacy, but by and large, people just want the job done.

[00:33:58] They want it done well, they want it done [00:34:00] fast. And if you can deliver that, and AI is part of that recipe. Then that's a win. So I don't really know, what that means for school. ~~I would not ~~it's hard to give advice. Cause like you need to pass your classes, ~~we're not in, ~~Actually ~~hell ~~you could drop out.

[00:34:13] And if you're sweet at AI, ~~there's~~ there's infinite opportunity, like the AI world does not care. If you have a degree, if you could automate tasks effectively, I do believe that there is a lot of opportunity that is, increasingly independent of a degree. But I'm not telling you to drop out.

[00:34:28] And if you're not going to drop out, you should pass your classes. And if your classes have like final exams that involve do this, programming task and you're not allowed to use AI, then ~~I think~~ that's a little retro, but it is the reality. And so you got to do it, but I don't code by hand really any more at all.

[00:34:45] I pretty much only go to an AI, describe what I want, Sometimes I will, curate context. So I might bring ~~like ~~documentation from something that I'm using, or I might, if it's a code base that I'm already working in, I [00:35:00] might, copy one class in and another class in and be like, Hey, I want to.

[00:35:03] Make a new method that does this. I don't want to implement the caching pattern from over here or whatever. And it's a much better typer than I am and frankly, a better coder as well, but the typing alone, it~~ like~~ saves a ton of time, just to have it generated quickly.

[00:35:19] So yeah, ~~I don't really, ~~not knowing enough about ~~the, ~~The specifics of the context of, the classes and the requirements,~~ whatever.~~ I can't say with confidence, like ignore this ~~or don't, ~~or don't do that. But I will say, generally speaking, people don't care if you're using AI, they want, the classic thing is like good, fast and cheap.

[00:35:40] You can only pick two. And AI is starting to break that paradigm, or another way to say that is~~ of course, like ~~good, fast, and cheap will be redefined. So you can still only pick two, but relative to traditional good, fast, and cheap, you can only pick two with AI. I feel like I routinely deliver good, fast, and cheap, and people don't have to pick.

[00:35:59] And if you can deliver [00:36:00] that people will love you for it. And nobody's really like, Oh, it's cheating to use chat GPT. ~~Nobody has that ~~very few have that attitude. And I probably wouldn't work at that sort of place because unless there's a really good reason, like we're a military contractor or, whatever.

[00:36:15] You can come up with reasons, but unless there's a really good reason, if it's just like the boss doesn't like it or something, then I would be like, eh, this is. Yeah that's great. Thank you. You bet. Another thing, by the way, this is like the easiest thing in the world, but in so many organizations right now would radically change how they do things.

[00:36:32] Clawed for sheets. It's just a integrated API call ~~that you,~~ that they've wrapped up into a typical spreadsheet function. And it's ~~fucking~~ amazing. It can structure data for you. It can answer questions for you can fill in gaps in data. It can do all sorts of, things that you might want to do.

[00:36:50] And it takes two seconds to install. And especially with their cheap version, it's like insanely cheap too. They even have a caching layer in there. ~~It's all ~~it's extremely useful tool. And this is [00:37:00] like 0. 1%. Of businesses have probably installed cloud for sheets right now. If you literally just went in and we're like, Hey, if we thought about using cloud for sheets, yeah.

[00:37:10] What's that? Nobody will know. And ~~it's like an,~~ it's immediate alpha. Okay. Mindful of time, scout all applications. I describe myself as an AI scout. I literally spend all my time trying new stuff, reading research, trying to understand what's going on from all angles, definitely find it remarkable.

[00:37:26] How often people just can't be bothered to try a new thing. Yeah. The fact that all of you, that nobody raised their hand for cursor is a little bit of a warning sign for that. Go try some of these new things. There's an AI app for everything these days, and anytime I have an unfamiliar task that I haven't really done in a while, whether it's like making slides, for example, ~~or~~ these slides actually predate Gamma getting quite good, but Gamma.

[00:37:52] app is a really good little slide maker now. If I'm editing video, Descript has all sorts of cool AI tools for helping to edit video. [00:38:00] Suno and this other one, Udio, make unbelievably sick music. Listen to this. ~~Hold on one second. Did my sound?~~

[00:38:05] ~~Okay. Listen to this.~~ This is AI.

[00:38:06] Turn the data true to light, AI. Cut the code just like a knife, so fly. Maybe this bot's got that blaze. It's got teaching machines the human way AI. Learning deeper than the sea, no lie. Giving robots that fresh shot.

[00:38:27] I don't know about you, but like it's getting good enough that I actually would listen to it for just enjoyment of music. Like it's not even just a novelty anymore. The lyrics are honestly pretty good too. My favorite part is when it goes to deep minds and ~~it's ~~this was made by a friend of mine, but I asked him to did you ask for that?

[00:38:44] And he said, no, I basically just did nothing.~~ And that's it. ~~I just gave it a quick prompt. It took no effort. So it's definitely worth going out and trying a lot of these new products. There's a million of them popping up all the time. ~~And it's,~~ again, it's something that other people won't do.

[00:38:56] And ~~it's something, ~~it's certainly something that~~ like~~ mid and late career people [00:39:00] won't do. So it's something that you with~~ your ~~your youthful energy can go out and do ~~that. ~~Other people will find ~~like~~ remarkable that you do that. So~~ I think~~ it's an extremely easy way to get an edge and, you can just again, routinely blow people's minds.

[00:39:14] I'm sure you guys are familiar with Repl. it at least somewhat. Their Ghostwriter product is pretty cool. Watch that space because~~ I think~~ they're going to have a lot more stuff coming soon. I've been trying Julius recently. It's another kind of ~~coding, ~~real time coding thing. I gave it a prompt earlier today.

[00:39:28] Maybe I could just show this how easy this was. I just went to it and said ~~I wanted to, yeah, okay. Look how easy this was. ~~I wanted to get audio of off a video simple as that. But all I had was the YouTube URL. So I said, can you fetch the audio from a YouTube video and give me an MP3? It wrote the code, executed the code, gave me a download link, worked flawlessly first time.

[00:39:51] No problem. Amazing. ~~This. ~~Now, why would I do this instead of chat GPT? First of all, I'm not sure if chat GPT would do it. It may or may not refuse me. So that's one [00:40:00] issue. Second. Chatsubd can execute code, but~~ it's a little bit,~~ this is like even more robust code execution as part of the process. If you went to Claude, it might write you this code, but then you'd still have the question of like, where am I going to execute this code?

[00:40:11] And maybe that's no problem for you because you've got like development environments all over the place, whatever other people don't. The ability to just go do this in a browser and get this done in 15 seconds, how much of a pain ~~in the ass~~ would this be? I'm sure you've seen Devin as well.

[00:40:23] Who's seen Devin? A couple hands. Okay, cool. Yeah. Devin is a coding agent ~~where it~~ actually this is the beginning of the agent moment. ~~From what I've heard, ~~I haven't used it yet,~~ but from,~~ cause it's like still waitlisted, but from what I've heard, it's like, Getting good, but not quite good yet.

[00:40:35] But ~~it's ~~you can start to see how it's happening. It will actually hit a bug, not know what to do. Go to the documentation online, read the documentation, come back and try to fix the bug based on reading the documentation. ~~It's ~~it's starting to run this actual loop process of just keep going.

[00:40:49] Just cause I failed doesn't mean I'm done. Just like a person, I'm not going to just give up because I hit a first bug. I got to keep going. I got to try something else. I got to go find some new information. I got to find another approach. Maybe I got to change [00:41:00] my plan. That's what Devon, and there's also an open Devon, which you can go download and run on your own environment, another one coming soon.

[00:41:07] It'll be magic. dev. Brush past that for now, because nobody really knows what it is other than that it's allegedly a huge deal. Okay. Almost there. And then I can do a couple of questions. ~~I think~~ I've covered most of it. This evals expert concept is pretty similar to, or it's at least intimately related to delegation mode, but it can apply in a lot of contexts, but basically is this AI thing working?

[00:41:31] How would we know if it's not working or if something changes and it's not working as well as it used to be? What are the ~~like~~ things that we want it to always do? What are the things that we want it to never do? And how are we sure that it's always never doing that,~~ right?~~ It becomes quite challenging.

[00:41:44] So setting up, this is basically like a unit testing type of thing, except you're doing it potentially on an ongoing basis for all the inputs and outputs of these systems. And this can really help you ~~like ~~assure yourselves and assure ~~like the, ~~the company that you're working with [00:42:00] that This is actually working, ~~right?~~

[00:42:01] And we have visibility into what's happening and we can quantify, we said we never wanted to do X. As of right now, we're seeing that it is doing X 3 percent of the time. Is that tolerable? Is that intolerable? That obviously depends on context. There's going to be judgment calls to be made, but you need to have those ability to describe what is actually happening in order to have an informed discussion.

[00:42:26] So these are sometimes called benchmarks~~ and it's a set of~~ evals and benchmarks are the same thing. Benchmarks are more like public evals tend to be, ~~this~~ again, the same thing, but more internal. I'll show you an example ~~of a, of a, when it waymark really quick, what account is this in?~~

[00:42:35] ~~Excuse me.~~

[00:42:35] ~~It's not in this account.~~

[00:42:35] ~~This one also ties to Clawd3, so it'll be, or Clawd4Sheets, so it's worth a real quick look if I can find it super quick.~~

[00:42:35] Okay, ~~this is, ~~for Waymark we have an AI write video scripts, and we have all these things that we want it to do and not do, and you don't have to worry about the details of this too much, but here's the sort of thing that you can do. Let's say, for example, we give the AI a template for a video, and its job is to write a script that fits that template.

[00:42:58] I've installed Cloud for Sheets here, that's what I'm [00:43:00] using. I use the meta prompt to help flesh out all these instructions, and now I'm going to give it a script structure that was the input to the AI, and then the output, and what I want to check in this particular eval is, are you using, and we serve small businesses, so these are all like.

[00:43:19] So we want to confirm ~~that they're using,~~ that the AI is using the business contact information in the same way that it's being used in the original script, because if they put it in the wrong place, the videos end up looking weird and whatever. So this is a highly idiosyncratic problem. A lot of this stuff is going to be idiosyncratic.

[00:43:35] The businesses you guys are going to work at are all going to be very idiosyncratic. Everybody has their own weird shit and nobody likes looking at this stuff, to sit there and say. Oh, hey, can you go through a hundred of these and check to see if the AI did any contact information in the wrong place?

[00:43:49] People hate that kind of work, super tedious, super time consuming. That also means it's expensive and people aren't very good at it because it just gets super boring and their minds go elsewhere and just in every way, it sucks. [00:44:00] This is ~~like ~~definitely the perfect job for AI. So here's what that looks like.

[00:44:03] We have a JSON structure ~~who cares ~~doesn't really matter. And now I'm literally just calling the cloud for sheets function and saying, here is this full script. This was just my template instruction. And I actually now put the variables here. So you can see the variables are now populated and now we're calling to Claude and it's going to tell us at the end of the thing are there any places where contact information is incorrectly used?

[00:44:29] I had to workshop this for a while to get it to work, but here you have it. Okay. This is the expected answer. So we're like actually testing in this. Phase ~~that, that it,~~ that Claude can do the job, but now we're going to start to use it on all of the stuff that we do. So we can monitor how often do we see that the AI script has contact information in the wrong place.

[00:44:48] So here it's like, Oh, expert hair regeneration. But here we have, Oh, in the actual one that it wrote, it gave a URL that according to our rules is a violation. So boom, it flags it. Here's another example of [00:45:00] that. This one, there was one, it had call today with a phone number, but in the. One that it wrote, it didn't have any contact information.

[00:45:06] So that's a, ~~the~~ violation in our rules, we get to define our rules. That's a violation going the other way. And this one, there weren't a violation. So it's just, ~~it's~~ told to return true. It returns true. So anyway, all this is to say, being a person who can set up these evals, To quantify what is happening.

[00:45:23] Oh, you want to make sure that our AI is never doing this. Okay, cool. ~~Here's a,~~ let's set up a framework can be as simple as called for sheets. There are way more advanced and complicated tools out there, but a simple call for sheets where it's give me five examples of where it's doing it. Right. And five examples of where it's doing it wrong.

[00:45:38] I'll use the meta prompt. I'll workshop the prompt. I'll set it up. I'll be able to demonstrate that it can evaluate things accurately. Okay. And then we can use that going forward. However we want, that isn't again, another skill that ~~like~~ people don't even have their heads wrapped around what they need.

[00:45:52] So if you can understand how to use evals and actually develop them, you're going to be like a unicorn in 99 percent of [00:46:00] businesses in the country today. Okay. I'm going to land the plane here. Cause I know we're just about at time. You're going to be the one that knows this, the mid career and late career people are not, they're going to have expertise.

[00:46:09] Of course, that you're not going to have. But I think you really want to position yourself as somebody who understands this stuff and can help lead them through understanding, what they don't know. These can be really basic questions like, what are the new jobs~~ that, that are,~~ that companies are hiring for an AI?

[00:46:26] There is this concept of the AI engineer. If you haven't heard that look that up, cause that's kind of software engineering, but with a heavy AI emphasis. AI emphasis. It's like a software engineer that can really use the AI tools and models well and actually build models into products. The ML ops specialist is like fine tuning models, curating data sets, some of this benchmarking and eval type stuff.

[00:46:47] And then the AI implementation specialist is ~~like~~ a little less technical. And by the way, these are all very much in flux and businesses call them different things, whatever. These are just kind of things that I observe. This would be somebody who's like, Oh, I'm going to help us~~ like~~ set up an internal chat [00:47:00] bot that ~~like~~ has access to our database or has access to all of our knowledge base.

[00:47:03] ~~So we can like, ~~see, here's a super simple example. I work with a company that has a thousand employees. They just wanted to set up a simple chat so that people can ask AI their day to day questions. Instead of having to go ask a person, they have a whole team that sits there and answers questions for the thousand employees.

[00:47:18] And now they're ~~like~~ able to shift ~~like ~~half of that work to ai. AI doesn't know all the answers, but we gave it 250 documents and we just, it's a very simple thing to set that up with a off the shelf product. I literally just used chat based.co for that one. Load the documents in couldn't be easier, but just knowing how to set that kind of stuff up is the job onto itself these days.

[00:47:38] I call that AI implementation. Also I think sometimes really helps to have these ~~like~~ simple mantras for people. At way mark, I say AI or die like. You're never going to, or, and I also say done for you beats do it yourself or thing used to be make your own video. You had to write all the copy.

[00:47:54] Now the AI writes it for you. It's a 10 times better experience. So having these little ~~like~~ mantras that people can wrap their [00:48:00] heads around when they're very unfamiliar with the technology really helps three general ones that I say are summary, not strategy, ~~meaning like~~ this kind of goes back to the Eureka moments.

[00:48:09] A lot of times business leaders will be like, can I have this thing? Help me with business strategy. And I always tell them, no, that's unfortunately that's still on you. You can have it do a lot of routine stuff, repetitive stuff, ~~the stuff that, ~~the work that nobody wants to do, or the work that you'd like to scale,~~ but you can't~~ that stuff.

[00:48:22] It can do. But it can't do your business strategy. Similarly, process, not product. Whatever it is in your business that is like your unique product, don't delegate that to the AI. Delegate all the other stuff to the AI that sucks to do,~~ right?~~ Or that's at least ~~like~~ not that awesome to do. But whatever makes you super special, hold on to that, double down on that as humans and~~ have the, you know,~~ use AI and ~~like~~ all the other places again, convert, not create,~~ right?~~

[00:48:46] Similar, these are just ~~kind of ~~simple ways that you can communicate to people like what the AI is good for and what it's not really so good for. This may change and that leads us to the last thing, which is just continue to update your understanding, continue to update ~~your. ~~Your world model on this, [00:49:00] we are only a couple years in new AI.

[00:49:03] I would say really, we're just one year into AI GPT four was really the moment when it went from not that useful to like often very useful to ~~a, ~~a naive user that's only been one year. So the systems are still going to get a lot better. The state of the art is going to change, the tail of the cognitive tape is going to change.

[00:49:22] All this stuff is going to happen, it's going to continue to evolve. So just keep in mind, ~~like~~ we are nowhere near a static, fixed end state right now, you're going to have to keep evolving with it. And follow people that, cause this is definitely a community thing,~~ right?~~

[00:49:35] These are. Going back to the breadth on the tail of the cognitive tape, the AIs are bigger than us in very profound ways. They know more than us. They can speak all the languages. ~~So they can do a, ~~because ~~they get,~~ they are so big, the surface area is so vast. It takes a community to understand what is going on.

[00:49:50] Even a full year after GPT 4 was released, People are still finding better ways to prompt it that are bringing out new state of the art. So this is a very short [00:50:00] list, but these are people~~ I obviously myself, but people~~ that I really highly recommend specifically for coding. The student McKay Wrigley on Twitter posts super good demos.

[00:50:07] There's quite a few like him. Another one that I really respect a lot is Swicks on Twitter and his podcast is called latent space. He also has a blog, really good content. And if you are ~~like~~ less into coding, but more into just kind of general business and knowledge work, then I think the number one commenter in today's world is a guy named Ethan Malik.

[00:50:26] He's a professor at Wharton and super prolific, just came out with a book. And ~~you're like, ~~if there's one,~~ like ~~business school professor that your future boss will maybe have, ~~I've~~ heard of, it might be this guy. So it would definitely be a good one for you to know about. To,~~ and I like ~~these people, I trust to have good content.

[00:50:40] There's a lot of AI snake oil out there these days of, ~~you know, 90, ~~99 percent of people are using chat GPT wrong by my course. Don't buy that course. ~~All this stuff,~~ all the real information you need is free. And it's like freely available from good thought leaders by the products, by GPT four, don't buy the like, ~~see~~ prompting secrets.

[00:50:56] Cause they're not, there's not really any prompting secrets you can't get for [00:51:00] free. All right. ~~Sorry to go a little long~~ happy to do a couple questions~~ and then I can get you on with your evening.~~

[00:51:02] ~~I don't hear anything. If anybody wants to ask a question, either speak up or step up.~~

[00:51:02] Can you tell me something about,~~ like,~~ how AI is used in finance stocks, like in the finance sector a little bit? Yeah to be honest, it's not an area I know a ton about and a big reason for that is that a lot of it is held much more closely and proprietary as trade secret in finance than in other areas.

[00:51:21] This is actually something I just heard Swick say the other day. He was like, in finance, everybody is keeping everything a secret. Whereas~~ in. And, ~~in like software, people are sharing methods. ~~So ~~an open sourcing stuff all over the place. So ~~there, ~~certainly like high frequency trading has been an AI game for years and I'm not sure how much language models are breaking into like highly quantitative finance.

[00:51:49] ~~Those tend to be like~~ speed is really super important in finance, at least for trading style finance. So those systems tend to be ~~like~~ much more narrowly focused versus the like broad [00:52:00] purpose, AI is that are dominating the news today. Bloomberg tried to train their own. Model based on all the proprietary data that they had, and they found that GPT 4 was better, even though they had all this, special data and all the, whatever that they thought this would give us an advantage in the end, still, they found GPT 4 was better.

[00:52:20] So that's an interesting data point.~~ I think there's also there's~~ finance, obviously a huge thing, right? ~~You could also think about ~~the mortgage industry is like part of finance. I think there's definitely a ton of opportunity for language models to do like document review, ~~is this,~~ are these documents in order?

[00:52:34] Is anything missing? Does anything seem wrong? There's~~ like~~ a lot of ~~kind of~~ task automation work that you could do there. ~~Yeah, I think we're Underwriting, has been a checklist,~~ anything where there's like a checklist, right? Did the person submit this? Did the person correctly fill this form out?

[00:52:46] ~~Did this, ~~does this match with this and this other form? All those sorts of things ~~are probably going to, ~~those are like ways in which finance is ~~like~~ similar to other business. And in that way I think that'll happen in finance just like it happens in other places. But the more sort of, trading you get, the more [00:53:00] it's like a lot of hedge fund and, big bank secrets.

[00:53:03] Where can I go to find more information on setting up my own AI to train, say I had a lot of data and results from people that took action on that data. I wanted to train an AI to do that.

[00:53:18] ~~I~~

[00:53:18] ~~mean,~~ it depends on how much data you have and it depends on how technical you want to get. You are going to be almost for sure limited to fine tuning because the amount of data that it takes to train a model from scratch. And here I'm referring again to general purpose language models. If you're talking about narrow, you know, you can train a linear regression, that's a model.

[00:53:38] In some sense, you can train a really simple thing with very little data. But assuming we're talking about like relatively general purpose language model type things, they take a huge amount of data to train from scratch. So you're going to be much more likely fine tuning one. And ~~there are, ~~the easiest way to do it would be to use the open AI platform.

[00:53:58] They have a fine tuning [00:54:00] API. ~~You can,~~ it doesn't cost much, a couple bucks typically like a hundred examples. Is enough if you want to just do one task, but one of the things to realize about fine tuning is that they've trained these models. Like it's a, high art to train these chat bots, to handle everything in the way that they do tons of trade offs, tons of data, to get it, to say no to the things that it's supposed to say no to and do the things it's supposed to do.

[00:54:25] And, there's a ton that goes into that fine tuning is relatively easy. If you are narrowing down to one task. So with Waymark, for example, we do fine tune GPT 3. 5 to be our script writer. And we have hundreds of examples. The quality of the data is probably the most important thing. Because that's what your model is learning from.

[00:54:47] If there are mistakes in there, it's going to learn those mistakes. So you really need to have high quality data. That again comes back to the the evals are really important there. How do I know what's high quality? I might define a bunch of rules as to what's high quality. And then I have to actually.

[00:54:58] Examine all the things with all those rules [00:55:00] in mind. Maybe I can help with that. So I would say, keep in mind, you're going to be fundamentally narrowing the scope of what the system can do when you fine tune it, it is possible to fine tune in a way that can do more than one task, standard starting place would be like a few hundred examples, maybe even fewer than that could be a few, 10 of examples to start probably you're to really get good performance.

[00:55:20] You might need a few hundred. If it's a hard task or there's a lot of, edge cases or whatever, you might need a few thousand, but somewhere in that range is enough to do reasonably good fine tuning on the open AI platform. And. You just need to assemble high quality data and run it through.

[00:55:37] You can also do that open source. You could get a llama model or, any number of open source models with techniques that are generally called PEFT, P E F T parameter efficient, fine tuning. Basically what they do, you've heard of course, like GB3 has 175 billion parameters, whatever.

[00:55:55] That's like a lot, ~~to,~~ to mess with. Today's models are smaller. You might have a llama [00:56:00] model. That's 7 billion. That's still a lot to mess with. If you use parameter efficient fine tuning, basically most of those are just held in place constant and then just ~~like~~ 3 percent of them or whatever are changed to fine tune it just to do whatever you want it to do.

[00:56:12] You can run that in a Google collab notebook and, could be like very limited compute requirements. So techniques there are called LoRa, QLoRa. Those are examples of parameter efficient fine tuning. ~~I think ~~LoRa is like, Low rank, something low rank, meaning it's ~~like~~ not changing that much of the matrices.

[00:56:34] Yeah, that's a pretty good start. That'll take you pretty far. Just know that when you do it, you're going to be narrowing what it can do. If you bring it a hundred examples of writing a script and then you come ask it to write you a recipe for, a birthday cake or whatever, it literally can't do it.

[00:56:47] It's lost that ability entirely. Now, like our waymark script writer can only write scripts. It can do nothing else. That's in a way good because we don't want it to do anything else. But it's in a way bad because ~~if you think that I'm gonna I've tried to train a model to write as me. And, ~~oh, this is a good tip actually [00:57:00] also too.

[00:57:00] So I have this podcast, I do an intro essay for each podcast. I literally write, one page and I read it. After doing that a bunch of times, I started to think, man, maybe I could fine tune a model to write as me. Very hard to do. I write a lot of different things. It's tough. What works a lot better is taking 30 of my essays, dropping them into cloud three, and then saying, use the style of all these essays and write me something new.

[00:57:24] And then it will do a much better job. So in a lot of cases, you may think you need fine tuning. You really don't. You just need to give the AI a lot of examples. Every case is different. So just keep in mind that~~ like~~ fine tuning is not always the answer. And especially I find that software engineers want to go to more technical solutions often than are really necessary.

[00:57:43] If I were to go talk to a software engineer and say, like, how can I get AI to write as me? They're going to immediately be like, Oh let's fine tune. ~~Let's, ~~let's do this. We could use Laura. We could use this path to blah, blah, blah. The actual best answer today is give cloud three, a ton of examples and have it do it.

[00:57:59] And it's going to beat [00:58:00] any fine tuning you're going to do. Keep that in mind. ~~That's awesome. Any other questions? Great. Thank you so much for the talk. Yeah. I just~~ Go out there and establish yourselves as the AI experts that these, dinosaurs will never become and they will love you for it. ~~You, they're, this is something that, it's on the, it's on ~~the infinite interns is like, It's on the horizon. It's here.

[00:58:13] It's not quite here. But you're in a moment where if you can establish that you're the person to go to for this kind of technology, ~~they're, everybody's going to,~~ all the business leaders are going to be looking for that. The next talk I'm giving is, a group of business leaders that are all ~~like ~~mid and late career, they're all worth millions of dollars.

[00:58:28] But they're all asking these questions. So if you can answer these questions for them, then they will hire you. Awesome. Thank you so much. My pleasure guys. ~~Have a great evening.~~ I hope this was helpful. ~~Yeah. Bye for now.~~

