# Legislation Scott

[00:00:00] California state Senator Scott Wiener. Welcome to the cognitive revolution. Thank you for having me.

I'm excited for this conversation. So you are the sponsor, or I guess as it's known in California parlance, the author of the proposed legislation SB 10 47, which attempts to get the government's arms around this question of what is going on the frontier of AI development. And,~~ um,~~ has certainly caused a lot of,~~ uh,~~ consideration and attracted a lot of interest lately.

So I'm really interested to get your story on ~~kind of ~~how you got interested in this. ~~You know, ~~what you,~~ um,~~ aim to do with this legislation, and then we can get into the weeds on some of the concerns that people have and some,~~ uh,~~ some possible ways to even make it better. How's that sound? Sure. It sounds great.

Thanks for having me and thanks for,~~ uh,~~ talking with me about,~~ uh, about, ~~about, um, Yes, I , uh, the, uh, honor representing San Francisco in the state Senate and I'm very proud that San Francisco is,~~ uh,~~ the beating heart of,~~ uh,~~ AI [00:01:00] innovation,~~ uh,~~ so much amazing and inspiring work is happening,~~ uh,~~ in San Francisco,~~ uh,~~ because I,~~ uh,~~ I am a San Franciscoan and represent San Francisco, I am San Francisco.

~~Uh, ~~surrounded,~~ uh,~~ by some incredibly,~~ uh,~~ brilliant,~~ uh,~~ AI minds,~~ uh, and, ~~and folk, not just folks senior levels, but frontline,~~ uh,~~ technologists who are doing the work every day I'm thinking about, I've had great opportunities to talk about, ~~um. ~~Policy issues surrounding AI and probably about a year and a half ago in a number of different settings, folks in the world started talking to me about safety and the need ~~for ~~for policymakers to try to get ahead.

Of safety issues, rather than playing catch up, which is what we often do when the horse is already out of the barn. ~~Uh, ~~and so we started to just have a lot of conversations about what that might. Look like, and what makes sense and how do we. Absolutely [00:02:00] promote,~~ um,~~ innovation,~~ uh,~~ the last thing we would ever want to do.

The last thing I would want to do is to Keith. ~~Um, ~~innovation, I would do that and also make it,~~ uh, uh, ~~safe ~~and, and, and, ~~and really address that issue. So we had an enormous number of,~~ uh,~~ conversations, lots of outreach,~~ um, uh, ~~last September before we,~~ uh,~~ recessed. For the year,~~ um,~~ I put what we call, I put into print, we introduced a formal,~~ uh, uh, ~~piece of legislation, really an outline of what we're looking at in terms of an AIC bill.

I put that out there so that it would be just floating up there for months and months. ~~Uh, ~~I wanted to be completely transparent about what we were doing, welcome lots of broad based feedback,~~ uh, and so we,~~ we introduced that outline, send it around to a bunch of people,~~ uh,~~ and then,~~ uh,~~ in February,~~ uh,~~ introduced the formal,~~ uh,~~ bill.

~~Uh, ~~and, ~~you know, my, the, my goals, my, ~~my intentions ~~are, ~~are really threefold. First,~~ uh,~~ Set some basic safety,~~ uh,~~ and mitigation requirements that [00:03:00] are reasonable, light touch and not micromanaging. I'll mention not micromanaging this, light touch, reasonable safety and mediation requirements that are super doable,~~ uh,~~ and super possible for,~~ uh,~~ developers of LLIs and frontier models to accomplish.

~~Um, ~~we want to focus on only,~~ uh,~~ the most capable models~~ only the, these.~~ Upcoming really large, powerful models that are beyond what's possible today,~~ uh,~~ with, ~~you know, ~~tools like Google and other tools that we have,~~ uh,~~ we're talking about future really large models. ~~Uh, ~~and then how do we,~~ um,~~ protect and foster,~~ uh,~~ innovation, including open source.

~~Uh, um, ~~model developments,~~ um,~~ and how do we do it in a way,~~ uh,~~ that takes you to a cut. So ~~those are, ~~those are our goals and that's how it came about. Cool. Do you want to give us just a tiny bit of background on your legislative career? ~~I think it might be helpful because I think, you know, our audience is very AI obsessed.~~

~~They will know GPT 4. They'll have a sense for, you know, how many flops, you know, might, uh, go into the next generation of models, but I don't think they'll necessarily know much about you or the kinds of things that you. Worked on. So just for context, a little bit of that might be helpful.~~ Yeah. So I'm just like to teach you.

It's by San Francisco from you [00:04:00] supposed to make seven for same reasons. A lot of where people came out here to find my community,~~ uh,~~ practice law ~~for, ~~for many years. ~~Um, ~~very involved in the LGBT community. ~~Uh, ~~and ultimately,~~ um,~~ Ran for was elected to the San Francisco board of supervisors, which is like our city council back in 2010 representing the district of used to represent did a lot of work on the board.

Around housing, public transportation and. ~~Uh, ~~Me and two issues ~~and, ~~and so forth. ~~Um, ~~and then 2016 I was elected to the California State Senate representing all of San Francisco and Greater San Mateo County Rights in south of San Francisco. ~~Uh, uh, ~~state Senate District State, California are huge overrepresented by a million.

~~Uh, ~~people,~~ uh,~~ then I served for a number of years, and that's the chair of the Senate housing committee. And I currently serve as chair of the Senate budget committee. I got acquitted by the church just in time for a massive budget deficit, which is always a lot of fun. And my focus,~~ um, you know, I, I'm, ~~I'm [00:05:00] quite best off my work for a housing policy for trying to relieve barriers to building more homes.

We've made way, way too far to build housing. So we try to make it easier and faster to build homes, but it's hard work to make sure we're adequately funding. Presentation and a lot of work around,~~ um,~~ help tear,~~ uh,~~ access,~~ uh,~~ including single tear. Health care, a lot of work around climate, energy issues, criminal justice reform, so forth.

A note here, I, back in 2018, after the Trump administration got rid of net neutrality protections, I don't know, but it seems like everyone doesn't know that in reality, the notion that you or I should decide where we go on the internet, and AT&amp; T, Verizon, Comcast, and other internet service providers should not be telling us or manipulating us into where to go or not go on the internet.

~~Uh, ~~and so it's something that,~~ uh,~~ I offered and we were able to tell formulas and that mentality [00:06:00] loss,~~ uh,~~ which is a song, some country and some people either in democratizing,~~ uh,~~ the internet and not having concentration of power,~~ uh,~~ so making sure that,~~ um,~~ there is a democratic place. ~~Okay, cool. ~~Thank you.

~~Um, ~~so for this bill, one thing that is ~~kind of ~~striking is that. It really does focus on the frontier, and it leaves, I would say, a number of questions, which ~~I would imagine~~ you probably agree are important for another debate. At another time, those would be things like the future of professional licensing, the overall impact on the workforce, the potential for algorithmic bias or discrimination.

~~Um, ~~none of those are really addressed in the bill at all. So ~~I guess~~ the question would be ~~like, ~~why, ~~you know, ~~is that something you think you'll come back to later? ~~Um, ~~and ~~how, ~~how have you assembled a coalition to support this given that, ~~you know, ~~from what I understand of the coalition, I would imagine some of those folks would be pretty concerned with those issues as well.

Yeah, and,~~ um,~~ one thing,~~ uh,~~ it's important to know that it's just [00:07:00] about AI policy, but AI policy is you can't do everything in one built. ~~Um, ~~and so we have,~~ uh, um, ~~a number of members of the legislature who have been or are working on AI,~~ uh,~~ related,~~ uh,~~ issues. ~~Um, ~~And various bills that are moving forward now, so our bill focuses on,~~ um,~~ safety evaluation and mitigations for the very largest, ~~uh.~~

Models that are being developed, there is a bill. That's coming in the state assembly by my colleague, Assemblymember Rebecca R. Cahill, relating to algorithmic. Discrimination, there are a few different bills around watermarking. ~~Um, ~~of AI images, there are a couple bills on AI generated revenge porn,~~ uh,~~ there are bills around,~~ um,~~ our government contracting around AI related services and government use of AI.

~~Uh, ~~and there are others as well, but there [00:08:00] are quite a few AI bills moving forward. ~~Uh, ~~and SB 1047 is simply one of them relating to safety in particular. ~~Gotcha.~~ Okay. Do you have a established position on these other bills? Is there any, ~~you know, kind of ~~broader worldview that you'd want to share? Yeah, ~~I mean, ~~I generally, ~~you know, I, ~~I'm a supporter of artificial intelligence.

~~I think that ~~it has so much potential to make the world a better place, to make people's day to day lives better,~~ uh,~~ to,~~ uh,~~ address some of the biggest issues of our time around,~~ uh,~~ climate or various huge healthcare,~~ uh,~~ issues. The potential ~~is ~~is limitless in a lot of ways, and I am so excited about what I can do to make huge life.

~~Uh, ~~better, but we also know that there are,~~ um, uh, ~~impacts that we need to be mindful of. And so I am supportive of addressing algorithmic discrimination. The last thing we need is to have. ~~Um, ~~[00:09:00] they, I make the discrimination that exists. ~~Uh, ~~in the world,~~ uh,~~ even more pronounced. ~~Uh, ~~and so that needs to be addressed.

I think it's important for people,~~ um,~~ to be able to know if an image is fake,~~ um,~~ or not. There could be so much disinformation,~~ uh, if, you know, ~~around audio and,~~ uh,~~ visual,~~ um, uh, uh, you know, some ~~fakes. And ~~so, uh, you know, ~~deep fakes are a real issue. We need to address it. ~~Um, ~~and, ~~you know, ~~in terms of impacts on the workforce, it's a ~~really, ~~really hard issue.

Technology always impacts, or often impacts,~~ uh,~~ job classifications and workforce. But we're looking at impacts on the workforce at a ~~very, ~~very large scale. ~~Uh, ~~and I know that there are limitations. Theoretical perfect world. ~~Um, ~~if AI made,~~ uh,~~ Work,~~ uh,~~ or made it possible for less work to exist,~~ um,~~ in an ideal world, the benefits would be spread around to everyone and everyone could just ~~like, ~~work less and have more free time.

~~Uh, ~~but we know that [00:10:00] in this society, in this world, we are very bad about spreading around. ~~Uh, ~~Benefits and what I don't want to see happen is that AI generates huge economic benefits that are enjoyed by a few, whereas most people are made worse off because they no longer have jobs. We need to be intentional about what the future looks like in terms of AI's impact on the workforce.

That is a huge mega issue that's not going to be solved by a bill or one bill, but that's something we need to focus on. ~~Okay, cool. Well, then let's narrow our, we, there might be a bunch of shows in my future about various, um, proposed AI legislation, but let's, you know, narrow our focus back down to SP 1047 for now.~~

~~Um, ~~you said that, ~~you know, ~~goal is to have a focus on the frontier. ~~The, you know, ~~the goal is to have a light touch. How would you describe ~~the, in like plain language,~~ the burden that the law would place on frontier developers? And how would you say they have reacted to it so far? It's been striking to me that I haven't really heard like an official position from, ~~you know, ~~the companies that would seem most likely to be directly impacted in the near term.

Sure. ~~Um, ~~[00:11:00] so I'll describe,~~ um,~~ what the bill does, and I'll also describe,~~ um,~~ after that, all the elements that I rejected that some people wanted to put in,~~ uh,~~ the bill. My goal here, ~~what, uh,~~ is to be light touch, is to,~~ uh, not, ~~not to manage, not to,~~ you know, ~~undo burdens upon people. That's something I don't like passing laws that meet.

People's lives harder just for the sake of making us harder. That's not what that's not who I am. Not what I ever want. I do. I want the laws that have my name on them to actually make the world a better place. So the bill, it applies to models that are ~~Um, ~~the size threshold, we use the same size threshold, but the Biden executive order, I'd say, to use, which is 10 and 26 plot.

And because we don't want to tie everything to where we are in 2024, we want this to be flexible over time. [00:12:00] If you're developing that model of that scale, then ~~when you. ~~Before you train the model and before you finalize it and make it available to other people, you need to perform a safety evaluation on the bill.

There are various kinds of evaluations. Red teaming is one of them, but there are others as well. ~~Um, ~~so you have to perform that evaluation if the evaluation testing shows that there is a significant~~ Um, ~~risk of ~~like, ~~the model leading to catastrophic harm, I'll tell you in a minute how we can find catastrophic harm.

Then you need to take reasonable mitigations to reduce that risk, not to eliminate risk. ~~Uh, ~~eliminating risk in life is usually not possible and when we try to eliminate risk that,~~ um,~~ will take a lot of ~~the, ~~the joy and innovation of,~~ Like,~~ we all be sitting in a coffin in our basement, basically doing nothing.

Life is about risk. ~~Um, ~~and ~~so, uh, ~~but if there, we want people to at least try to mitigate or [00:13:00] reduce,~~ uh, those, uh,~~ those risks,~~ um, the, um,~~ When we talk about catastrophic harms, how we define it in the bill is that it will,~~ um,~~ cause, lead to the deployment or development, deployment of nuclear, chemical, biological,~~ um,~~ weapons,~~ uh,~~ that it will,~~ uh, um, harm, ~~cause harm, damage.

To critical infrastructure,~~ uh,~~ 500, 000, 000 worth,~~ um,~~ or that it will create,~~ uh,~~ trigger some sort of cybercrime or some sort of crime that would be a crime that a human did it causing 500, 000, 000 or more in damage. So we're not talking about small things. We're talking about ~~large. ~~Large scale,~~ um,~~ destruction and damage.

Some people say that threshold should be higher. Some people say, wait, why 500Million? Is a 50Million enough? And that's top of the conversation we could have. And then there are other ones that are sort of equivalent in scale and destruction,~~ um,~~ to, to those,~~ um,~~ and when someone engages in the safety evaluation, you then have to certify to the Department of Technology that you've [00:14:00] done so.

~~Um, ~~and if then one of those arms occurs from your model.~~ Um, ~~then,~~ uh,~~ and you have not done the safety testing,~~ um,~~ or you already did the safety testing and it showed that there was something really bad that was likely to happen and you didn't do anything about it, then the attorney general can sue you.

~~Um, so, uh, ~~I also just want to note,~~ um,~~ that there are a number of things that I rejected, including the bill,~~ um,~~ because I wanted this to be, like, touched. The first is. ~~Um, ~~there are some who wanted us to be able to call private right of action so that anyone could file a lawsuit against the developer if something were to happen.

I did not accept that idea. We limited enforcement to the attorney general of California. ~~Um, ~~to,~~ uh, you know, uh, you know, ~~what. There's limited resources, so they will focus their enforcement on the really bad actors, not just on someone.~~ You know, I was trying, uh, uh, We also, ~~there are people who wanted us to have a [00:15:00] licensing requirement, so that you can't train or release any of these, a lot of these models,~~ um,~~ until,~~ uh,~~ unless the state gets permission and gives you a license.

I did not want to go in that direction, but I don't want the state to be in the middle of all that. ~~We, uh, ~~some people wanted us to ban,~~ trying to ban~~ certain kinds of models. We didn't go there. ~~Um, ~~and some people wanted what's called strict liability. Strict liability means if you cause a harm, you're automatically liable, whether or not you acted reasonably.

~~Um, ~~I rejected that as well. ~~So that, those are,~~ that's basically what this, that element of the bill does, and some of the items that we did not. ~~Um, ~~I do just want to address one thing, because there was some conversations,~~ uh,~~ on Twitter, saying some ~~very inaccurate, people said some ~~very inaccurate things about the bill.

First, they said the bill was being fast tracked, which is false, the bill. We 1st analysis,~~ like,~~ 9 months ago, and it's on the regular slow go. And so there's opportunity for additional dialogue for amendments. ~~Um, you know, ~~and. And so it's not [00:16:00] a gas trap, it's a super transparent process. And the other piece is always,~~ uh,~~ some people say, oh, this developer is going to go to prison for this.

~~Um, ~~they'll make honest mistakes and go to prison, which is also false. ~~Um, ~~the only criminal aspect of the bill is about perjury. If you literally, intentionally, maliciously lie to the government, That's perjury. It's just like it would be if you lie on a driver's license application or any other documents you actually intentionally lie, misrepresent.

That can be charged as perjury. ~~Um, ~~it's not about if you click faith mistake or an honest mistake, or you just submit something that, ~~you know, ~~was even some sloppy,~~ um,~~ that would not rise to the level. So anyway, that's I'll stop there. Yes, that's an important distinction. So just to make sure I get it in layman's terms, basically, obviously, these frontier models are very.

Unusual technology in [00:17:00] that we don't really know what they can do until they've been created. And then, ~~you know, ~~we're still figuring out new things and better ways to prompt GPT 4 a full year after its public release, ~~you know, ~~18 months after it was finished training. So there's this whole process. What you're saying there is that a developer is responsible for doing some amount of,~~ like,~~ reasonable testing to try to get their own arms around what can this thing do.

And then it's their job to report, obviously, honestly, to the government that we have tested this. Again, I'm a little fuzzy on like exactly how much they're supposed to test, how exhaustive that's supposed to be. Maybe you can ~~kind of ~~clarify that for me, but if they do an honest testing and reporting and they say, Hey, we don't think we have a problem here.

We can go ahead and distribute this model. And then something happens. They could be sued would be the base case, and only if it was later found that they actually knew better and were covering up information that they had willfully not reported, would they [00:18:00] be subject to criminal liability. Do I have that right?

~~Uh, ~~yes. ~~And, um, and, but,~~ but the suit lawsuit was only needed by the attorneys out of California, not by,~~ uh,~~ Other random people. So it's very limited, very focused support by one,~~ uh, uh, ~~law enforcement agency. Gotcha. Okay. So how have the developers reacted to this? I feel like there's a lot of,~~ um,~~ cross talk in that dimension as well.

~~You know, ~~some people are of course saying, Oh, this is a ploy by the developers to create a regulatory capture environment, others are, ~~you know, ~~saying that it's going to prevent them from doing what they want to do. ~~Um, What, ~~what has your conversation with the developers been like?~~ Um, ~~it's been a wide array.

We're talking to all sorts of people. And there are a lot of people who,~~ uh,~~ some publicly, but many quietly say, this is the right approach, ~~you know, ~~thanks for doing this largely. They don't want to be public,~~ uh,~~ about it. I'm learning a lot about the politics of,~~ uh,~~ of Silicon Valley. And,~~ uh, you know, ~~there are a lot of these people want to be funded.

~~You know, they, ~~they, and so [00:19:00] there are a lot who don't want to be public about. Their support,~~ um,~~ some do, though,~~ um,~~ and we've had some ~~very, ~~very prominent folks in the AI world who are supporting the bill. ~~Um, ~~we've had a lot of meetings, both with the large labs, with the megatech companies,~~ uh,~~ but also with, ~~you know, ~~smaller,~~ uh,~~ developers.

The large,~~ um, uh, ~~tech companies,~~ uh, you know, there,~~ there's been ~~this, ~~this narrative that this is about regulatory capture and trying to box out small folks and startups ~~that ~~that is completely untrue. And it's not what the bill will in any way do. ~~Uh, ~~none of the big tech companies are supporting the bill.

We're in conversation with them to try to get feedback, but none of them are supporting the bill. And in fact, TechNet, which is the trade association for all the big tech companies,~~ uh,~~ the Meadows and Googles, Amazon, several of us, is opposing,~~ um, the, ~~the bill. ~~Um, and so, uh, uh, you know, it, so, and, and, and this,~~ the ideas in this book did not come from big tech companies, it actually came from a lot ~~of, Some, you know, from my, uh,~~ technologists,~~ um, and, ~~and folks who focus on AI safety,~~ um, uh, we, uh, uh, ~~we have a number of startups that are supporting,~~ uh,~~ [00:20:00] Lidl.

~~Um, uh, so, you know, ~~I think that also speaks volumes. A lot of folks were just sort of.~~ You know, ~~watching and staying away. ~~Um, ~~we have a particular,~~ uh, you know, ~~conversation with the open source,~~ uh,~~ community. ~~Um, uh, ~~folks who are, ~~you know, ~~think, who believe that open source should be treated differently ~~from ~~from other models.

And we're an active conversation with folks in that community. I think open source has enormous,~~ uh,~~ potential value. Turns out,~~ uh, uh, ~~scurrying innovation and democratizing,~~ um,~~ access to these models, but there are, of course, also risks if you release a very powerful,~~ um, uh, ~~open source,~~ uh,~~ model, ~~um. ~~People could do amazing things with it, could also potentially do horrible things.

~~Uh, ~~so that's a continuing conversation we definitely are looking to, uh,~~ you know, ~~undermine open source. ~~Let me give you just a, I'll do this as rapid fire as I can give you as much of the airtime as I can. Yeah. Um. ~~Objections that I've heard. You mentioned that the big tech companies in their network are opposing one of the other objections I've heard has been that well, all the leading developers are basically doing this [00:21:00] already anyway.

So what's the point? ~~Right? ~~They've all put out model cards and ~~kind of ~~walked us through. ~~Like, ~~here's what we're doing for testing. Anthropix got the responsible scaling policy. Open AI has answered that Google is getting there, too. ~~Um, you know, ~~even meta with their open source approach has done pretty thorough testing.

So it's just to hear ~~like, ~~what are they saying ~~that, ~~that they find a problem with? And ~~you know, ~~how would you answer ~~the, ~~the opposite end of ~~that ~~that says, if all the big guys are doing this, why do we need to make it a law? Yeah,~~ so, uh, ~~just to be clear,~~ the,~~ the big tech companies that may have not come out individually,~~ uh,~~ since you were having conversations with, I think ~~all, ~~all of them,~~ um,~~ asked some questions.

And the big folks, with the small folks, with the academics, with advocates, with everyone, it's super ~~open, uh,~~ open door. ~~Um, ~~and yeah, there is testing, so for the folks who are doing,~~ like,~~ reasonable good testing, this bill will have,~~ um,~~ will have very, ~~very, um,~~ limited impact, because they're already, Doing it.

And so it's not an issue for them. ~~Um, ~~there are,~~ uh,~~ other labs that we're aware of who are maybe not [00:22:00] doing,~~ uh,~~ the best,~~ uh, uh, ~~testing,~~ um,~~ there's, ~~you know, ~~we've heard some, this has been listed on Twitter, so I'm not, ~~you know, ~~seeing this on Twitter, some folks who have concerns ~~about, ~~about metas,~~ um,~~ safety,~~ uh,~~ testing, whether it's really where he's, ~~you know, ~~To be,~~ um,~~ and so I'm just repeating what I've seen online.

So ~~I think that~~ there are diverse views about,~~ um,~~ whether all of the large labs are doing Testing to the degree that they need to. ~~Uh, ~~and I've also found In life, when it comes to, ~~you know, ~~any industry, any ecosystem,~~ um,~~ It's good to have minimum standards and not just trust that every lab is going to do the right thing.

There are plenty of labs that will do the right thing. They're taking safety very seriously, will go above and beyond on testing and on guardrails and mitigations. But that doesn't mean that they all will. ~~Um, ~~and I think it's good to have a level of feel where they're the minimum standard. And again, this is light [00:23:00] touch.

Safety rules,~~ uh,~~ we're not looking at the micromanage, but we want to make sure everyone, if you're creating a model of this scale, and these are huge, powerful models, just at least do that baseline safety testing.

~~So, ~~On the open source question in particular, this is obviously one of the hottest topics. The cause around models that have ~~kind of ~~similar power to what 10 to the 26 would get you in 2024 has a lot of people projecting out a couple of years and saying,~~ well,~~ look, we're getting a lot better at this stuff.

It's going to become. pretty affordable to create something of that power over the next couple of years. And so the threshold effectively in terms of like dollar budget falls from the rare air today, where we're talking hundreds of millions of dollars and relatively few companies can do it to maybe a couple of years from now, just a couple of million dollars, maybe like a ton of companies can do it, or even, ~~you know, ~~groups or clubs or whatever could rally that many resources.[00:24:00] 

It seems to me like that is right. Unless there's some sort of Conceptual breakthrough that would allow people to really definitively say that their stuff is safe. ~~So is there a, ~~is that sort of a long-term bullet you're willing to bite as of right now, where you basically would just say ~~like, ~~Hey, look,~~ if,~~ if there's no way to say that this stuff is safe, then you really just can't open source it into the public.

Is that kind of your default position and hope that there is a breakthrough?~~ Well, ~~we know that,~~ uh, you know, no,~~ no one can predict the future, but we know ~~that, ~~that right now, and. In the future, I think we can see~~ that ~~that it's going to be incredibly expensive to develop models of this scale, this magnitude.

And so we're talking about, ~~you know, ~~significant undertakings to,~~ um,~~ to develop models at this scale. ~~Um, ~~and so we think it's reasonable to say, let's just do a safety test. So at least again, not eliminating risks to at least,~~ uh,~~ try to mitigate,~~ uh, the. ~~The risk,~~ um,~~ with respect to open source in particular,~~ we,~~ we want to protect open source.

~~I~~ mean, [00:25:00] very, very clear about that. We're not looking to eliminate open source. That's not what I want to do. It's not what we're going to do or working right now, as we speak in good faith,~~ you know, ~~with a lot of people who are a lot of really smart people in the open source space,~~ uh,~~ to try to figure out how we can.

Address some of their concerns in a way that still promotes,~~ uh, you know, ~~safety in the open source,~~ uh, uh, ~~context. ~~Uh, ~~and we know that when people, ~~you know, ~~if you take a very powerful open source,~~ uh,~~ model. When you're building on that, there's a good chance that what you're building on top of it. Maybe very small in terms of turning that model into ~~like ~~some sort of ~~real, ~~real world application.

~~Uh, ~~and so we do think that it's the model developer is best positions. ~~Um, ~~to,~~ uh,~~ to do that safety analysis,~~ uh,~~ however, we, ~~you know, ~~are, ~~you know, ~~really actively receiving feedback from open source,~~ uh,~~ developers and experts,~~ uh,~~ to [00:26:00] make sure that we are,~~ uh, you know, ~~I don't want to, I'm not looking to create any kind of structure that people can't meet.

~~Uh, ~~so we're going to continue to work on the open source issue and I'm going to continue to have an open door on it.

What are the best ideas? I've heard there would be the idea of maybe getting a little more precise on how much more you can do on an open source model before the responsibility becomes yours.

~~Um, ~~right now, if I read the legislation correctly, it's like open source developer puts out a model. Anybody ~~kind of ~~does downstream additional training or whatever on ~~that, ~~that's still a derivative model. And so the original open source developer retains the responsibility. Some of the ideas I've heard have been like.

If you do, ~~you know, ~~10 percent additional compute, then maybe you should now own the ball. The original open source developer can be off the hook. Is that the kind of thing that you're entertaining now? No, I think that's a fair conversation. ~~Um, that, that is a, a, a fair conversation.~~ And we're, ~~you know, ~~we're open to exploring that and other ideas with the open [00:27:00] source.

~~Um, ~~Folks. Okay. ~~Um,~~

~~do you have time for one more question before we break? Okay, I'll maybe edit this one to go a little earlier, but~~ one other idea that's just like super simple is What if we just made it illegal to distribute AIs that do certain things?

~~What's the ~~what's the fault in that idea? Yeah, ~~there there are and I I you know, we Um, ~~there's certainly a case to be made that there are certain types of activities that the question not be, how the hell ~~I, I don't, ~~I don't think we want to be,~~ uh, you know, ~~creating biological weapons, for example, it's probably the most commonly discussed,~~ um,~~ idea,~~ but, um,~~ but we don't really know.

First of all, what the capabilities of these models are going to be in 2 years, 5 years, 10 years. ~~So, ~~even if we decided there are certain things we want to ban, we may not even know what it is we would want to be. And so we decided in this bill. Rather than saying what you've had, you can't develop in terms of capabilities, must require safety evaluation.

And again, I'm not going to pretend that this bill is going to ~~like ~~solve all the essential problems that could ever result from,~~ uh,~~ for an AI [00:28:00] model. ~~Um, ~~but, and I think it at least takes that,~~ um,~~ step of requiring some introspection and evaluation. And then if you discover,~~ uh,~~ oh wait, this is going to generate biological weapons, Trying to mitigate that,~~ um,~~ and the conversation about whether to restrict or ban ~~particularly ~~particular AI capabilities.

That will be an ongoing conversation, but it's certainly a fair conversation to have

I was surprised also that there's no,~~ um,~~ AI sort of having to identify itself when it goes out into the world. This is ~~like, uh, um, ~~evolved. No, Harari. The first rule should be a, I must identify itself as AI.

Is that just something that you think will be handled by other? Legislation and ~~I, ~~I sort of, I do have a list of those in front of me and I think ~~that ~~that might be a bill pending this year. I can't remember. ~~Um, ~~but that is certainly,~~ uh, I think, uh, something that's ~~an active topic of,~~ uh,~~ of conversation.

And I think it's in it. That's, there's a dull in that this year, but I could be wrong about that. But ~~that's, ~~that's certainly an important issue. 

~~I'm going to have to pick and choose questions here, so I'm just, uh, choosing which ones seem like they're the most important. ~~I [00:29:00] think one of the big ideas that people really worry about in AI broadly is the idea of an arms race between countries, certainly,~~ uh,~~ but even between firms within the United States.

The bill ~~sort of. You know, ~~obviously insists on a certain amount of testing, but in a world where, ~~you know, ~~it's really easy to switch from one language model to another, there is, I think, risk of a sort of winner take all dynamic between the companies where, you know, giving the testing kind of the shortest possible window that they could give it starts to maybe become attractive because I think we might even see this like in the next.

A couple of weeks here, Google's going to have their event, they're going to release something. And then it's expected that like opening, I was going to try to ~~kind of ~~come back over the top and steal their thunder. And so you can imagine inside the labs, the conversation might be like, look, we're launching at this date when, ~~you know, ~~we can most effectively, ~~you know, ~~outcompete our rival and you've got to have your testing done by then.

And, ~~you know, ~~whatever we can do by then, like we'll [00:30:00] call it reasonable. ~~Um, ~~do you worry about that at all? And do you have any ideas or has there been any conversation about things that legislation could do to try to mitigate this sort of winner take all AI arms race dynamic that seems like it is starting to take shape?

You mean it's harming people cutting corners? Yeah, exactly. ~~Well, I mean, ~~ultimately. Under this bill,~~ if, if, ~~if someone cuts corners on safety testing, those really shoddy testing or seize problems, and just sort of, ~~you know, uh, ~~ingeniously pretends not to see them,~~ um,~~ they're going to have to file a certification, and they,~~ uh,~~ and the nature would be that if something terrible happens, that could lead to liability.

The attorney general could file a lawsuit. ~~So, ~~yeah, people. ~~Um, ~~especially, ~~you know, ~~again, this is light touch. We're not micromanaging. We're not. ~~It's very, it's sort of.~~ Just saying broad parameters,~~ um,~~ and people can certainly violate the law. act in [00:31:00] shoddy, irresponsible ways if they want to, but then there's going to be a risk to them.

Maybe they'll get away with it, but that's all, ~~you know, ~~life often works that way. They'll find from people to problematic things and get away with it, but sometimes they don't.~~ Uh, ~~and so they're, and we're talking about catastrophic harms,~~ uh,~~ so they Developers will need to be mindful and I think ultimately, ~~you know, ~~a lot, ~~you know, many ~~many developers want to do the right thing.

~~They want, ~~they want, they don't want to release a model that, that's going to cause catastrophic harm. ~~Um, ~~and so I, I think that we will have,~~ um,~~ good compliance. The penalties. are, how would you describe the penalties? Cause I see in the bill, like a 10 to 30 percent sort of fine, 10 to 30 percent of what it costs to train the model, I believe.

And then I think there's like additional sort of possibility for kind of punitive damages or whatever. ~~Um, ~~but I could imagine a situation where you're at Google, for example, and you're like, man, opening eyes, eating our lunch. ~~Um, ~~how much did this thing [00:32:00] cost to train again? Oh, it costs 500 million to train.

Okay. That means we could be on the hook for one 50. ~~Um, ~~it's worth it, right? ~~Like, ~~do you think that the financial penalties, ~~you know, ~~sort of probabilistic and ~~kind of ~~deferred into the future as they ~~are, ~~are enough to be really persuasive to these companies?~~ Um, ~~I think so, and especially, ~~you know, uh, ~~cumulative damages, it has, and cumulative damages requires, ~~you know, ~~very high, it's a high threshold of malice.

~~Uh, ~~and if punitive damages are triggered,~~ uh, that is, uh,~~ those are typically based on ~~the, um, what, uh,~~ your net worth. And so punitive damages fought against me, as a public servant, who doesn't have a lot of money, are going to look very different than punitive damages against you. ~~You go, uh,~~ a trillion dollar,~~ uh,~~ corporation or whatever the size is,~~ uh,~~ and so ~~we do, uh, um, I think, um, I think, ~~I think the incentives ~~are, ~~are good,~~ uh, in, ~~in the bill.

I started my career very briefly in the mortgage space and it just happened to be at the time when the mortgage,~~ uh,~~ industry ~~kind of ~~blew up the American economy and then it became clear that,~~ like,~~ While there were a lot [00:33:00] of people ~~sort of~~ nominally watching after the behavior of the lenders and, ~~you know, ~~all the different steps in the value chain, like that had ~~kind of ~~eroded to be more of a rubber stamp.

And ~~I think~~ that is a big part of what the safety community Most worries about it's ~~like, ~~okay, these people are going to be under pressure that, ~~you know, it's, ~~it's such an easy change to switch from one API call to the other. ~~Right. I mean, ~~I can switch from open AI to Google to anthropic literally in one line of code.

So it does create these sort of, you have the best model, like you ~~kind of ~~win the day dynamics. And then there's this sense of ~~like, ~~okay,~~ well,~~ we're racing, ~~you know, ~~maybe ~~we, ~~we want to do this, but do we really want to do it? Do we really want to find everything we could find that we really want to take all the time.

That we could take,~~ um,~~ it seems like there is ~~sort of ~~likely to be a space where, ~~you know, ~~they would, it would be possible for the developers to have ~~like ~~a reasonable defense, ~~you know, ~~look at all the things we did. This is like very reasonable. But then somebody who's like really in the know. would say ~~like, well, ~~think about all the things you didn't do, ~~you know, ~~and here's all the things that ~~I, ~~I really think you should have done and you [00:34:00] didn't, but it's still ~~kind of ~~reasonable.

This leads me to, I think one of the most common suggestions that I heard in preparing for this, which is a more forceful requirement for truly independent third party testing. ~~Um, ~~what has there been any conversation around That sort of thing where, ~~you know, ~~who provides that testing under what circumstances, how long they have exactly what sort of access they have is a lot of particulars there.

But I do worry along with a lot of these safety people that.~~ You know, the, ~~the motivation or the sort of standard of reasonableness could be met. And yet we might want,~~ like,~~ a higher standard if we're really talking about, ~~you know, ~~catastrophic or ~~as, ~~as you well know,~~ like,~~ the safety community even worries about extinction level risks,~~ um, you know, ~~is reasonable enough or ~~should we, ~~should we sort of say, hey, you have to allow in these sort of, ~~you know, ~~red hat testers to go in and,~~ um, you know, ~~Dig deeper than you might dig on your own.

Now, ~~I mean, ~~the bill, and again, ~~we are, ~~we are walking a fine year where we want to have good safety protocols,~~ um,~~ [00:35:00] without,~~ uh,~~ being too intrusive, right? And we're already, there are people who are criticizing us for being too intrusive. I don't think we are at all. Other people are both saying it should be more intrusive.

As you know, I better than I do. There are some philosophical divides within. That AI world right now on this issue, the bill. ~~Um, ~~calls to developers to use fair credit testing. ~~Uh, ~~quote, when appropriate, and I understand ~~that ~~that is a little,~~ uh,~~ vague and certainly an issue. We'll continue to take a look at,~~ um,~~ we're open to the possibility of a greater role for 3rd party.

~~Uh, ~~testing,~~ um, uh. ~~But,~~ uh, you know. ~~We also want to know more about who those 3rd party testers would be. And I think as you,~~ uh,~~ alluded to earlier, and you said, ~~you know, ~~some big accounting firm that comes in and checks the box, or ~~is it, ~~is it real? Our goal here is just to have good testing. ~~Uh, ~~and we're open to different ways,~~ uh,~~ of approaching that again with,~~ uh,~~ and I know you're going to get sick of me [00:36:00] using this phrase, but it's real, as long as it is,~~ um,~~ late touch,~~ um, uh, that is, ~~that is our goal for DATA,~~ uh,~~ micromanage.

And there are people who would like us to micromanage, right? They want people who want you to have to give permission from the government and get a license before you,~~ uh,~~ train or,~~ uh,~~ Or anyway, release a model of this scale. ~~Uh, ~~and I respect that point of view, but that's not where we toasted on this bill.

Yeah. I do think it's a tough one to figure out exactly how you would specify who the third parties should be. And, ~~you know, ~~again, exactly what kind of access. They should have,~~ um,~~ I had a personal experience as a member of the GPT 4 red team, and I was really taken aback by what a leap it was, and the public hadn't really seen anything like it.

My feeling at the time was that the testing that they were doing appeared to be inadequate, and I became sort of a,~~ uh, Like ~~lost in the wilderness, ~~you know, ~~like under NDA on the one hand, like not trying to blow up their spot on the other hand, but like [00:37:00] feeling like I was one of just a couple of people in the world that had, ~~you know, ~~the time to devote.

And I've literally dropped everything else I was doing and worked on it full time for a while. ~~Um, ~~and I felt like, where do I go? ~~So, I think ~~one of the problems that I had was that I was totally ~~at the, ~~at the pleasure of open AI. I do think one thing that would be really useful would be some,~~ um,~~ protections for the third party testers, ~~you know, some, ~~some perhaps like accreditation or some, ~~you know, sort of~~ status that they need to achieve, but also some ability for them to ~~like ~~report their findings without being, ~~you know, ~~at the risk of immediately being cut off because the access dance that they have that it offers is a really.

Tricky one.~~ I mean,~~ I think so. It's totally different. Like next. I authored a law that was passed in sign last year. ~~Um, ~~it was a law to require large corporations to disclose their carbon emissions, including from their supply chain. It was a ~~huge, ~~huge fight with a lot of the large,~~ uh, uh, ~~business associations.

We got it passed in the governor's side and the law puts California in the forefront on it. [00:38:00] And. In that law, we require these disclosures and disclosures have to be audited at the California Resource Board, sort of certifies the auditors that are qualified and trusted to do that auditing,~~ um,~~ different contexts, not just subject matter, but also because,~~ um,~~ carbon emissions are not necessarily like trade secrets and,~~ uh, you know, ~~are not I guess they could be, but are typically not particularly sensitive in terms of some sort of confidential part of a business model or a technology.

Here, obviously, there's a much higher sensitivity in terms of this is like the core products ~~of the ~~of the business and,~~ uh,~~ and there's a lot of confidentiality issues as well. So it's a little touchier, but I think that this is,~~ uh,~~ I absolutely talked to the conversation about how to make sure the testing is as solid as possible.

What about sort of a similar transparency requirement for model development as well? ~~Like, ~~what [00:39:00] if you were to say you have to just disclose how big your training runs are going to be? ~~You know, ~~if you're going to do 2 times 10 to 26,~~ like,~~ okay, but you got to put it on record. That would maybe help address these sort of race dynamics where the different developers don't know,~~ like,~~ just how fast the other companies are going, how big,~~ um,~~ and you might also imagine extending that to, Things that people have seen models do.

~~I'm a big, you know, ~~I do think there is definitely a place for trade secrets. I don't think, ~~you know, ~~total transparency makes sense if only because then we're, ~~you know, ~~also sharing all our secrets with all the governments of the world, which ~~we, ~~we probably don't want to do, but I've often thought, does it really harm the company?

If somebody says, Hey, I saw an AI do X and it ~~kind of ~~freaked me out. ~~Um, ~~cause I was under an NDA where I couldn't even disclose that sort of thing. And, ~~you know, ~~people have been fired, of course, for, ~~you know, ~~saying that they think their AIs are conscious and whatnot. ~~Um, ~~I don't necessarily think they are, but I also don't rule it out.

~~Um, ~~it certainly seems like something we should be discussing. I wonder what you think about these sort of, are there ways ~~to, ~~to put [00:40:00] strategic transparency, either Requirements on the company or sort of opportunities for very careful, like narrow, but hopefully illuminating disclosure for employees or testers that could help everybody at least get a sense for kind of what's going on across the environment.

~~I mean, those are~~ Is that possible? Yes. And is that worthy of a conversation? Yes. In this bill, we're really focused on just creating, which does not exist now, the baseline requirements to do safety testing, which a number of labs are already doing, and some are probably doing better and more consistently and thoroughly than others.

~~Uh, ~~and we want to make sure that labs that are creating these huge models,~~ uh,~~ are doing that baseline testing. Are there other things that are based? That may be reasonable to put on top of that, potentially,~~ um,~~ but, ~~you know, ~~we're,~~ um,~~ really focused on, let's just ~~like, ~~let's do this, take this important steps,~~ uh,~~ that,~~ uh, you know, is, ~~[00:41:00] is already going to be controversial, and we're already seeing that controversy,~~ uh,~~ so that's what we're focused on.

Yeah, makes sense. ~~Um, ~~

so on the core testing concept, I hear you. Bottom line seems to be. We want to make sure that this testing is happening. We want to make sure that it's good quality testing.

Want to create the right incentives for that openness. You've expressed several times to discussion about potentially a bigger role for third parties or a more kind of forceful insistence on that. Where should people come to have that conversation with you? Particularly those that are motivated to do that testing that feel like they have something to contribute, but are currently ~~kind of ~~boxed out ~~of, ~~of contributing in the way that they would like to, yeah, so people can reach out ~~to, to my, ~~to my office,~~ uh,~~ we're super easy to find.

They can also message me on Twitter. ~~Uh, ~~Scott underscore Wiener, IP40 and Wiener, I can message you there, I can pick you up at the office,~~ uh,~~ we really welcome,~~ uh,~~ feedback [00:42:00] and ideas,~~ um,~~ and people have been very gracious,~~ uh,~~ and helpful in, ~~you know, ~~pointing out maybe things in the building could be better or things or problems that we may not have anticipated, and we very much welcome that feedback.

~~Thank you so much. Cool. Thanks. ~~Yeah. I can never guarantee that everyone's going to be in complete agreement with everything and any bill that I author, but,~~ um,~~ I do,~~ uh,~~ really value feedback to make the bill as good as it can be. And there are times, I've had bills where there are, there, there's one philosophy of legislating saying, if you're opposing my bill, then I'm not going to listen to anything you have to say.

I don't subscribe to that philosophy. My view is that even if you are fighting me on a bill, if you then come forward and say, Hey. There's something you missed, and this is going to play out in a problematic way. There are times when I'll say, oh my god, yeah, you're totally right, thank you for bringing that to my attention.

We changed the bill. Even though the person is going to continue to oppose it, because my take is I just want my bills to be as good as they can be. 

~~Well, ~~I appreciate,~~ um,~~ what a complicated situation this is, what, how much [00:43:00] uncertainty there is on so many of the key questions. And I know you're balancing a lot in terms of,~~ um,~~ Different constituencies and trying to, ~~you know, ~~create something that has a positive effect and still has a light touch.

~~Um, ~~I definitely appreciate the fact that you're focused on these frontier tail, sometimes dismissed as speculative, but I don't think,~~ uh,~~ wisely dismissed as speculative risks. ~~Um, so. ~~I appreciate the work that you're putting into this and the openness that you've expressed,~~ uh,~~ to further input as well.

~~Um, ~~any other closing thoughts from you?~~ Uh, no, I just, uh, um, ~~I appreciate it. That, first of all, I'm excited about what AI has to offer and I,~~ uh,~~ appreciate the engagement that we're,~~ uh,~~ seeing. I would also just, one thing I was asked, my only request is,~~ uh,~~ this is true in lathe, not just, ~~you know, ~~an AI policy, and that's just about our builds.

~~Uh, ~~but if you see something on Twitter or elsewhere online about the build,~~ uh,~~ just, Please don't assume ~~that ~~that's what the bill does, because we've seen some real inaccurate information about [00:44:00] the bill on Twitter in particular over the last week. ~~I'm not, ~~I'm not saying it's all malicious. There are some people who just see things or hear things and then just post about it.

~~Um, and, ~~and feel free to reach out to us and ask,~~ uh,~~ because we're happy to,~~ uh,~~ answer questions. 

California State Senator Scott Weiner, sponsor of SB 1047. Thank you for being part of the Cognitive Revolution 

